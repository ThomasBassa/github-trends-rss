<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: C++, This week</title><link>https://github.com/trending/c++?since=weekly</link><description>The top repositories on GitHub for c++, measured weekly</description><pubDate>Sun, 09 Feb 2020 01:09:47 GMT</pubDate><lastBuildDate>Sun, 09 Feb 2020 01:09:47 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>google/googletest #1 in C++, This week</title><link>https://github.com/google/googletest</link><description>&lt;p&gt;&lt;i&gt;Googletest - Google Testing and Mocking Framework&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-test" class="anchor" aria-hidden="true" href="#google-test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Test&lt;/h1&gt;
&lt;h4&gt;&lt;a id="user-content-oss-builds-status" class="anchor" aria-hidden="true" href="#oss-builds-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OSS Builds Status:&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/google/googletest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3889293d486af33499741bd12a3a1ce11deb4c93/68747470733a2f2f6170692e7472617669732d63692e6f72672f676f6f676c652f676f6f676c65746573742e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://api.travis-ci.org/google/googletest.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/GoogleTestAppVeyor/googletest/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a3469255f3fcdead1593919251ab6f438744e9be/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f346f3338706c743078626f31756263382f6272616e63682f6d61737465723f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/4o38plt0xbo1ubc8/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-announcements" class="anchor" aria-hidden="true" href="#announcements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Announcements:&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-release-110x" class="anchor" aria-hidden="true" href="#release-110x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Release 1.10.x&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/google/googletest/releases/tag/release-1.10.0"&gt;Release 1.10.x&lt;/a&gt;
is now available.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-coming-soon" class="anchor" aria-hidden="true" href="#coming-soon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Coming Soon&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Post 1.10.x googletest will follow
&lt;a href="https://abseil.io/about/philosophy" rel="nofollow"&gt;Abseil Live at Head philosophy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;We are also planning to take a dependency on
&lt;a href="https://github.com/abseil/abseil-cpp"&gt;Abseil&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-welcome-to-google-test-googles-c-test-framework" class="anchor" aria-hidden="true" href="#welcome-to-google-test-googles-c-test-framework"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to &lt;strong&gt;Google Test&lt;/strong&gt;, Google's C++ test framework!&lt;/h2&gt;
&lt;p&gt;This repository is a merger of the formerly separate GoogleTest and GoogleMock
projects. These were so closely related that it makes sense to maintain and
release them together.&lt;/p&gt;
&lt;p&gt;Please subscribe to the mailing list at &lt;a href="mailto:googletestframework@googlegroups.com"&gt;googletestframework@googlegroups.com&lt;/a&gt; for
questions, discussions, and development.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started:&lt;/h3&gt;
&lt;p&gt;The information for &lt;strong&gt;Google Test&lt;/strong&gt; is available in the
&lt;a href="googletest/docs/primer.md"&gt;Google Test Primer&lt;/a&gt; documentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Google Mock&lt;/strong&gt; is an extension to Google Test for writing and using C++ mock
classes. See the separate &lt;a href="googlemock/README.md"&gt;Google Mock documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More detailed documentation for googletest is in its interior
&lt;a href="googletest/README.md"&gt;googletest/README.md&lt;/a&gt; file.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An &lt;a href="https://en.wikipedia.org/wiki/XUnit" rel="nofollow"&gt;xUnit&lt;/a&gt; test framework.&lt;/li&gt;
&lt;li&gt;Test discovery.&lt;/li&gt;
&lt;li&gt;A rich set of assertions.&lt;/li&gt;
&lt;li&gt;User-defined assertions.&lt;/li&gt;
&lt;li&gt;Death tests.&lt;/li&gt;
&lt;li&gt;Fatal and non-fatal failures.&lt;/li&gt;
&lt;li&gt;Value-parameterized tests.&lt;/li&gt;
&lt;li&gt;Type-parameterized tests.&lt;/li&gt;
&lt;li&gt;Various options for running the tests.&lt;/li&gt;
&lt;li&gt;XML test report generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-platforms" class="anchor" aria-hidden="true" href="#platforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Platforms&lt;/h2&gt;
&lt;p&gt;Google test has been used on a variety of platforms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Mac OS X&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;li&gt;Cygwin&lt;/li&gt;
&lt;li&gt;MinGW&lt;/li&gt;
&lt;li&gt;Windows Mobile&lt;/li&gt;
&lt;li&gt;Symbian&lt;/li&gt;
&lt;li&gt;PlatformIO&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-who-is-using-google-test" class="anchor" aria-hidden="true" href="#who-is-using-google-test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Who Is Using Google Test?&lt;/h2&gt;
&lt;p&gt;In addition to many internal projects at Google, Google Test is also used by the
following notable projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href="http://www.chromium.org/" rel="nofollow"&gt;Chromium projects&lt;/a&gt; (behind the Chrome browser
and Chrome OS).&lt;/li&gt;
&lt;li&gt;The &lt;a href="http://llvm.org/" rel="nofollow"&gt;LLVM&lt;/a&gt; compiler.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/protobuf"&gt;Protocol Buffers&lt;/a&gt;, Google's data
interchange format.&lt;/li&gt;
&lt;li&gt;The &lt;a href="http://opencv.org/" rel="nofollow"&gt;OpenCV&lt;/a&gt; computer vision library.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tiny-dnn/tiny-dnn"&gt;tiny-dnn&lt;/a&gt;: header only,
dependency-free deep learning framework in C++11.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-related-open-source-projects" class="anchor" aria-hidden="true" href="#related-open-source-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related Open Source Projects&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/nholthaus/gtest-runner"&gt;GTest Runner&lt;/a&gt; is a Qt5 based
automated test-runner and Graphical User Interface with powerful features for
Windows and Linux platforms.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ospector/gtest-gbar"&gt;Google Test UI&lt;/a&gt; is test runner that
runs your test binary, allows you to track its progress via a progress bar, and
displays a list of test failures. Clicking on one shows failure text. Google
Test UI is written in C#.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/kinow/gtest-tap-listener"&gt;GTest TAP Listener&lt;/a&gt; is an event
listener for Google Test that implements the
&lt;a href="https://en.wikipedia.org/wiki/Test_Anything_Protocol" rel="nofollow"&gt;TAP protocol&lt;/a&gt; for test
result output. If your test runner understands TAP, you may find it useful.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/gtest-parallel"&gt;gtest-parallel&lt;/a&gt; is a test runner that
runs tests from your binary in parallel to provide significant speed-up.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=DavidSchuldenfrei.gtest-adapter" rel="nofollow"&gt;GoogleTest Adapter&lt;/a&gt;
is a VS Code extension allowing to view Google Tests in a tree view, and
run/debug your tests.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/matepek/vscode-catch2-test-adapter"&gt;Catch2 and Google Test Explorer&lt;/a&gt;
is a VS Code extension allowing to view Google Tests in a tree view, and
run/debug your tests.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pypi.org/project/cornichon/" rel="nofollow"&gt;Cornichon&lt;/a&gt; is a small Gherkin DSL parser
that generates stub code for Google Test.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;p&gt;Google Test is designed to have fairly minimal requirements to build and use
with your projects, but there are some. If you notice any problems on your
platform, please notify
&lt;a href="https://groups.google.com/forum/#!forum/googletestframework" rel="nofollow"&gt;googletestframework@googlegroups.com&lt;/a&gt;.
Patches for fixing them are welcome!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-build-requirements" class="anchor" aria-hidden="true" href="#build-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build Requirements&lt;/h3&gt;
&lt;p&gt;These are the base requirements to build and use Google Test from a source
package:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://bazel.build/" rel="nofollow"&gt;Bazel&lt;/a&gt; or &lt;a href="https://cmake.org/" rel="nofollow"&gt;CMake&lt;/a&gt;. NOTE: Bazel is
the build system that googletest is using internally and tests against.
CMake is community-supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;a C++11-standard-compliant compiler&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-change" class="anchor" aria-hidden="true" href="#contributing-change"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing change&lt;/h2&gt;
&lt;p&gt;Please read the &lt;a href="CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for details on how to
contribute to this project.&lt;/p&gt;
&lt;p&gt;Happy testing!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google</author><guid isPermaLink="false">https://github.com/google/googletest</guid><pubDate>Sun, 09 Feb 2020 00:01:00 GMT</pubDate></item><item><title>trojan-gfw/trojan #2 in C++, This week</title><link>https://github.com/trojan-gfw/trojan</link><description>&lt;p&gt;&lt;i&gt;An unidentifiable mechanism that helps you bypass GFW.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-trojan" class="anchor" aria-hidden="true" href="#trojan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;trojan&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://dev.azure.com/GreaterFire/Trojan-GFW/_build/latest?definitionId=5&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/62a97cc683a7827db7b89819aa6351799d28d40f/68747470733a2f2f6465762e617a7572652e636f6d2f47726561746572466972652f54726f6a616e2d4746572f5f617069732f6275696c642f7374617475732f74726f6a616e2d6766772e74726f6a616e3f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/GreaterFire/Trojan-GFW/_apis/build/status/trojan-gfw.trojan?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;An unidentifiable mechanism that helps you bypass GFW.&lt;/p&gt;
&lt;p&gt;Trojan features multiple protocols over &lt;code&gt;TLS&lt;/code&gt; to avoid both active/passive detections and ISP &lt;code&gt;QoS&lt;/code&gt; limitations.&lt;/p&gt;
&lt;p&gt;Trojan is not a fixed program or protocol. It's an idea, an idea that imitating the most common service, to an extent that it behaves identically, could help you get across the Great FireWall permanently, without being identified ever. We are the GreatER Fire; we ship Trojan Horses.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentations" class="anchor" aria-hidden="true" href="#documentations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentations&lt;/h2&gt;
&lt;p&gt;An online documentation can be found &lt;a href="https://trojan-gfw.github.io/trojan/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;br&gt;
Installation guide on various platforms can be found in the &lt;a href="https://github.com/trojan-gfw/trojan/wiki/Binary-&amp;amp;-Package-Distributions"&gt;wiki&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cmake.org/" rel="nofollow"&gt;CMake&lt;/a&gt; &amp;gt;= 3.7.2&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.boost.org/" rel="nofollow"&gt;Boost&lt;/a&gt; &amp;gt;= 1.66.0&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.openssl.org/" rel="nofollow"&gt;OpenSSL&lt;/a&gt; &amp;gt;= 1.1.0&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dev.mysql.com/downloads/connector/c/" rel="nofollow"&gt;libmysqlclient&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;GPLv3&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>trojan-gfw</author><guid isPermaLink="false">https://github.com/trojan-gfw/trojan</guid><pubDate>Sun, 09 Feb 2020 00:02:00 GMT</pubDate></item><item><title>godotengine/godot #3 in C++, This week</title><link>https://github.com/godotengine/godot</link><description>&lt;p&gt;&lt;i&gt;Godot Engine – Multi-platform 2D and 3D game engine&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://godotengine.org" rel="nofollow"&gt;&lt;img src="/logo.png" alt="Godot Engine logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-godot-engine" class="anchor" aria-hidden="true" href="#godot-engine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Godot Engine&lt;/h2&gt;
&lt;p&gt;Homepage: &lt;a href="https://godotengine.org" rel="nofollow"&gt;https://godotengine.org&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-2d-and-3d-cross-platform-game-engine" class="anchor" aria-hidden="true" href="#2d-and-3d-cross-platform-game-engine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2D and 3D cross-platform game engine&lt;/h4&gt;
&lt;p&gt;Godot Engine is a feature-packed, cross-platform game engine to create 2D and
3D games from a unified interface. It provides a comprehensive set of common
tools, so that users can focus on making games without having to reinvent the
wheel. Games can be exported in one click to a number of platforms, including
the major desktop platforms (Linux, Mac OSX, Windows) as well as mobile
(Android, iOS) and web-based (HTML5) platforms.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-free-open-source-and-community-driven" class="anchor" aria-hidden="true" href="#free-open-source-and-community-driven"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Free, open source and community-driven&lt;/h4&gt;
&lt;p&gt;Godot is completely free and open source under the very permissive MIT license.
No strings attached, no royalties, nothing. The users' games are theirs, down
to the last line of engine code. Godot's development is fully independent and
community-driven, empowering users to help shape their engine to match their
expectations. It is supported by the Software Freedom Conservancy
not-for-profit.&lt;/p&gt;
&lt;p&gt;Before being open sourced in February 2014, Godot had been developed by Juan
Linietsky and Ariel Manzur (both still maintaining the project) for several
years as an in-house engine, used to publish several work-for-hire titles.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/godotengine/godot-design/master/screenshots/editor_tps_demo_1920x1080.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/godotengine/godot-design/master/screenshots/editor_tps_demo_1920x1080.jpg" alt="Screenshot of a 3D scene in Godot Engine" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-the-engine" class="anchor" aria-hidden="true" href="#getting-the-engine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting the engine&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-binary-downloads" class="anchor" aria-hidden="true" href="#binary-downloads"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Binary downloads&lt;/h4&gt;
&lt;p&gt;Official binaries for the Godot editor and the export templates can be found
&lt;a href="https://godotengine.org/download" rel="nofollow"&gt;on the homepage&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-compiling-from-source" class="anchor" aria-hidden="true" href="#compiling-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiling from source&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://docs.godotengine.org/en/latest/development/compiling/" rel="nofollow"&gt;See the official docs&lt;/a&gt;
for compilation instructions for every supported platform.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-community-and-contributing" class="anchor" aria-hidden="true" href="#community-and-contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community and contributing&lt;/h3&gt;
&lt;p&gt;Godot is not only an engine but an ever-growing community of users and engine
developers. The main community channels are listed &lt;a href="https://godotengine.org/community" rel="nofollow"&gt;on the homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To get in touch with the developers, the best way is to join the
&lt;a href="https://webchat.freenode.net/?channels=godotengine" rel="nofollow"&gt;#godotengine IRC channel&lt;/a&gt;
on Freenode.&lt;/p&gt;
&lt;p&gt;To get started contributing to the project, see the &lt;a href="CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-documentation-and-demos" class="anchor" aria-hidden="true" href="#documentation-and-demos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation and demos&lt;/h3&gt;
&lt;p&gt;The official documentation is hosted on &lt;a href="https://docs.godotengine.org" rel="nofollow"&gt;ReadTheDocs&lt;/a&gt;.
It is maintained by the Godot community in its own &lt;a href="https://github.com/godotengine/godot-docs"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.godotengine.org/en/latest/classes/" rel="nofollow"&gt;class reference&lt;/a&gt;
is also accessible from within the engine.&lt;/p&gt;
&lt;p&gt;The official demos are maintained in their own &lt;a href="https://github.com/godotengine/godot-demo-projects"&gt;GitHub repository&lt;/a&gt;
as well.&lt;/p&gt;
&lt;p&gt;There are also a number of other learning resources provided by the community,
such as text and video tutorials, demos, etc. Consult the &lt;a href="https://godotengine.org/community" rel="nofollow"&gt;community channels&lt;/a&gt;
for more info.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/godotengine/godot" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1749bd189cb7e200d00f73c7aea7c12a81c51213/68747470733a2f2f7472617669732d63692e6f72672f676f646f74656e67696e652f676f646f742e7376673f6272616e63683d6d6173746572" alt="Travis Build Status" data-canonical-src="https://travis-ci.org/godotengine/godot.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/akien-mga/godot" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/26a2ec75e7db307fe6519108365c8b912d18753c/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6266696968717136627978736a7878682f6272616e63682f6d61737465723f7376673d74727565" alt="AppVeyor Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/bfiihqq6byxsjxxh/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codetriage.com/godotengine/godot" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6094dc9f409fe501c72fccae301f6cbc69f8746c/68747470733a2f2f7777772e636f64657472696167652e636f6d2f676f646f74656e67696e652f676f646f742f6261646765732f75736572732e737667" alt="Code Triagers Badge" data-canonical-src="https://www.codetriage.com/godotengine/godot/badges/users.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://hosted.weblate.org/engage/godot-engine/?utm_source=widget" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2124b3de8872ae25a95093b43a55c35dbbaee1d4/68747470733a2f2f686f737465642e7765626c6174652e6f72672f776964676574732f676f646f742d656e67696e652f2d2f676f646f742f7376672d62616467652e737667" alt="Translate on Weblate" data-canonical-src="https://hosted.weblate.org/widgets/godot-engine/-/godot/svg-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>godotengine</author><guid isPermaLink="false">https://github.com/godotengine/godot</guid><pubDate>Sun, 09 Feb 2020 00:03:00 GMT</pubDate></item><item><title>yuzu-emu/yuzu #4 in C++, This week</title><link>https://github.com/yuzu-emu/yuzu</link><description>&lt;p&gt;&lt;i&gt;Nintendo Switch Emulator&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-yuzu-emulator" class="anchor" aria-hidden="true" href="#yuzu-emulator"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;yuzu emulator&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/yuzu-emu/yuzu" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4cb736274fea28e3a2584b80afddb3da04027bd1/68747470733a2f2f7472617669732d63692e6f72672f79757a752d656d752f79757a752e7376673f6272616e63683d6d6173746572" alt="Travis CI Build Status" data-canonical-src="https://travis-ci.org/yuzu-emu/yuzu.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://dev.azure.com/yuzu-emu/yuzu/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bd081ce462b0bdcbda19d17fc4fe0d70b3ad8079/68747470733a2f2f6465762e617a7572652e636f6d2f79757a752d656d752f79757a752f5f617069732f6275696c642f7374617475732f79757a752532306d61696e6c696e653f6272616e63684e616d653d6d6173746572" alt="Azure Mainline CI Build Status" data-canonical-src="https://dev.azure.com/yuzu-emu/yuzu/_apis/build/status/yuzu%20mainline?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;yuzu is an experimental open-source emulator for the Nintendo Switch from the creators of &lt;a href="https://citra-emu.org/" rel="nofollow"&gt;Citra&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is written in C++ with portability in mind, with builds actively maintained for Windows and Linux. The emulator is capable of running several commercial games.&lt;/p&gt;
&lt;p&gt;yuzu only emulates a subset of Switch hardware and therefore most commercial games &lt;strong&gt;do not&lt;/strong&gt; run at full speed or are not fully functional.&lt;/p&gt;
&lt;p&gt;Do you want to check which games are compatible and which ones are not? Please visit our &lt;a href="https://yuzu-emu.org/game/" rel="nofollow"&gt;Compatibility page&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;yuzu is licensed under the GPLv2 (or any later version). Refer to the license.txt file included.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://yuzu-emu.org/" rel="nofollow"&gt;website&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;For development discussion, please join us on &lt;a href="https://discord.gg/XQV6dn9" rel="nofollow"&gt;Discord&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;Most of the development happens on GitHub. It's also where &lt;a href="https://github.com/yuzu-emu/yuzu"&gt;our central repository&lt;/a&gt; is hosted.&lt;/p&gt;
&lt;p&gt;If you want to contribute please take a look at the &lt;a href="CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt; and &lt;a href="https://github.com/yuzu-emu/yuzu/wiki/Developer-Information"&gt;Developer Information&lt;/a&gt;. You should as well contact any of the developers on Discord in order to know about the current state of the emulator.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-building" class="anchor" aria-hidden="true" href="#building"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;a href="https://github.com/yuzu-emu/yuzu/wiki/Building-For-Windows"&gt;Windows Build&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;a href="https://github.com/yuzu-emu/yuzu/wiki/Building-For-Linux"&gt;Linux Build&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;a href="https://github.com/yuzu-emu/yuzu/wiki/Building-for-macOS"&gt;macOS Build&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h3&gt;
&lt;p&gt;We happily accept monetary donations or donated games and hardware. Please see our &lt;a href="https://yuzu-emu.org/donate/" rel="nofollow"&gt;donations page&lt;/a&gt; for more information on how you can contribute to yuzu. Any donations received will go towards things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Switch consoles to explore and reverse-engineer the hardware&lt;/li&gt;
&lt;li&gt;Switch games for testing, reverse-engineering, and implementing new features&lt;/li&gt;
&lt;li&gt;Web hosting and infrastructure setup&lt;/li&gt;
&lt;li&gt;Software licenses (e.g. Visual Studio, IDA Pro, etc.)&lt;/li&gt;
&lt;li&gt;Additional hardware (e.g. GPUs as-needed to improve rendering support, other peripherals to add support for, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also more than gladly accept used Switch consoles, preferably ones with firmware 3.0.0 or lower! If you would like to give yours away, don't hesitate to join our &lt;a href="https://discord.gg/VXqngT3" rel="nofollow"&gt;Discord&lt;/a&gt; and talk to bunnei. You may also contact: &lt;a href="mailto:donations@yuzu-emu.org"&gt;donations@yuzu-emu.org&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yuzu-emu</author><guid isPermaLink="false">https://github.com/yuzu-emu/yuzu</guid><pubDate>Sun, 09 Feb 2020 00:04:00 GMT</pubDate></item><item><title>opencv/opencv #5 in C++, This week</title><link>https://github.com/opencv/opencv</link><description>&lt;p&gt;&lt;i&gt;Open Source Computer Vision Library&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-opencv-open-source-computer-vision-library" class="anchor" aria-hidden="true" href="#opencv-open-source-computer-vision-library"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenCV: Open Source Computer Vision Library&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Homepage: &lt;a href="https://opencv.org" rel="nofollow"&gt;https://opencv.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Docs: &lt;a href="https://docs.opencv.org/master/" rel="nofollow"&gt;https://docs.opencv.org/master/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Q&amp;amp;A forum: &lt;a href="http://answers.opencv.org" rel="nofollow"&gt;http://answers.opencv.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracking: &lt;a href="https://github.com/opencv/opencv/issues"&gt;https://github.com/opencv/opencv/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h3&gt;
&lt;p&gt;Please read the &lt;a href="https://github.com/opencv/opencv/wiki/How_to_contribute"&gt;contribution guidelines&lt;/a&gt; before starting work on a pull request.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-summary-of-the-guidelines" class="anchor" aria-hidden="true" href="#summary-of-the-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary of the guidelines:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;One pull request per issue;&lt;/li&gt;
&lt;li&gt;Choose the right base branch;&lt;/li&gt;
&lt;li&gt;Include tests and documentation;&lt;/li&gt;
&lt;li&gt;Clean up "oops" commits before submitting;&lt;/li&gt;
&lt;li&gt;Follow the &lt;a href="https://github.com/opencv/opencv/wiki/Coding_Style_Guide"&gt;coding style guide&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>opencv</author><guid isPermaLink="false">https://github.com/opencv/opencv</guid><pubDate>Sun, 09 Feb 2020 00:05:00 GMT</pubDate></item><item><title>llvm/llvm-project #6 in C++, This week</title><link>https://github.com/llvm/llvm-project</link><description>&lt;p&gt;&lt;i&gt;The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. Note: the repository does not accept github pull requests at this moment. Please submit your patches at http://reviews.llvm.org.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-the-llvm-compiler-infrastructure" class="anchor" aria-hidden="true" href="#the-llvm-compiler-infrastructure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The LLVM Compiler Infrastructure&lt;/h1&gt;
&lt;p&gt;This directory and its subdirectories contain source code for LLVM,
a toolkit for the construction of highly optimized compilers,
optimizers, and runtime environments.&lt;/p&gt;
&lt;p&gt;The README briefly describes how to get started with building LLVM.
For more information on how to contribute to the LLVM project, please
take a look at the
&lt;a href="https://llvm.org/docs/Contributing.html" rel="nofollow"&gt;Contributing to LLVM&lt;/a&gt; guide.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-with-the-llvm-system" class="anchor" aria-hidden="true" href="#getting-started-with-the-llvm-system"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started with the LLVM System&lt;/h2&gt;
&lt;p&gt;Taken from &lt;a href="https://llvm.org/docs/GettingStarted.html" rel="nofollow"&gt;https://llvm.org/docs/GettingStarted.html&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h3&gt;
&lt;p&gt;Welcome to the LLVM project!&lt;/p&gt;
&lt;p&gt;The LLVM project has multiple components. The core of the project is
itself called "LLVM". This contains all of the tools, libraries, and header
files needed to process intermediate representations and converts it into
object files.  Tools include an assembler, disassembler, bitcode analyzer, and
bitcode optimizer.  It also contains basic regression tests.&lt;/p&gt;
&lt;p&gt;C-like languages use the &lt;a href="http://clang.llvm.org/" rel="nofollow"&gt;Clang&lt;/a&gt; front end.  This
component compiles C, C++, Objective C, and Objective C++ code into LLVM bitcode
-- and from there into object files, using LLVM.&lt;/p&gt;
&lt;p&gt;Other components include:
the &lt;a href="https://libcxx.llvm.org" rel="nofollow"&gt;libc++ C++ standard library&lt;/a&gt;,
the &lt;a href="https://lld.llvm.org" rel="nofollow"&gt;LLD linker&lt;/a&gt;, and more.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-the-source-code-and-building-llvm" class="anchor" aria-hidden="true" href="#getting-the-source-code-and-building-llvm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting the Source Code and Building LLVM&lt;/h3&gt;
&lt;p&gt;The LLVM Getting Started documentation may be out of date.  The &lt;a href="http://clang.llvm.org/get_started.html" rel="nofollow"&gt;Clang
Getting Started&lt;/a&gt; page might have more
accurate information.&lt;/p&gt;
&lt;p&gt;This is an example workflow and configuration to get and build the LLVM source:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Checkout LLVM (including related subprojects like Clang):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;git clone https://github.com/llvm/llvm-project.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or, on windows, &lt;code&gt;git clone --config core.autocrlf=false  https://github.com/llvm/llvm-project.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure and build LLVM and Clang:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;cd llvm-project&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;mkdir build&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;cd build&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;cmake -G &amp;lt;generator&amp;gt; [options] ../llvm&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Some common generators are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Ninja&lt;/code&gt; --- for generating &lt;a href="https://ninja-build.org" rel="nofollow"&gt;Ninja&lt;/a&gt;
build files. Most llvm developers use Ninja.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Unix Makefiles&lt;/code&gt; --- for generating make-compatible parallel makefiles.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Visual Studio&lt;/code&gt; --- for generating Visual Studio projects and
solutions.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Xcode&lt;/code&gt; --- for generating Xcode projects.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some Common options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-DLLVM_ENABLE_PROJECTS='...'&lt;/code&gt; --- semicolon-separated list of the LLVM
subprojects you'd like to additionally build. Can include any of: clang,
clang-tools-extra, libcxx, libcxxabi, libunwind, lldb, compiler-rt, lld,
polly, or debuginfo-tests.&lt;/p&gt;
&lt;p&gt;For example, to build LLVM, Clang, libcxx, and libcxxabi, use
&lt;code&gt;-DLLVM_ENABLE_PROJECTS="clang;libcxx;libcxxabi"&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-DCMAKE_INSTALL_PREFIX=directory&lt;/code&gt; --- Specify for &lt;em&gt;directory&lt;/em&gt; the full
pathname of where you want the LLVM tools and libraries to be installed
(default &lt;code&gt;/usr/local&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-DCMAKE_BUILD_TYPE=type&lt;/code&gt; --- Valid options for &lt;em&gt;type&lt;/em&gt; are Debug,
Release, RelWithDebInfo, and MinSizeRel. Default is Debug.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-DLLVM_ENABLE_ASSERTIONS=On&lt;/code&gt; --- Compile with assertion checks enabled
(default is Yes for Debug builds, No for all other build types).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run your build tool of choice!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The default target (i.e. &lt;code&gt;ninja&lt;/code&gt; or &lt;code&gt;make&lt;/code&gt;) will build all of LLVM.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;check-all&lt;/code&gt; target (i.e. &lt;code&gt;ninja check-all&lt;/code&gt;) will run the
regression tests to ensure everything is in working order.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CMake will generate build targets for each tool and library, and most
LLVM sub-projects generate their own &lt;code&gt;check-&amp;lt;project&amp;gt;&lt;/code&gt; target.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Running a serial build will be &lt;em&gt;slow&lt;/em&gt;.  To improve speed, try running a
parallel build. That's done by default in Ninja; for &lt;code&gt;make&lt;/code&gt;, use
&lt;code&gt;make -j NNN&lt;/code&gt; (NNN is the number of parallel jobs, use e.g. number of
CPUs you have.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For more information see &lt;a href="https://llvm.org/docs/CMake.html" rel="nofollow"&gt;CMake&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Consult the
&lt;a href="https://llvm.org/docs/GettingStarted.html#getting-started-with-llvm" rel="nofollow"&gt;Getting Started with LLVM&lt;/a&gt;
page for detailed information on configuring and compiling LLVM. You can visit
&lt;a href="https://llvm.org/docs/GettingStarted.html#directory-layout" rel="nofollow"&gt;Directory Layout&lt;/a&gt;
to learn about the layout of the source code tree.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>llvm</author><guid isPermaLink="false">https://github.com/llvm/llvm-project</guid><pubDate>Sun, 09 Feb 2020 00:06:00 GMT</pubDate></item><item><title>NVIDIAGameWorks/kaolin #7 in C++, This week</title><link>https://github.com/NVIDIAGameWorks/kaolin</link><description>&lt;p&gt;&lt;i&gt;A PyTorch Library for Accelerating 3D Deep Learning Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/kaolin.png"&gt;&lt;img src="assets/kaolin.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-kaolin-a-pytorch-library-for-accelerating-3d-deep-learning-research" class="anchor" aria-hidden="true" href="#kaolin-a-pytorch-library-for-accelerating-3d-deep-learning-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://kaolin.readthedocs.io/en/latest/" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://arxiv.org/abs/1911.05063" rel="nofollow"&gt;Paper&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kaolin is a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Fun fact&lt;/em&gt;: The name &lt;em&gt;Kaolin&lt;/em&gt;, it’s from Kaolinite, a form of plasticine (clay) that is sometimes used in 3D modeling.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#functionality"&gt;Functionality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation-and-usage"&gt;Installation And Usage&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supported-platforms"&gt;Supported Platforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#install-kaolin"&gt;Install Kaolin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#verify-installation"&gt;Verify Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#building-the-documentation"&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#running-unittests"&gt;Running Unittests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#main-modules"&gt;Main Modules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-functionality" class="anchor" aria-hidden="true" href="#functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Functionality&lt;/h2&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/kaolin_applications.png"&gt;&lt;img src="assets/kaolin_applications.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Currently, the (beta) release contains several processing functions for 3D deep learning on meshes, voxels, signed distance functions, and pointclouds. Loading of several popular datasets (eg. ShapeNet, ModelNet, SHREC) are supported out-of-the-box. We also implement several 3D conversion and transformation operations (both within and across the aforementioned representations).&lt;/p&gt;
&lt;p&gt;Kaolin supports several 3D tasks such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Differentiable rendering (see &lt;a href="https://github.com/hiroharu-kato/neural_renderer"&gt;Neural Mesh Renderer&lt;/a&gt;, its &lt;a href="https://github.com/daniilidis-group/neural_renderer"&gt;PyTorch port&lt;/a&gt;, &lt;a href="https://github.com/ShichenLiu/SoftRas"&gt;Soft Rasterizer&lt;/a&gt;, &lt;a href="https://nv-tlabs.github.io/DIB-R/" rel="nofollow"&gt;Differentiable Interpolation-based Renderer&lt;/a&gt;, and a modular and extensible abstract DifferentiableRenderer specification).&lt;/li&gt;
&lt;li&gt;Single-image based mesh reconstruction (&lt;a href="https://github.com/nywang16/Pixel2Mesh"&gt;Pixel2Mesh&lt;/a&gt;, &lt;a href="https://github.com/EdwardSmith1884/GEOMetrics"&gt;GEOMetrics&lt;/a&gt;, &lt;a href="https://github.com/autonomousvision/occupancy_networks"&gt;OccupancyNets&lt;/a&gt;, and more...)&lt;/li&gt;
&lt;li&gt;Pointcloud classification and segmentation (&lt;a href="https://github.com/fxia22/pointnet.pytorch"&gt;PointNet&lt;/a&gt;, &lt;a href="https://github.com/erikwijmans/Pointnet2_PyTorch"&gt;PoinNet++&lt;/a&gt;, &lt;a href="https://github.com/muhanzhang/pytorch_DGCNN"&gt;DGCNN&lt;/a&gt;, ...)&lt;/li&gt;
&lt;li&gt;Mesh classification and segmentation (&lt;a href="https://github.com/ranahanocka/MeshCNN"&gt;MeshCNN&lt;/a&gt;, &lt;a href="https://github.com/tkipf/pygcn"&gt;GCN&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;3D superresolution on voxel grids (&lt;a href="https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-Resolution-3D-Object-Representation"&gt;ODM&lt;/a&gt;, VoxelUNet, and more...)&lt;/li&gt;
&lt;li&gt;Basic graphics utilities (lighting, shading, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-model-zoo" class="anchor" aria-hidden="true" href="#model-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Zoo&lt;/h2&gt;
&lt;p&gt;Kaolin curates a large &lt;em&gt;model zoo&lt;/em&gt; containing reference implementations of popular 3D DL architectures. Head over &lt;a href="kaolin/models"&gt;here&lt;/a&gt; to check them out.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: For several of these models, the implementation is due to the original authors. We build a bridge to our library and wherever possible, we introduce optimization. If you use any of the models in the model zoo, or the graphics packages (eg. differentiable renderers such as &lt;a href="https://arxiv.org/abs/1711.07566" rel="nofollow"&gt;NMR&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1904.01786" rel="nofollow"&gt;SoftRas&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1908.01210" rel="nofollow"&gt;DIB-R&lt;/a&gt;), please cite the original papers in addition to Kaolin. For convenience, BibTeX citation formats for each of the original papers are included in the documentation for each model provided.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and Usage&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The API is currently somewhat unstable, as we're making constant changes. (It's a beta release)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Python &amp;gt;= 3.6&lt;/li&gt;
&lt;li&gt;CUDA &amp;gt;= 10.0.130 (with &lt;code&gt;nvcc&lt;/code&gt; installed)&lt;/li&gt;
&lt;li&gt;Display Driver &amp;gt;= 410.48&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Windows support is in the works and is currently considered experimental.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;numpy &amp;gt;= 1.17&lt;/li&gt;
&lt;li&gt;PyTorch &amp;gt;= 1.2 and Torchvision (see &lt;a href="http://pytorch.org" rel="nofollow"&gt;pytorch.org&lt;/a&gt; for installation instructions)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;We highly recommend installing Kaolin inside of a virtual environment (such as ones created using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;). In this example, we show how to create a &lt;code&gt;conda&lt;/code&gt; virtual environment for installing kaolin.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ conda create --name kaolin python=3.6
$ conda activate kaolin&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies-1" class="anchor" aria-hidden="true" href="#dependencies-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;p&gt;Install PyTorch and Torchvision by following instructions from &lt;a href="https://pytorch.org/" rel="nofollow"&gt;https://pytorch.org/&lt;/a&gt;. Numpy will be installed as part of the Pytorch installation. Note that the setup file does not automatically install these dependencies.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-install-kaolin" class="anchor" aria-hidden="true" href="#install-kaolin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install Kaolin&lt;/h4&gt;
&lt;p&gt;Now, you can install the library. From the root directory of this repo (i.e., the directory containing this &lt;code&gt;README&lt;/code&gt; file), run&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-for-general-use" class="anchor" aria-hidden="true" href="#for-general-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;For General Use&lt;/h5&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python setup.py build_ext --inplace   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; optional, allows importing kaolin from the kaolin root directory&lt;/span&gt;
$ python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-for-development-modifying-kaolin-code" class="anchor" aria-hidden="true" href="#for-development-modifying-kaolin-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;For Development (modifying kaolin code)&lt;/h5&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python setup.py develop&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note, if modifying or adding Cython files, ensure that Cython is installed and set the following environment variable &lt;code&gt;USE_CYTHON=1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;During installation, the &lt;em&gt;packman&lt;/em&gt; package manager will
download the nv-usd package to &lt;code&gt;~/packman-repo/&lt;/code&gt; containing the necessary packages for reading and writing Universal Scene Description (USD) files.&lt;/p&gt;
&lt;p&gt;Note, if you are using an heterogeneous GPUs setup set the architectures for which you want to compile the cuda code using the environment variable.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ &lt;span class="pl-k"&gt;export&lt;/span&gt; TORCH_CUDA_ARCH_LIST=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;7.0 7.5&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-verify-installation" class="anchor" aria-hidden="true" href="#verify-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Verify installation&lt;/h3&gt;
&lt;p&gt;To verify that &lt;code&gt;kaolin&lt;/code&gt; has been installed, fire up your python interpreter, and execute the following commands.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; kaolin &lt;span class="pl-k"&gt;as&lt;/span&gt; kal
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;print&lt;/span&gt;(kal.&lt;span class="pl-c1"&gt;__version__&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-building-the-documentation" class="anchor" aria-hidden="true" href="#building-the-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building the Documentation&lt;/h3&gt;
&lt;p&gt;To delve deeper into the library, build the documentation. From the root directory of the repository (i.e., the directory containing this &lt;code&gt;README&lt;/code&gt; file), execute the following.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; docs
$ sphinx-build &lt;span class="pl-c1"&gt;.&lt;/span&gt; _build&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-running-unittests" class="anchor" aria-hidden="true" href="#running-unittests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Unittests&lt;/h3&gt;
&lt;p&gt;To run unittests, from the root directory of the repository (i.e., the directory containing this &lt;code&gt;README&lt;/code&gt; file), execute the following commands.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ pytest tests/&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-main-modules" class="anchor" aria-hidden="true" href="#main-modules"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Modules&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;rep&lt;/strong&gt;: Supported 3D asset representations include: Triangle Meshes, Quad Meshes, Voxel Grids, Point Clouds, Signed Distance Functions (SDF).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;conversions&lt;/strong&gt;: Supports conversion across all popular 3D representations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;models&lt;/strong&gt;: Provided models include the following. For each implementation, we also provide a link to the &lt;em&gt;original implementation which it was ported from&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DGCNN [&lt;a href="https://arxiv.org/abs/1801.07829v1" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/WangYueFt/dgcnn"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DIB-R [&lt;a href="https://arxiv.org/abs/1908.01210" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/nv-tlabs/DIB-R/"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GEOMetrics [&lt;a href="https://arxiv.org/abs/1901.11461" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/EdwardSmith1884/GEOMetrics"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Image2Mesh [&lt;a href="https://arxiv.org/abs/1711.10669" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/jhonykaesemodel/image2mesh"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Occupancy Network [&lt;a href="https://arxiv.org/abs/1812.03828" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/autonomousvision/occupancy_networks"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pixel2Mesh [&lt;a href="https://arxiv.org/abs/1804.01654" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/nywang16/Pixel2Mesh"&gt;Original implementation (TensorFlow)&lt;/a&gt;], [&lt;a href="https://github.com/EdwardSmith1884/GEOMetrics"&gt;Re-implementation that we borrow from&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;PointNet [&lt;a href="https://arxiv.org/abs/1612.00593" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/charlesq34/pointnet"&gt;Original implementation&lt;/a&gt;], [&lt;a href="https://github.com/fxia22/pointnet.pytorch/"&gt;Re-implementation (we borrow from here)&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;PointNet++ [&lt;a href="https://arxiv.org/abs/1706.02413" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/charlesq34/pointnet2"&gt;Original implementation (we borrow  from here)&lt;/a&gt;], [&lt;a href="https://github.com/erikwijmans/Pointnet2_PyTorch"&gt;Re-implementation (we borrow from here)&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;MeshEncoder: A simple mesh encoder architecture.&lt;/li&gt;
&lt;li&gt;GraphResNet: MeshEncoder with residual connections.&lt;/li&gt;
&lt;li&gt;OccupancyNetworks [&lt;a href="https://arxiv.org/abs/1812.03828" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/autonomousvision/occupancy_networks"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;MeshCNN: [&lt;a href="https://arxiv.org/abs/1809.05910" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/ranahanocka/MeshCNN"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VoxelGAN [&lt;a href="http://3dgan.csail.mit.edu/papers/3dgan_nips.pdf" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/zck119/3dgan-release"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;AtlasNet [&lt;a href="https://arxiv.org/abs/1802.05384" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/ThibaultGROUEIX/AtlasNet"&gt;Original implementation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;And many more to come!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: For several of these models, the implementation is due to the original authors. We build a bridge to our library and wherever possible, we introduce optimization. If you use any of the models in the model zoo, or the graphics packages (eg. differentiable renderers such as &lt;a href="https://arxiv.org/abs/1711.07566" rel="nofollow"&gt;NMR&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1904.01786" rel="nofollow"&gt;SoftRas&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1908.01210" rel="nofollow"&gt;DIB-R&lt;/a&gt;), please cite the original papers in addition to Kaolin. For convenience, BibTeX citation formats for each of the original papers are included in the documentation for each model provided.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;graphics&lt;/strong&gt;: Kaolin provides a flexible and modular framework for building differentiable renderers, making it simple to replace individual components with new ones. Kaolin also provides implementations of the following differentiable renderers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DIB-R [&lt;a href="https://arxiv.org/abs/1908.01210" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/nv-tlabs/DIB-R/"&gt;Original implementation (we borrow from here)&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;SoftRas [&lt;a href="https://arxiv.org/abs/1904.01786" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/ShichenLiu/SoftRas"&gt;Original implementation (we borrow from here)&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Neural 3D Mesh Renderer [&lt;a href="https://arxiv.org/abs/1711.07566" rel="nofollow"&gt;Paper&lt;/a&gt;], [&lt;a href="https://github.com/hiroharu-kato/neural_renderer"&gt;Original Chainer implementation&lt;/a&gt;], [&lt;a href="https://github.com/daniilidis-group/neural_renderer"&gt;PyTorch re-implementation (we borrow from here)&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;metrics&lt;/strong&gt;: Implemented metrics and loss functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mesh: Triangle Distance, Chamfer Distance, Edge Length regularization, Laplacian regularization, Point to Surface distance, Normal consistency&lt;/li&gt;
&lt;li&gt;Point Cloud: Sided Distance, Chamfer Distance, Directed Distance&lt;/li&gt;
&lt;li&gt;Voxel Grid: Intersection Over Union (3D IoU), F-Score&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;Take a look at some of our examples!! Examples include differentiable renderers, voxel superresolution, etc. Begin &lt;a href="examples"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We will (very soon) host our docs online. Stay tuned for the link. Until then, please follow instructions from &lt;a href="#building-the-documentation"&gt;above&lt;/a&gt; to build docs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://krrish94.github.io/" rel="nofollow"&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;,
&lt;a href="https://github.com/EdwardSmith1884"&gt;Edward Smith&lt;/a&gt;,
&lt;a href="https://www.linkedin.com/in/jflafleche" rel="nofollow"&gt;Jean-Francois Lafleche&lt;/a&gt;,
&lt;a href="https://ca.linkedin.com/in/clement-fuji-tsang-b8028a82" rel="nofollow"&gt;Clement Fuji Tsang&lt;/a&gt;,
&lt;a href="https://sites.google.com/site/artemrozantsev/" rel="nofollow"&gt;Artem Rozantsev&lt;/a&gt;,
&lt;a href="http://www.cs.toronto.edu/~wenzheng/" rel="nofollow"&gt;Wenzheng Chen&lt;/a&gt;,
&lt;a href="https://github.com/TommyX12"&gt;Tommy Xiang&lt;/a&gt;,
&lt;a href="https://blogs.nvidia.com/blog/author/revlebaredian/" rel="nofollow"&gt;Rev Lebaredian&lt;/a&gt;,
&lt;a href="https://ca.linkedin.com/in/gavstate" rel="nofollow"&gt;Gavriel State&lt;/a&gt;,
&lt;a href="https://www.cs.utoronto.ca/~fidler/" rel="nofollow"&gt;Sanja Fidler&lt;/a&gt;,&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="Acknowledgements.txt"&gt;Acknowledgements&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We would like to thank &lt;a href="https://amlankar.github.io" rel="nofollow"&gt;Amlan Kar&lt;/a&gt; for suggesting the need for this library. We also thank &lt;a href="http://ankurhanda.github.io" rel="nofollow"&gt;Ankur Handa&lt;/a&gt; for his advice during the initial and final stages of the project. Many thanks to &lt;a href="https://scholar.google.com/citations?user=VVIAoY0AAAAJ&amp;amp;hl=en" rel="nofollow"&gt;Joanh Philion&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/daiqing-li-23873789?originalSubdomain=ca" rel="nofollow"&gt;Daiqing Li&lt;/a&gt;, &lt;a href="https://ca.linkedin.com/in/mark-brophy-3a298382" rel="nofollow"&gt;Mark Brophy&lt;/a&gt;, &lt;a href="http://www.cs.toronto.edu/~jungao/" rel="nofollow"&gt;Jun Gao&lt;/a&gt;, and &lt;a href="http://www.cs.toronto.edu/~linghuan/" rel="nofollow"&gt;Huan Ling&lt;/a&gt; who performed detailed internal reviews, and provided constructive comments. We also thank &lt;a href="https://ca.linkedin.com/in/gavstate" rel="nofollow"&gt;Gavriel State&lt;/a&gt; for all his help during the project.&lt;/p&gt;
&lt;p&gt;Most importantly, we thank all 3D DL researchers who have made their code available as open-source. The field could use a lot more of it!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license-and-copyright" class="anchor" aria-hidden="true" href="#license-and-copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License and Copyright&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="COPYRIGHT"&gt;COPYRIGHT&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you find this library useful, consider citing the following paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{kaolin2019arxiv,
    author = {J., {Krishna Murthy} and Smith, Edward and Lafleche, Jean-Francois and {Fuji Tsang}, Clement and Rozantsev, Artem and Chen, Wenzheng and Xiang, Tommy and Lebaredian, Rev and Fidler, Sanja},
    title = {Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research},
    journal = {arXiv:1911.05063},
    year = {2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-contributors-for-borrowed-sources" class="anchor" aria-hidden="true" href="#contributors-for-borrowed-sources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors for borrowed sources&lt;/h2&gt;
&lt;p&gt;Here is a list of all authors on relevant research papers that Kaolin borrows code from. Without the efforts of these folks (and their willingness to release their implementations under permissive open-source licenses), Kaolin would not have been possible.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kornia: Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee and Gary Bradski. [&lt;a href="https://arxiv.org/pdf/1910.02190.pdf" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/kornia/kornia"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Occupancy Networks: Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger. [&lt;a href="https://avg.is.tuebingen.mpg.de/publications/occupancy-networks" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/autonomousvision/occupancy_networks"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation: Edward Smith, Scott Fujimoto, David Meger. [&lt;a href="https://papers.nips.cc/paper/7883-multi-view-silhouette-and-depth-decomposition-for-high-resolution-3d-object-representation.pdf" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-Resolution-3D-Object-Representation"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pytorch Chamfer Distance: Christian Diller. [&lt;a href="https://github.com/chrdiller/pyTorchChamferDistance"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GEOMetrics: Edward Smith, Scott Fujimoto, Adriana Romero, David Meger. [&lt;a href="https://arxiv.org/abs/1901.11461" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/EdwardSmith1884/GEOMetrics"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DeepSDF: Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove. [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/facebookresearch/DeepSDF"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;PointGAN: Fei Xia. [&lt;a href="https://github.com/fxia22/pointGAN"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;AtlasNet: Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry. [&lt;a href="https://arxiv.org/abs/1802.05384" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/ThibaultGROUEIX/AtlasNet"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;PointNet: Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas. Also, Fei Xia (reimplementation). [&lt;a href="https://arxiv.org/abs/1612.00593" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/fxia22/pointnet.pytorch"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;MeshCNN: Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, Daniel Cohen-Or. [&lt;a href="https://arxiv.org/abs/1809.05910" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/ranahanocka/MeshCNN"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DGCNN: Muhan Zhang, Zhicheng Cui, Marion Neumann, Yixin Chen. [&lt;a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/muhanzhang/pytorch_DGCNN"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Neural 3D Mesh Renderer: Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada. Also, Nikos Kolotouros (for reimplementation). [&lt;a href="https://arxiv.org/abs/1711.07566" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/daniilidis-group/neural_renderer"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;SoftRasterizer: Shichen Liu, Tianye Li, Weikai Chen, Hao Li. [&lt;a href="https://arxiv.org/abs/1904.01786" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/ShichenLiu/SoftRas"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DIB-R: Wenzheng Chen, Jun Gao, Huan Ling, Edward J. Smith, Jaakko Lehtinen, Alec Jacobson, Sanja Fidler. [&lt;a href="https://arxiv.org/abs/1908.01210" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/nv-tlabs/DIB-R"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;PointNet++: Charles R. Qi, Li (Eric) Yi, Hao Su, Leonidas J. Guibas. Also, Erik Wijmans (reimplementation). [&lt;a href="https://arxiv.org/abs/1706.02413" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/erikwijmans/Pointnet2_PyTorch"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling': Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B. Tenenbaum. [&lt;a href="http://arxiv.org/abs/1610.07584" rel="nofollow"&gt;Paper&lt;/a&gt;] [&lt;a href="https://github.com/zck119/3dgan-release"&gt;Code&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIAGameWorks</author><guid isPermaLink="false">https://github.com/NVIDIAGameWorks/kaolin</guid><pubDate>Sun, 09 Feb 2020 00:07:00 GMT</pubDate></item><item><title>CMU-Perceptual-Computing-Lab/openpose #8 in C++, This week</title><link>https://github.com/CMU-Perceptual-Computing-Lab/openpose</link><description>&lt;p&gt;&lt;i&gt;OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href=".github/Logo_main_black.png"&gt;&lt;img src=".github/Logo_main_black.png" width="300" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;code&gt;Default Config&lt;/code&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;code&gt;CUDA (+Python)&lt;/code&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;code&gt;CPU (+Python)&lt;/code&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;code&gt;OpenCL (+Python)&lt;/code&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;code&gt;Debug&lt;/code&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;code&gt;Unity&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;strong&gt;&lt;code&gt;Linux&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13586d37f7b024fb7dac328651ab0a946d4d68ff/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f31" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/76b55d876291ec7c54720a7436ad3726e45444b9/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f32" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3c65775203ce7948f62d4993a19fb2462f9231fe/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f33" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/10ec70280ad0d9923d9a30cb65b7037df80d921b/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f34" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/4" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4d4cedc4adc768de4b58bc9753a7a135b346a6b/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f35" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/5" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c894cf78c90ff13512b0f549e7d3c039416057f9/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f36" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/6" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;strong&gt;&lt;code&gt;MacOS&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9a18a89e972840fb95ff2a13b79068c3223a10b6/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f37" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9a18a89e972840fb95ff2a13b79068c3223a10b6/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f37" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/7" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/86c23b7d4e74b3e7955f6c474842d1e6c7614bd1/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f38" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/8" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/53af875f18f51b7c2c608e7c8e6ab3969b6906a2/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f39" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/9" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/CMU-Perceptual-Computing-Lab/openpose" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/51ab556036ce41f8f8e250de53a4306802dea16d/68747470733a2f2f7472617669732d6d61747269782d6261646765732e6865726f6b756170702e636f6d2f7265706f732f434d552d5065726365707475616c2d436f6d707574696e672d4c61622f6f70656e706f73652f6272616e636865732f6d61737465722f3130" alt="Status" data-canonical-src="https://travis-matrix-badges.herokuapp.com/repos/CMU-Perceptual-Computing-Lab/openpose/branches/master/10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;strong&gt;&lt;code&gt;Windows&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/25a10cf1906e5ba6519601577180759a5779e17b/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f356c6565736378786477656e37376b672f6272616e63682f6d61737465723f7376673d74727565" alt="Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose"&gt;&lt;strong&gt;OpenPose&lt;/strong&gt;&lt;/a&gt; represents the &lt;strong&gt;first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It is &lt;strong&gt;authored by &lt;a href="https://www.gineshidalgo.com" rel="nofollow"&gt;Gines Hidalgo&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~zhecao" rel="nofollow"&gt;Zhe Cao&lt;/a&gt;, &lt;a href="http://www.cs.cmu.edu/~tsimon" rel="nofollow"&gt;Tomas Simon&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=sFQD3k4AAAAJ&amp;amp;hl=en" rel="nofollow"&gt;Shih-En Wei&lt;/a&gt;, &lt;a href="https://jhugestar.github.io" rel="nofollow"&gt;Hanbyul Joo&lt;/a&gt;, and &lt;a href="http://www.cs.cmu.edu/~yaser" rel="nofollow"&gt;Yaser Sheikh&lt;/a&gt;&lt;/strong&gt;. Currently, it is being &lt;strong&gt;maintained by &lt;a href="https://www.gineshidalgo.com" rel="nofollow"&gt;Gines Hidalgo&lt;/a&gt; and &lt;a href="https://www.raaj.tech" rel="nofollow"&gt;Yaadhav Raaj&lt;/a&gt;&lt;/strong&gt;. In addition, OpenPose would not be possible without the &lt;a href="http://domedb.perception.cs.cmu.edu" rel="nofollow"&gt;&lt;strong&gt;CMU Panoptic Studio dataset&lt;/strong&gt;&lt;/a&gt;. We would also like to thank all the people who helped OpenPose in any way. The main contributors are listed in &lt;a href="doc/contributors.md"&gt;doc/contributors.md&lt;/a&gt;.&lt;/p&gt;

&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/pose_face_hands.gif"&gt;&lt;img src="doc/media/pose_face_hands.gif" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;sup&gt;Authors &lt;a href="https://www.gineshidalgo.com" rel="nofollow"&gt;Gines Hidalgo&lt;/a&gt; (left) and &lt;a href="https://jhugestar.github.io" rel="nofollow"&gt;Hanbyul Joo&lt;/a&gt; (right) in front of the &lt;a href="http://domedb.perception.cs.cmu.edu" rel="nofollow"&gt;CMU Panoptic Studio&lt;/a&gt;&lt;/sup&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2D real-time multi-person keypoint detection&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;15 or 18 or &lt;strong&gt;25-keypoint body/foot keypoint estimation&lt;/strong&gt;. &lt;strong&gt;Running time invariant to number of detected people&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;6-keypoint foot keypoint estimation&lt;/strong&gt;. Integrated together with the 25-keypoint body/foot keypoint detector.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2x21-keypoint hand keypoint estimation&lt;/strong&gt;. Currently, &lt;strong&gt;running time depends&lt;/strong&gt; on &lt;strong&gt;number of detected people&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;70-keypoint face keypoint estimation&lt;/strong&gt;. Currently, &lt;strong&gt;running time depends&lt;/strong&gt; on &lt;strong&gt;number of detected people&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3D real-time single-person keypoint detection&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;3-D triangulation from multiple single views.&lt;/li&gt;
&lt;li&gt;Synchronization of Flir cameras handled.&lt;/li&gt;
&lt;li&gt;Compatible with Flir/Point Grey cameras, but provided C++ demos to add your custom input.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calibration toolbox&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Easy estimation of distortion, intrinsic, and extrinsic camera parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single-person tracking&lt;/strong&gt; for further speed up or visual smoothing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Image, video, webcam, Flir/Point Grey and IP camera. Included C++ demos to add your custom input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), and/or keypoints as array class.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Ubuntu (14, 16), Windows (8, 10), Mac OSX, Nvidia TX2.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training and datasets&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"&gt;&lt;strong&gt;OpenPose Training&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/" rel="nofollow"&gt;&lt;strong&gt;Foot dataset website&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Others&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Available: command-line demo, C++ wrapper, and C++ API.&lt;/li&gt;
&lt;li&gt;&lt;a href="doc/modules/python_module.md"&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin"&gt;&lt;strong&gt;Unity Plugin&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CUDA (Nvidia GPU), OpenCL (AMD GPU), and CPU-only (no GPU) versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-latest-features" class="anchor" aria-hidden="true" href="#latest-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latest Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Sep 2019: &lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"&gt;&lt;strong&gt;Training code released&lt;/strong&gt;&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;Jan 2019: &lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin"&gt;&lt;strong&gt;Unity plugin released&lt;/strong&gt;&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;Jan 2019: &lt;a href="doc/modules/python_module.md"&gt;&lt;strong&gt;Improved Python API&lt;/strong&gt;&lt;/a&gt; released! Including body, face, hands, and all the functionality of the C++ API!&lt;/li&gt;
&lt;li&gt;Dec 2018: &lt;a href="https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset" rel="nofollow"&gt;&lt;strong&gt;Foot dataset released&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1812.08008" rel="nofollow"&gt;&lt;strong&gt;new paper released&lt;/strong&gt;&lt;/a&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For further details, check &lt;a href="doc/released_features.md"&gt;all released features&lt;/a&gt; and &lt;a href="doc/release_notes.md"&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-body-and-foot-estimation" class="anchor" aria-hidden="true" href="#body-and-foot-estimation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Body and Foot Estimation&lt;/h3&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/dance_foot.gif"&gt;&lt;img src="doc/media/dance_foot.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;sup&gt;Testing the &lt;a href="https://www.youtube.com/watch?v=2DiQUX11YaY" rel="nofollow"&gt;&lt;i&gt;Crazy Uptown Funk flashmob in Sydney&lt;/i&gt;&lt;/a&gt; video sequence with OpenPose&lt;/sup&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-3-d-reconstruction-module-body-foot-face-and-hands" class="anchor" aria-hidden="true" href="#3-d-reconstruction-module-body-foot-face-and-hands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3-D Reconstruction Module (Body, Foot, Face, and Hands)&lt;/h3&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/openpose3d.gif"&gt;&lt;img src="doc/media/openpose3d.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;sup&gt;Testing the 3D Reconstruction Module of OpenPose&lt;/sup&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-body-foot-face-and-hands-estimation" class="anchor" aria-hidden="true" href="#body-foot-face-and-hands-estimation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Body, Foot, Face, and Hands Estimation&lt;/h3&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/pose_face.gif"&gt;&lt;img src="doc/media/pose_face.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/pose_hands.gif"&gt;&lt;img src="doc/media/pose_hands.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;sup&gt;Authors &lt;a href="https://www.gineshidalgo.com" rel="nofollow"&gt;Gines Hidalgo&lt;/a&gt; (left image) and &lt;a href="http://www.cs.cmu.edu/~tsimon" rel="nofollow"&gt;Tomas Simon&lt;/a&gt; (right image) testing OpenPose&lt;/sup&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-unity-plugin" class="anchor" aria-hidden="true" href="#unity-plugin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unity Plugin&lt;/h3&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/unity_main.png"&gt;&lt;img src="doc/media/unity_main.png" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/unity_body_foot.png"&gt;&lt;img src="doc/media/unity_body_foot.png" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/unity_hand_face.png"&gt;&lt;img src="doc/media/unity_hand_face.png" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;sup&gt;&lt;a href="http://tianyizhao.com" rel="nofollow"&gt;Tianyi Zhao&lt;/a&gt; and &lt;a href="https://www.gineshidalgo.com" rel="nofollow"&gt;Gines Hidalgo&lt;/a&gt; testing their &lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin"&gt;OpenPose Unity Plugin&lt;/a&gt;&lt;/sup&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-runtime-analysis" class="anchor" aria-hidden="true" href="#runtime-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Runtime Analysis&lt;/h3&gt;
&lt;p&gt;Inference time comparison between the 3 available pose estimation libraries: OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN:&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="doc/media/openpose_vs_competition.png"&gt;&lt;img src="doc/media/openpose_vs_competition.png" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
This analysis was performed using the same images for each algorithm and a batch size of 1. Each analysis was repeated 1000 times and then averaged. This was all performed on a system with a Nvidia 1080 Ti and CUDA 8. Megvii (Face++) and MSRA GitHub repositories were excluded because they only provide pose estimation results given a cropped person. However, they suffer the same problem than Alpha-Pose and Mask R-CNN, their runtimes grow linearly with the number of people.
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#features"&gt;Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#latest-features"&gt;Latest Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#results"&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation-reinstallation-and-uninstallation"&gt;Installation, Reinstallation and Uninstallation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#output"&gt;Output&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speeding-up-openpose-and-benchmark"&gt;Speeding Up OpenPose and Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-code-and-foot-dataset"&gt;Training Code and Foot Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#send-us-failure-cases-and-feedback"&gt;Send Us Failure Cases and Feedback!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-installation-reinstallation-and-uninstallation" class="anchor" aria-hidden="true" href="#installation-reinstallation-and-uninstallation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation, Reinstallation and Uninstallation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Windows portable version&lt;/strong&gt;: Simply download and use the latest version from the &lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases"&gt;Releases&lt;/a&gt; section.&lt;/p&gt;
&lt;p&gt;Otherwise, check &lt;a href="doc/installation.md"&gt;doc/installation.md&lt;/a&gt; for instructions on how to build OpenPose from source.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;Most users do not need the OpenPose C++/Python API, but can simply use the OpenPose Demo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenPose Demo&lt;/strong&gt;: To easily process images/video/webcam and display/save the results. See &lt;a href="doc/demo_overview.md"&gt;doc/demo_overview.md&lt;/a&gt;. E.g., run OpenPose in a video with:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# Ubuntu
./build/examples/openpose/openpose.bin --video examples/media/video.avi
:: Windows - Portable Demo
bin\OpenPoseDemo.exe --video examples\media\video.avi
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Calibration toolbox&lt;/strong&gt;: To easily calibrate your cameras for 3-D OpenPose or any other stereo vision task. See &lt;a href="doc/modules/calibration_module.md"&gt;doc/modules/calibration_module.md&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenPose C++ API&lt;/strong&gt;: If you want to read a specific input, and/or add your custom post-processing function, and/or implement your own display/saving, check the C++ API tutorial on &lt;a href="examples/tutorial_api_cpp/"&gt;examples/tutorial_api_cpp/&lt;/a&gt; and &lt;a href="doc/library_introduction.md"&gt;doc/library_introduction.md&lt;/a&gt;. You can create your custom code on &lt;a href="examples/user_code/"&gt;examples/user_code/&lt;/a&gt; and quickly compile it with CMake when compiling the whole OpenPose project. Quickly &lt;strong&gt;add your custom code&lt;/strong&gt;: See &lt;a href="examples/user_code/README.md"&gt;examples/user_code/README.md&lt;/a&gt; for further details.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenPose Python API&lt;/strong&gt;: Analogously to the C++ API, find the tutorial for the Python API on &lt;a href="examples/tutorial_api_python/"&gt;examples/tutorial_api_python/&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adding an extra module&lt;/strong&gt;: Check &lt;a href="./doc/library_add_new_module.md"&gt;doc/library_add_new_module.md&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Standalone face or hand detector&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Face&lt;/strong&gt; keypoint detection &lt;strong&gt;without body&lt;/strong&gt; keypoint detection: If you want to speed it up (but also reduce amount of detected faces), check the OpenCV-face-detector approach in &lt;a href="doc/standalone_face_or_hand_keypoint_detector.md"&gt;doc/standalone_face_or_hand_keypoint_detector.md&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use your own face/hand detector&lt;/strong&gt;: You can use the hand and/or face keypoint detectors with your own face or hand detectors, rather than using the body detector. E.g., useful for camera views at which the hands are visible but not the body (OpenPose detector would fail). See &lt;a href="doc/standalone_face_or_hand_keypoint_detector.md"&gt;doc/standalone_face_or_hand_keypoint_detector.md&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-output" class="anchor" aria-hidden="true" href="#output"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Output&lt;/h2&gt;
&lt;p&gt;Output (format, keypoint index ordering, etc.) in &lt;a href="doc/output.md"&gt;doc/output.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speeding-up-openpose-and-benchmark" class="anchor" aria-hidden="true" href="#speeding-up-openpose-and-benchmark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speeding Up OpenPose and Benchmark&lt;/h2&gt;
&lt;p&gt;Check the OpenPose Benchmark as well as some hints to speed up and/or reduce the memory requirements for OpenPose on &lt;a href="doc/speed_up_openpose.md"&gt;doc/speed_up_openpose.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-training-code-and-foot-dataset" class="anchor" aria-hidden="true" href="#training-code-and-foot-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training Code and Foot Dataset&lt;/h2&gt;
&lt;p&gt;For training OpenPose, check &lt;a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"&gt;github.com/CMU-Perceptual-Computing-Lab/openpose_train&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the foot dataset, check the &lt;a href="https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/" rel="nofollow"&gt;foot dataset website&lt;/a&gt; and new &lt;a href="https://arxiv.org/abs/1812.08008" rel="nofollow"&gt;OpenPose paper&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-send-us-failure-cases-and-feedback" class="anchor" aria-hidden="true" href="#send-us-failure-cases-and-feedback"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Send Us Failure Cases and Feedback!&lt;/h2&gt;
&lt;p&gt;Our library is open source for research purposes, and we want to continuously improve it! So please, let us know if...&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;... you find videos or images where OpenPose does not seems to work well. Feel free to send them to &lt;a href="mailto:openposecmu@gmail.com"&gt;openposecmu@gmail.com&lt;/a&gt; (email only for failure cases!), we will use them to improve the quality of the algorithm!&lt;/li&gt;
&lt;li&gt;... you find any bug (in functionality or speed).&lt;/li&gt;
&lt;li&gt;... you added some functionality to some class or some new Worker subclass which we might potentially incorporate.&lt;/li&gt;
&lt;li&gt;... you know how to speed up or improve any part of the library.&lt;/li&gt;
&lt;li&gt;... you have a request about possible functionality.&lt;/li&gt;
&lt;li&gt;... etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Just comment on GitHub or make a pull request and we will answer as soon as possible! Send us an email if you use the library to make a cool demo or YouTube video!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please cite these papers in your publications if it helps your research. The body-foot model and any additional functionality (calibration, 3-D reconstruction, etc.) use &lt;code&gt;[Cao et al. 2018]&lt;/code&gt;; the hand and face keypoint detectors use &lt;code&gt;[Cao et al. 2018]&lt;/code&gt; and &lt;code&gt;[Simon et al. 2017]&lt;/code&gt; (the face detector was trained using the same procedure than for hands); and the old (deprecated) body-only model uses &lt;code&gt;[Cao et al. 2017]&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{cao2018openpose,
  author = {Zhe Cao and Gines Hidalgo and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  booktitle = {arXiv preprint arXiv:1812.08008},
  title = {Open{P}ose: realtime multi-person 2{D} pose estimation using {P}art {A}ffinity {F}ields},
  year = {2018}
}

@inproceedings{simon2017hand,
  author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},
  year = {2017}
}

@inproceedings{cao2017realtime,
  author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year = {2017}
}

@inproceedings{wei2016cpm,
  author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},
  booktitle = {CVPR},
  title = {Convolutional pose machines},
  year = {2016}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Links to the papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1812.08008" rel="nofollow"&gt;OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1704.07809" rel="nofollow"&gt;Hand Keypoint Detection in Single Images using Multiview Bootstrapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1611.08050" rel="nofollow"&gt;Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1602.00134" rel="nofollow"&gt;Convolutional Pose Machines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;OpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the &lt;a href="LICENSE"&gt;license&lt;/a&gt; for further details. Interested in a commercial license? Check this &lt;a href="https://flintbox.com/public/project/47343/" rel="nofollow"&gt;FlintBox link&lt;/a&gt;. For commercial queries, use the &lt;code&gt;Directly Contact Organization&lt;/code&gt; section from the &lt;a href="https://flintbox.com/public/project/47343/" rel="nofollow"&gt;FlintBox link&lt;/a&gt; and also send a copy of that message to &lt;a href="mailto:yaser@cs.cmu.edu"&gt;Yaser Sheikh&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CMU-Perceptual-Computing-Lab</author><guid isPermaLink="false">https://github.com/CMU-Perceptual-Computing-Lab/openpose</guid><pubDate>Sun, 09 Feb 2020 00:08:00 GMT</pubDate></item><item><title>notepad-plus-plus/notepad-plus-plus #9 in C++, This week</title><link>https://github.com/notepad-plus-plus/notepad-plus-plus</link><description>&lt;p&gt;&lt;i&gt;Notepad++ official repository&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-what-is-notepad-" class="anchor" aria-hidden="true" href="#what-is-notepad-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Notepad++ ?&lt;/h1&gt;
&lt;p&gt;&lt;a href="../../releases/latest"&gt;&lt;img src="https://camo.githubusercontent.com/fb15f9cc76c3ccb070d9cdc5ea1e2b39338c2a68/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6e6f74657061642d706c75732d706c75732f6e6f74657061642d706c75732d706c75732e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/github/release/notepad-plus-plus/notepad-plus-plus.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://ci.appveyor.com/project/donho/notepad-plus-plus" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0e1e31c839d423021549460c71d45633c767d5dd/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f6e6f74657061642d706c75732d706c75732f6e6f74657061642d706c75732d706c75733f6272616e63683d6d6173746572267376673d74727565" alt="Appveyor build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/github/notepad-plus-plus/notepad-plus-plus?branch=master&amp;amp;svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://community.notepad-plus-plus.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d16dfd4d6c7a0213e966f6ed52faebbd615febdf/68747470733a2f2f6e6f74657061642d706c75732d706c75732e6f72672f6173736574732f696d616765732f4e7070436f6d6d756e69747942616467652e737667" alt="Join the disscussions at https://community.notepad-plus-plus.org/" data-canonical-src="https://notepad-plus-plus.org/assets/images/NppCommunityBadge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://gitter.im/notepad-plus-plus/notepad-plus-plus?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/notepad-plus-plus/notepad-plus-plus" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notepad++ is a free (free as in both "free speech" and "free beer") source code
editor and Notepad replacement that supports several programming languages and
natural languages. Running in the MS Windows environment, its use is governed by
&lt;a href="LICENSE"&gt;GPL License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="https://notepad-plus-plus.org/" rel="nofollow"&gt;Notepad++ official site&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-notepad-release-key" class="anchor" aria-hidden="true" href="#notepad-release-key"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notepad++ Release Key&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Since the release of version 7.6.5 Notepad++ is signed using GPG with the following key:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Signer:&lt;/strong&gt; Notepad++&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;E-mail:&lt;/strong&gt; &lt;a href="mailto:don.h@free.fr"&gt;don.h@free.fr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key ID:&lt;/strong&gt; 0x8D84F46E&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key fingerprint:&lt;/strong&gt; 14BC E436 2749 B2B5 1F8C 7122 6C42 9F1D 8D84 F46E&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key type:&lt;/strong&gt; RSA 4096/4096&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Created:&lt;/strong&gt; 2019-03-11&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expiries:&lt;/strong&gt; 2021-03-10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/notepad-plus-plus/notepad-plus-plus/blob/master/nppGpgPub.asc"&gt;https://github.com/notepad-plus-plus/notepad-plus-plus/blob/master/nppGpgPub.asc&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-build-notepad" class="anchor" aria-hidden="true" href="#build-notepad"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build Notepad++&lt;/h2&gt;
&lt;p&gt;Please follow &lt;a href="BUILD.md"&gt;build guide&lt;/a&gt; to build Notepad++ from source.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;Code contribution is welcome. Here are some &lt;a href="CONTRIBUTING.md"&gt;rules&lt;/a&gt; that your should follow to make your contribution accepted easily.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/notepad-plus-plus/notepad-plus-plus/graphs/contributors"&gt;Notepad++ Contributors&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>notepad-plus-plus</author><guid isPermaLink="false">https://github.com/notepad-plus-plus/notepad-plus-plus</guid><pubDate>Sun, 09 Feb 2020 00:09:00 GMT</pubDate></item><item><title>pytorch/pytorch #10 in C++, This week</title><link>https://github.com/pytorch/pytorch</link><description>&lt;p&gt;&lt;i&gt;Tensors and Dynamic neural networks in Python with strong GPU acceleration&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/pytorch/blob/master/docs/source/_static/img/pytorch-logo-dark.png"&gt;&lt;img src="https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/pytorch-logo-dark.png" alt="PyTorch Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;PyTorch is a Python package that provides two high-level features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tensor computation (like NumPy) with strong GPU acceleration&lt;/li&gt;
&lt;li&gt;Deep neural networks built on a tape-based autograd system&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#more-about-pytorch"&gt;More about PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#binaries"&gt;Binaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#from-source"&gt;From Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker-image"&gt;Docker Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#building-the-documentation"&gt;Building the Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#previous-versions"&gt;Previous Versions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#communication"&gt;Communication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#releases-and-contributing"&gt;Releases and Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-team"&gt;The Team&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;System&lt;/th&gt;
&lt;th align="center"&gt;2.7&lt;/th&gt;
&lt;th align="center"&gt;3.5&lt;/th&gt;
&lt;th align="center"&gt;3.6&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Linux CPU&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://ci.pytorch.org/jenkins/job/pytorch-master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3c79e1fb9ca53dc2ce86114435a912f8f43ab4ef/68747470733a2f2f63692e7079746f7263682e6f72672f6a656e6b696e732f6a6f622f7079746f7263682d6d61737465722f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://ci.pytorch.org/jenkins/job/pytorch-master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3c79e1fb9ca53dc2ce86114435a912f8f43ab4ef/68747470733a2f2f63692e7079746f7263682e6f72672f6a656e6b696e732f6a6f622f7079746f7263682d6d61737465722f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Linux GPU&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://ci.pytorch.org/jenkins/job/pytorch-master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3c79e1fb9ca53dc2ce86114435a912f8f43ab4ef/68747470733a2f2f63692e7079746f7263682e6f72672f6a656e6b696e732f6a6f622f7079746f7263682d6d61737465722f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://ci.pytorch.org/jenkins/job/pytorch-master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3c79e1fb9ca53dc2ce86114435a912f8f43ab4ef/68747470733a2f2f63692e7079746f7263682e6f72672f6a656e6b696e732f6a6f622f7079746f7263682d6d61737465722f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://ci.pytorch.org/jenkins/job/pytorch-master/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Windows CPU / GPU&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-trigger/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6c61691bfa9e7595127af4a153e2bf61445f9a46/68747470733a2f2f63692e7079746f7263682e6f72672f6a656e6b696e732f6a6f622f7079746f7263682d6275696c64732f6a6f622f7079746f7263682d77696e2d7773323031362d63756461392d6375646e6e372d7079332d747269676765722f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-trigger/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Linux (ppc64le) CPU&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e810d96a781734cec199da617c69fb7acf77ee6a/68747470733a2f2f706f77657263692e6f73756f736c2e6f72672f6a6f622f7079746f7263682d6d61737465722d6e696768746c792d7079332d6c696e75782d70706336346c652f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://powerci.osuosl.org/job/pytorch-master-nightly-py3-linux-ppc64le/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Linux (ppc64le) GPU&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;td align="center"&gt;—&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://powerci.osuosl.org/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f29f86236868fb605decfb7558cf66843eb396cf/68747470733a2f2f706f77657263692e6f73756f736c2e6f72672f6a6f622f7079746f7263682d6c696e75782d6375646139322d6375646e6e372d7079332d6d70692d6275696c642d746573742d6770752f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://powerci.osuosl.org/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;See also the &lt;a href="https://ezyang.github.io/pytorch-ci-hud/build/pytorch-master" rel="nofollow"&gt;ci.pytorch.org HUD&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-more-about-pytorch" class="anchor" aria-hidden="true" href="#more-about-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More About PyTorch&lt;/h2&gt;
&lt;p&gt;At a granular level, PyTorch is a library that consists of the following components:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://pytorch.org/docs/stable/torch.html" rel="nofollow"&gt;&lt;strong&gt;torch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;a Tensor library like NumPy, with strong GPU support&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://pytorch.org/docs/stable/autograd.html" rel="nofollow"&gt;&lt;strong&gt;torch.autograd&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://pytorch.org/docs/stable/jit.html" rel="nofollow"&gt;&lt;strong&gt;torch.jit&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html" rel="nofollow"&gt;&lt;strong&gt;torch.nn&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;a neural networks library deeply integrated with autograd designed for maximum flexibility&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://pytorch.org/docs/stable/multiprocessing.html" rel="nofollow"&gt;&lt;strong&gt;torch.multiprocessing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://pytorch.org/docs/stable/data.html" rel="nofollow"&gt;&lt;strong&gt;torch.utils&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;DataLoader and other utility functions for convenience&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Usually PyTorch is used either as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a replacement for NumPy to use the power of GPUs.&lt;/li&gt;
&lt;li&gt;a deep learning research platform that provides maximum flexibility and speed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Elaborating further:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-a-gpu-ready-tensor-library" class="anchor" aria-hidden="true" href="#a-gpu-ready-tensor-library"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A GPU-Ready Tensor Library&lt;/h3&gt;
&lt;p&gt;If you use NumPy, then you have used Tensors (a.k.a. ndarray).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./docs/source/_static/img/tensor_illustration.png"&gt;&lt;img src="./docs/source/_static/img/tensor_illustration.png" alt="Tensor illustration" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerates the
computation by a huge amount.&lt;/p&gt;
&lt;p&gt;We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, math operations, linear algebra, reductions.
And they are fast!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-dynamic-neural-networks-tape-based-autograd" class="anchor" aria-hidden="true" href="#dynamic-neural-networks-tape-based-autograd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamic Neural Networks: Tape-Based Autograd&lt;/h3&gt;
&lt;p&gt;PyTorch has a unique way of building neural networks: using and replaying a tape recorder.&lt;/p&gt;
&lt;p&gt;Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world.
One has to build a neural network, and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.&lt;/p&gt;
&lt;p&gt;With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
&lt;a href="https://github.com/twitter/torch-autograd"&gt;torch-autograd&lt;/a&gt;,
&lt;a href="https://github.com/HIPS/autograd"&gt;autograd&lt;/a&gt;,
&lt;a href="https://chainer.org" rel="nofollow"&gt;Chainer&lt;/a&gt;, etc.&lt;/p&gt;
&lt;p&gt;While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/pytorch/blob/master/docs/source/_static/img/dynamic_graph.gif"&gt;&lt;img src="https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif" alt="Dynamic graph" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-first" class="anchor" aria-hidden="true" href="#python-first"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python First&lt;/h3&gt;
&lt;p&gt;PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt; / &lt;a href="https://www.scipy.org/" rel="nofollow"&gt;SciPy&lt;/a&gt; / &lt;a href="https://scikit-learn.org" rel="nofollow"&gt;scikit-learn&lt;/a&gt; etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as Cython and Numba.
Our goal is to not reinvent the wheel where appropriate.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-imperative-experiences" class="anchor" aria-hidden="true" href="#imperative-experiences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Imperative Experiences&lt;/h3&gt;
&lt;p&gt;PyTorch is designed to be intuitive, linear in thought and easy to use.
When you execute a line of code, it gets executed. There isn't an asynchronous view of the world.
When you drop into a debugger, or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fast-and-lean" class="anchor" aria-hidden="true" href="#fast-and-lean"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fast and Lean&lt;/h3&gt;
&lt;p&gt;PyTorch has minimal framework overhead. We integrate acceleration libraries
such as &lt;a href="https://software.intel.com/mkl" rel="nofollow"&gt;Intel MKL&lt;/a&gt; and NVIDIA (cuDNN, NCCL) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
(TH, THC, THNN, THCUNN) are mature and have been tested for years.&lt;/p&gt;
&lt;p&gt;Hence, PyTorch is quite fast – whether you run small or large neural networks.&lt;/p&gt;
&lt;p&gt;The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We've written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-extensions-without-pain" class="anchor" aria-hidden="true" href="#extensions-without-pain"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extensions Without Pain&lt;/h3&gt;
&lt;p&gt;Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward
and with minimal abstractions.&lt;/p&gt;
&lt;p&gt;You can write new neural network layers in Python using the torch API
&lt;a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" rel="nofollow"&gt;or your favorite NumPy-based libraries such as SciPy&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
No wrapper code needs to be written. You can see &lt;a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" rel="nofollow"&gt;a tutorial here&lt;/a&gt; and &lt;a href="https://github.com/pytorch/extension-cpp"&gt;an example here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-binaries" class="anchor" aria-hidden="true" href="#binaries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Binaries&lt;/h3&gt;
&lt;p&gt;Commands to install from binaries via Conda or pip wheels are on our website:
&lt;a href="https://pytorch.org" rel="nofollow"&gt;https://pytorch.org&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-nvidia-jetson-platforms" class="anchor" aria-hidden="true" href="#nvidia-jetson-platforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA Jetson platforms&lt;/h4&gt;
&lt;p&gt;Python wheels for NVIDIA's Jetson Nano, Jetson TX2, and Jetson AGX Xavier are available via the following URLs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stable binaries:
&lt;ul&gt;
&lt;li&gt;Python 2.7: &lt;a href="https://nvidia.box.com/v/torch-stable-cp27-jetson-jp42" rel="nofollow"&gt;https://nvidia.box.com/v/torch-stable-cp27-jetson-jp42&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python 3.6: &lt;a href="https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42" rel="nofollow"&gt;https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rolling weekly binaries:
&lt;ul&gt;
&lt;li&gt;Python 2.7: &lt;a href="https://nvidia.box.com/v/torch-weekly-cp27-jetson-jp42" rel="nofollow"&gt;https://nvidia.box.com/v/torch-weekly-cp27-jetson-jp42&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python 3.6: &lt;a href="https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42" rel="nofollow"&gt;https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They require JetPack 4.2 and above, and @dusty-nv maintains them&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-from-source" class="anchor" aria-hidden="true" href="#from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From Source&lt;/h3&gt;
&lt;p&gt;If you are installing from source, you will need a C++14 compiler. Also, we highly recommend installing an &lt;a href="https://www.anaconda.com/distribution/#download-section" rel="nofollow"&gt;Anaconda&lt;/a&gt; environment.
You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.&lt;/p&gt;
&lt;p&gt;Once you have &lt;a href="https://www.anaconda.com/distribution/#download-section" rel="nofollow"&gt;Anaconda&lt;/a&gt; installed, here are the instructions.&lt;/p&gt;
&lt;p&gt;If you want to compile with CUDA support, install&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow"&gt;NVIDIA CUDA&lt;/a&gt; 9 or above&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/cudnn" rel="nofollow"&gt;NVIDIA cuDNN&lt;/a&gt; v7 or above&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to disable CUDA support, export environment variable &lt;code&gt;USE_CUDA=0&lt;/code&gt;.
Other potentially useful environment variables may be found in &lt;code&gt;setup.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to &lt;a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" rel="nofollow"&gt;are available here&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-install-dependencies" class="anchor" aria-hidden="true" href="#install-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install Dependencies&lt;/h4&gt;
&lt;p&gt;Common (only install &lt;code&gt;typing&lt;/code&gt; for Python &amp;lt;3.5)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Linux&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Add LAPACK support for the GPU if needed&lt;/span&gt;
conda install -c pytorch magma-cuda90 &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or [magma-cuda92 | magma-cuda100 | magma-cuda101 ] depending on your cuda version&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-get-the-pytorch-source" class="anchor" aria-hidden="true" href="#get-the-pytorch-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get the PyTorch Source&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone --recursive https://github.com/pytorch/pytorch
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; pytorch
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; if you are updating an existing checkout&lt;/span&gt;
git submodule sync
git submodule update --init --recursive&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-install-pytorch" class="anchor" aria-hidden="true" href="#install-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install PyTorch&lt;/h4&gt;
&lt;p&gt;On Linux&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; CMAKE_PREFIX_PATH=&lt;span class="pl-smi"&gt;${CONDA_PREFIX&lt;span class="pl-k"&gt;:-&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;dirname &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;which conda&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/../&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;}&lt;/span&gt;
python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On macOS&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; CMAKE_PREFIX_PATH=&lt;span class="pl-smi"&gt;${CONDA_PREFIX&lt;span class="pl-k"&gt;:-&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;dirname &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;which conda&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/../&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;}&lt;/span&gt;
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Each CUDA version only supports one particular XCode version. The following combinations have been reported to work with PyTorch.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CUDA version&lt;/th&gt;
&lt;th&gt;XCode version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10.0&lt;/td&gt;
&lt;td&gt;XCode 9.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.1&lt;/td&gt;
&lt;td&gt;XCode 10.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On Windows&lt;/p&gt;
&lt;p&gt;At least Visual Studio 2017 Update 3 (version 15.3.3 with the toolset 14.11) and &lt;a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" rel="nofollow"&gt;NVTX&lt;/a&gt; are needed.&lt;/p&gt;
&lt;p&gt;If the version of Visual Studio 2017 is higher than 15.4.5, installing of "VC++ 2017 version 15.4 v14.11 toolset" is strongly recommended.
&lt;br&gt; If the version of Visual Studio 2017 is lesser than 15.3.3, please update Visual Studio 2017 to the latest version along with installing "VC++ 2017 version 15.4 v14.11 toolset".
&lt;br&gt; There is no guarantee of the correct building with VC++ 2017 toolsets, others than version 15.4 v14.11.
&lt;br&gt; "VC++ 2017 version 15.4 v14.11 toolset" might be installed onto already installed Visual Studio 2017 by running its installation once again and checking the corresponding checkbox under "Individual components"/"Compilers, build tools, and runtimes".&lt;/p&gt;
&lt;p&gt;NVTX is a part of CUDA distributive, where it is called "Nsight Compute". To install it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Be sure that CUDA with Nsight Compute is installed after Visual Studio 2017.&lt;/p&gt;
&lt;p&gt;Currently VS 2017, VS 2019 and Ninja are supported as the generator of CMake. If &lt;code&gt;ninja.exe&lt;/code&gt; is detected in &lt;code&gt;PATH&lt;/code&gt;, then Ninja will be used as the default generator, otherwise it will use VS 2017.
&lt;br&gt; If Ninja is selected as the generator, the latest MSVC which is newer than VS 2015 (14.0) will get selected as the underlying toolchain if you have Python &amp;gt; 3.5, otherwise VS 2015 will be selected so you'll have to activate the environment. If you use CMake &amp;lt;= 3.14.2 and has VS 2019 installed, then even if you specify VS 2017 as the generator, VS 2019 will get selected as the generator.&lt;/p&gt;
&lt;p&gt;CUDA and MSVC have strong version dependencies, so even if you use VS 2017 / 2019, you will get build errors like &lt;code&gt;nvcc fatal : Host compiler targets unsupported OS&lt;/code&gt;. For this kind of problem, please install the corresponding VS toolchain in the table below and then you can either specify the toolset during activation (recommended) or set &lt;code&gt;CUDAHOSTCXX&lt;/code&gt; to override the cuda host compiler (not recommended if there are big version differences).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CUDA version&lt;/th&gt;
&lt;th&gt;Newest supported VS version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;9.0 / 9.1&lt;/td&gt;
&lt;td&gt;Visual Studio 2017 Update 4 (15.4) (&lt;code&gt;_MSC_VER&lt;/code&gt; &amp;lt;= 1911)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9.2&lt;/td&gt;
&lt;td&gt;Visual Studio 2017 Update 5 (15.5) (&lt;code&gt;_MSC_VER&lt;/code&gt; &amp;lt;= 1912)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.0&lt;/td&gt;
&lt;td&gt;Visual Studio 2017 (15.X) (&lt;code&gt;_MSC_VER&lt;/code&gt; &amp;lt; 1920)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.1&lt;/td&gt;
&lt;td&gt;Visual Studio 2019 (16.X) (&lt;code&gt;_MSC_VER&lt;/code&gt; &amp;lt; 1930)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight highlight-source-batchfile"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;cmd&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; [Optional] Only add the next two lines if you need Python 2.7. If you use Python 3, ignore these two lines.&lt;/span&gt;
&lt;span class="pl-k"&gt;set&lt;/span&gt; &lt;span class="pl-smi"&gt;MSSdk&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;
&lt;span class="pl-k"&gt;set&lt;/span&gt; &lt;span class="pl-smi"&gt;FORCE_PY27_BUILD&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; [Optional] If you want to build with VS 2019 generator, please change the value in the next line to `Visual Studio 16 2019`.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; Note: This value is useless if Ninja is detected. However, you can force that by using `set USE_NINJA=OFF`.&lt;/span&gt;
&lt;span class="pl-k"&gt;set&lt;/span&gt; &lt;span class="pl-smi"&gt;CMAKE_GENERATOR&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;Visual Studio &lt;span class="pl-c1"&gt;15&lt;/span&gt; &lt;span class="pl-c1"&gt;2017&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; Read the content in the previous section carefully before you proceed.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; "Visual Studio 2017 Developer Command Prompt" will be run automatically.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; Make sure you have CMake &amp;gt;= 3.12 before you do this when you use the Visual Studio generator.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; It's an essential step if you use Python 3.5.&lt;/span&gt;
&lt;span class="pl-k"&gt;set&lt;/span&gt; &lt;span class="pl-smi"&gt;CMAKE_GENERATOR_TOOLSET_VERSION&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;14.11
&lt;span class="pl-k"&gt;set&lt;/span&gt; &lt;span class="pl-smi"&gt;DISTUTILS_USE_SDK&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; /f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;usebackq tokens=*&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-smi"&gt;%i in (`"%&lt;/span&gt;ProgramFiles(x86)&lt;span class="pl-smi"&gt;%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,16^) -products * -latest -property installationPath`) do call "%&lt;/span&gt;i\VC\Auxiliary\Build\vcvarsall.bat&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt; x64 -vcvars_ver=&lt;span class="pl-smi"&gt;%CMAKE_GENERATOR_TOOLSET_VERSION%&lt;/span&gt;&lt;span class="pl-ii"&gt;&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;::&lt;/span&gt; [Optional] If you want to override the cuda host compiler&lt;/span&gt;
&lt;span class="pl-k"&gt;set&lt;/span&gt; &lt;span class="pl-smi"&gt;CUDAHOSTCXX&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.11.25503\bin\HostX64\x64\cl.exe

python setup.py install
&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-adjust-build-options-optional" class="anchor" aria-hidden="true" href="#adjust-build-options-optional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adjust Build Options (Optional)&lt;/h5&gt;
&lt;p&gt;You can adjust the configuration of cmake variables optionally (without building first), by doing
the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done
with such a step.&lt;/p&gt;
&lt;p&gt;On Linux&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; CMAKE_PREFIX_PATH=&lt;span class="pl-smi"&gt;${CONDA_PREFIX&lt;span class="pl-k"&gt;:-&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;dirname &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;which conda&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/../&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;}&lt;/span&gt;
python setup.py build --cmake-only
ccmake build  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or cmake-gui build&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On macOS&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; CMAKE_PREFIX_PATH=&lt;span class="pl-smi"&gt;${CONDA_PREFIX&lt;span class="pl-k"&gt;:-&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;dirname &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;which conda&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;/../&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;}&lt;/span&gt;
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or cmake-gui build&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-docker-image" class="anchor" aria-hidden="true" href="#docker-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker Image&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-using-pre-built-images" class="anchor" aria-hidden="true" href="#using-pre-built-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using pre-built images&lt;/h4&gt;
&lt;p&gt;You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-building-the-image-yourself" class="anchor" aria-hidden="true" href="#building-the-image-yourself"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building the image yourself&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Must be built with a docker version &amp;gt; 18.06&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; is supplied to build images with cuda support and cudnn v7.
You can pass &lt;code&gt;PYTHON_VERSION=x.y&lt;/code&gt; make variable to specify which Python version is to be used by Miniconda, or leave it
unset to use the default.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make -f docker.Makefile
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; images are tagged as docker.io/${your_docker_username}/pytorch&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-building-the-documentation" class="anchor" aria-hidden="true" href="#building-the-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building the Documentation&lt;/h3&gt;
&lt;p&gt;To build documentation in various formats, you will need &lt;a href="http://www.sphinx-doc.org" rel="nofollow"&gt;Sphinx&lt;/a&gt; and the
readthedocs theme.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd docs/
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then build the documentation by running &lt;code&gt;make &amp;lt;format&amp;gt;&lt;/code&gt; from the
&lt;code&gt;docs/&lt;/code&gt; folder. Run &lt;code&gt;make&lt;/code&gt; to get a list of all available output formats.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-previous-versions" class="anchor" aria-hidden="true" href="#previous-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Previous Versions&lt;/h3&gt;
&lt;p&gt;Installation instructions and binaries for previous PyTorch versions may be found
on &lt;a href="https://pytorch.org/previous-versions" rel="nofollow"&gt;our website&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;Three pointers to get you started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/" rel="nofollow"&gt;Tutorials: get you started with understanding and using PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pytorch/examples"&gt;Examples: easy to understand pytorch code across all domains&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/" rel="nofollow"&gt;The API Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;forums: discuss implementations, research, etc. &lt;a href="https://discuss.pytorch.org" rel="nofollow"&gt;https://discuss.pytorch.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc.&lt;/li&gt;
&lt;li&gt;Slack: The &lt;a href="https://pytorch.slack.com/" rel="nofollow"&gt;PyTorch Slack&lt;/a&gt; hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration etc. If you are a beginner looking for help, the primary medium is &lt;a href="https://discuss.pytorch.org" rel="nofollow"&gt;PyTorch Forums&lt;/a&gt;. If you need a slack invite, please fill this form: &lt;a href="https://goo.gl/forms/PP1AGvNHpSaJP8to1" rel="nofollow"&gt;https://goo.gl/forms/PP1AGvNHpSaJP8to1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;newsletter: no-noise, one-way email newsletter with important announcements about pytorch. You can sign-up here: &lt;a href="https://eepurl.com/cbG0rv" rel="nofollow"&gt;https://eepurl.com/cbG0rv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-releases-and-contributing" class="anchor" aria-hidden="true" href="#releases-and-contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Releases and Contributing&lt;/h2&gt;
&lt;p&gt;PyTorch has a 90 day release cycle (major releases). Please let us know if you encounter a bug by &lt;a href="https://github.com/pytorch/pytorch/issues"&gt;filing an issue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.&lt;/p&gt;
&lt;p&gt;If you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us.
Sending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-team" class="anchor" aria-hidden="true" href="#the-team"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Team&lt;/h2&gt;
&lt;p&gt;PyTorch is a community driven project with several skillful engineers and researchers contributing to it.&lt;/p&gt;
&lt;p&gt;PyTorch is currently maintained by &lt;a href="https://apaszke.github.io/" rel="nofollow"&gt;Adam Paszke&lt;/a&gt;, &lt;a href="https://github.com/colesbury"&gt;Sam Gross&lt;/a&gt;, &lt;a href="http://soumith.ch" rel="nofollow"&gt;Soumith Chintala&lt;/a&gt; and &lt;a href="https://github.com/gchanan"&gt;Gregory Chanan&lt;/a&gt; with major contributions coming from hundreds of talented individuals in various forms and means.
A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.&lt;/p&gt;
&lt;p&gt;Note: this project is unrelated to &lt;a href="https://github.com/hughperkins/pytorch"&gt;hughperkins/pytorch&lt;/a&gt; with the same name. Hugh is a valuable contributor in the Torch community and has helped with many things Torch and PyTorch.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;PyTorch is BSD-style licensed, as found in the LICENSE file.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/pytorch</guid><pubDate>Sun, 09 Feb 2020 00:10:00 GMT</pubDate></item><item><title>wszqkzqk/deepin-wine-ubuntu #11 in C++, This week</title><link>https://github.com/wszqkzqk/deepin-wine-ubuntu</link><description>&lt;p&gt;&lt;i&gt;Deepin Wine for Ubuntu/Debian&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deepin-wine-for-ubuntu-and-debian" class="anchor" aria-hidden="true" href="#deepin-wine-for-ubuntu-and-debian"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deepin wine for Ubuntu and Debian&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-一项目介绍" class="anchor" aria-hidden="true" href="#一项目介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;一、项目介绍&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Deepin-wine 环境的 Ubuntu/Debian 移植版&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;使用deepin原版二进制文件，解决依赖问题&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;仅供个人研究学习使用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;刚刚适配debian，可能在安装或运行上还存在问题,欢迎反馈！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;声明：
因为使用本仓库的任何内容所导致的任何后果与本人无关，若你无法对使用该仓库后的任何后果负责，请不要使用本仓库的任何内容。
本仓库所有者不拥有该仓库任何二进制文件的版权，所有由本人编写的非二进制文件以GPL开源协议开源。若该仓库的文件侵犯了您的法律权益，请联系&lt;a href="mailto:1726509538@qq.com"&gt;1726509538@qq.com&lt;/a&gt;，我会删除侵权内容并道歉。
该项目得以实现的全部功劳来自于深度操作系统开发人员的辛勤努力，本人只是将其成果适配到Ubuntu/Debian平台以让这一伟大成果能为更多人所共享，本人对深度操作系统的开发人员致以崇高的敬意。
2.18-19适配计划可能会推迟（因暂时无必要性，实际上新版软件改依赖即可安装，不太影响使用。）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-二软件架构" class="anchor" aria-hidden="true" href="#二软件架构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;二、软件架构&lt;/h2&gt;
&lt;p&gt;软件架构说明&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-1安装教程" class="anchor" aria-hidden="true" href="#1安装教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（1）安装教程&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-在线安装目前尚有bug" class="anchor" aria-hidden="true" href="#在线安装目前尚有bug"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;在线安装(目前尚有bug)&lt;/h4&gt;
&lt;p&gt;直接使用在线安装脚本，安装最新的Release版本:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;wget -qO- https://raw.githubusercontent.com/wszqkzqk/deepin-wine-ubuntu/master/online_install.sh &lt;span class="pl-k"&gt;|&lt;/span&gt; bash -e&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-本地安装ubuntudebian通用" class="anchor" aria-hidden="true" href="#本地安装ubuntudebian通用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;本地安装(Ubuntu/Debian通用)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;克隆 (&lt;code&gt;git clone https://github.com/wszqkzqk/deepin-wine-ubuntu.git&lt;/code&gt;) 或&lt;a href="https://github.com/wszqkzqk/deepin-wine-ubuntu/archive/master.zip"&gt;下载&lt;/a&gt;到本地。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在中国推荐用下面的地址，速度更快： (&lt;code&gt;git clone https://gitee.com/wszqkzqk/deepin-wine-for-ubuntu.git&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当然也可以选择下载releases：&lt;a href="https://github.com/wszqkzqk/deepin-wine-ubuntu/releases"&gt;Github&lt;/a&gt; 或者
&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-for-ubuntu/releases" rel="nofollow"&gt;Gitee&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压后切换到解压文件目录，在终端中运行（授予可执行权限后）： &lt;code&gt;./install.sh&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;KDE或其他按照普通安装方式安装后运行出现X错误的桌面环境执行 &lt;code&gt;./KDE-install.sh&lt;/code&gt;  ）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-2使用说明" class="anchor" aria-hidden="true" href="#2使用说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（2）使用说明&lt;/h3&gt;
&lt;p&gt;下载并安装所需要的deepin-wine容器 &lt;em&gt;（建议在终端下使用dpkg -i安装容器，否则容易误报依赖错误）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;可使用deepin发布的最新版容器安装包：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.qq.im/" rel="nofollow"&gt;QQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.qq.office/" rel="nofollow"&gt;TIM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.qq.im.light/" rel="nofollow"&gt;QQ轻聊版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.wechat/" rel="nofollow"&gt;微信&lt;/a&gt; 如果出现依赖错误，请下载&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.wechat_2.6.8.65deepin0_i386.deb" rel="nofollow"&gt;这个版本&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.foxmail/" rel="nofollow"&gt;Foxmail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.baidu.pan/" rel="nofollow"&gt;百度网盘&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.cn.360.yasuo/" rel="nofollow"&gt;360压缩&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.cn.com.winrar/" rel="nofollow"&gt;WinRAR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.thunderspeed/" rel="nofollow"&gt;迅雷极速版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.taobao.aliclient.qianniu/" rel="nofollow"&gt;千牛卖家工作台&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其它deepin-wine容器：&lt;a href="https://mirrors.aliyun.com/deepin/pool/non-free/d/" rel="nofollow"&gt;阿里云镜像下载&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-若版本不兼容可选择下载安装以下旧版包文件" class="anchor" aria-hidden="true" href="#若版本不兼容可选择下载安装以下旧版包文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;若版本不兼容，可选择下载安装以下旧版包文件：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.qq.im_9.1.8deepin0_i386.deb" rel="nofollow"&gt;QQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.qq.office_2.0.0deepin4_i386.deb" rel="nofollow"&gt;TIM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.qq.im.light_7.9.14308deepin8_i386.deb" rel="nofollow"&gt;QQ轻聊版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.wechat_2.6.8.65deepin0_i386.deb" rel="nofollow"&gt;微信&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.foxmail_7.2deepin3_i386.deb" rel="nofollow"&gt;Foxmail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.baidu.pan_5.7.3deepin0_i386.deb" rel="nofollow"&gt;百度网盘&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.cn.360.yasuo_4.0.0.1060deepin3_i386.deb" rel="nofollow"&gt;360压缩&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.cn.com.winrar_5.3.0deepin2_i386.deb" rel="nofollow"&gt;WinRAR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu/raw/master/deepin.com.thunderspeed_7.10.35.366deepin18_i386.deb" rel="nofollow"&gt;迅雷极速版&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意：&lt;strong&gt;1.目前基本适配最新版KDE，但测试可能不足(已测试可用：最新版KDE neon、Kubuntu 18.04/18.0.1/18.0.2)，欢迎大家测试反馈（建议将KDE升级到最新版）&lt;/strong&gt;
&lt;strong&gt;2.部分桌面环境无法正常使用视频通话功能！&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-3关于托盘" class="anchor" aria-hidden="true" href="#3关于托盘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（3）关于托盘&lt;/h3&gt;
&lt;p&gt;Ubuntu 18.04 下（Gnome 桌面）：&lt;/p&gt;
&lt;p&gt;安装 Gnome Shell 插件：&lt;a href="https://extensions.gnome.org/extension/1031/topicons/" rel="nofollow"&gt;TopIcons Plus&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-4tim-的可选操作--flash-的安装方法" class="anchor" aria-hidden="true" href="#4tim-的可选操作--flash-的安装方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（4）TIM 的可选操作--Flash 的安装方法&lt;/h3&gt;
&lt;p&gt;*本操作无必要性&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载需要的 install_flash_player_ppapi.exe&lt;/li&gt;
&lt;li&gt;将下载的安装文件放入 ~/.deepinwine/Deepin-TIM/drive_c 下，即 TIM 所在 Wine C 盘根目录&lt;/li&gt;
&lt;li&gt;打开一个 Terminal ，执行：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;WINEPREFIX=~/.deepinwine/Deepin-TIM deepin-wine "c:\\install_flash_player_ppapi.exe"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;然后按提示进行安装、重启 TIM 即可。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-5手动更改配置winecfg" class="anchor" aria-hidden="true" href="#5手动更改配置winecfg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（5）手动更改配置（winecfg）&lt;/h3&gt;
&lt;p&gt;执行 &lt;code&gt;WINEPREFIX=~/.deepinwine/容器名称 deepin-wine winecfg&lt;/code&gt; 即可，也可以用此方法来调整缩放问题&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-6解决系统非中文语言环境时软件无法设置为中文" class="anchor" aria-hidden="true" href="#6解决系统非中文语言环境时软件无法设置为中文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（6）解决系统非中文语言环境时软件无法设置为中文&lt;/h3&gt;
&lt;p&gt;在/opt/deepinwine/tools/run.sh 中将 WINE_CMD 那一行修改为 WINE_CMD="LC_ALL=zh_CN.UTF-8 deepin-wine"&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-7卸载方法" class="anchor" aria-hidden="true" href="#7卸载方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（7）卸载方法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;执行uninstall.sh即可&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-8微信更新问题" class="anchor" aria-hidden="true" href="#8微信更新问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（8）微信更新问题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;如果出现微信提示跟新问题执行这一条语句即可&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt; wget -qO- https://deepin-wine.i-m.dev/setup.sh | sudo sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-9最近问题" class="anchor" aria-hidden="true" href="#9最近问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（9）最近问题&lt;/h3&gt;
&lt;p&gt;部分环境出现deepin最新容器无法安装的问题，主要是因为这套软件包为了保证兼容性，保持了较旧的软件包版本，使得某些deepin新打包的容器无法正常获得依赖关系，可以到&lt;a href="https://gitee.com/wszqkzqk/deepin-wine-containers-for-ubuntu" rel="nofollow"&gt;这里下载兼容包&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;另外，希望大家遇到问题时，先检测自己的deepin-wine环境是否升级到了最新版。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-10wine-应用程序全局快捷键无效的解决方案" class="anchor" aria-hidden="true" href="#10wine-应用程序全局快捷键无效的解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;（10）&lt;a href="https://blog.diqigan.cn/posts/wine-global-hotkey-problem.html" rel="nofollow"&gt;wine 应用程序全局快捷键无效的解决方案&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-1-安装-xdotool" class="anchor" aria-hidden="true" href="#1-安装-xdotool"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. 安装 xdotool&lt;/h4&gt;
&lt;p&gt;直接在命令行运行以下命令即可:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo apt install --no-install-recommends xdotool&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-2-编写-xdotool-脚本" class="anchor" aria-hidden="true" href="#2-编写-xdotool-脚本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. 编写 xdotool 脚本&lt;/h4&gt;
&lt;p&gt;*思路: Wine 应用在后台无法接收到快捷键状态, 此时借助 xdotool 向 Wine 应用发送模拟按键信息即可. *&lt;/p&gt;
&lt;p&gt;在合适的位置新建一个脚本文件 "open_wechat.sh", 写入以下内容:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#!&lt;/span&gt;/bin/sh&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;在当前运行的应用中找到名为WeChat.exe的应用程序，并向它发送按键事件"ctrl+alt+W"&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;WeChat的可执行文件名为WeChat.exe，如果是其它应用程序就修改成其它应用程序的可执行文件名, 应用名称大小写敏感, 一个字母都不能错!&lt;/span&gt;
xdotool key --window &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;xdotool search --limit 1 --all --pid &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;pgrep WeChat.exe&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;ctrl+alt+W&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;赋予脚本可执行权限:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;chmod +x open_wechat.sh&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果此时你的微信正好运行在后台, 执行这个脚本就可以把它召唤到前台. 如果没有, 请检查脚本是否有错误.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-3-设置快捷键" class="anchor" aria-hidden="true" href="#3-设置快捷键"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. 设置快捷键&lt;/h4&gt;
&lt;p&gt;图形界面依次打开 "设置" -&amp;gt; "设备" -&amp;gt; "键盘", 点击列表最底部的 "+" 号添加自定义快捷键.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/49c287ecaca6578b73e5f7cc184c0e35aa6a845c/68747470733a2f2f692e6c6f6c692e6e65742f323031392f31312f30392f624645634c715839667034754a656c2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/49c287ecaca6578b73e5f7cc184c0e35aa6a845c/68747470733a2f2f692e6c6f6c692e6e65742f323031392f31312f30392f624645634c715839667034754a656c2e706e67" alt="快捷键设置" data-canonical-src="https://i.loli.net/2019/11/09/bFEcLqX9fp4uJel.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;名称随便, 填写 "打开微信" 即可;&lt;/li&gt;
&lt;li&gt;命令填写刚才编写的脚本的&lt;strong&gt;全路径&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;快捷键设置自己想用的快捷键即可, 建议于应用内部快捷键相同;&lt;/li&gt;
&lt;li&gt;最后点击"添加"即可.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-4-验证" class="anchor" aria-hidden="true" href="#4-验证"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. 验证&lt;/h4&gt;
&lt;p&gt;到这里已经设置成功了, 打开微信, 切换到后台, 然后按下刚才设置的快捷键就能召唤应用至前台. 如果不能, 请检查自己前面的设置是否有误.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-三参与贡献" class="anchor" aria-hidden="true" href="#三参与贡献"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;三、参与贡献&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Fork 本项目&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="2"&gt;
&lt;li&gt;新建 Feat_xxx 分支&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="3"&gt;
&lt;li&gt;提交代码&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="4"&gt;
&lt;li&gt;新建 Pull Request&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start="5"&gt;
&lt;li&gt;捐赠：&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://bbs.deepin.org/forum.php?mod=viewthread&amp;amp;tid=40784&amp;amp;extra=page%3D1" rel="nofollow"&gt;捐赠开发者&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;捐赠打包者（这个其实不需要，我的贡献并不多，支持deepin就好了，不过有人在问，我就加上吧）
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wszqkzqk/deepin-wine-ubuntu/master/donate.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/wszqkzqk/deepin-wine-ubuntu/master/donate.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wszqkzqk/deepin-wine-ubuntu/master/donate.png"&gt;&lt;img src="https://raw.githubusercontent.com/wszqkzqk/deepin-wine-ubuntu/master/donate.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;欢迎大家积极反馈！&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;*本软件最近更新可能有点缓慢
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wszqkzqk</author><guid isPermaLink="false">https://github.com/wszqkzqk/deepin-wine-ubuntu</guid><pubDate>Sun, 09 Feb 2020 00:11:00 GMT</pubDate></item><item><title>tensorflow/tensorflow #12 in C++, This week</title><link>https://github.com/tensorflow/tensorflow</link><description>&lt;p&gt;&lt;i&gt;An Open Source Machine Learning Framework for Everyone&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67" data-canonical-src="https://www.tensorflow.org/images/tf_logo_social.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.tensorflow.org/api_docs/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6bcb2a5bacdee6fffb24776a79c21bc3eae19ecc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6170692d7265666572656e63652d626c75652e737667" alt="Documentation" data-canonical-src="https://img.shields.io/badge/api-reference-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href="https://www.tensorflow.org/" rel="nofollow"&gt;TensorFlow&lt;/a&gt; is an end-to-end open source platform
for machine learning. It has a comprehensive, flexible ecosystem of
&lt;a href="https://www.tensorflow.org/resources/tools" rel="nofollow"&gt;tools&lt;/a&gt;,
&lt;a href="https://www.tensorflow.org/resources/libraries-extensions" rel="nofollow"&gt;libraries&lt;/a&gt;, and
&lt;a href="https://www.tensorflow.org/community" rel="nofollow"&gt;community&lt;/a&gt; resources that lets
researchers push the state-of-the-art in ML and developers easily build and
deploy ML-powered applications.&lt;/p&gt;
&lt;p&gt;TensorFlow was originally developed by researchers and engineers working on the
Google Brain team within Google's Machine Intelligence Research organization to
conduct machine learning and deep neural networks research. The system is
general enough to be applicable in a wide variety of other domains, as well.&lt;/p&gt;
&lt;p&gt;TensorFlow provides stable &lt;a href="https://www.tensorflow.org/api_docs/python" rel="nofollow"&gt;Python&lt;/a&gt;
and &lt;a href="https://www.tensorflow.org/api_docs/cc" rel="nofollow"&gt;C++&lt;/a&gt; APIs, as well as
non-guaranteed backward compatible API for
&lt;a href="https://www.tensorflow.org/api_docs" rel="nofollow"&gt;other languages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Keep up-to-date with release announcements and security updates by subscribing
to
&lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!forum/announce" rel="nofollow"&gt;announce@tensorflow.org&lt;/a&gt;.
See all the &lt;a href="https://www.tensorflow.org/community/forums" rel="nofollow"&gt;mailing lists&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;See the &lt;a href="https://www.tensorflow.org/install" rel="nofollow"&gt;TensorFlow install guide&lt;/a&gt; for the
&lt;a href="https://www.tensorflow.org/install/pip" rel="nofollow"&gt;pip package&lt;/a&gt;, to
&lt;a href="https://www.tensorflow.org/install/gpu" rel="nofollow"&gt;enable GPU support&lt;/a&gt;, use a
&lt;a href="https://www.tensorflow.org/install/docker" rel="nofollow"&gt;Docker container&lt;/a&gt;, and
&lt;a href="https://www.tensorflow.org/install/source" rel="nofollow"&gt;build from source&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To install the current release, which includes support for
&lt;a href="https://www.tensorflow.org/install/gpu" rel="nofollow"&gt;CUDA-enabled GPU cards&lt;/a&gt; &lt;em&gt;(Ubuntu and
Windows)&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install tensorflow
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A smaller CPU-only package is also available:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install tensorflow-cpu
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To update TensorFlow to the latest version, add &lt;code&gt;--upgrade&lt;/code&gt; flag to the above
commands.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Nightly binaries are available for testing using the
&lt;a href="https://pypi.python.org/pypi/tf-nightly" rel="nofollow"&gt;tf-nightly&lt;/a&gt; and
&lt;a href="https://pypi.python.org/pypi/tf-nightly-cpu" rel="nofollow"&gt;tf-nightly-cpu&lt;/a&gt; packages on PyPi.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-try-your-first-tensorflow-program" class="anchor" aria-hidden="true" href="#try-your-first-tensorflow-program"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Try your first TensorFlow program&lt;/em&gt;&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; tf.add(&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;).numpy()
&lt;span class="pl-c1"&gt;3&lt;/span&gt;
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; hello &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.constant(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Hello, TensorFlow!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; hello.numpy()
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Hello, TensorFlow!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more examples, see the
&lt;a href="https://www.tensorflow.org/tutorials/" rel="nofollow"&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;If you want to contribute to TensorFlow, be sure to review the
&lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;. This project adheres to TensorFlow's
&lt;a href="CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you are expected to
uphold this code.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We use &lt;a href="https://github.com/tensorflow/tensorflow/issues"&gt;GitHub issues&lt;/a&gt; for
tracking requests and bugs, please see
&lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss" rel="nofollow"&gt;TensorFlow Discuss&lt;/a&gt;
for general questions and discussion, and please direct specific questions to
&lt;a href="https://stackoverflow.com/questions/tagged/tensorflow" rel="nofollow"&gt;Stack Overflow&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The TensorFlow project strives to abide by generally accepted best practices in
open-source software development:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bestpractices.coreinfrastructure.org/projects/1486" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3e88ed14c6c25049b897c18ec0634849eedd41ed/68747470733a2f2f626573747072616374696365732e636f7265696e6672617374727563747572652e6f72672f70726f6a656374732f313438362f6261646765" alt="CII Best Practices" data-canonical-src="https://bestpractices.coreinfrastructure.org/projects/1486/badge" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="CODE_OF_CONDUCT.md"&gt;&lt;img src="https://camo.githubusercontent.com/8315e511f8eb4651470540d6951fd05099251fc5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76312e3425323061646f707465642d6666363962342e737667" alt="Contributor Covenant" data-canonical-src="https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-continuous-build-status" class="anchor" aria-hidden="true" href="#continuous-build-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Continuous build status&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-official-builds" class="anchor" aria-hidden="true" href="#official-builds"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official Builds&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Build Type&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Artifacts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4a4fdcfa00ca9b58e25faeb12b3c33e76b1a958e/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f7562756e74752d63632e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pypi.org/project/tf-nightly/" rel="nofollow"&gt;PyPI&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/23974bb3500725d9002d3d90db6969209727e67a/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f7562756e74752d6770752d7079332e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pypi.org/project/tf-nightly-gpu/" rel="nofollow"&gt;PyPI&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux XLA&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f87640221ff02de68733023965fbc4158915ae34/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f7562756e74752d786c612e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a870cac7ec5bb83225157f2d8cd29402c5c225c2/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f6d61636f732d7079322d63632e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pypi.org/project/tf-nightly/" rel="nofollow"&gt;PyPI&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Windows CPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6bb46296f5a577e642e90c2790cd986fed948001/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f77696e646f77732d6370752e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pypi.org/project/tf-nightly/" rel="nofollow"&gt;PyPI&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Windows GPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dcef118987c7dc26851bd1bf06209a73a321eade/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f77696e646f77732d6770752e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pypi.org/project/tf-nightly-gpu/" rel="nofollow"&gt;PyPI&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Android&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ac624987d37e4b45b7d48af47da9a2c59aa91f98/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f616e64726f69642e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://bintray.com/google/tensorflow/tensorflow/_latestVersion" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c70326c7b5968c1c60c4736aca22ceea7caee29/68747470733a2f2f6170692e62696e747261792e636f6d2f7061636b616765732f676f6f676c652f74656e736f72666c6f772f74656e736f72666c6f772f696d616765732f646f776e6c6f61642e737667" alt="Download" data-canonical-src="https://api.bintray.com/packages/google/tensorflow/tensorflow/images/download.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Raspberry Pi 0 and 1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3b1d713c0f336323888f3af38b245d48437bfff6/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f72706930312d7079322e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/206345bc65f075349a30f0417d98696db7a3c991/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f72706930312d7079332e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv6l.whl" rel="nofollow"&gt;Py2&lt;/a&gt; &lt;a href="https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl" rel="nofollow"&gt;Py3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Raspberry Pi 2 and 3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d9ad01af09347b9b3d51598136966906db5078f6/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f72706932332d7079322e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3005cda539dbd99ffdd30b4a3a53632ca0b3157d/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f74656e736f72666c6f772d6b6f6b6f726f2d6275696c642d6261646765732f72706932332d7079332e737667" alt="Status" data-canonical-src="https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv7l.whl" rel="nofollow"&gt;Py2&lt;/a&gt; &lt;a href="https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl" rel="nofollow"&gt;Py3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-community-supported-builds" class="anchor" aria-hidden="true" href="#community-supported-builds"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community Supported Builds&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Build Type&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Artifacts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux AMD ROCm GPU&lt;/strong&gt; Nightly&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/43be7f8a67c3b3b7d3a36a1e643080d50f0d0161/687474703a2f2f6d6c2d63692e616d642e636f6d3a32313039362f6a6f622f74656e736f72666c6f772d726f636d2d6e696768746c792f62616467652f69636f6e" alt="Build Status" data-canonical-src="http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ml-ci.amd.com:21096/job/tensorflow-rocm-nightly/lastSuccessfulBuild/" rel="nofollow"&gt;Nightly&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux AMD ROCm GPU&lt;/strong&gt; Stable Release&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4b7df6fd69523c6ac58fa4e18020fbb11ccefb3a/687474703a2f2f6d6c2d63692e616d642e636f6d3a32313039362f6a6f622f74656e736f72666c6f772d726f636d2d72656c656173652f62616467652f69636f6e" alt="Build Status" data-canonical-src="http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Release &lt;a href="http://ml-ci.amd.com:21096/job/tensorflow-rocm-release/lastSuccessfulBuild/" rel="nofollow"&gt;1.15&lt;/a&gt; / &lt;a href="http://ml-ci.amd.com:21096/job/tensorflow-rocm-v2-release/lastSuccessfulBuild/" rel="nofollow"&gt;2.x&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux s390x&lt;/strong&gt; Nightly&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6bf78ecce19b8d70fd6e2b8b69e9b471f34bd6b4/687474703a2f2f69626d7a2d63692e6f73756f736c2e6f72672f6a6f622f54656e736f72466c6f775f49424d5a5f43492f62616467652f69636f6e" alt="Build Status" data-canonical-src="http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/" rel="nofollow"&gt;Nightly&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux s390x CPU&lt;/strong&gt; Stable Release&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f42181ec41d0488d09a5ee573fdbc99984aa1145/687474703a2f2f69626d7a2d63692e6f73756f736c2e6f72672f6a6f622f54656e736f72466c6f775f49424d5a5f52656c656173655f4275696c642f62616467652f69636f6e" alt="Build Status" data-canonical-src="http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_Release_Build/" rel="nofollow"&gt;Release&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux ppc64le CPU&lt;/strong&gt; Nightly&lt;/td&gt;
&lt;td&gt;&lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/54cb21daa12029bb61f4e7d7bc95e0e335ceda2e/68747470733a2f2f706f77657263692e6f73756f736c2e6f72672f6a6f622f54656e736f72466c6f775f50504336344c455f4350555f4275696c642f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Nightly_Artifact/" rel="nofollow"&gt;Nightly&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux ppc64le CPU&lt;/strong&gt; Stable Release&lt;/td&gt;
&lt;td&gt;&lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3b254e3e5afa48aeb9c1ae1a0ae2638d34154b62/68747470733a2f2f706f77657263692e6f73756f736c2e6f72672f6a6f622f54656e736f72466c6f775f50504336344c455f4350555f52656c656173655f4275696c642f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Release &lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/" rel="nofollow"&gt;1.15&lt;/a&gt; / &lt;a href="https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/" rel="nofollow"&gt;2.x&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux ppc64le GPU&lt;/strong&gt; Nightly&lt;/td&gt;
&lt;td&gt;&lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e8e2513df82fb86f96c447d323ea2cb0dd70b05c/68747470733a2f2f706f77657263692e6f73756f736c2e6f72672f6a6f622f54656e736f72466c6f775f50504336344c455f4750555f4275696c642f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Nightly_Artifact/" rel="nofollow"&gt;Nightly&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux ppc64le GPU&lt;/strong&gt; Stable Release&lt;/td&gt;
&lt;td&gt;&lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7f1f37aee090217fc7a43ea95ca9b0de6939c163/68747470733a2f2f706f77657263692e6f73756f736c2e6f72672f6a6f622f54656e736f72466c6f775f50504336344c455f4750555f52656c656173655f4275696c642f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Release &lt;a href="https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/" rel="nofollow"&gt;1.15&lt;/a&gt; / &lt;a href="https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_GPU_Release_Build/" rel="nofollow"&gt;2.x&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux CPU with Intel® MKL-DNN&lt;/strong&gt; Nightly&lt;/td&gt;
&lt;td&gt;&lt;a href="https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c40f3ad1a2607800e59b7883ad6b02bd7f14a571/68747470733a2f2f74656e736f72666c6f772d63692e696e74656c2e636f6d2f6a6f622f74656e736f72666c6f772d6d6b6c2d6275696c642d77686c2d6e696768746c792f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/" rel="nofollow"&gt;Nightly&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux CPU with Intel® MKL-DNN&lt;/strong&gt; Stable Release&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b2c71a19d8201755f18351c512a056932c4398cf/68747470733a2f2f74656e736f72666c6f772d63692e696e74656c2e636f6d2f6a6f622f74656e736f72666c6f772d6d6b6c2d6275696c642d72656c656173652d77686c2f62616467652f69636f6e"&gt;&lt;img src="https://camo.githubusercontent.com/b2c71a19d8201755f18351c512a056932c4398cf/68747470733a2f2f74656e736f72666c6f772d63692e696e74656c2e636f6d2f6a6f622f74656e736f72666c6f772d6d6b6c2d6275696c642d72656c656173652d77686c2f62616467652f69636f6e" alt="Build Status" data-canonical-src="https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/badge/icon" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Release &lt;a href="https://pypi.org/project/intel-tensorflow/1.15.0/" rel="nofollow"&gt;1.15&lt;/a&gt; / &lt;a href="https://pypi.org/project/intel-tensorflow/" rel="nofollow"&gt;2.x&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Red Hat® Enterprise Linux® 7.6 CPU &amp;amp; GPU&lt;/strong&gt; &lt;br&gt; Python 2.7, 3.6&lt;/td&gt;
&lt;td&gt;&lt;a href="https://jenkins-tensorflow.apps.ci.centos.org/job/tensorflow-rhel7-3.6/2/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/54054605d50d10a330786d70ac221bb8874caa5d/68747470733a2f2f6a656e6b696e732d74656e736f72666c6f772e617070732e63692e63656e746f732e6f72672f6275696c645374617475732f69636f6e3f6a6f623d74656e736f72666c6f772d7268656c372d332e36266275696c643d32" alt="Build Status" data-canonical-src="https://jenkins-tensorflow.apps.ci.centos.org/buildStatus/icon?job=tensorflow-rhel7-3.6&amp;amp;build=2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://tensorflow.pypi.thoth-station.ninja/index/" rel="nofollow"&gt;1.13.1 PyPI&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;TensorFlow.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/" rel="nofollow"&gt;TensorFlow tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/models/tree/master/official"&gt;TensorFlow official models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/examples"&gt;TensorFlow examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/specializations/tensorflow-in-practice" rel="nofollow"&gt;TensorFlow in Practice from Coursera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187" rel="nofollow"&gt;Intro to TensorFlow for Deep Learning from Udacity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udacity.com/course/intro-to-tensorflow-lite--ud190" rel="nofollow"&gt;Introduction to TensorFlow Lite from Udacity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.tensorflow.org" rel="nofollow"&gt;TensorFlow blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/tensorflow" rel="nofollow"&gt;TensorFlow Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ" rel="nofollow"&gt;TensorFlow YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/community/roadmap" rel="nofollow"&gt;TensorFlow roadmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/about/bib" rel="nofollow"&gt;TensorFlow white papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorboard"&gt;TensorBoard visualization toolkit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learn more about the
&lt;a href="https://www.tensorflow.org/community" rel="nofollow"&gt;TensorFlow community&lt;/a&gt; and how to
&lt;a href="https://www.tensorflow.org/community/contribute" rel="nofollow"&gt;contribute&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/tensorflow</guid><pubDate>Sun, 09 Feb 2020 00:12:00 GMT</pubDate></item><item><title>bigtreetech/BIGTREETECH-SKR-V1.3 #13 in C++, This week</title><link>https://github.com/bigtreetech/BIGTREETECH-SKR-V1.3</link><description>&lt;p&gt;&lt;i&gt;32bit board with LPC1768, support marlin2.0 and smoothieware, support lcd2004/12864, On-board TMC2130 SPI interface and TMC2208 UART interface no additional wiring is required&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bigtreetech-skr-v13" class="anchor" aria-hidden="true" href="#bigtreetech-skr-v13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BIGTREETECH-SKR-V1.3&lt;/h1&gt;
&lt;p&gt;32bit board with LPC1768, support marlin2.0 and smoothieware, support lcd2004/12864, On-board TMC2130 SPI interface and TMC2208 UART interface no additional wiring is&lt;/p&gt;
&lt;p&gt;BIGTREETECH SKR V1.3 YouTube Video：&lt;a href="https://www.youtube.com/watch?v=oaXfXkPYHpw&amp;amp;t=8s" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bigtreetech</author><guid isPermaLink="false">https://github.com/bigtreetech/BIGTREETECH-SKR-V1.3</guid><pubDate>Sun, 09 Feb 2020 00:13:00 GMT</pubDate></item><item><title>grpc/grpc #14 in C++, This week</title><link>https://github.com/grpc/grpc</link><description>&lt;p&gt;&lt;i&gt;The C based gRPC (C++, Python, Ruby, Objective-C, PHP, C#)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-grpc---an-rpc-library-and-framework" class="anchor" aria-hidden="true" href="#grpc---an-rpc-library-and-framework"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;gRPC - An RPC library and framework&lt;/h1&gt;
&lt;p&gt;gRPC is a modern, open source, high-performance remote procedure call (RPC) framework that can run anywhere. gRPC enables client and server applications to communicate transparently, and simplifies the building of connected systems.&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;b&gt;Homepage:&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://grpc.io/" rel="nofollow"&gt;grpc.io&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;b&gt;Mailing List:&lt;/b&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://groups.google.com/forum/#!forum/grpc-io" rel="nofollow"&gt;grpc-io@googlegroups.com&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;a href="https://gitter.im/grpc/grpc?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/584e0cdb40890a4a0118ed01be26ad2063ac1033/68747470733a2f2f6261646765732e6769747465722e696d2f677270632f677270632e737667" alt="Join the chat at https://gitter.im/grpc/grpc" data-canonical-src="https://badges.gitter.im/grpc/grpc.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-to-start-using-grpc" class="anchor" aria-hidden="true" href="#to-start-using-grpc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;To start using gRPC&lt;/h1&gt;
&lt;p&gt;To maximize usability, gRPC supports the standard method for adding dependencies to a user's chosen language (if there is one).
In most languages, the gRPC runtime comes as a package available in a user's language package manager.&lt;/p&gt;
&lt;p&gt;For instructions on how to use the language-specific gRPC runtime for a project, please refer to these documents&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="src/cpp"&gt;C++&lt;/a&gt;: follow the instructions under the &lt;code&gt;src/cpp&lt;/code&gt; directory&lt;/li&gt;
&lt;li&gt;&lt;a href="src/csharp"&gt;C#&lt;/a&gt;: NuGet package &lt;code&gt;Grpc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/grpc/grpc-dart"&gt;Dart&lt;/a&gt;: pub package &lt;code&gt;grpc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/grpc/grpc-go"&gt;Go&lt;/a&gt;: &lt;code&gt;go get google.golang.org/grpc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/grpc/grpc-java"&gt;Java&lt;/a&gt;: Use JARs from Maven Central Repository&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/grpc/grpc-node"&gt;Node&lt;/a&gt;: &lt;code&gt;npm install grpc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="src/objective-c"&gt;Objective-C&lt;/a&gt;: Add &lt;code&gt;gRPC-ProtoRPC&lt;/code&gt; dependency to podspec&lt;/li&gt;
&lt;li&gt;&lt;a href="src/php"&gt;PHP&lt;/a&gt;: &lt;code&gt;pecl install grpc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="src/python/grpcio"&gt;Python&lt;/a&gt;: &lt;code&gt;pip install grpcio&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="src/ruby"&gt;Ruby&lt;/a&gt;: &lt;code&gt;gem install grpc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/grpc/grpc-web"&gt;WebJS&lt;/a&gt;: follow the grpc-web instructions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Per-language quickstart guides and tutorials can be found in the &lt;a href="https://grpc.io/docs/" rel="nofollow"&gt;documentation section on the grpc.io website&lt;/a&gt;. Code examples are available in the &lt;a href="examples"&gt;examples&lt;/a&gt; directory.&lt;/p&gt;
&lt;p&gt;Precompiled bleeding-edge package builds of gRPC &lt;code&gt;master&lt;/code&gt; branch's &lt;code&gt;HEAD&lt;/code&gt; are uploaded daily to &lt;a href="https://packages.grpc.io" rel="nofollow"&gt;packages.grpc.io&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-to-start-developing-grpc" class="anchor" aria-hidden="true" href="#to-start-developing-grpc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;To start developing gRPC&lt;/h1&gt;
&lt;p&gt;Contributions are welcome!&lt;/p&gt;
&lt;p&gt;Please read &lt;a href="CONTRIBUTING.md"&gt;How to contribute&lt;/a&gt; which will guide you through the entire workflow of how to build the source code, how to run the tests, and how to contribute changes to
the gRPC codebase.
The "How to contribute" document also contains info on how the contribution process works and contains best practices for creating contributions.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h1&gt;
&lt;p&gt;Sometimes things go wrong. Please check out the &lt;a href="TROUBLESHOOTING.md"&gt;Troubleshooting guide&lt;/a&gt; if you are experiencing issues with gRPC.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Performance&lt;/h1&gt;
&lt;p&gt;See the &lt;a href="http://performance-dot-grpc-testing.appspot.com/explore?dashboard=5636470266134528" rel="nofollow"&gt;Performance dashboard&lt;/a&gt; for performance numbers of the latest released version.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-concepts" class="anchor" aria-hidden="true" href="#concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concepts&lt;/h1&gt;
&lt;p&gt;See &lt;a href="CONCEPTS.md"&gt;gRPC Concepts&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-about-this-repository" class="anchor" aria-hidden="true" href="#about-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About This Repository&lt;/h1&gt;
&lt;p&gt;This repository contains source code for gRPC libraries implemented in multiple languages written on top of a shared C core library &lt;a href="src/core"&gt;src/core&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Libraries in different languages may be in various states of development. We are seeking contributions for all of these libraries:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;Source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Shared C [core library]&lt;/td&gt;
&lt;td&gt;&lt;a href="src/core"&gt;src/core&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C++&lt;/td&gt;
&lt;td&gt;&lt;a href="src/cpp"&gt;src/cpp&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ruby&lt;/td&gt;
&lt;td&gt;&lt;a href="src/ruby"&gt;src/ruby&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;&lt;a href="src/python"&gt;src/python&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PHP&lt;/td&gt;
&lt;td&gt;&lt;a href="src/php"&gt;src/php&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C# (core library based)&lt;/td&gt;
&lt;td&gt;&lt;a href="src/csharp"&gt;src/csharp&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Objective-C&lt;/td&gt;
&lt;td&gt;&lt;a href="src/objective-c"&gt;src/objective-c&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;Source repo&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Java&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/grpc/grpc-java"&gt;grpc-java&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Go&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/grpc/grpc-go"&gt;grpc-go&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NodeJS&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/grpc/grpc-node"&gt;grpc-node&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WebJS&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/grpc/grpc-web"&gt;grpc-web&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dart&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/grpc/grpc-dart"&gt;grpc-dart&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;.NET (pure C# impl.)&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/grpc/grpc-dotnet"&gt;grpc-dotnet&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>grpc</author><guid isPermaLink="false">https://github.com/grpc/grpc</guid><pubDate>Sun, 09 Feb 2020 00:14:00 GMT</pubDate></item><item><title>electron/electron #15 in C++, This week</title><link>https://github.com/electron/electron</link><description>&lt;p&gt;&lt;i&gt;:electron: Build cross-platform desktop apps with JavaScript, HTML, and CSS&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://electronjs.org" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/627c774e3070482b180c3abd858ef2145d46303b/68747470733a2f2f656c656374726f6e6a732e6f72672f696d616765732f656c656374726f6e2d6c6f676f2e737667" alt="Electron Logo" data-canonical-src="https://electronjs.org/images/electron-logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://circleci.com/gh/electron/electron/tree/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/40e80eb4e1baf46eb68d558cc957c7d925655484/68747470733a2f2f636972636c6563692e636f6d2f67682f656c656374726f6e2f656c656374726f6e2f747265652f6d61737465722e7376673f7374796c653d736869656c64" alt="CircleCI Build Status" data-canonical-src="https://circleci.com/gh/electron/electron/tree/master.svg?style=shield" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/electron-bot/electron-ljo26/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7becf5009becc1f184f06079973315c142541373/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f346c6767693964706a6331716f62376b2f6272616e63682f6d61737465723f7376673d74727565" alt="AppVeyor Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/4lggi9dpjc1qob7k/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://david-dm.org/electron/electron?type=dev" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/89729600f77228357e4be6fe22cd0ecd6940b267/68747470733a2f2f64617669642d646d2e6f72672f656c656374726f6e2f656c656374726f6e2f6465762d7374617475732e737667" alt="devDependency Status" data-canonical-src="https://david-dm.org/electron/electron/dev-status.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="memo" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png"&gt;📝&lt;/g-emoji&gt; Available Translations: &lt;g-emoji class="g-emoji" alias="cn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e8-1f1f3.png"&gt;🇨🇳&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="taiwan" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f9-1f1fc.png"&gt;🇹🇼&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="brazil" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e7-1f1f7.png"&gt;🇧🇷&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="es" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ea-1f1f8.png"&gt;🇪🇸&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="kr" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f0-1f1f7.png"&gt;🇰🇷&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="jp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ef-1f1f5.png"&gt;🇯🇵&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="ru" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f7-1f1fa.png"&gt;🇷🇺&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="fr" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1eb-1f1f7.png"&gt;🇫🇷&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="thailand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f9-1f1ed.png"&gt;🇹🇭&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="netherlands" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f3-1f1f1.png"&gt;🇳🇱&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="tr" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f9-1f1f7.png"&gt;🇹🇷&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="indonesia" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ee-1f1e9.png"&gt;🇮🇩&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="ukraine" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1fa-1f1e6.png"&gt;🇺🇦&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="czech_republic" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e8-1f1ff.png"&gt;🇨🇿&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="it" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ee-1f1f9.png"&gt;🇮🇹&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="poland" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f5-1f1f1.png"&gt;🇵🇱&lt;/g-emoji&gt;.
View these docs in other languages at &lt;a href="https://github.com/electron/i18n/tree/master/content/"&gt;electron/i18n&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Electron framework lets you write cross-platform desktop applications
using JavaScript, HTML and CSS. It is based on &lt;a href="https://nodejs.org/" rel="nofollow"&gt;Node.js&lt;/a&gt; and
&lt;a href="https://www.chromium.org" rel="nofollow"&gt;Chromium&lt;/a&gt; and is used by the &lt;a href="https://github.com/atom/atom"&gt;Atom
editor&lt;/a&gt; and many other &lt;a href="https://electronjs.org/apps" rel="nofollow"&gt;apps&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Follow &lt;a href="https://twitter.com/electronjs" rel="nofollow"&gt;@ElectronJS&lt;/a&gt; on Twitter for important
announcements.&lt;/p&gt;
&lt;p&gt;This project adheres to the Contributor Covenant
&lt;a href="https://github.com/electron/electron/tree/master/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;.
By participating, you are expected to uphold this code. Please report unacceptable
behavior to &lt;a href="mailto:coc@electronjs.org"&gt;coc@electronjs.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;To install prebuilt Electron binaries, use &lt;a href="https://docs.npmjs.com/" rel="nofollow"&gt;&lt;code&gt;npm&lt;/code&gt;&lt;/a&gt;.
The preferred method is to install Electron as a development dependency in your
app:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm install electron --save-dev [--save-exact]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;--save-exact&lt;/code&gt; flag is recommended for Electron prior to version 2, as it does not follow semantic
versioning. As of version 2.0.0, Electron follows semver, so you don't need &lt;code&gt;--save-exact&lt;/code&gt; flag. For info on how to manage Electron versions in your apps, see
&lt;a href="docs/tutorial/electron-versioning.md"&gt;Electron versioning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more installation options and troubleshooting tips, see
&lt;a href="docs/tutorial/installation.md"&gt;installation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start--electron-fiddle" class="anchor" aria-hidden="true" href="#quick-start--electron-fiddle"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start &amp;amp; Electron Fiddle&lt;/h2&gt;
&lt;p&gt;Use &lt;a href="https://github.com/electron/fiddle"&gt;&lt;code&gt;Electron Fiddle&lt;/code&gt;&lt;/a&gt;
to build, run, and package small Electron experiments, to see code examples for all of Electron's APIs, and
to try out different versions of Electron. It's designed to make the start of your journey with
Electron easier.&lt;/p&gt;
&lt;p&gt;Alternatively, clone and run the
&lt;a href="https://github.com/electron/electron-quick-start"&gt;electron/electron-quick-start&lt;/a&gt;
repository to see a minimal Electron app in action:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/electron/electron-quick-start
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; electron-quick-start
npm install
npm start&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-resources-for-learning-electron" class="anchor" aria-hidden="true" href="#resources-for-learning-electron"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources for learning Electron&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://electronjs.org/docs" rel="nofollow"&gt;electronjs.org/docs&lt;/a&gt; - All of Electron's documentation&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/electron/fiddle"&gt;electron/fiddle&lt;/a&gt; - A tool to build, run, and package small Electron experiments&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/electron/electron-quick-start"&gt;electron/electron-quick-start&lt;/a&gt; - A very basic starter Electron app&lt;/li&gt;
&lt;li&gt;&lt;a href="https://electronjs.org/community#boilerplates" rel="nofollow"&gt;electronjs.org/community#boilerplates&lt;/a&gt; - Sample starter apps created by the community&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/electron/simple-samples"&gt;electron/simple-samples&lt;/a&gt; - Small applications with ideas for taking them further&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/electron/electron-api-demos"&gt;electron/electron-api-demos&lt;/a&gt; - An Electron app that teaches you how to use Electron&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hokein/electron-sample-apps"&gt;hokein/electron-sample-apps&lt;/a&gt; - Small demo apps for the various Electron APIs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-programmatic-usage" class="anchor" aria-hidden="true" href="#programmatic-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Programmatic usage&lt;/h2&gt;
&lt;p&gt;Most people use Electron from the command line, but if you require &lt;code&gt;electron&lt;/code&gt; inside
your &lt;strong&gt;Node app&lt;/strong&gt; (not your Electron app) it will return the file path to the
binary. Use this to spawn Electron from Node scripts:&lt;/p&gt;
&lt;div class="highlight highlight-source-js"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;const&lt;/span&gt; &lt;span class="pl-c1"&gt;electron&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;require&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;electron&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;const&lt;/span&gt; &lt;span class="pl-c1"&gt;proc&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;require&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;child_process&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; will print something similar to /Users/maf/.../Electron&lt;/span&gt;
&lt;span class="pl-en"&gt;console&lt;/span&gt;.&lt;span class="pl-c1"&gt;log&lt;/span&gt;(electron)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;//&lt;/span&gt; spawn Electron&lt;/span&gt;
&lt;span class="pl-k"&gt;const&lt;/span&gt; &lt;span class="pl-c1"&gt;child&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-smi"&gt;proc&lt;/span&gt;.&lt;span class="pl-en"&gt;spawn&lt;/span&gt;(electron)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-mirrors" class="anchor" aria-hidden="true" href="#mirrors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mirrors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://npm.taobao.org/mirrors/electron" rel="nofollow"&gt;China&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation-translations" class="anchor" aria-hidden="true" href="#documentation-translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation Translations&lt;/h2&gt;
&lt;p&gt;Find documentation translations in &lt;a href="https://github.com/electron/i18n"&gt;electron/i18n&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;If you are interested in reporting/fixing issues and contributing directly to the code base, please see &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information on what we're looking for and how to get started.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-community" class="anchor" aria-hidden="true" href="#community"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community&lt;/h2&gt;
&lt;p&gt;Info on reporting bugs, getting help, finding third-party tools and sample apps,
and more can be found in the &lt;a href="docs/tutorial/support.md#finding-support"&gt;support document&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/electron/electron/blob/master/LICENSE"&gt;MIT&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When using the Electron or other GitHub logos, be sure to follow the &lt;a href="https://github.com/logos"&gt;GitHub logo guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>electron</author><guid isPermaLink="false">https://github.com/electron/electron</guid><pubDate>Sun, 09 Feb 2020 00:15:00 GMT</pubDate></item><item><title>FreeCAD/FreeCAD #16 in C++, This week</title><link>https://github.com/FreeCAD/FreeCAD</link><description>&lt;p&gt;&lt;i&gt;This is the official source code of FreeCAD, a free and opensource multiplatform 3D parametric modeler. Issues are managed on our own bug tracker at https://www.freecadweb.org/tracker&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/63b30fc4f3e337edd83037c0448237ec2c4ba4f8/68747470733a2f2f7777772e667265656361647765622e6f72672f696d616765732f6c6f676f2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/63b30fc4f3e337edd83037c0448237ec2c4ba4f8/68747470733a2f2f7777772e667265656361647765622e6f72672f696d616765732f6c6f676f2e706e67" alt="Logo" data-canonical-src="https://www.freecadweb.org/images/logo.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-your-own-3d-parametric-modeler" class="anchor" aria-hidden="true" href="#your-own-3d-parametric-modeler"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Your own 3D parametric modeler&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.freecadweb.org" rel="nofollow"&gt;Website&lt;/a&gt; •
&lt;a href="https://www.freecadweb.org/wiki/" rel="nofollow"&gt;Documentation&lt;/a&gt; •
&lt;a href="https://forum.freecadweb.org/" rel="nofollow"&gt;Forum&lt;/a&gt; •
&lt;a href="https://www.freecadweb.org/tracker/" rel="nofollow"&gt;Bug tracker&lt;/a&gt; •
&lt;a href="https://github.com/FreeCAD/FreeCAD"&gt;Git repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/freecad/freecad/releases/latest"&gt;&lt;img src="https://camo.githubusercontent.com/b5f084f81caa7d0df1d75059bb1c406668ba3875/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f667265656361642f667265656361642e737667" alt="Release" data-canonical-src="https://img.shields.io/github/release/freecad/freecad.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/FreeCAD/FreeCAD/branches" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5767a057682c450959243caf277cbae4e0d903af/68747470733a2f2f7472617669732d63692e6f72672f467265654341442f467265654341442e7376673f6272616e63683d6d6173746572" alt="Master" data-canonical-src="https://travis-ci.org/FreeCAD/FreeCAD.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://crowdin.com/project/freecad" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da7aaf3b289bb4cbb1420620665efe723641a35b/68747470733a2f2f64333232637174353834626f346f2e636c6f756466726f6e742e6e65742f667265656361642f6c6f63616c697a65642e737667" alt="Crowdin" data-canonical-src="https://d322cqt584bo4o.cloudfront.net/freecad/localized.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://gitter.im/freecad/freecad?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2292cb2bcc8bb0e1a59a73bef7c5bb8a95e62838/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f667265656361642f667265656361642e737667" alt="Gitter" data-canonical-src="https://img.shields.io/gitter/room/freecad/freecad.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://lgtm.com/projects/g/FreeCAD/FreeCAD/context:python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0d560ccdcb107a314c3c92498d5d4a558a32aa45/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f467265654341442f467265654341442e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: Python" data-canonical-src="https://img.shields.io/lgtm/grade/python/g/FreeCAD/FreeCAD.svg?logo=lgtm&amp;amp;logoWidth=18" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/908facad45da3ec0a6cdec1779e00a02c958ed23/687474703a2f2f7777772e667265656361647765622e6f72672f77696b692f696d616765732f7468756d622f372f37322f467265656361643031365f73637265656e73686f74312e6a70672f38303070782d467265656361643031365f73637265656e73686f74312e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/908facad45da3ec0a6cdec1779e00a02c958ed23/687474703a2f2f7777772e667265656361647765622e6f72672f77696b692f696d616765732f7468756d622f372f37322f467265656361643031365f73637265656e73686f74312e6a70672f38303070782d467265656361643031365f73637265656e73686f74312e6a7067" alt="screenshot" data-canonical-src="http://www.freecadweb.org/wiki/images/thumb/7/72/Freecad016_screenshot1.jpg/800px-Freecad016_screenshot1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Freedom to build what you want&lt;/strong&gt;  FreeCAD is an open-source parametric 3D
modeler made primarily to design real-life objects of any size.
Parametric modeling allows you to easily modify your design by going back into
your model history and changing its parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create 3D from 2D &amp;amp; back&lt;/strong&gt; FreeCAD allows you to sketch geometry constrained
2D shapes and use them as a base to build other objects.
It contains many components to adjust dimensions or extract design details from
3D models to create high quality production ready drawings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Designed for your needs&lt;/strong&gt; FreeCAD is designed to fit a wide range of uses
including product design, mechanical engineering and architecture.
Whether you are a hobbyist, a programmer, an experienced CAD user,
a student or a teacher, you will feel right at home with FreeCAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross platform&lt;/strong&gt; FreeCAD runs on Windows, Mac and Linux&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Underlying technology&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OpenCASCADE&lt;/strong&gt; A powerful geometry kernel, the most important component of FreeCAD&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coin3D library&lt;/strong&gt; Open Inventor-compliant 3D scene representation model&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; FreeCAD offers a broad Python API&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qt&lt;/strong&gt; Graphical User Interface built with Qt&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;Precompiled (installable) packages are available for Windows and Mac on the
&lt;a href="https://github.com/FreeCAD/FreeCAD/releases"&gt;Releases page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On most Linux distributions, FreeCAD is directly installable from the
software center application.&lt;/p&gt;
&lt;p&gt;Other options are described at the &lt;a href="http://www.freecadweb.org/wiki/Download" rel="nofollow"&gt;wiki Download page&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-build-status-" class="anchor" aria-hidden="true" href="#build-status-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build Status &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3b460c8fb4d10addb3bd6a91bf1467f031427445/68747470733a2f2f626c6f672e7472617669732d63692e636f6d2f696d616765732f7472617669732d6d6173636f742d32303070782e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/3b460c8fb4d10addb3bd6a91bf1467f031427445/68747470733a2f2f626c6f672e7472617669732d63692e636f6d2f696d616765732f7472617669732d6d6173636f742d32303070782e706e67" height="30" data-canonical-src="https://blog.travis-ci.com/images/travis-mascot-200px.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Master&lt;/th&gt;
&lt;th align="center"&gt;0.18&lt;/th&gt;
&lt;th align="center"&gt;Translation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/FreeCAD/FreeCAD/branches" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5767a057682c450959243caf277cbae4e0d903af/68747470733a2f2f7472617669732d63692e6f72672f467265654341442f467265654341442e7376673f6272616e63683d6d6173746572" alt="Master" data-canonical-src="https://travis-ci.org/FreeCAD/FreeCAD.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://travis-ci.org/FreeCAD/FreeCAD/branches" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/08e7f8f9912aa2bfc37350cdfde7c0560668032d/68747470733a2f2f7472617669732d63692e6f72672f467265654341442f467265654341442e7376673f6272616e63683d72656c65617365732f467265654341442d302d3138" alt="0.18" data-canonical-src="https://travis-ci.org/FreeCAD/FreeCAD.svg?branch=releases/FreeCAD-0-18" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://crowdin.com/project/freecad" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da7aaf3b289bb4cbb1420620665efe723641a35b/68747470733a2f2f64333232637174353834626f346f2e636c6f756466726f6e742e6e65742f667265656361642f6c6f63616c697a65642e737667" alt="Crowdin" data-canonical-src="https://d322cqt584bo4o.cloudfront.net/freecad/localized.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-compiling" class="anchor" aria-hidden="true" href="#compiling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiling&lt;/h2&gt;
&lt;p&gt;Compiling FreeCAD requires installation of several libraries and their
development files such as OpenCASCADe, Coin and Qt, listed in the
pages below. Once this is done, FreeCAD can be simply compiled with
cMake. On Windows, these libraries are bundled and offered by the
FreeCAD team in a convenient package. On Linux, they are usually found
in your distribution's repositories, and on Mac OSX and other platforms
you will usually need to compile them yourself.&lt;/p&gt;
&lt;p&gt;The pages below contain up-to-date build instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/CompileOnUnix" rel="nofollow"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/CompileOnWindows" rel="nofollow"&gt;Windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/CompileOnMac" rel="nofollow"&gt;Mac OSX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/CompileOnCygwin" rel="nofollow"&gt;Cygwin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/CompileOnMinGW" rel="nofollow"&gt;MinGW&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-usage--getting-help" class="anchor" aria-hidden="true" href="#usage--getting-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage &amp;amp; Getting help&lt;/h2&gt;
&lt;p&gt;The FreeCAD wiki contains documentation on
general FreeCAD usage, Python scripting, and development. These
pages might help you get started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/Getting_started" rel="nofollow"&gt;Getting started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/Feature_list" rel="nofollow"&gt;Features list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/FAQ" rel="nofollow"&gt;Frequent questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/Workbench_Concept" rel="nofollow"&gt;Workbenches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/Power_users_hub" rel="nofollow"&gt;Scripting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.freecadweb.org/wiki/Developer_hub" rel="nofollow"&gt;Development&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="http://forum.freecadweb.org" rel="nofollow"&gt;FreeCAD forum&lt;/a&gt; is also a great place
to find help and solve specific problems you might encounter when
learning to use FreeCAD.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>FreeCAD</author><guid isPermaLink="false">https://github.com/FreeCAD/FreeCAD</guid><pubDate>Sun, 09 Feb 2020 00:16:00 GMT</pubDate></item><item><title>dmlc/xgboost #17 in C++, This week</title><link>https://github.com/dmlc/xgboost</link><description>&lt;p&gt;&lt;i&gt;Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library,  for Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink and DataFlow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content---extreme-gradient-boosting" class="anchor" aria-hidden="true" href="#--extreme-gradient-boosting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png"&gt;&lt;img src="https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png" width="135/" style="max-width:100%;"&gt;&lt;/a&gt;  eXtreme Gradient Boosting&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://xgboost-ci.net/blue/organizations/jenkins/xgboost/activity" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/603cf77d12c2d5f7dd7d7e337c2189f4a48df644/68747470733a2f2f7867626f6f73742d63692e6e65742f6a6f622f7867626f6f73742f6a6f622f6d61737465722f62616467652f69636f6e3f7374796c653d706c6173746963" alt="Build Status" data-canonical-src="https://xgboost-ci.net/job/xgboost/job/master/badge/icon?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/dmlc/xgboost" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/38efd7690715e8ef81dc1f68dc4482d896f7fddb/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f646d6c632f7867626f6f73742e7376673f6c6162656c3d6275696c64266c6f676f3d747261766973266272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://img.shields.io/travis/dmlc/xgboost.svg?label=build&amp;amp;logo=travis&amp;amp;branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/tqchen/xgboost" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/071225dea3921f4e6211620f0efe86dbd2dedcba/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f357970613876616564366b706d6c69383f7376673d74727565" alt="Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/5ypa8vaed6kpmli8?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://xgboost.readthedocs.org" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2b86bb53873d0f268be973ff78ee651e63dfd988/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7867626f6f73742f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/xgboost/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="./LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/9c627262a98da9ed0c90405f220e42f88a8f865f/687474703a2f2f646d6c632e6769746875622e696f2f696d672f617061636865322e737667" alt="GitHub license" data-canonical-src="http://dmlc.github.io/img/apache2.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="http://cran.r-project.org/web/packages/xgboost" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13d2ca861dc6fcbba5d8159805e3584465c3fcfe/687474703a2f2f7777772e722d706b672e6f72672f6261646765732f76657273696f6e2f7867626f6f7374" alt="CRAN Status Badge" data-canonical-src="http://www.r-pkg.org/badges/version/xgboost" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/xgboost/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/74102606894f9f5e019c4192cc61749d601f7832/68747470733a2f2f62616467652e667572792e696f2f70792f7867626f6f73742e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/xgboost.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://optuna.org" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c4d78d0d752ebf6d6d7a0438612866569acad23d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f7074756e612d696e74656772617465642d626c7565" alt="Optuna" data-canonical-src="https://img.shields.io/badge/Optuna-integrated-blue" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://xgboost.ai/community" rel="nofollow"&gt;Community&lt;/a&gt; |
&lt;a href="https://xgboost.readthedocs.org" rel="nofollow"&gt;Documentation&lt;/a&gt; |
&lt;a href="demo/README.md"&gt;Resources&lt;/a&gt; |
&lt;a href="CONTRIBUTORS.md"&gt;Contributors&lt;/a&gt; |
&lt;a href="NEWS.md"&gt;Release Notes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;XGBoost is an optimized distributed gradient boosting library designed to be highly &lt;em&gt;&lt;strong&gt;efficient&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;flexible&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;portable&lt;/strong&gt;&lt;/em&gt;.
It implements machine learning algorithms under the &lt;a href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="nofollow"&gt;Gradient Boosting&lt;/a&gt; framework.
XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
The same code runs on major distributed environment (Kubernetes, Hadoop, SGE, MPI, Dask) and can solve problems beyond billions of examples.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;© Contributors, 2019. Licensed under an &lt;a href="https://github.com/dmlc/xgboost/blob/master/LICENSE"&gt;Apache-2&lt;/a&gt; license.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribute-to-xgboost" class="anchor" aria-hidden="true" href="#contribute-to-xgboost"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribute to XGBoost&lt;/h2&gt;
&lt;p&gt;XGBoost has been developed and used by a group of active community members. Your help is very valuable to make the package better for everyone.
Checkout the &lt;a href="https://xgboost.ai/community" rel="nofollow"&gt;Community Page&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tianqi Chen and Carlos Guestrin. &lt;a href="http://arxiv.org/abs/1603.02754" rel="nofollow"&gt;XGBoost: A Scalable Tree Boosting System&lt;/a&gt;. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016&lt;/li&gt;
&lt;li&gt;XGBoost originates from research project at University of Washington.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;Become a sponsor and get a logo here. See details at &lt;a href="https://xgboost.ai/sponsors" rel="nofollow"&gt;Sponsoring the XGBoost Project&lt;/a&gt;. The funds are used to defray the cost of continuous integration and testing infrastructure (&lt;a href="https://xgboost-ci.net" rel="nofollow"&gt;https://xgboost-ci.net&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-open-source-collective-sponsors" class="anchor" aria-hidden="true" href="#open-source-collective-sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Open Source Collective sponsors&lt;/h2&gt;
&lt;p&gt;&lt;a href="#backers"&gt;&lt;img src="https://camo.githubusercontent.com/600d7cffb949fdb88bf4a8a357f44a8f5774f080/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f6261636b6572732f62616467652e737667" alt="Backers on Open Collective" data-canonical-src="https://opencollective.com/xgboost/backers/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#sponsors"&gt;&lt;img src="https://camo.githubusercontent.com/05a7e1c0ef4d76131ead9d925b900efcc91fdb9f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f72732f62616467652e737667" alt="Sponsors on Open Collective" data-canonical-src="https://opencollective.com/xgboost/sponsors/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sponsors-1" class="anchor" aria-hidden="true" href="#sponsors-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h3&gt;
&lt;p&gt;[&lt;a href="https://opencollective.com/xgboost#sponsor" rel="nofollow"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/nvidia.jpg" alt="NVIDIA" width="72" height="72" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/1/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5425f4f7a5a11f5cdcd3704e4e2e8a875a5017ca/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f312f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/1/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/2/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a9dfc128ee9badfdc106cbc6f41215702ad91a36/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f322f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/2/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/3/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e3c86cbeb1a6f42478b6cd825353eddacfb397c3/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f332f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/3/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/4/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7ee872020f6386590f14fb89ff6b761bdf0d55e8/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f342f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/4/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/5/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c4d77c183a346544a11775759032df56e454d5ce/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f352f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/5/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/6/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/074f7c3780632690d3b41a7136c074fe3669653f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f362f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/6/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/7/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b54c81a82768315d57bb932eff82383e087abde8/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f372f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/7/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/8/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e997de57b0e32e8e3c7deef3c649fb558a8b0c5f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f382f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/8/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/xgboost/sponsor/9/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1c94c93a3d5c1773dd82de990bc0807bb1334e8b/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f73706f6e736f722f392f6176617461722e737667" data-canonical-src="https://opencollective.com/xgboost/sponsor/9/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-backers" class="anchor" aria-hidden="true" href="#backers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Backers&lt;/h3&gt;
&lt;p&gt;[&lt;a href="https://opencollective.com/xgboost#backer" rel="nofollow"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/xgboost#backers" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d85dbabb0c98d21db761d36f9531ed06a8af68e5/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f7867626f6f73742f6261636b6572732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/xgboost/backers.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-other-sponsors" class="anchor" aria-hidden="true" href="#other-sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other sponsors&lt;/h2&gt;
&lt;p&gt;The sponsors in this list are donating cloud hours in lieu of cash donation.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aws.amazon.com/" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/aws.png" alt="Amazon Web Services" width="72" height="72" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dmlc</author><guid isPermaLink="false">https://github.com/dmlc/xgboost</guid><pubDate>Sun, 09 Feb 2020 00:17:00 GMT</pubDate></item><item><title>tindy2013/subconverter #18 in C++, This week</title><link>https://github.com/tindy2013/subconverter</link><description>&lt;p&gt;&lt;i&gt;Utility to convert between various subscription format&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-subconverter" class="anchor" aria-hidden="true" href="#subconverter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;subconverter&lt;/h1&gt;
&lt;p&gt;Utility to convert between various subscription format.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.com/tindy2013/subconverter" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1be5a935b702523f91ac96f43cda82a93c8ddc5d/68747470733a2f2f7472617669732d63692e636f6d2f74696e6479323031332f737562636f6e7665727465722e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/tindy2013/subconverter.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/tindy2013/subconverter/tags"&gt;&lt;img src="https://camo.githubusercontent.com/7e63413445e476fb83aaf39d2c69c93e366f0dbd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7461672f74696e6479323031332f737562636f6e7665727465722e737667" alt="GitHub tag (latest SemVer)" data-canonical-src="https://img.shields.io/github/tag/tindy2013/subconverter.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/tindy2013/subconverter/releases"&gt;&lt;img src="https://camo.githubusercontent.com/7cdc654f5528d89a9ffdc32bc276ddaa80d0ed3b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f74696e6479323031332f737562636f6e7665727465722e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/github/release/tindy2013/subconverter.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/tindy2013/subconverter/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/9d586e4d2d7cfd4f01b3fb13a6a842ffcb784463/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696e6479323031332f737562636f6e7665727465722e737667" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/tindy2013/subconverter.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tindy2013/subconverter/blob/master/README-docker.md"&gt;Docker README&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/tindy2013/subconverter/blob/master/README-cn.md"&gt;中文文档&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#subconverter"&gt;subconverter&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supported-types"&gt;Supported Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#quick-usage"&gt;Quick Usage&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#access-interface"&gt;Access Interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#description"&gt;Description&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#advanced-usage"&gt;Advanced Usage&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#read-before-continue"&gt;Read Before Continue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#advanced-details"&gt;Advanced Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuration-file"&gt;Configuration File&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#external-configuration-file"&gt;External Configuration File&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#auto-upload"&gt;Auto Upload&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-supported-types" class="anchor" aria-hidden="true" href="#supported-types"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported Types&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th align="center"&gt;As Source&lt;/th&gt;
&lt;th align="center"&gt;As Target&lt;/th&gt;
&lt;th&gt;Target Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Clash&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;clash&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClashR&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;clashr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Quantumult&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;quan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Quantumult X&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;quanx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SS (SIP002)&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;ss&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SS Android&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;sssub&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSD&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;ssd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSR&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;ssr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Surfboard&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;surfboard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Surge 2&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;surge&amp;amp;ver=2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Surge 3&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;surge&amp;amp;ver=3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Surge 4&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;surge&amp;amp;ver=4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V2Ray&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td align="center"&gt;✔&lt;/td&gt;
&lt;td&gt;v2ray&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Notice&lt;/strong&gt;：Shadowrocket users should use &lt;code&gt;ss&lt;/code&gt;, &lt;code&gt;ssr&lt;/code&gt; or &lt;code&gt;v2ray&lt;/code&gt; as target.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-quick-usage" class="anchor" aria-hidden="true" href="#quick-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Usage&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Using default groups and rulesets configuration directly, without changing any settings&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-access-interface" class="anchor" aria-hidden="true" href="#access-interface"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Access Interface&lt;/h3&gt;
&lt;pre lang="TXT"&gt;&lt;code&gt;http://127.0.0.1:25500/sub?target=%TARGET%&amp;amp;url=%URL%&amp;amp;config=%CONFIG%
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th align="center"&gt;Needed&lt;/th&gt;
&lt;th align="left"&gt;Example&lt;/th&gt;
&lt;th&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;target&lt;/td&gt;
&lt;td align="center"&gt;Yes&lt;/td&gt;
&lt;td align="left"&gt;clash&lt;/td&gt;
&lt;td&gt;Target subscription type. Acquire from Target Name in &lt;a href="#Supported_Types"&gt;Supported Types&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;url&lt;/td&gt;
&lt;td align="center"&gt;Yes&lt;/td&gt;
&lt;td align="left"&gt;https%3A%2F%2Fwww.xxx.com&lt;/td&gt;
&lt;td&gt;Subscription to convert. Supports URLs and file paths. Process with &lt;a href="https://www.urlencoder.org/" rel="nofollow"&gt;URLEncode&lt;/a&gt; first.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;config&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;https%3A%2F%2Fwww.xxx.com&lt;/td&gt;
&lt;td&gt;External configuration file path. Supports URLs and file paths. Process with &lt;a href="https://www.urlencoder.org/" rel="nofollow"&gt;URLEncode&lt;/a&gt; first. More examples can be found in &lt;a href="https://github.com/lzdnico/subconverteriniexample"&gt;this&lt;/a&gt; repository. Default is to load configurations from &lt;code&gt;pref.ini&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If you need to merge two or more subscription, you should connect them with '|' before the URLEncode process.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre lang="TXT"&gt;&lt;code&gt;You have 2 subscriptions and you want to merge them and generate a Clash subscription:
1. https://dler.cloud/subscribe/ABCDE?clash=vmess
2. https://rich.cloud/subscribe/ABCDE?clash=vmess

First use '|' to separate 2 subscriptions:
https://dler.cloud/subscribe/ABCDE?clash=vmess|https://rich.cloud/subscribe/ABCDE?clash=vmess

Then process it with URLEncode to get %URL%:
https%3A%2F%2Fdler.cloud%2Fsubscribe%2FABCDE%3Fclash%3Dvmess%7Chttps%3A%2F%2Frich.cloud%2Fsubscribe%2FABCDE%3Fclash%3Dvmess

Then fill %TARGET% and %URL% in Access Interface with actual values:
http://127.0.0.1:25500/sub?target=clash&amp;amp;url=https%3A%2F%2Fdler.cloud%2Fsubscribe%2FABCDE%3Fclash%3Dvmess%7Chttps%3A%2F%2Frich.cloud%2Fsubscribe%2FABCDE%3Fclash%3Dvmess

Finally subscribe this link in Clash and you are done!
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-advanced-usage" class="anchor" aria-hidden="true" href="#advanced-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced Usage&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;If you are not satisfied with the default groups and rulesets, you can try out advanced usage.
Customize more settings in Access Interface and &lt;code&gt;pref.ini&lt;/code&gt; to satisfy various needs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-read-before-continue" class="anchor" aria-hidden="true" href="#read-before-continue"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Read Before Continue&lt;/h3&gt;
&lt;p&gt;It is strongly recommended to read the following articles before continuing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Related to &lt;code&gt;pref.ini&lt;/code&gt;: &lt;a href="https://en.wikipedia.org/wiki/INI_file" rel="nofollow"&gt;INI file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Related to &lt;code&gt;Clash&lt;/code&gt; configurations: &lt;a href="https://en.wikipedia.org/wiki/YAML#Syntax" rel="nofollow"&gt;YAML Syntax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Often needed: &lt;a href="https://github.com/ziishaned/learn-regex/blob/master/README.md"&gt;Learn Regular Expression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;When you want to write an ISSUE: &lt;a href="http://www.catb.org/~esr/faqs/smart-questions.html" rel="nofollow"&gt;How To Ask Questions The Smart Way&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Subconverter only guaranteed to work with default configurations.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-advanced-details" class="anchor" aria-hidden="true" href="#advanced-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced Details&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-access-interface-1" class="anchor" aria-hidden="true" href="#access-interface-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Access Interface&lt;/h4&gt;
&lt;pre lang="txt"&gt;&lt;code&gt;http://127.0.0.1:25500/sub?target=%TARGET%&amp;amp;url=%URL%&amp;amp;emoji=%EMOJI%····
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-description-1" class="anchor" aria-hidden="true" href="#description-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th align="center"&gt;Needed&lt;/th&gt;
&lt;th align="left"&gt;Example&lt;/th&gt;
&lt;th align="left"&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;target&lt;/td&gt;
&lt;td align="center"&gt;Yes&lt;/td&gt;
&lt;td align="left"&gt;quan&lt;/td&gt;
&lt;td align="left"&gt;Target subscription type. Acquire from Target Name in &lt;a href="#Supported_Types"&gt;Supported Types&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;url&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;https%3A%2F%2Fwww.xxx.com&lt;/td&gt;
&lt;td align="left"&gt;Subscription to convert. Supports URLs and file paths. Process with &lt;a href="https://www.urlencoder.org/" rel="nofollow"&gt;URLEncode&lt;/a&gt; first. **Not needed ONLY WHEN YOU HAVE SET &lt;code&gt;default_urls&lt;/code&gt; IN &lt;code&gt;pref.ini&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;config&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;https%3A%2F%2Fwww.xxx.com&lt;/td&gt;
&lt;td align="left"&gt;External configuration file path. Supports URLs and file paths. Process with &lt;a href="https://www.urlencoder.org/" rel="nofollow"&gt;URLEncode&lt;/a&gt; first. More examples can be found in &lt;a href="https://github.com/lzdnico/subconverteriniexample"&gt;this&lt;/a&gt; repository. Default is to load configurations from &lt;code&gt;pref.ini&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;upload&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;true / false&lt;/td&gt;
&lt;td align="left"&gt;Upload generated configuration to &lt;code&gt;Gist repository&lt;/code&gt;. &lt;code&gt;gistconf.ini&lt;/code&gt; must be filled before uploading. Default is &lt;code&gt;false&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;upload_path&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;MySS.yaml&lt;/td&gt;
&lt;td align="left"&gt;File name when uploaded to &lt;code&gt;Gist&lt;/code&gt;. Process with &lt;a href="https://www.urlencoder.org/" rel="nofollow"&gt;URLEncode&lt;/a&gt; first.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;emoji&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;true / false&lt;/td&gt;
&lt;td align="left"&gt;Adding Emoji to node remarks. Default is &lt;code&gt;true&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;group&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;MySS&lt;/td&gt;
&lt;td align="left"&gt;Set a custom group for generated configuration. Often needed in SSD/SSR subscription.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tfo&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;true / false&lt;/td&gt;
&lt;td align="left"&gt;Enable TCP Fast Open for all nodes. Default is &lt;code&gt;false&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;udp&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;true / false&lt;/td&gt;
&lt;td align="left"&gt;Enable UDP for all nodes. Default is &lt;code&gt;false&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scv&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;true / false&lt;/td&gt;
&lt;td align="left"&gt;Enable Skip Cert Verify for all nodes. Default is &lt;code&gt;false&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;true / false&lt;/td&gt;
&lt;td align="left"&gt;Generate Surge Node List or Clash Proxy Provider. Default is &lt;code&gt;false&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sort&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;true / false&lt;/td&gt;
&lt;td align="left"&gt;Sort nodes in alphabetical order. Default is &lt;code&gt;false&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;include&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;See &lt;code&gt;include_remarks&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Exclude nodes which remarks match the following patterns. Supports regular expression. Process with &lt;a href="https://www.urlencoder.org/" rel="nofollow"&gt;URLEncode&lt;/a&gt; first. &lt;strong&gt;WILL OVERRIDE THE SAME SETTING IN &lt;code&gt;pref.ini&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;exclude&lt;/td&gt;
&lt;td align="center"&gt;No&lt;/td&gt;
&lt;td align="left"&gt;See &lt;code&gt;exclude_remarks&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;Only include nodes which remarks match the following patterns. Supports regular expression. Process with &lt;a href="https://www.urlencoder.org/" rel="nofollow"&gt;URLEncode&lt;/a&gt; first. &lt;strong&gt;WILL OVERRIDE THE SAME SETTING IN &lt;code&gt;pref.ini&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre lang="TXT"&gt;&lt;code&gt;You have the following subscription: `https://dler.cloud/subscribe/ABCDE?clash=vmess`, and you want to convert it to Surge 4 subscription, set UDP and TFO to enabled,
add Emoji to node remarks and filter out unused nodes named "剩余流量：1024G" and "官网地址：dler.cloud".

First find all needed arguments: 
target=surge, ver=4,  tfo=true, udp=true, emoji=true, exclude=(流量|官网)
url=https://dler.cloud/subscribe/ABCDE?clash=vmess

Then process any argument that requires URLEncode: 
exclude=%28%E6%B5%81%E9%87%8F%7C%E5%AE%98%E7%BD%91%29
url=https%3A%2F%2Fdler.cloud%2Fsubscribe%2FABCDE%3Fclash%3Dvmess

Then merge everything into a single URL: 
http://127.0.0.1:25500/sub?surge&amp;amp;ver=4&amp;amp;tfo=true&amp;amp;udp=true&amp;amp;emoji=true&amp;amp;exclude=%28%E6%B5%81%E9%87%8F%7C%E5%AE%98%E7%BD%91%29&amp;amp;url=https%3A%2F%2Fdler.cloud%2Fsubscribe%2FABCDE%3Fclash%3Dvmess

Finally subscribe this link in Surge and you are done!
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-configuration-file" class="anchor" aria-hidden="true" href="#configuration-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration File&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Check comments inside &lt;a href="https://github.com/tindy2013/subconverter/blob/master/base/pref.ini"&gt;pref.ini&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-external-configuration-file" class="anchor" aria-hidden="true" href="#external-configuration-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;External Configuration File&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Most settings works the same as the ones with the same name inside &lt;code&gt;pref.ini&lt;/code&gt;, you can check &lt;a href="https://github.com/tindy2013/subconverter/blob/master/base/config/example_external_config.ini"&gt;the example configuration file&lt;/a&gt; and comments inside &lt;a href="https://github.com/tindy2013/subconverter/blob/master/base/pref.ini"&gt;pref.ini&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Any setting defined in the external configuration file will &lt;strong&gt;override&lt;/strong&gt; the ones from &lt;code&gt;pref.ini&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, if you have the following lines inside the external configuration file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;emoji=(流量|时间|应急),🏳️‍🌈
emoji=阿根廷,🇦🇷
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then instead of the Emojis defined in &lt;code&gt;pref.ini&lt;/code&gt;, the program will only use the newly defined ones.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-auto-upload" class="anchor" aria-hidden="true" href="#auto-upload"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Auto Upload&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Upload Gist automatically&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Add a &lt;a href="https://github.com/settings/tokens/new"&gt;Personal Access Token&lt;/a&gt; into &lt;a href="./gistconf.ini"&gt;gistconf.ini&lt;/a&gt; in the root directory, then add &lt;code&gt;&amp;amp;upload=true&lt;/code&gt; to the local subscription link, then when you access this link, the program will automatically update the content to Gist repository.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;div class="highlight highlight-source-ini"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;[common]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;;&lt;/span&gt;uncomment the following line and enter your token to enable upload function&lt;/span&gt;
&lt;span class="pl-k"&gt;token&lt;/span&gt; = xxxxxxxxxxxxxxxxxxxxxxxx(Your Personal Access Token)&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tindy2013</author><guid isPermaLink="false">https://github.com/tindy2013/subconverter</guid><pubDate>Sun, 09 Feb 2020 00:18:00 GMT</pubDate></item><item><title>dusty-nv/jetson-inference #19 in C++, This week</title><link>https://github.com/dusty-nv/jetson-inference</link><description>&lt;p&gt;&lt;i&gt;Guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-header.jpg"&gt;&lt;img src="https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-header.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deploying-deep-learning" class="anchor" aria-hidden="true" href="#deploying-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploying Deep Learning&lt;/h1&gt;
&lt;p&gt;Welcome to our instructional guide for inference and realtime &lt;a href="#api-reference"&gt;DNN vision&lt;/a&gt; library for NVIDIA &lt;strong&gt;&lt;a href="http://www.nvidia.com/object/embedded-systems.html" rel="nofollow"&gt;Jetson Nano/TX1/TX2/Xavier&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This repo uses NVIDIA &lt;strong&gt;&lt;a href="https://developer.nvidia.com/tensorrt" rel="nofollow"&gt;TensorRT&lt;/a&gt;&lt;/strong&gt; for efficiently deploying neural networks onto the embedded Jetson platform, improving performance and power efficiency using graph optimizations, kernel fusion, and FP16/INT8 precision.&lt;/p&gt;
&lt;p&gt;Vision primitives, such as &lt;a href="c/imageNet.h"&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt; for image recognition, &lt;a href="c/detectNet.h"&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt; for object localization, and &lt;a href="c/segNet.h"&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt; for semantic segmentation, inherit from the shared &lt;a href="c/tensorNet.h"&gt;&lt;code&gt;tensorNet&lt;/code&gt;&lt;/a&gt; object.  Examples are provided for streaming from live camera feed and processing images.  See the &lt;strong&gt;&lt;a href="#api-reference"&gt;API Reference&lt;/a&gt;&lt;/strong&gt; section for detailed reference documentation of the C++ and Python libraries.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-primitives.png"&gt;&lt;img src="https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-primitives.png" width="800" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are multiple tracks of the tutorial that you can choose to follow, including &lt;a href="#hello-ai-world"&gt;Hello AI World&lt;/a&gt; for running inference and transfer learning onboard your Jetson, or the full &lt;a href="#two-days-to-a-demo-digits"&gt;Two Days to a Demo&lt;/a&gt; tutorial for training on a PC or server with DIGITS.&lt;/p&gt;
&lt;p&gt;It's recommended to walk through the Hello AI World module first to familiarize yourself with machine learning and inference with TensorRT, before proceeding to training in the cloud with DIGITS.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hello-ai-world"&gt;Hello AI World&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#two-days-to-a-demo-digits"&gt;Two Days to a Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#api-reference"&gt;API Reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#code-examples"&gt;Code Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pre-trained-models"&gt;Pre-Trained Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommended-system-requirements"&gt;System Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#extra-resources"&gt;Extra Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;gt;   Jetson Nano Developer Kit and JetPack 4.3 is now supported in the repo. &lt;br&gt;
&amp;gt;   See our latest technical blog including benchmarks, &lt;a href="https://devblogs.nvidia.com/jetson-nano-ai-computing/" rel="nofollow"&gt;&lt;code&gt;Jetson Nano Brings AI Computing to Everyone&lt;/code&gt;&lt;/a&gt;. &lt;br&gt;
&amp;gt;   Hello AI World now supports Python and onboard training with PyTorch!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-hello-ai-world" class="anchor" aria-hidden="true" href="#hello-ai-world"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hello AI World&lt;/h2&gt;
&lt;p&gt;Hello AI World can be run completely onboard your Jetson, including inferencing with TensorRT and transfer learning with PyTorch.  The inference portion of Hello AI World - which includes coding your own image classification application for C++ or Python, object detection, and live camera demos - can be run on your Jetson in roughly two hours or less, while transfer learning is best left to leave running overnight.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/jetpack-setup-2.md"&gt;Setting up Jetson with JetPack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/building-repo-2.md"&gt;Building the Project from Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-console-2.md"&gt;Classifying Images with ImageNet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/imagenet-console-2.md#using-the-console-program-on-jetson"&gt;Using the Console Program on Jetson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-example-python-2.md"&gt;Coding Your Own Image Recognition Program (Python)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-example-2.md"&gt;Coding Your Own Image Recognition Program (C++)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-camera-2.md"&gt;Running the Live Camera Recognition Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-console-2.md"&gt;Locating Objects with DetectNet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/detectnet-console-2.md#detecting-objects-from-the-command-line"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-camera-2.md"&gt;Running the Live Camera Detection Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-example-2.md"&gt;Coding Your Own Object Detection Program&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-console-2.md"&gt;Semantic Segmentation with SegNet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/segnet-console-2.md#segmenting-images-from-the-command-line"&gt;Segmenting Images from the Command Line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-camera-2.md"&gt;Running the Live Camera Segmentation Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/pytorch-transfer-learning.md"&gt;Transfer Learning with PyTorch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/pytorch-cat-dog.md"&gt;Re-training on the Cat/Dog Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/pytorch-plants.md"&gt;Re-training on the PlantCLEF Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/pytorch-collect.md"&gt;Collecting your own Datasets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-two-days-to-a-demo-digits" class="anchor" aria-hidden="true" href="#two-days-to-a-demo-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Two Days to a Demo (DIGITS)&lt;/h2&gt;
&lt;p&gt;The full tutorial includes training in the cloud or PC, and inference on the Jetson with TensorRT, and can take roughly two days or more depending on system setup, downloading the datasets, and the training speed of your GPU.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/digits-workflow.md"&gt;DIGITS Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/digits-setup.md"&gt;DIGITS System Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/jetpack-setup.md"&gt;Setting up Jetson with JetPack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/building-repo.md"&gt;Building the Project from Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-console.md"&gt;Classifying Images with ImageNet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/imagenet-console.md#using-the-console-program-on-jetson"&gt;Using the Console Program on Jetson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-example.md"&gt;Coding Your Own Image Recognition Program&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-camera.md"&gt;Running the Live Camera Recognition Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-training.md"&gt;Re-Training the Network with DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-training.md#downloading-image-recognition-dataset"&gt;Downloading Image Recognition Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-training.md#customizing-the-object-classes"&gt;Customizing the Object Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-training.md#importing-classification-dataset-into-digits"&gt;Importing Classification Dataset into DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-training.md#creating-image-classification-model-with-digits"&gt;Creating Image Classification Model with DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-training.md#testing-classification-model-in-digits"&gt;Testing Classification Model in DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-snapshot.md"&gt;Downloading Model Snapshot to Jetson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-custom.md"&gt;Loading Custom Models on Jetson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-training.md"&gt;Locating Objects with DetectNet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/detectnet-training.md#detection-data-formatting-in-digits"&gt;Detection Data Formatting in DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-training.md#downloading-the-detection-dataset"&gt;Downloading the Detection Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-training.md#importing-the-detection-dataset-into-digits"&gt;Importing the Detection Dataset into DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-training.md#creating-detectnet-model-with-digits"&gt;Creating DetectNet Model with DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-training.md#testing-detectnet-model-inference-in-digits"&gt;Testing DetectNet Model Inference in DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-snapshot.md"&gt;Downloading the Detection Model to Jetson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-snapshot.md#detectnet-patches-for-tensorrt"&gt;DetectNet Patches for TensorRT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-console.md"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-console.md#multi-class-object-detection-models"&gt;Multi-class Object Detection Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/detectnet-camera.md"&gt;Running the Live Camera Detection Demo on Jetson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-dataset.md"&gt;Semantic Segmentation with SegNet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/segnet-dataset.md#downloading-aerial-drone-dataset"&gt;Downloading Aerial Drone Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-dataset.md#importing-the-aerial-dataset-into-digits"&gt;Importing the Aerial Dataset into DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-pretrained.md"&gt;Generating Pretrained FCN-Alexnet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-training.md"&gt;Training FCN-Alexnet with DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-training.md#testing-inference-model-in-digits"&gt;Testing Inference Model in DIGITS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-patches.md"&gt;FCN-Alexnet Patches for TensorRT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/segnet-console.md"&gt;Running Segmentation Models on Jetson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-api-reference" class="anchor" aria-hidden="true" href="#api-reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;API Reference&lt;/h2&gt;
&lt;p&gt;Below are links to reference documentation for the &lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/index.html" rel="nofollow"&gt;C++&lt;/a&gt; and &lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/python/jetson.html" rel="nofollow"&gt;Python&lt;/a&gt; libraries from the repo:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-jetson-inference" class="anchor" aria-hidden="true" href="#jetson-inference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;jetson-inference&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/group__deepVision.html" rel="nofollow"&gt;C++&lt;/a&gt;&lt;/th&gt;
&lt;th&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/python/jetson.inference.html" rel="nofollow"&gt;Python&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Image Recognition&lt;/td&gt;
&lt;td&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/classimageNet.html" rel="nofollow"&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/python/jetson.inference.html#imageNet" rel="nofollow"&gt;&lt;code&gt;imageNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Object Detection&lt;/td&gt;
&lt;td&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/classdetectNet.html" rel="nofollow"&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/python/jetson.inference.html#detectNet" rel="nofollow"&gt;&lt;code&gt;detectNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Segmentation&lt;/td&gt;
&lt;td&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/classsegNet.html" rel="nofollow"&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/pytorch/docs/html/python/jetson.inference.html#segNet" rel="nofollow"&gt;&lt;code&gt;segNet&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-jetson-utils" class="anchor" aria-hidden="true" href="#jetson-utils"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;jetson-utils&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/group__util.html" rel="nofollow"&gt;C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rawgit.com/dusty-nv/jetson-inference/python/docs/html/python/jetson.utils.html" rel="nofollow"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These libraries are able to be used in external projects by linking to &lt;code&gt;libjetson-inference&lt;/code&gt; and &lt;code&gt;libjetson-utils&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-code-examples" class="anchor" aria-hidden="true" href="#code-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code Examples&lt;/h2&gt;
&lt;p&gt;Introductory code walkthroughs of using the library are covered during these steps of the Hello AI World tutorial:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/imagenet-example-python-2.md"&gt;Coding Your Own Image Recognition Program (Python)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/imagenet-example-2.md"&gt;Coding Your Own Image Recognition Program (C++)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional C++ and Python samples for running the networks on static images and live camera streams can be found here:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Images&lt;/th&gt;
&lt;th&gt;Camera&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;C++ (&lt;a href="examples/"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt;)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   Image Recognition&lt;/td&gt;
&lt;td&gt;&lt;a href="examples/imagenet-console/imagenet-console.cpp"&gt;&lt;code&gt;imagenet-console&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="examples/imagenet-camera/imagenet-camera.cpp"&gt;&lt;code&gt;imagenet-camera&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   Object Detection&lt;/td&gt;
&lt;td&gt;&lt;a href="examples/detectnet-console/detectnet-console.cpp"&gt;&lt;code&gt;detectnet-console&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="examples/detectnet-camera/detectnet-camera.cpp"&gt;&lt;code&gt;detectnet-camera&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   Segmentation&lt;/td&gt;
&lt;td&gt;&lt;a href="examples/segnet-console/segnet-console.cpp"&gt;&lt;code&gt;segnet-console&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="examples/segnet-camera/segnet-camera.cpp"&gt;&lt;code&gt;segnet-camera&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Python (&lt;a href="python/examples/"&gt;&lt;code&gt;python/examples&lt;/code&gt;&lt;/a&gt;)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   Image Recognition&lt;/td&gt;
&lt;td&gt;&lt;a href="python/examples/imagenet-console.py"&gt;&lt;code&gt;imagenet-console.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="python/examples/imagenet-camera.py"&gt;&lt;code&gt;imagenet-camera.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   Object Detection&lt;/td&gt;
&lt;td&gt;&lt;a href="python/examples/detectnet-console.py"&gt;&lt;code&gt;detectnet-console.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="python/examples/detectnet-camera.py"&gt;&lt;code&gt;detectnet-camera.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   Segmentation&lt;/td&gt;
&lt;td&gt;&lt;a href="python/examples/segnet-console.py"&gt;&lt;code&gt;segnet-console.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="python/examples/segnet-camera.py"&gt;&lt;code&gt;segnet-camera.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt;:  for working with numpy arrays, see &lt;a href="https://github.com/dusty-nv/jetson-utils/blob/master/python/examples/cuda-from-numpy.py"&gt;&lt;code&gt;cuda-from-numpy.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/dusty-nv/jetson-utils/blob/master/python/examples/cuda-to-numpy.py"&gt;&lt;code&gt;cuda-to-numpy.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These examples will automatically be compiled while &lt;a href="docs/building-repo-2.md"&gt;Building the Project from Source&lt;/a&gt;, and are able to run the pre-trained models listed below in addition to custom models provided by the user.  Launch each example with &lt;code&gt;--help&lt;/code&gt; for usage info.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Trained Models&lt;/h2&gt;
&lt;p&gt;The project comes with a number of pre-trained models that are available through the &lt;a href="docs/building-repo-2.md#downloading-models"&gt;&lt;strong&gt;Model Downloader&lt;/strong&gt;&lt;/a&gt; tool:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-image-recognition" class="anchor" aria-hidden="true" href="#image-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Recognition&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Network&lt;/th&gt;
&lt;th&gt;CLI argument&lt;/th&gt;
&lt;th&gt;NetworkType enum&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AlexNet&lt;/td&gt;
&lt;td&gt;&lt;code&gt;alexnet&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ALEXNET&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GoogleNet&lt;/td&gt;
&lt;td&gt;&lt;code&gt;googlenet&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;GOOGLENET&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GoogleNet-12&lt;/td&gt;
&lt;td&gt;&lt;code&gt;googlenet-12&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;GOOGLENET_12&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-18&lt;/td&gt;
&lt;td&gt;&lt;code&gt;resnet-18&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;RESNET_18&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-50&lt;/td&gt;
&lt;td&gt;&lt;code&gt;resnet-50&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;RESNET_50&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-101&lt;/td&gt;
&lt;td&gt;&lt;code&gt;resnet-101&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;RESNET_101&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-152&lt;/td&gt;
&lt;td&gt;&lt;code&gt;resnet-152&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;RESNET_152&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VGG-16&lt;/td&gt;
&lt;td&gt;&lt;code&gt;vgg-16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;VGG-16&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VGG-19&lt;/td&gt;
&lt;td&gt;&lt;code&gt;vgg-19&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;VGG-19&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Inception-v4&lt;/td&gt;
&lt;td&gt;&lt;code&gt;inception-v4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;INCEPTION_V4&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-object-detection" class="anchor" aria-hidden="true" href="#object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object Detection&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Network&lt;/th&gt;
&lt;th&gt;CLI argument&lt;/th&gt;
&lt;th&gt;NetworkType enum&lt;/th&gt;
&lt;th&gt;Object classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SSD-Mobilenet-v1&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ssd-mobilenet-v1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;SSD_MOBILENET_V1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;91 (&lt;a href="data/networks/ssd_coco_labels.txt"&gt;COCO classes&lt;/a&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSD-Mobilenet-v2&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ssd-mobilenet-v2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;SSD_MOBILENET_V2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;91 (&lt;a href="data/networks/ssd_coco_labels.txt"&gt;COCO classes&lt;/a&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SSD-Inception-v2&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ssd-inception-v2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;SSD_INCEPTION_V2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;91 (&lt;a href="data/networks/ssd_coco_labels.txt"&gt;COCO classes&lt;/a&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DetectNet-COCO-Dog&lt;/td&gt;
&lt;td&gt;&lt;code&gt;coco-dog&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;COCO_DOG&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;dogs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DetectNet-COCO-Bottle&lt;/td&gt;
&lt;td&gt;&lt;code&gt;coco-bottle&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;COCO_BOTTLE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bottles&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DetectNet-COCO-Chair&lt;/td&gt;
&lt;td&gt;&lt;code&gt;coco-chair&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;COCO_CHAIR&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;chairs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DetectNet-COCO-Airplane&lt;/td&gt;
&lt;td&gt;&lt;code&gt;coco-airplane&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;COCO_AIRPLANE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;airplanes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ped-100&lt;/td&gt;
&lt;td&gt;&lt;code&gt;pednet&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;PEDNET&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pedestrians&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;multiped-500&lt;/td&gt;
&lt;td&gt;&lt;code&gt;multiped&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;PEDNET_MULTI&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pedestrians, luggage&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;facenet-120&lt;/td&gt;
&lt;td&gt;&lt;code&gt;facenet&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FACENET&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;faces&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-semantic-segmentation" class="anchor" aria-hidden="true" href="#semantic-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semantic Segmentation&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Resolution&lt;/th&gt;
&lt;th&gt;CLI Argument&lt;/th&gt;
&lt;th align="center"&gt;Accuracy&lt;/th&gt;
&lt;th align="center"&gt;Jetson Nano&lt;/th&gt;
&lt;th align="center"&gt;Jetson Xavier&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;512x256&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-512x256&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;83.3%&lt;/td&gt;
&lt;td align="center"&gt;48 FPS&lt;/td&gt;
&lt;td align="center"&gt;480 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;1024x512&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-1024x512&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;87.3%&lt;/td&gt;
&lt;td align="center"&gt;12 FPS&lt;/td&gt;
&lt;td align="center"&gt;175 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;2048x1024&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-cityscapes-2048x1024&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;89.6%&lt;/td&gt;
&lt;td align="center"&gt;3 FPS&lt;/td&gt;
&lt;td align="center"&gt;47 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://deepscene.cs.uni-freiburg.de/" rel="nofollow"&gt;DeepScene&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;576x320&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-deepscene-576x320&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;96.4%&lt;/td&gt;
&lt;td align="center"&gt;26 FPS&lt;/td&gt;
&lt;td align="center"&gt;360 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://deepscene.cs.uni-freiburg.de/" rel="nofollow"&gt;DeepScene&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;864x480&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-deepscene-864x480&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;96.9%&lt;/td&gt;
&lt;td align="center"&gt;14 FPS&lt;/td&gt;
&lt;td align="center"&gt;190 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://lv-mhp.github.io/" rel="nofollow"&gt;Multi-Human&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;512x320&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-mhp-512x320&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;86.5%&lt;/td&gt;
&lt;td align="center"&gt;34 FPS&lt;/td&gt;
&lt;td align="center"&gt;370 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://lv-mhp.github.io/" rel="nofollow"&gt;Multi-Human&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;640x360&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-mhp-512x320&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;87.1%&lt;/td&gt;
&lt;td align="center"&gt;23 FPS&lt;/td&gt;
&lt;td align="center"&gt;325 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="nofollow"&gt;Pascal VOC&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;320x320&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-voc-320x320&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;85.9%&lt;/td&gt;
&lt;td align="center"&gt;45 FPS&lt;/td&gt;
&lt;td align="center"&gt;508 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="nofollow"&gt;Pascal VOC&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;512x320&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-voc-512x320&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;88.5%&lt;/td&gt;
&lt;td align="center"&gt;34 FPS&lt;/td&gt;
&lt;td align="center"&gt;375 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://rgbd.cs.princeton.edu/" rel="nofollow"&gt;SUN RGB-D&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;512x400&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-sun-512x400&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;64.3%&lt;/td&gt;
&lt;td align="center"&gt;28 FPS&lt;/td&gt;
&lt;td align="center"&gt;340 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://rgbd.cs.princeton.edu/" rel="nofollow"&gt;SUN RGB-D&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;640x512&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-resnet18-sun-640x512&lt;/code&gt;&lt;/td&gt;
&lt;td align="center"&gt;65.1%&lt;/td&gt;
&lt;td align="center"&gt;17 FPS&lt;/td&gt;
&lt;td align="center"&gt;224 FPS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;If the resolution is omitted from the CLI argument, the lowest resolution model is loaded&lt;/li&gt;
&lt;li&gt;Accuracy indicates the pixel classification accuracy across the model's validation dataset&lt;/li&gt;
&lt;li&gt;Performance is measured for GPU FP16 mode with JetPack 4.2.1, &lt;code&gt;nvpmodel 0&lt;/code&gt; (MAX-N)&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;Legacy Segmentation Models&lt;/summary&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Network&lt;/th&gt;
&lt;th&gt;CLI Argument&lt;/th&gt;
&lt;th&gt;NetworkType enum&lt;/th&gt;
&lt;th&gt;Classes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Cityscapes (2048x2048)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-alexnet-cityscapes-hd&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FCN_ALEXNET_CITYSCAPES_HD&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cityscapes (1024x1024)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-alexnet-cityscapes-sd&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FCN_ALEXNET_CITYSCAPES_SD&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pascal VOC (500x356)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-alexnet-pascal-voc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FCN_ALEXNET_PASCAL_VOC&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Synthia (CVPR16)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-cvpr&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_CVPR&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Synthia (Summer-HD)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-summer-hd&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_SUMMER_HD&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Synthia (Summer-SD)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-alexnet-synthia-summer-sd&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FCN_ALEXNET_SYNTHIA_SUMMER_SD&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Aerial-FPV (1280x720)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;fcn-alexnet-aerial-fpv-720p&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;FCN_ALEXNET_AERIAL_FPV_720p&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/details&gt;
&lt;h2&gt;&lt;a id="user-content-recommended-system-requirements" class="anchor" aria-hidden="true" href="#recommended-system-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommended System Requirements&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Training GPU:&lt;/strong&gt;  Maxwell, Pascal, Volta, or Turing-based GPU (ideally with at least 6GB video memory)&lt;br&gt;
                        optionally, AWS P2/P3 instance or Microsoft Azure N-series&lt;br&gt;
                        Ubuntu 16.04/18.04 x86_64&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deployment:&lt;/strong&gt;      Jetson Nano Developer Kit with JetPack 4.2 or newer (Ubuntu 18.04 aarch64).&lt;br&gt;
                        Jetson Xavier Developer Kit with JetPack 4.0 or newer (Ubuntu 18.04 aarch64)&lt;br&gt;
                        Jetson TX2 Developer Kit with JetPack 3.0 or newer (Ubuntu 16.04 aarch64).&lt;br&gt;
                        Jetson TX1 Developer Kit with JetPack 2.3 or newer (Ubuntu 16.04 aarch64).&lt;/p&gt;
&lt;p&gt;Note that TensorRT samples from the repo are intended for deployment onboard Jetson, however when cuDNN and TensorRT have been installed on the host side, the TensorRT samples in the repo can be compiled for PC.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-extra-resources" class="anchor" aria-hidden="true" href="#extra-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extra Resources&lt;/h2&gt;
&lt;p&gt;In this area, links and resources for deep learning are listed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.github.com/dusty-nv/ros_deep_learning"&gt;ros_deep_learning&lt;/a&gt; - TensorRT inference ROS nodes&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NVIDIA-AI-IOT"&gt;NVIDIA AI IoT&lt;/a&gt; - NVIDIA Jetson GitHub repositories&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.eLinux.org/Jetson" rel="nofollow"&gt;Jetson eLinux Wiki&lt;/a&gt; - Jetson eLinux Wiki&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-legacy-links" class="anchor" aria-hidden="true" href="#legacy-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Legacy Links&lt;/h2&gt;
&lt;details open=""&gt;
&lt;summary&gt;Since the documentation has been re-organized, below are links mapping the previous content to the new locations.&lt;/summary&gt;
       (click on the arrow above to hide this section)
&lt;h3&gt;&lt;a id="user-content-digits-workflow" class="anchor" aria-hidden="true" href="#digits-workflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DIGITS Workflow&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/digits-workflow.md"&gt;DIGITS Workflow&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-system-setup" class="anchor" aria-hidden="true" href="#system-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;System Setup&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-running-jetpack-on-the-host" class="anchor" aria-hidden="true" href="#running-jetpack-on-the-host"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running JetPack on the Host&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/jetpack-setup.md"&gt;JetPack Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-ubuntu-on-the-host" class="anchor" aria-hidden="true" href="#installing-ubuntu-on-the-host"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Ubuntu on the Host&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md#installing-ubuntu-on-the-host"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-setting-up-host-training-pc-with-ngc-container" class="anchor" aria-hidden="true" href="#setting-up-host-training-pc-with-ngc-container"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setting up host training PC with NGC container&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md#setting-up-host-training-pc-with-ngc-container"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-the-nvidia-driver" class="anchor" aria-hidden="true" href="#installing-the-nvidia-driver"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing the NVIDIA driver&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md#installing-the-nvidia-driver"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-docker" class="anchor" aria-hidden="true" href="#installing-docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing Docker&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md#installing-docker"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-ngc-sign-up" class="anchor" aria-hidden="true" href="#ngc-sign-up"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NGC Sign-up&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md#ngc-sign-up"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-setting-up-data-and-job-directories" class="anchor" aria-hidden="true" href="#setting-up-data-and-job-directories"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setting up data and job directories&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md#setting-up-data-and-job-directories"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-starting-digits-container" class="anchor" aria-hidden="true" href="#starting-digits-container"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting DIGITS container&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-setup.md#starting-digits-container"&gt;DIGITS Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-natively-setting-up-digits-on-the-host" class="anchor" aria-hidden="true" href="#natively-setting-up-digits-on-the-host"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natively setting up DIGITS on the Host&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-native.md"&gt;DIGITS Native Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-nvidia-driver-on-the-host" class="anchor" aria-hidden="true" href="#installing-nvidia-driver-on-the-host"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing NVIDIA Driver on the Host&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-native.md#installing-nvidia-driver-on-the-host"&gt;DIGITS Native Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-cudnn-on-the-host" class="anchor" aria-hidden="true" href="#installing-cudnn-on-the-host"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing cuDNN on the Host&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-native.md#installing-cudnn-on-the-host"&gt;DIGITS Native Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-nvcaffe-on-the-host" class="anchor" aria-hidden="true" href="#installing-nvcaffe-on-the-host"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing NVcaffe on the Host&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-native.md#installing-nvcaffe-on-the-host"&gt;DIGITS Native Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-digits-on-the-host" class="anchor" aria-hidden="true" href="#installing-digits-on-the-host"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing DIGITS on the Host&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-native.md#installing-digits-on-the-host"&gt;DIGITS Native Setup&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-starting-the-digits-server" class="anchor" aria-hidden="true" href="#starting-the-digits-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting the DIGITS Server&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/digits-native.md#starting-the-digits-server"&gt;DIGITS Native Setup&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-building-from-source-on-jetson" class="anchor" aria-hidden="true" href="#building-from-source-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building from Source on Jetson&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/building-repo.md"&gt;Building the Repo from Source&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-cloning-the-repo" class="anchor" aria-hidden="true" href="#cloning-the-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cloning the Repo&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/building-repo.md#cloning-the-repo"&gt;Building the Repo from Source&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-configuring-with-cmake" class="anchor" aria-hidden="true" href="#configuring-with-cmake"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuring with CMake&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/building-repo.md#configuring-with-cmake"&gt;Building the Repo from Source&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-compiling-the-project" class="anchor" aria-hidden="true" href="#compiling-the-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiling the Project&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/building-repo.md#compiling-the-project"&gt;Building the Repo from Source&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-digging-into-the-code" class="anchor" aria-hidden="true" href="#digging-into-the-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Digging Into the Code&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/building-repo.md#digging-into-the-code"&gt;Building the Repo from Source&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-classifying-images-with-imagenet" class="anchor" aria-hidden="true" href="#classifying-images-with-imagenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classifying Images with ImageNet&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-console.md"&gt;Classifying Images with ImageNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-using-the-console-program-on-jetson" class="anchor" aria-hidden="true" href="#using-the-console-program-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using the Console Program on Jetson&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-console.md#using-the-console-program-on-jetson"&gt;Classifying Images with ImageNet&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-running-the-live-camera-recognition-demo" class="anchor" aria-hidden="true" href="#running-the-live-camera-recognition-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running the Live Camera Recognition Demo&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-camera.md"&gt;Running the Live Camera Recognition Demo&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-re-training-the-network-with-digits" class="anchor" aria-hidden="true" href="#re-training-the-network-with-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Re-training the Network with DIGITS&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-training.md"&gt;Re-Training the Recognition Network&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-downloading-image-recognition-dataset" class="anchor" aria-hidden="true" href="#downloading-image-recognition-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading Image Recognition Dataset&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-training.md#downloading-image-recognition-dataset"&gt;Re-Training the Recognition Network&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-customizing-the-object-classes" class="anchor" aria-hidden="true" href="#customizing-the-object-classes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Customizing the Object Classes&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-training.md#customizing-the-object-classes"&gt;Re-Training the Recognition Network&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-importing-classification-dataset-into-digits" class="anchor" aria-hidden="true" href="#importing-classification-dataset-into-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Importing Classification Dataset into DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-training.md#importing-classification-dataset-into-digits"&gt;Re-Training the Recognition Network&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-creating-image-classification-model-with-digits" class="anchor" aria-hidden="true" href="#creating-image-classification-model-with-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating Image Classification Model with DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-training.md#creating-image-classification-model-with-digits"&gt;Re-Training the Recognition Network&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-testing-classification-model-in-digits" class="anchor" aria-hidden="true" href="#testing-classification-model-in-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing Classification Model in DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-training.md#testing-classification-model-in-digits"&gt;Re-Training the Recognition Network&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-downloading-model-snapshot-to-jetson" class="anchor" aria-hidden="true" href="#downloading-model-snapshot-to-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading Model Snapshot to Jetson&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-snapshot.md"&gt;Downloading Model Snapshots to Jetson&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-loading-custom-models-on-jetson" class="anchor" aria-hidden="true" href="#loading-custom-models-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Loading Custom Models on Jetson&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/imagenet-custom.md"&gt;Loading Custom Models on Jetson&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-locating-object-coordinates-using-detectnet" class="anchor" aria-hidden="true" href="#locating-object-coordinates-using-detectnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Locating Object Coordinates using DetectNet&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-detection-data-formatting-in-digits" class="anchor" aria-hidden="true" href="#detection-data-formatting-in-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Detection Data Formatting in DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#detection-data-formatting-in-digits"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-downloading-the-detection-dataset" class="anchor" aria-hidden="true" href="#downloading-the-detection-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading the Detection Dataset&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#downloading-the-detection-dataset"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-importing-the-detection-dataset-into-digits" class="anchor" aria-hidden="true" href="#importing-the-detection-dataset-into-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Importing the Detection Dataset into DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#importing-the-detection-dataset-into-digits"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-creating-detectnet-model-with-digits" class="anchor" aria-hidden="true" href="#creating-detectnet-model-with-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating DetectNet Model with DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#creating-detectnet-model-with-digits"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-selecting-detectnet-batch-size" class="anchor" aria-hidden="true" href="#selecting-detectnet-batch-size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Selecting DetectNet Batch Size&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#selecting-detectnet-batch-size"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-specifying-the-detectnet-prototxt" class="anchor" aria-hidden="true" href="#specifying-the-detectnet-prototxt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Specifying the DetectNet Prototxt&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#specifying-the-detectnet-prototxt"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-training-the-model-with-pretrained-googlenet" class="anchor" aria-hidden="true" href="#training-the-model-with-pretrained-googlenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training the Model with Pretrained Googlenet&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#training-the-model-with-pretrained-googlenet"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-testing-detectnet-model-inference-in-digits" class="anchor" aria-hidden="true" href="#testing-detectnet-model-inference-in-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing DetectNet Model Inference in DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-training.md#testing-detectnet-model-inference-in-digits"&gt;Locating Object Coordinates using DetectNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-downloading-the-model-snapshot-to-jetson" class="anchor" aria-hidden="true" href="#downloading-the-model-snapshot-to-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading the Model Snapshot to Jetson&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-snapshot.md"&gt;Downloading the Detection Model to Jetson&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-detectnet-patches-for-tensorrt" class="anchor" aria-hidden="true" href="#detectnet-patches-for-tensorrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DetectNet Patches for TensorRT&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-snapshot.md#detectnet-patches-for-tensorrt"&gt;Downloading the Detection Model to Jetson&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-processing-images-from-the-command-line-on-jetson" class="anchor" aria-hidden="true" href="#processing-images-from-the-command-line-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Processing Images from the Command Line on Jetson&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-console.md"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-launching-with-a-pretrained-model" class="anchor" aria-hidden="true" href="#launching-with-a-pretrained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Launching With a Pretrained Model&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-console.md#launching-with-a-pretrained-model"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-pretrained-detectnet-models-available" class="anchor" aria-hidden="true" href="#pretrained-detectnet-models-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained DetectNet Models Available&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-console.md#pretrained-detectnet-models-available"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-running-other-ms-coco-models-on-jetson" class="anchor" aria-hidden="true" href="#running-other-ms-coco-models-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Other MS-COCO Models on Jetson&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-console.md#running-other-ms-coco-models-on-jetson"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-running-pedestrian-models-on-jetson" class="anchor" aria-hidden="true" href="#running-pedestrian-models-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Pedestrian Models on Jetson&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-console.md#running-pedestrian-models-on-jetson"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-multi-class-object-detection-models" class="anchor" aria-hidden="true" href="#multi-class-object-detection-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-class Object Detection Models&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-console.md#multi-class-object-detection-models"&gt;Detecting Objects from the Command Line&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-running-the-live-camera-detection-demo-on-jetson" class="anchor" aria-hidden="true" href="#running-the-live-camera-detection-demo-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running the Live Camera Detection Demo on Jetson&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/detectnet-camera.md"&gt;Running the Live Camera Detection Demo&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-segmentation-with-segnet" class="anchor" aria-hidden="true" href="#image-segmentation-with-segnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Segmentation with SegNet&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/segnet-dataset.md"&gt;Semantic Segmentation with SegNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-downloading-aerial-drone-dataset" class="anchor" aria-hidden="true" href="#downloading-aerial-drone-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading Aerial Drone Dataset&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/segnet-dataset.md#downloading-aerial-drone-dataset"&gt;Semantic Segmentation with SegNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-importing-the-aerial-dataset-into-digits" class="anchor" aria-hidden="true" href="#importing-the-aerial-dataset-into-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Importing the Aerial Dataset into DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/segnet-dataset.md#importing-the-aerial-dataset-into-digits"&gt;Semantic Segmentation with SegNet&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-generating-pretrained-fcn-alexnet" class="anchor" aria-hidden="true" href="#generating-pretrained-fcn-alexnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Pretrained FCN-Alexnet&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/segnet-pretrained.md"&gt;Generating Pretrained FCN-Alexnet&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training-fcn-alexnet-with-digits" class="anchor" aria-hidden="true" href="#training-fcn-alexnet-with-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training FCN-Alexnet with DIGITS&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/segnet-training.md"&gt;Training FCN-Alexnet with DIGITS&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-testing-inference-model-in-digits" class="anchor" aria-hidden="true" href="#testing-inference-model-in-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing Inference Model in DIGITS&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/segnet-training.md#testing-inference-model-in-digits"&gt;Training FCN-Alexnet with DIGITS&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fcn-alexnet-patches-for-tensorrt" class="anchor" aria-hidden="true" href="#fcn-alexnet-patches-for-tensorrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-Alexnet Patches for TensorRT&lt;/h4&gt;
&lt;p&gt;See &lt;a href="docs/segnet-patches.md"&gt;FCN-Alexnet Patches for TensorRT&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-running-segmentation-models-on-jetson" class="anchor" aria-hidden="true" href="#running-segmentation-models-on-jetson"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Segmentation Models on Jetson&lt;/h3&gt;
&lt;p&gt;See &lt;a href="docs/segnet-console.md"&gt;Running Segmentation Models on Jetson&lt;/a&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;p align="center"&gt;&lt;sup&gt;© 2016-2019 NVIDIA | &lt;/sup&gt;&lt;a href="#deploying-deep-learning"&gt;&lt;sup&gt;Table of Contents&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dusty-nv</author><guid isPermaLink="false">https://github.com/dusty-nv/jetson-inference</guid><pubDate>Sun, 09 Feb 2020 00:19:00 GMT</pubDate></item><item><title>bigtreetech/BIGTREETECH-SKR-mini-E3 #20 in C++, This week</title><link>https://github.com/bigtreetech/BIGTREETECH-SKR-mini-E3</link><description>&lt;p&gt;&lt;i&gt;BIGTREETECH SKR-mini-E3 motherboard is a ultra-quiet, low-power, high-quality 3D printing machine control board. It is launched by the 3D printing team of Shenzhen BIGTREE technology co., LTD. This board is specially tailored for Ender 3 printer, perfectly replacing the original Ender3 printer motherboard.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bigtreetech-skr-mini-e3" class="anchor" aria-hidden="true" href="#bigtreetech-skr-mini-e3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BIGTREETECH-SKR-mini-E3&lt;/h1&gt;
&lt;p&gt;BIGTREETECH SKR-mini-E3 motherboard is a ultra-quiet, low-power, high-quality 3D printing machine control board. It is launched by the 3D printing team of Shenzhen BIGTREE technology co., LTD. This board is specially tailored for Ender 3 printer, perfectly replacing the original Ender3 printer motherboard.&lt;/p&gt;
&lt;p&gt;Note: If your motherbord version is BIGTREETECH-SKR-mini-E3 V1.0, please check the BIGTREETECH-SKR-mini-E3 V1.0 document which is in the firmware and hardware files.
If your motherbord version is BTT SKR mini E3 V1.2, please check the BTT SKR mini E3 V1.2 document which is in the firmware and hardware files.
The refer to check the main differences of the BTT SKR mini E3 V1.2：
&lt;a href="https://github.com/bigtreetech/BIGTREETECH-SKR-mini-E3/blob/master/hardware/BTT%20SKR%20MINI%20E3%20V1.2/The%20Notement%20of%20BTT%20SKR%20MINI%20E3%20V1.2.pdf"&gt;The notement of BTT SKR MINI E3 V1.2&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bigtreetech</author><guid isPermaLink="false">https://github.com/bigtreetech/BIGTREETECH-SKR-mini-E3</guid><pubDate>Sun, 09 Feb 2020 00:20:00 GMT</pubDate></item><item><title>radareorg/cutter #21 in C++, This week</title><link>https://github.com/radareorg/cutter</link><description>&lt;p&gt;&lt;i&gt;Free and Open Source Reverse Engineering Platform powered by radare2&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/radareorg/cutter/master/src/img/cutter.svg?sanitize=true"&gt;&lt;img width="150" height="150" align="left" alt="Cutter logo" src="https://raw.githubusercontent.com/radareorg/cutter/master/src/img/cutter.svg?sanitize=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-cutter" class="anchor" aria-hidden="true" href="#cutter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cutter&lt;/h1&gt;
&lt;p&gt;Cutter is a free and open-source reverse engineering framework powered by &lt;a href="https://github.com/radareorg/radare2"&gt;radare2&lt;/a&gt; . Its goal is making an advanced, customizable and FOSS reverse-engineering platform while keeping the user experience at mind. Cutter is created by reverse engineers for reverse engineers.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.com/radareorg/cutter" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/109ebd02b50ded2b6d5d39f655045bcf0530151b/68747470733a2f2f7472617669732d63692e636f6d2f7261646172656f72672f6375747465722e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.com/radareorg/cutter.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/radareorg/cutter/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9e2669b527deb0a2c87684480dcd3a5e97f8e63b/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f7339726b7831646e33757934626664782f6272616e63682f6d61737465723f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/s9rkx1dn3uy4bfdx/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lgtm.com/projects/g/radareorg/cutter/alerts/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/263bbf1a01c01cd26d881c44735b7a30cd4b950a/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f616c657274732f672f7261646172656f72672f6375747465722e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Total alerts" data-canonical-src="https://img.shields.io/lgtm/alerts/g/radareorg/cutter.svg?logo=lgtm&amp;amp;logoWidth=18" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/radareorg/cutter/master/docs/source/images/screenshot.png"&gt;&lt;img src="https://raw.githubusercontent.com/radareorg/cutter/master/docs/source/images/screenshot.png" alt="Screenshot" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-learn-more-at-httpscutterre" class="anchor" aria-hidden="true" href="#learn-more-at-httpscutterre"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learn more at &lt;a href="https://cutter.re" rel="nofollow"&gt;https://cutter.re&lt;/a&gt;.&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-downloading-a-release" class="anchor" aria-hidden="true" href="#downloading-a-release"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading a release&lt;/h2&gt;
&lt;p&gt;Cutter is available for all platforms (Linux, macOS, Windows).
You can download the latest release &lt;a href="https://github.com/radareorg/cutter/releases"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;macOS: Download the latest &lt;code&gt;.dmg&lt;/code&gt; file or use &lt;a href="https://github.com/Homebrew/homebrew-cask"&gt;Homebrew Cask&lt;/a&gt; &lt;code&gt;brew cask install cutter&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Windows: Download the latest Zip archive.&lt;/li&gt;
&lt;li&gt;Linux: Download the latest AppImage file. Then just make it executable and run it:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;chmod +x &amp;lt;appimage_file&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./&amp;lt;appimage_file&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-building-from-sources" class="anchor" aria-hidden="true" href="#building-from-sources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building from sources&lt;/h2&gt;
&lt;p&gt;To build Cutter on your local machine, please follow this guide: &lt;a href="https://cutter.re/docs/building.html" rel="nofollow"&gt;Building from source&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h2&gt;
&lt;p&gt;To deploy &lt;em&gt;cutter&lt;/em&gt; using a pre-built &lt;code&gt;Dockerfile&lt;/code&gt;, it's possible to use the &lt;a href="docker"&gt;provided configuration&lt;/a&gt;. The corresponding &lt;code&gt;README.md&lt;/code&gt; file also contains instructions on how to get started using the docker image with minimal effort.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;You can find our documentation in our &lt;a href="https://cutter.re/docs/" rel="nofollow"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-plugins" class="anchor" aria-hidden="true" href="#plugins"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Plugins&lt;/h2&gt;
&lt;p&gt;Cutter supports both Python and Native C++ plugins. Want to extend Cutter with Plugins? Read the &lt;a href="https://cutter.re/docs/plugins" rel="nofollow"&gt;Plugins&lt;/a&gt; section on our documentation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-official-and-community-plugins" class="anchor" aria-hidden="true" href="#official-and-community-plugins"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official and Community Plugins&lt;/h3&gt;
&lt;p&gt;Our community built many plugins and useful scripts for Cutter such as the native integration of Ghidra decompiler or the plugin to visualize DynamoRIO code coverage. You can find more plugins in the &lt;a href="https://github.com/radareorg/cutter-plugins"&gt;following list&lt;/a&gt;. Don't hesitate to extend it with your own plugins and scripts for Cutter.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-help" class="anchor" aria-hidden="true" href="#help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help&lt;/h2&gt;
&lt;p&gt;The best place to obtain help from &lt;em&gt;Cutter&lt;/em&gt; developers and community is to contact us on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Telegram:&lt;/strong&gt; &lt;a href="https://t.me/r2cutter" rel="nofollow"&gt;https://t.me/r2cutter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IRC:&lt;/strong&gt; #cutter on irc.freenode.net&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Twitter:&lt;/strong&gt; &lt;a href="https://twitter.com/r2gui" rel="nofollow"&gt;@r2gui&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>radareorg</author><guid isPermaLink="false">https://github.com/radareorg/cutter</guid><pubDate>Sun, 09 Feb 2020 00:21:00 GMT</pubDate></item><item><title>commaai/openpilot #22 in C++, This week</title><link>https://github.com/commaai/openpilot</link><description>&lt;p&gt;&lt;i&gt;open source driving agent&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="#"&gt;&lt;img src="https://camo.githubusercontent.com/2e695e85be3e3e19836c442c85e38a0583958cf0/68747470733a2f2f692e696d6775722e636f6d2f55656c556a4b41682e706e67" alt="" data-canonical-src="https://i.imgur.com/UelUjKAh.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-openpilot"&gt;What is openpilot?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#integration-with-stock-features"&gt;Integration with Stock Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#supported-hardware"&gt;Supported Hardware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#supported-cars"&gt;Supported Cars&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community-maintained-cars-and-features"&gt;Community Maintained Cars and Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation-instructions"&gt;Installation Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#limitations-of-openpilot-alc-and-ldw"&gt;Limitations of openpilot ALC and LDW&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#limitations-of-openpilot-acc-and-fcw"&gt;Limitations of openpilot ACC and FCW&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#limitations-of-openpilot-dm"&gt;Limitations of openpilot DM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#user-data-and-comma-account"&gt;User Data and comma Account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#safety-and-testing"&gt;Safety and Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing-on-pc"&gt;Testing on PC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community-and-contributing"&gt;Community and Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#directory-structure"&gt;Directory Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#licensing"&gt;Licensing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-openpilot" class="anchor" aria-hidden="true" href="#what-is-openpilot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is openpilot?&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://github.com/commaai/openpilot"&gt;openpilot&lt;/a&gt; is an open source driver assistance system. Currently, openpilot performs the functions of Adaptive Cruise Control (ACC), Automated Lane Centering (ALC), Forward Collision Warning (FCW) and Lane Departure Warning (LDW) for a growing variety of supported &lt;a href="#supported-cars"&gt;car makes, models and model years&lt;/a&gt;. In addition, while openpilot is engaged, a camera based Driver Monitoring (DM) feature alerts distracted and asleep drivers.&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=mgAbfr42oI8" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/361caac74d26e25fcbaa0fe269c740b62fb1d478/68747470733a2f2f692e696d6775722e636f6d2f6b4174543645692e706e67" data-canonical-src="https://i.imgur.com/kAtT6Ei.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=394rJKeh76k" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/247bdab178792afa9dee41f49cade65bcd988ae9/68747470733a2f2f692e696d6775722e636f6d2f6c5474386353322e706e67" data-canonical-src="https://i.imgur.com/lTt8cS2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=1iNOc3cq8cs" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/20c854357def20c0bfa1811dbc8b89ee3a4db4bc/68747470733a2f2f692e696d6775722e636f6d2f414e6e755370652e706e67" data-canonical-src="https://i.imgur.com/ANnuSpe.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Vr6NgrB-zHw" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e4c193b6261c5703c81f9c9da68413f967d57bf7/68747470733a2f2f692e696d6775722e636f6d2f517970616e75712e706e67" data-canonical-src="https://i.imgur.com/Qypanuq.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Ug41KIKF0oo" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e92d00445815efa69bbfc688629598e3b41e301a/68747470733a2f2f692e696d6775722e636f6d2f3363615a37784d2e706e67" data-canonical-src="https://i.imgur.com/3caZ7xM.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NVR_CdG1FRg" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f19f0e68e031d562f3aa212d6a00817d73f8dd4c/68747470733a2f2f692e696d6775722e636f6d2f62415a4f77716c2e706e67" data-canonical-src="https://i.imgur.com/bAZOwql.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tkEvIdzdfUE" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d301588a8594e01b4b33af77a9a87d9ae192c66a/68747470733a2f2f692e696d6775722e636f6d2f4546494e457a472e706e67" data-canonical-src="https://i.imgur.com/EFINEzG.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=_P-N1ewNne4" title="YouTube" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c960c2e48511730a6c928ea3ba4fdebb85b76f0c/68747470733a2f2f692e696d6775722e636f6d2f674179417132322e706e67" data-canonical-src="https://i.imgur.com/gAyAq22.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-stock-features" class="anchor" aria-hidden="true" href="#integration-with-stock-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with Stock Features&lt;/h2&gt;
&lt;p&gt;In all supported cars:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock Lane Keep Assist (LKA) and stock ALC are replaced by openpilot ALC, which only functions when openpilot is engaged by the user.&lt;/li&gt;
&lt;li&gt;Stock LDW is replaced by openpilot LDW.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, on specific supported cars (see ACC column in &lt;a href="#supported-cars"&gt;supported cars&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock ACC is replaced by openpilot ACC.&lt;/li&gt;
&lt;li&gt;openpilot FCW operates in addition to stock FCW.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;openpilot should preserve all other vehicle's stock features, including, but are not limited to: FCW, Automatic Emergency Braking (AEB), auto high-beam, blind spot warning, and side collision warning.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supported-hardware" class="anchor" aria-hidden="true" href="#supported-hardware"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported Hardware&lt;/h2&gt;
&lt;p&gt;At the moment, openpilot supports the &lt;a href="https://comma.ai/shop/products/eon-dashcam-devkit" rel="nofollow"&gt;EON DevKit&lt;/a&gt; and the &lt;a href="https://comma.ai/shop/products/comma-two-devkit" rel="nofollow"&gt;comma two&lt;/a&gt;. A &lt;a href="https://comma.ai/shop/products/car-harness" rel="nofollow"&gt;car harness&lt;/a&gt; is recommended to connect the EON or comma two to the car. In the future, we'd like to support other platforms as well, like gaming PCs.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supported-cars" class="anchor" aria-hidden="true" href="#supported-cars"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported Cars&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Make&lt;/th&gt;
&lt;th&gt;Model (US Market Reference)&lt;/th&gt;
&lt;th&gt;Supported Package&lt;/th&gt;
&lt;th&gt;ACC&lt;/th&gt;
&lt;th&gt;No ACC accel below&lt;/th&gt;
&lt;th&gt;No ALC below&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Acura&lt;/td&gt;
&lt;td&gt;ILX 2016-18&lt;/td&gt;
&lt;td&gt;AcuraWatch Plus&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;25mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Acura&lt;/td&gt;
&lt;td&gt;RDX 2016-18&lt;/td&gt;
&lt;td&gt;AcuraWatch Plus&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chrysler&lt;/td&gt;
&lt;td&gt;Pacifica 2017-18&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;9mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chrysler&lt;/td&gt;
&lt;td&gt;Pacifica Hybrid 2017-18&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;9mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chrysler&lt;/td&gt;
&lt;td&gt;Pacifica Hybrid 2019-20&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;39mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Accord 2018-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;3mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Accord Hybrid 2018-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;3mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Civic Hatchback 2017-19&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Civic Sedan/Coupe 2016-18&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Civic Sedan/Coupe 2019&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;2mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;CR-V 2015-16&lt;/td&gt;
&lt;td&gt;Touring&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;CR-V 2017-19&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;CR-V Hybrid 2017-2019&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Fit 2018-19&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Odyssey 2018-20&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Passport 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Pilot 2016-18&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Pilot 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Ridgeline 2017-19&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hyundai&lt;/td&gt;
&lt;td&gt;Elantra 2017-19&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;SCC + LKAS&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;19mph&lt;/td&gt;
&lt;td&gt;34mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hyundai&lt;/td&gt;
&lt;td&gt;Genesis 2018&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;19mph&lt;/td&gt;
&lt;td&gt;34mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hyundai&lt;/td&gt;
&lt;td&gt;Santa Fe 2019&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jeep&lt;/td&gt;
&lt;td&gt;Grand Cherokee 2016-18&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;9mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jeep&lt;/td&gt;
&lt;td&gt;Grand Cherokee 2019&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;39mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kia&lt;/td&gt;
&lt;td&gt;Optima 2019&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;SCC + LKAS&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kia&lt;/td&gt;
&lt;td&gt;Sorento 2018&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kia&lt;/td&gt;
&lt;td&gt;Stinger 2018&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;SCC + LKAS&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;CT Hybrid 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;ES 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;ES Hybrid 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;IS 2017-2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;22mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;IS Hybrid 2017&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;RX 2016-17&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;RX 2020&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;RX Hybrid 2016-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Subaru&lt;/td&gt;
&lt;td&gt;Crosstrek 2018-19&lt;/td&gt;
&lt;td&gt;EyeSight&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Subaru&lt;/td&gt;
&lt;td&gt;Impreza 2019-20&lt;/td&gt;
&lt;td&gt;EyeSight&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Avalon 2016&lt;/td&gt;
&lt;td&gt;TSS-P&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Avalon 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Camry 2018-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Camry Hybrid 2018-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;C-HR 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;C-HR Hybrid 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Corolla 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Corolla 2020&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Corolla Hatchback 2019-20&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Corolla Hybrid 2020&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Highlander 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Highlander Hybrid 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Prius 2016&lt;/td&gt;
&lt;td&gt;TSS-P&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Prius 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Prius Prime 2017-20&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 2016&lt;/td&gt;
&lt;td&gt;TSS-P&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 Hybrid 2016&lt;/td&gt;
&lt;td&gt;TSS-P&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 Hybrid 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 Hybrid 2019-20&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Sienna 2018&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Stock&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Volkswagen&lt;/td&gt;
&lt;td&gt;Golf 2016-19&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Driver Assistance&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Requires a &lt;a href="https://comma.ai/shop/products/panda-obd-ii-dongle" rel="nofollow"&gt;panda&lt;/a&gt; and open sourced &lt;a href="https://github.com/commaai/neo/tree/master/giraffe/hyundai"&gt;Hyundai giraffe&lt;/a&gt;, designed for the 2019 Sante Fe; pinout may differ for other Hyundai and Kia models. &lt;br&gt;
&lt;sup&gt;2&lt;/sup&gt;28mph for Camry 4CYL L, 4CYL LE and 4CYL SE which don't have Full-Speed Range Dynamic Radar Cruise Control. &lt;br&gt;
&lt;sup&gt;3&lt;/sup&gt;Requires a &lt;a href="https://community.comma.ai/wiki/index.php/Volkswagen#Integration_at_R242_Camera" rel="nofollow"&gt;custom connector&lt;/a&gt; for the &lt;a href="https://comma.ai/shop/products/car-harness" rel="nofollow"&gt;car harness&lt;/a&gt; &lt;br&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-community-maintained-cars-and-features" class="anchor" aria-hidden="true" href="#community-maintained-cars-and-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community Maintained Cars and Features&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Make&lt;/th&gt;
&lt;th&gt;Model (US Market Reference)&lt;/th&gt;
&lt;th&gt;Supported Package&lt;/th&gt;
&lt;th&gt;ACC&lt;/th&gt;
&lt;th&gt;No ACC accel below&lt;/th&gt;
&lt;th&gt;No ALC below&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Buick&lt;/td&gt;
&lt;td&gt;Regal 2018&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cadillac&lt;/td&gt;
&lt;td&gt;ATS 2018&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chevrolet&lt;/td&gt;
&lt;td&gt;Malibu 2017&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chevrolet&lt;/td&gt;
&lt;td&gt;Volt 2017-18&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GMC&lt;/td&gt;
&lt;td&gt;Acadia Denali 2018&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Holden&lt;/td&gt;
&lt;td&gt;Astra 2017&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;openpilot&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;sup&gt;4&lt;/sup&gt;When disconnecting the Driver Support Unit (DSU), openpilot ACC will replace stock ACC. For DSU locations, see &lt;a href="https://community.comma.ai/wiki/index.php/Toyota" rel="nofollow"&gt;Toyota Wiki page&lt;/a&gt;. &lt;em&gt;&lt;strong&gt;NOTE: disconnecting the DSU disables Automatic Emergency Braking (AEB).&lt;/strong&gt;&lt;/em&gt; &lt;br&gt;
&lt;sup&gt;5&lt;/sup&gt;&lt;a href="https://community.comma.ai/wiki/index.php/Comma_Pedal" rel="nofollow"&gt;Comma Pedal&lt;/a&gt; is used to provide stop-and-go capability to some of the openpilot-supported cars that don't currently support stop-and-go. Here is how to &lt;a href="https://medium.com/@jfrux/comma-pedal-building-with-macrofab-6328bea791e8" rel="nofollow"&gt;build a Comma Pedal&lt;/a&gt;. &lt;em&gt;&lt;strong&gt;NOTE: The Comma Pedal is not officially supported by &lt;a href="https://comma.ai" rel="nofollow"&gt;comma&lt;/a&gt;.&lt;/strong&gt;&lt;/em&gt; &lt;br&gt;
&lt;sup&gt;6&lt;/sup&gt;Requires a &lt;a href="https://comma.ai/shop/products/panda-obd-ii-dongle" rel="nofollow"&gt;panda&lt;/a&gt; and &lt;a href="https://zoneos.com/volt/" rel="nofollow"&gt;community built giraffe&lt;/a&gt;. &lt;em&gt;&lt;strong&gt;NOTE: disconnecting the ASCM disables Automatic Emergency Braking (AEB).&lt;/strong&gt;&lt;/em&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Community Maintained Cars and Features are not verified by comma to meet our &lt;a href="SAFETY.md"&gt;safety model&lt;/a&gt;. Be extra cautious using them. They are only available after enabling the toggle in &lt;code&gt;Settings-&amp;gt;Developer-&amp;gt;Enable Community Features&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation-instructions" class="anchor" aria-hidden="true" href="#installation-instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Instructions&lt;/h2&gt;
&lt;p&gt;Install openpilot on a EON by entering &lt;code&gt;https://openpilot.comma.ai&lt;/code&gt; during the installer setup.&lt;/p&gt;
&lt;p&gt;Follow this &lt;a href="https://youtu.be/3nlkomHathI" rel="nofollow"&gt;video instructions&lt;/a&gt; to properly mount the EON on the windshield. Note: openpilot features an automatic pose calibration routine and openpilot performance should not be affected by small pitch and yaw misalignments caused by imprecise EON mounting.&lt;/p&gt;
&lt;p&gt;Before placing the device on your windshield, check the state and local laws and ordinances where you drive. Some state laws prohibit or restrict the placement of objects on the windshield of a motor vehicle.&lt;/p&gt;
&lt;p&gt;You will be able to engage openpilot after reviewing the onboarding screens and finishing the calibration procedure.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-limitations-of-openpilot-alc-and-ldw" class="anchor" aria-hidden="true" href="#limitations-of-openpilot-alc-and-ldw"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Limitations of openpilot ALC and LDW&lt;/h2&gt;
&lt;p&gt;openpilot ALC and openpilot LDW do not automatically drive the vehicle or reduce the amount of attention that must be paid to operate your vehicle. The driver must always keep control of the steering wheel and be ready to correct the openpilot ALC action at all times.&lt;/p&gt;
&lt;p&gt;While changing lanes, openpilot is not capable of looking next to you or checking your blind spot. Only nudge the wheel to initiate a lane change after you have confirmed it's safe to do so.&lt;/p&gt;
&lt;p&gt;Many factors can impact the performance of openpilot ALC and openpilot LDW, causing them to be unable to function as intended. These include, but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poor visibility (heavy rain, snow, fog, etc.) or weather conditions that may interfere with sensor operation.&lt;/li&gt;
&lt;li&gt;The road facing camera is obstructed, covered or damaged by mud, ice, snow, etc.&lt;/li&gt;
&lt;li&gt;Obstruction caused by applying excessive paint or adhesive products (such as wraps, stickers, rubber coating, etc.) onto the vehicle.&lt;/li&gt;
&lt;li&gt;The EON is mounted incorrectly.&lt;/li&gt;
&lt;li&gt;When in sharp curves, like on-off ramps, intersections etc...; openpilot is designed to be limited in the amount of steering torque it can produce.&lt;/li&gt;
&lt;li&gt;In the presence of restricted lanes or construction zones.&lt;/li&gt;
&lt;li&gt;When driving on highly banked roads or in presence of strong cross-wind.&lt;/li&gt;
&lt;li&gt;Extremely hot or cold temperatures.&lt;/li&gt;
&lt;li&gt;Bright light (due to oncoming headlights, direct sunlight, etc.).&lt;/li&gt;
&lt;li&gt;Driving on hills, narrow, or winding roads.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The list above does not represent an exhaustive list of situations that may interfere with proper operation of openpilot components. It is the driver's responsibility to be in control of the vehicle at all times.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-limitations-of-openpilot-acc-and-fcw" class="anchor" aria-hidden="true" href="#limitations-of-openpilot-acc-and-fcw"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Limitations of openpilot ACC and FCW&lt;/h2&gt;
&lt;p&gt;openpilot ACC and openpilot FCW are not systems that allow careless or inattentive driving. It is still necessary for the driver to pay close attention to the vehicle’s surroundings and to be ready to re-take control of the gas and the brake at all times.&lt;/p&gt;
&lt;p&gt;Many factors can impact the performance of openpilot ACC and openpilot FCW, causing them to be unable to function as intended. These include, but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poor visibility (heavy rain, snow, fog, etc.) or weather conditions that may interfere with sensor operation.&lt;/li&gt;
&lt;li&gt;The road facing camera or radar are obstructed, covered, or damaged by mud, ice, snow, etc.&lt;/li&gt;
&lt;li&gt;Obstruction caused by applying excessive paint or adhesive products (such as wraps, stickers, rubber coating, etc.) onto the vehicle.&lt;/li&gt;
&lt;li&gt;The EON is mounted incorrectly.&lt;/li&gt;
&lt;li&gt;Approaching a toll booth, a bridge or a large metal plate.&lt;/li&gt;
&lt;li&gt;When driving on roads with pedestrians, cyclists, etc...&lt;/li&gt;
&lt;li&gt;In presence of traffic signs or stop lights, which are not detected by openpilot at this time.&lt;/li&gt;
&lt;li&gt;When the posted speed limit is below the user selected set speed. openpilot does not detect speed limits at this time.&lt;/li&gt;
&lt;li&gt;In presence of vehicles in the same lane that are not moving.&lt;/li&gt;
&lt;li&gt;When abrupt braking maneuvers are required. openpilot is designed to be limited in the amount of deceleration and acceleration that it can produce.&lt;/li&gt;
&lt;li&gt;When surrounding vehicles perform close cut-ins from neighbor lanes.&lt;/li&gt;
&lt;li&gt;Driving on hills, narrow, or winding roads.&lt;/li&gt;
&lt;li&gt;Extremely hot or cold temperatures.&lt;/li&gt;
&lt;li&gt;Bright light (due to oncoming headlights, direct sunlight, etc.).&lt;/li&gt;
&lt;li&gt;Interference from other equipment that generates radar waves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The list above does not represent an exhaustive list of situations that may interfere with proper operation of openpilot components. It is the driver's responsibility to be in control of the vehicle at all times.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-limitations-of-openpilot-dm" class="anchor" aria-hidden="true" href="#limitations-of-openpilot-dm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Limitations of openpilot DM&lt;/h2&gt;
&lt;p&gt;openpilot DM should not be considered an exact measurements of the status of alertness of the driver.&lt;/p&gt;
&lt;p&gt;Many factors can impact the performance of openpilot DM, causing it to be unable to function as intended. These include, but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Low light conditions, such as driving at night or in dark tunnels.&lt;/li&gt;
&lt;li&gt;Bright light (due to oncoming headlights, direct sunlight, etc.).&lt;/li&gt;
&lt;li&gt;The driver face is partially or completely outside field of view of the driver facing camera.&lt;/li&gt;
&lt;li&gt;Right hand driving vehicles.&lt;/li&gt;
&lt;li&gt;The driver facing camera is obstructed, covered, or damaged.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The list above does not represent an exhaustive list of situations that may interfere with proper operation of openpilot components. A driver should not rely on openpilot DM to assess their level of attention.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-user-data-and-comma-account" class="anchor" aria-hidden="true" href="#user-data-and-comma-account"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User Data and comma Account&lt;/h2&gt;
&lt;p&gt;By default, openpilot uploads the driving data to our servers. You can also access your data by pairing with the comma connect app (&lt;a href="https://apps.apple.com/us/app/comma-connect/id1456551889" rel="nofollow"&gt;iOS&lt;/a&gt;, &lt;a href="https://play.google.com/store/apps/details?id=ai.comma.connect&amp;amp;hl=en_US" rel="nofollow"&gt;Android&lt;/a&gt;). We use your data to train better models and improve openpilot for everyone.&lt;/p&gt;
&lt;p&gt;openpilot is open source software: the user is free to disable data collection if they wish to do so.&lt;/p&gt;
&lt;p&gt;openpilot logs the road facing camera, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver facing camera is only logged if you explicitly opt-in in settings. The microphone is not recorded.&lt;/p&gt;
&lt;p&gt;By using openpilot, you agree to &lt;a href="https://my.comma.ai/privacy" rel="nofollow"&gt;our Privacy Policy&lt;/a&gt;. You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-safety-and-testing" class="anchor" aria-hidden="true" href="#safety-and-testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Safety and Testing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;openpilot observes ISO26262 guidelines, see &lt;a href="SAFETY.md"&gt;SAFETY.md&lt;/a&gt; for more detail.&lt;/li&gt;
&lt;li&gt;openpilot has software in the loop &lt;a href="run_docker_tests.sh"&gt;tests&lt;/a&gt; that run on every commit.&lt;/li&gt;
&lt;li&gt;The safety model code lives in panda and is written in C, see &lt;a href="https://github.com/commaai/panda#code-rigor"&gt;code rigor&lt;/a&gt; for more details.&lt;/li&gt;
&lt;li&gt;panda has software in the loop &lt;a href="https://github.com/commaai/panda/tree/master/tests/safety"&gt;safety tests&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Internally, we have a hardware in the loop Jenkins test suite that builds and unit tests the various processes.&lt;/li&gt;
&lt;li&gt;panda has additional hardware in the loop &lt;a href="https://github.com/commaai/panda/blob/master/Jenkinsfile"&gt;tests&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We run the latest openpilot in a testing closet containing 10 EONs continuously replaying routes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-testing-on-pc" class="anchor" aria-hidden="true" href="#testing-on-pc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing on PC&lt;/h2&gt;
&lt;p&gt;Check out the tools directory in master: lots of tools you can use to replay driving data, test and develop openpilot from your pc.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-community-and-contributing" class="anchor" aria-hidden="true" href="#community-and-contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community and Contributing&lt;/h2&gt;
&lt;p&gt;openpilot is developed by &lt;a href="https://comma.ai/" rel="nofollow"&gt;comma&lt;/a&gt; and by users like you. We welcome both pull requests and issues on &lt;a href="http://github.com/commaai/openpilot"&gt;GitHub&lt;/a&gt;. Bug fixes and new car ports are encouraged.&lt;/p&gt;
&lt;p&gt;You can add support for your car by following guides we have written for &lt;a href="https://medium.com/@comma_ai/how-to-write-a-car-port-for-openpilot-7ce0785eda84" rel="nofollow"&gt;Brand&lt;/a&gt; and &lt;a href="https://medium.com/@comma_ai/openpilot-port-guide-for-toyota-models-e5467f4b5fe6" rel="nofollow"&gt;Model&lt;/a&gt; ports. Generally, a car with adaptive cruise control and lane keep assist is a good candidate. &lt;a href="https://discord.comma.ai" rel="nofollow"&gt;Join our Discord&lt;/a&gt; to discuss car ports: most car makes have a dedicated channel.&lt;/p&gt;
&lt;p&gt;Want to get paid to work on openpilot? &lt;a href="https://comma.ai/jobs/" rel="nofollow"&gt;comma is hiring&lt;/a&gt;. We also have a &lt;a href="https://comma.ai/bounties.html" rel="nofollow"&gt;bounty program&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And &lt;a href="https://twitter.com/comma_ai" rel="nofollow"&gt;follow us on Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-directory-structure" class="anchor" aria-hidden="true" href="#directory-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directory Structure&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;.
├── apk                 # The apk files used for the UI
├── cereal              # The messaging spec and libs used for all logs on EON
├── common              # Library like functionality we've developed here
├── installer/updater   # Manages auto-updates of openpilot
├── opendbc             # Files showing how to interpret data from cars
├── panda               # Code used to communicate on CAN
├── phonelibs           # Libraries used on EON
├── pyextra             # Libraries used on EON
└── selfdrive           # Code needed to drive the car
    ├── assets          # Fonts and images for UI
    ├── athena          # Allows communication with the app
    ├── boardd          # Daemon to talk to the board
    ├── camerad         # Driver to capture images from the camera sensors
    ├── car             # Car specific code to read states and control actuators
    ├── common          # Shared C/C++ code for the daemons
    ├── controls        # Perception, planning and controls
    ├── debug           # Tools to help you debug and do car ports
    ├── locationd       # Soon to be home of precise location
    ├── logcatd         # Android logcat as a service
    ├── loggerd         # Logger and uploader of car data
    ├── modeld          # Driving and monitoring model runners
    ├── proclogd        # Logs information from proc
    ├── sensord         # IMU / GPS interface code
    ├── tests           # Unit tests, system tests and a car simulator
    └── ui              # The UI
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To understand how the services interact, see &lt;code&gt;cereal/service_list.yaml&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-licensing" class="anchor" aria-hidden="true" href="#licensing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Licensing&lt;/h2&gt;
&lt;p&gt;openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.&lt;/p&gt;
&lt;p&gt;Any user of this software shall indemnify and hold harmless comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys’ fees and costs) which arise out of, relate to or result from any use of this software by user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c93f9b3a1bbb98aee637825923b0d04c1de248e1/68747470733a2f2f64317162326e6235637a6e6174752e636c6f756466726f6e742e6e65742f73746172747570732f692f313036313135372d62633765396266336232343665636537333232653666666536353366366166382d6d656469756d5f6a70672e6a70673f6275737465723d31343538333633313330"&gt;&lt;img src="https://camo.githubusercontent.com/c93f9b3a1bbb98aee637825923b0d04c1de248e1/68747470733a2f2f64317162326e6235637a6e6174752e636c6f756466726f6e742e6e65742f73746172747570732f692f313036313135372d62633765396266336232343665636537333232653666666536353366366166382d6d656469756d5f6a70672e6a70673f6275737465723d31343538333633313330" width="75" data-canonical-src="https://d1qb2nb5cznatu.cloudfront.net/startups/i/1061157-bc7e9bf3b246ece7322e6ffe653f6af8-medium_jpg.jpg?buster=1458363130" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5cda36b7447fb30b53fefb059ccd3b28e46aa7c8/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a433837456a7847654d50726b547556525657566734772e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/5cda36b7447fb30b53fefb059ccd3b28e46aa7c8/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a433837456a7847654d50726b547556525657566734772e706e67" width="225" data-canonical-src="https://cdn-images-1.medium.com/max/1600/1*C87EjxGeMPrkTuVRVWVg4w.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>commaai</author><guid isPermaLink="false">https://github.com/commaai/openpilot</guid><pubDate>Sun, 09 Feb 2020 00:22:00 GMT</pubDate></item><item><title>facebookresearch/faiss #23 in C++, This week</title><link>https://github.com/facebookresearch/faiss</link><description>&lt;p&gt;&lt;i&gt;A library for efficient similarity search and clustering of dense vectors.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-faiss" class="anchor" aria-hidden="true" href="#faiss"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Faiss&lt;/h1&gt;
&lt;p&gt;Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed by &lt;a href="https://research.fb.com/category/facebook-ai-research-fair/" rel="nofollow"&gt;Facebook AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NEWS&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;NEW: version 1.6.1 (2019-11-29) bugfix.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: version 1.6.0 (2019-10-15) code structure reorg, support for codec interface.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: version 1.5.3 (2019-06-24) fix performance regression in IndexIVF.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: version 1.5.2 (2019-05-27) the license was relaxed to MIT from BSD+Patents. Read LICENSE for details.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: version 1.5.0 (2018-12-19) GPU binary flat index and binary HNSW index&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: version 1.4.0 (2018-08-30) no more crashes in pure Python code&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: version 1.3.0 (2018-07-12) support for binary indexes&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: latest commit (2018-02-22) supports on-disk storage of inverted indexes, see demos/demo_ondisk_ivf.py&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: latest commit (2018-01-09) includes an implementation of the HNSW indexing method, see benchs/bench_hnsw.py&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: there is now a Facebook public discussion group for Faiss users at &lt;a href="https://www.facebook.com/groups/faissusers/" rel="nofollow"&gt;https://www.facebook.com/groups/faissusers/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NEW: on 2017-07-30, the license on Faiss was relaxed to BSD from CC-BY-NC. Read LICENSE for details.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Faiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors.&lt;/p&gt;
&lt;p&gt;Most of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors. This generally comes at the cost of a less precise search but these methods can scale to billions of vectors in main memory on a single server.&lt;/p&gt;
&lt;p&gt;The GPU implementation can accept input from either CPU or GPU memory. On a server with GPUs, the GPU indexes can be used a drop-in replacement for the CPU indexes (e.g., replace &lt;code&gt;IndexFlatL2&lt;/code&gt; with &lt;code&gt;GpuIndexFlatL2&lt;/code&gt;) and copies to/from GPU memory are handled automatically. Results will be faster however if both input and output remain resident on the GPU. Both single and multi-GPU usage is supported.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-building" class="anchor" aria-hidden="true" href="#building"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building&lt;/h2&gt;
&lt;p&gt;The library is mostly implemented in C++, with optional GPU support provided via CUDA, and an optional Python interface. The CPU version requires a BLAS library. It compiles with a Makefile and can be packaged in a docker image. See &lt;a href="INSTALL.md"&gt;INSTALL.md&lt;/a&gt; for details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-faiss-works" class="anchor" aria-hidden="true" href="#how-faiss-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How Faiss works&lt;/h2&gt;
&lt;p&gt;Faiss is built around an index type that stores a set of vectors, and provides a function to search in them with L2 and/or dot product vector comparison. Some index types are simple baselines, such as exact search. Most of the available indexing structures correspond to various trade-offs with respect to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;search time&lt;/li&gt;
&lt;li&gt;search quality&lt;/li&gt;
&lt;li&gt;memory used per index vector&lt;/li&gt;
&lt;li&gt;training time&lt;/li&gt;
&lt;li&gt;need for external data for unsupervised training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The optional GPU implementation provides what is likely (as of March 2017) the fastest exact and approximate (compressed-domain) nearest neighbor search implementation for high-dimensional vectors, fastest Lloyd's k-means, and fastest small k-selection algorithm known. &lt;a href="https://arxiv.org/abs/1702.08734" rel="nofollow"&gt;The implementation is detailed here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-full-documentation-of-faiss" class="anchor" aria-hidden="true" href="#full-documentation-of-faiss"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Full documentation of Faiss&lt;/h2&gt;
&lt;p&gt;The following are entry points for documentation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the full documentation, including a &lt;a href="https://github.com/facebookresearch/faiss/wiki/Getting-started"&gt;tutorial&lt;/a&gt;, a &lt;a href="https://github.com/facebookresearch/faiss/wiki/FAQ"&gt;FAQ&lt;/a&gt; and a &lt;a href="https://github.com/facebookresearch/faiss/wiki/Troubleshooting"&gt;troubleshooting section&lt;/a&gt; can be found on the &lt;a href="http://github.com/facebookresearch/faiss/wiki"&gt;wiki page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the &lt;a href="http://rawgithub.com/facebookresearch/faiss/master/docs/html/annotated.html" rel="nofollow"&gt;doxygen documentation&lt;/a&gt; gives per-class information&lt;/li&gt;
&lt;li&gt;to reproduce results from our research papers, &lt;a href="https://arxiv.org/abs/1609.01882" rel="nofollow"&gt;Polysemous codes&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1702.08734" rel="nofollow"&gt;Billion-scale similarity search with GPUs&lt;/a&gt;, refer to the &lt;a href="benchs/README.md"&gt;benchmarks README&lt;/a&gt;. For &lt;a href="https://arxiv.org/abs/1804.09996" rel="nofollow"&gt;
Link and code: Fast indexing with graphs and compact regression codes&lt;/a&gt;, see the &lt;a href="benchs/link_and_code"&gt;link_and_code README&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;The main authors of Faiss are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jegou"&gt;Hervé Jégou&lt;/a&gt; initiated the Faiss project and wrote its first implementation&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mdouze"&gt;Matthijs Douze&lt;/a&gt; implemented most of the CPU Faiss&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wickedfoo"&gt;Jeff Johnson&lt;/a&gt; implemented all of the GPU Faiss&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/beauby"&gt;Lucas Hosseini&lt;/a&gt; implemented the binary indexes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;Reference to cite when you use Faiss in a research paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{JDH17,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1702.08734},
  year={2017}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-join-the-faiss-community" class="anchor" aria-hidden="true" href="#join-the-faiss-community"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Join the Faiss community&lt;/h2&gt;
&lt;p&gt;For public discussion of Faiss or for questions, there is a Facebook public discussion group at &lt;a href="https://www.facebook.com/groups/faissusers/" rel="nofollow"&gt;https://www.facebook.com/groups/faissusers/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We monitor the &lt;a href="http://github.com/facebookresearch/faiss/issues"&gt;issues page&lt;/a&gt; of the repository. You can report bugs, ask questions, etc.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Faiss is MIT-licensed.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>facebookresearch</author><guid isPermaLink="false">https://github.com/facebookresearch/faiss</guid><pubDate>Sun, 09 Feb 2020 00:23:00 GMT</pubDate></item><item><title>microsoft/LightGBM #24 in C++, This week</title><link>https://github.com/microsoft/LightGBM</link><description>&lt;p&gt;&lt;i&gt;A fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-lightgbm-light-gradient-boosting-machine" class="anchor" aria-hidden="true" href="#lightgbm-light-gradient-boosting-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LightGBM, Light Gradient Boosting Machine&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://lightgbm-ci.visualstudio.com/lightgbm-ci/_build/latest?definitionId=1" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/732c0350b7a73d8b927a296ea61386b2972a60d0/68747470733a2f2f6c6967687467626d2d63692e76697375616c73747564696f2e636f6d2f6c6967687467626d2d63692f5f617069732f6275696c642f7374617475732f4d6963726f736f66742e4c6967687447424d3f6272616e63684e616d653d6d6173746572" alt="Azure Pipelines Build Status" data-canonical-src="https://lightgbm-ci.visualstudio.com/lightgbm-ci/_apis/build/status/Microsoft.LightGBM?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/guolinke/lightgbm/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1faf46320dc9358b218369744022e80f0ad88b55/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f317973356f743430316d30666570366c2f6272616e63682f6d61737465723f7376673d74727565" alt="Appveyor Build Status" data-canonical-src="https://ci.appveyor.com/api/projects/status/1ys5ot401m0fep6l/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/microsoft/LightGBM" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/54050462b3d90f89a04a4cdc4485b002b10ad956/68747470733a2f2f7472617669732d63692e6f72672f6d6963726f736f66742f4c6967687447424d2e7376673f6272616e63683d6d6173746572" alt="Travis Build Status" data-canonical-src="https://travis-ci.org/microsoft/LightGBM.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lightgbm.readthedocs.io/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4f02737d4fa0e39f992a374c07598a95cf03db69/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6c6967687467626d2f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/lightgbm/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/microsoft/LightGBM/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/530c0380d54f0d00b695995f4cdb5d8c5b1da5af/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6963726f736f66742f6c6967687467626d2e737667" alt="License" data-canonical-src="https://img.shields.io/github/license/microsoft/lightgbm.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/lightgbm" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/34244ae628b4cb096fa26305abc1304e5d1b5e33/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6c6967687467626d2e7376673f6c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Python Versions" data-canonical-src="https://img.shields.io/pypi/pyversions/lightgbm.svg?logo=python&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/lightgbm" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e78e5fa3a797f79dfb9179ae5d4c34f5409d45b9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6967687467626d2e7376673f6c6f676f3d70797069266c6f676f436f6c6f723d7768697465" alt="PyPI Version" data-canonical-src="https://img.shields.io/pypi/v/lightgbm.svg?logo=pypi&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/Microsoft/LightGBM?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2050300eb5a1376830ec38321e6b1b63563a2ee4/68747470733a2f2f6261646765732e6769747465722e696d2f4d6963726f736f66742f4c6967687447424d2e737667" alt="Join Gitter at https://gitter.im/Microsoft/LightGBM" data-canonical-src="https://badges.gitter.im/Microsoft/LightGBM.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lightgbm-slack-autojoin.herokuapp.com" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/411ccf48382aa6bcc0d53b3ff624a2e43af9f986/68747470733a2f2f6c6967687467626d2d736c61636b2d6175746f6a6f696e2e6865726f6b756170702e636f6d2f62616467652e737667" alt="Slack" data-canonical-src="https://lightgbm-slack-autojoin.herokuapp.com/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Faster training speed and higher efficiency.&lt;/li&gt;
&lt;li&gt;Lower memory usage.&lt;/li&gt;
&lt;li&gt;Better accuracy.&lt;/li&gt;
&lt;li&gt;Support of parallel and GPU learning.&lt;/li&gt;
&lt;li&gt;Capable of handling large-scale data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For further details, please refer to &lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst"&gt;Features&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Benefitting from these advantages, LightGBM is being widely-used in many &lt;a href="https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions"&gt;winning solutions&lt;/a&gt; of machine learning competitions.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#comparison-experiment"&gt;Comparison experiments&lt;/a&gt; on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. What's more, &lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#parallel-experiment"&gt;parallel experiments&lt;/a&gt; show that LightGBM can achieve a linear speed-up by using multiple machines for training in specific settings.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-get-started-and-documentation" class="anchor" aria-hidden="true" href="#get-started-and-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Started and Documentation&lt;/h2&gt;
&lt;p&gt;Our primary documentation is at &lt;a href="https://lightgbm.readthedocs.io/" rel="nofollow"&gt;https://lightgbm.readthedocs.io/&lt;/a&gt; and is generated from this repository. If you are new to LightGBM, follow &lt;a href="https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html" rel="nofollow"&gt;the installation instructions&lt;/a&gt; on that site.&lt;/p&gt;
&lt;p&gt;Next you may want to read:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/LightGBM/tree/master/examples"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; showing command line usage of common tasks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt; and algorithms supported by LightGBM.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst"&gt;&lt;strong&gt;Parameters&lt;/strong&gt;&lt;/a&gt; is an exhaustive list of customization you can make.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst"&gt;&lt;strong&gt;Parallel Learning&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/GPU-Tutorial.rst"&gt;&lt;strong&gt;GPU Learning&lt;/strong&gt;&lt;/a&gt; can speed up computation.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/view/lauraepp/parameters" rel="nofollow"&gt;&lt;strong&gt;Laurae++ interactive documentation&lt;/strong&gt;&lt;/a&gt; is a detailed guide for hyperparameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Documentation for contributors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/README.rst"&gt;&lt;strong&gt;How we update readthedocs.io&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Check out the &lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Development-Guide.rst"&gt;&lt;strong&gt;Development Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;Please refer to changelogs at &lt;a href="https://github.com/microsoft/LightGBM/releases"&gt;GitHub releases&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;Some old update logs are available at &lt;a href="https://github.com/microsoft/LightGBM/blob/master/docs/Key-Events.md"&gt;Key Events&lt;/a&gt; page.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-external-unofficial-repositories" class="anchor" aria-hidden="true" href="#external-unofficial-repositories"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;External (Unofficial) Repositories&lt;/h2&gt;
&lt;p&gt;Julia-package: &lt;a href="https://github.com/Allardvm/LightGBM.jl"&gt;https://github.com/Allardvm/LightGBM.jl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;JPMML (Java PMML converter): &lt;a href="https://github.com/jpmml/jpmml-lightgbm"&gt;https://github.com/jpmml/jpmml-lightgbm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Treelite (model compiler for efficient deployment): &lt;a href="https://github.com/dmlc/treelite"&gt;https://github.com/dmlc/treelite&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;cuML Forest Inference Library (GPU-accelerated inference): &lt;a href="https://github.com/rapidsai/cuml"&gt;https://github.com/rapidsai/cuml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;m2cgen (model appliers for various languages): &lt;a href="https://github.com/BayesWitnesses/m2cgen"&gt;https://github.com/BayesWitnesses/m2cgen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;leaves (Go model applier): &lt;a href="https://github.com/dmitryikh/leaves"&gt;https://github.com/dmitryikh/leaves&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ONNXMLTools (ONNX converter): &lt;a href="https://github.com/onnx/onnxmltools"&gt;https://github.com/onnx/onnxmltools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;SHAP (model output explainer): &lt;a href="https://github.com/slundberg/shap"&gt;https://github.com/slundberg/shap&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MMLSpark (LightGBM on Spark): &lt;a href="https://github.com/Azure/mmlspark"&gt;https://github.com/Azure/mmlspark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kubeflow Fairing (LightGBM on Kubernetes): &lt;a href="https://github.com/kubeflow/fairing"&gt;https://github.com/kubeflow/fairing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ML.NET (.NET/C#-package): &lt;a href="https://github.com/dotnet/machinelearning"&gt;https://github.com/dotnet/machinelearning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LightGBM.NET (.NET/C#-package): &lt;a href="https://github.com/rca22/LightGBM.Net"&gt;https://github.com/rca22/LightGBM.Net&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dask-LightGBM (distributed and parallel Python-package): &lt;a href="https://github.com/dask/dask-lightgbm"&gt;https://github.com/dask/dask-lightgbm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ruby gem: &lt;a href="https://github.com/ankane/lightgbm"&gt;https://github.com/ankane/lightgbm&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ask a question &lt;a href="https://stackoverflow.com/questions/ask?tags=lightgbm" rel="nofollow"&gt;on Stack Overflow with the &lt;code&gt;lightgbm&lt;/code&gt; tag&lt;/a&gt;, we monitor this for new questions.&lt;/li&gt;
&lt;li&gt;Discuss on the &lt;a href="https://gitter.im/Microsoft/LightGBM" rel="nofollow"&gt;LightGBM Gitter&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Discuss on the &lt;a href="https://lightgbm.slack.com" rel="nofollow"&gt;LightGBM Slack team&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;Use &lt;a href="https://lightgbm-slack-autojoin.herokuapp.com/" rel="nofollow"&gt;this invite link&lt;/a&gt; to join the team.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Open &lt;strong&gt;bug reports&lt;/strong&gt; and &lt;strong&gt;feature requests&lt;/strong&gt; (not questions) on &lt;a href="https://github.com/microsoft/LightGBM/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-contribute" class="anchor" aria-hidden="true" href="#how-to-contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Contribute&lt;/h2&gt;
&lt;p&gt;LightGBM has been developed and used by many active community members. Your help is very valuable to make it better for everyone.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Contribute to the &lt;a href="https://github.com/microsoft/LightGBM/tree/master/tests"&gt;tests&lt;/a&gt; to make it more reliable.&lt;/li&gt;
&lt;li&gt;Contribute to the &lt;a href="https://github.com/microsoft/LightGBM/tree/master/docs"&gt;documentation&lt;/a&gt; to make it clearer for everyone.&lt;/li&gt;
&lt;li&gt;Contribute to the &lt;a href="https://github.com/microsoft/LightGBM/tree/master/examples"&gt;examples&lt;/a&gt; to share your experience with other users.&lt;/li&gt;
&lt;li&gt;Look for &lt;a href="https://github.com/microsoft/LightGBM/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"&gt;issues with tag "help wanted"&lt;/a&gt; and submit pull requests to address them.&lt;/li&gt;
&lt;li&gt;Add your stories and experience to &lt;a href="https://github.com/microsoft/LightGBM/blob/master/examples/README.md"&gt;Awesome LightGBM&lt;/a&gt;. If LightGBM helped you in a machine learning competition or some research application, we want to hear about it!&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/LightGBM/issues"&gt;Open an issue&lt;/a&gt; to report problems or recommend new features.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-microsoft-open-source-code-of-conduct" class="anchor" aria-hidden="true" href="#microsoft-open-source-code-of-conduct"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Microsoft Open Source Code of Conduct&lt;/h2&gt;
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reference-papers" class="anchor" aria-hidden="true" href="#reference-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference Papers&lt;/h2&gt;
&lt;p&gt;Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "&lt;a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree" rel="nofollow"&gt;LightGBM: A Highly Efficient Gradient Boosting Decision Tree&lt;/a&gt;". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.&lt;/p&gt;
&lt;p&gt;Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. "&lt;a href="http://papers.nips.cc/paper/6380-a-communication-efficient-parallel-algorithm-for-decision-tree" rel="nofollow"&gt;A Communication-Efficient Parallel Algorithm for Decision Tree&lt;/a&gt;". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.&lt;/p&gt;
&lt;p&gt;Huan Zhang, Si Si and Cho-Jui Hsieh. "&lt;a href="https://arxiv.org/abs/1706.08359" rel="nofollow"&gt;GPU Acceleration for Large-scale Tree Boosting&lt;/a&gt;". SysML Conference, 2018.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you use LightGBM in your GitHub projects, please add &lt;code&gt;lightgbm&lt;/code&gt; in the &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This project is licensed under the terms of the MIT license. See &lt;a href="https://github.com/microsoft/LightGBM/blob/master/LICENSE"&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/LightGBM</guid><pubDate>Sun, 09 Feb 2020 00:24:00 GMT</pubDate></item><item><title>scylladb/scylla #25 in C++, This week</title><link>https://github.com/scylladb/scylla</link><description>&lt;p&gt;&lt;i&gt;NoSQL data store using the seastar framework, compatible with Apache Cassandra&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-scylla" class="anchor" aria-hidden="true" href="#scylla"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scylla&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick-start&lt;/h2&gt;
&lt;p&gt;To get the build going quickly, Scylla offers a &lt;a href="tools/toolchain/README.md"&gt;frozen toolchain&lt;/a&gt;
which would build and run Scylla using a pre-configured Docker image.
Using the frozen toolchain will also isolate all of the installed
dependencies in a Docker container.
Assuming you have met the toolchain prerequisites, which is running
Docker in user mode, building and running is as easy as:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ ./tools/toolchain/dbuild ./configure.py
$ ./tools/toolchain/dbuild ninja build/release/scylla
$ ./tools/toolchain/dbuild ./build/release/scylla --developer-mode 1&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please see &lt;a href="HACKING.md"&gt;HACKING.md&lt;/a&gt; for detailed information on building and developing Scylla.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: GCC &amp;gt;= 8.1.1 is required to compile Scylla.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-scylla" class="anchor" aria-hidden="true" href="#running-scylla"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Scylla&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Run Scylla&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./build/release/scylla

&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;run Scylla with one CPU and ./tmp as work directory&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./build/release/scylla --workdir tmp --smp 1
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;For more run options:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;./build/release/scylla --help
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h2&gt;
&lt;p&gt;See &lt;a href="docs/testing.md"&gt;test.py manual&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-scylla-apis-and-compatibility" class="anchor" aria-hidden="true" href="#scylla-apis-and-compatibility"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scylla APIs and compatibility&lt;/h2&gt;
&lt;p&gt;By default, Scylla is compatible with Apache Cassandra and its APIs - CQL and
Thrift. There is also experimental support for the API of Amazon DynamoDB,
but being experimental it needs to be explicitly enabled to be used. For more
information on how to enable the experimental DynamoDB compatibility in Scylla,
and the current limitations of this feature, see
&lt;a href="docs/alternator/alternator.md"&gt;Alternator&lt;/a&gt; and
&lt;a href="docs/alternator/getting-started.md"&gt;Getting started with Alternator&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Documentation can be found in &lt;a href="./docs"&gt;./docs&lt;/a&gt; and on the
&lt;a href="https://github.com/scylladb/scylla/wiki"&gt;wiki&lt;/a&gt;. There is currently no clear
definition of what goes where, so when looking for something be sure to check
both.
Seastar documentation can be found &lt;a href="http://docs.seastar.io/master/index.html" rel="nofollow"&gt;here&lt;/a&gt;.
User documentation can be found &lt;a href="https://docs.scylladb.com/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-building-fedora-rpm" class="anchor" aria-hidden="true" href="#building-fedora-rpm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building Fedora RPM&lt;/h2&gt;
&lt;p&gt;As a pre-requisite, you need to install &lt;a href="https://fedoraproject.org/wiki/Mock" rel="nofollow"&gt;Mock&lt;/a&gt; on your machine:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Install mock:
sudo yum install mock

# Add user to the "mock" group:
usermod -a -G mock $USER &amp;amp;&amp;amp; newgrp mock
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, to build an RPM, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./dist/redhat/build_rpm.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The built RPM is stored in &lt;code&gt;/var/lib/mock/&amp;lt;configuration&amp;gt;/result&lt;/code&gt; directory.
For example, on Fedora 21 mock reports the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO: Done(scylla-server-0.00-1.fc21.src.rpm) Config(default) 20 minutes 7 seconds
INFO: Results and/or logs in: /var/lib/mock/fedora-21-x86_64/result
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-building-fedora-based-docker-image" class="anchor" aria-hidden="true" href="#building-fedora-based-docker-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building Fedora-based Docker image&lt;/h2&gt;
&lt;p&gt;Build a Docker image with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd dist/docker
docker build -t &amp;lt;image-name&amp;gt; .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the image with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -p $(hostname -i):9042:9042 -i -t &amp;lt;image name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-scylla" class="anchor" aria-hidden="true" href="#contributing-to-scylla"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to Scylla&lt;/h2&gt;
&lt;p&gt;&lt;a href="HACKING.md"&gt;Hacking howto&lt;/a&gt;
&lt;a href="CONTRIBUTING.md"&gt;Guidelines for contributing&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>scylladb</author><guid isPermaLink="false">https://github.com/scylladb/scylla</guid><pubDate>Sun, 09 Feb 2020 00:25:00 GMT</pubDate></item></channel></rss>