<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Cuda, This week</title><link>https://github.com/trending/cuda?since=weekly</link><description>The top repositories on GitHub for cuda, measured weekly</description><pubDate>Mon, 28 Oct 2019 04:49:49 GMT</pubDate><lastBuildDate>Mon, 28 Oct 2019 04:49:49 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>1400</ttl><item><title>hujie-frank/SENet #1 in Cuda, This week</title><link>https://github.com/hujie-frank/SENet</link><description>&lt;p&gt;&lt;i&gt;Squeeze-and-Excitation Networks&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-squeeze-and-excitation-networks-paper" class="anchor" aria-hidden="true" href="#squeeze-and-excitation-networks-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Squeeze-and-Excitation Networks &lt;sub&gt;(&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;)&lt;/sub&gt;&lt;/h1&gt;
&lt;p&gt;By Jie Hu&lt;sup&gt;[1]&lt;/sup&gt;, Li Shen&lt;sup&gt;[2]&lt;/sup&gt;, Gang Sun&lt;sup&gt;[1]&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://momenta.ai/" rel="nofollow"&gt;Momenta&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; and &lt;a href="http://www.robots.ox.ac.uk/~vgg/" rel="nofollow"&gt;University of Oxford&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-approach" class="anchor" aria-hidden="true" href="#approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Approach&lt;/h2&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-pipeline.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-pipeline.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align="center"&gt;
  Figure 1: Diagram of a Squeeze-and-Excitation building block.
&lt;/p&gt;
&lt;div align="center"&gt;
   &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-Inception-module.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-Inception-module.jpg" width="420" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-ResNet-module.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-ResNet-module.jpg" width="420" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align="center"&gt;
  Figure 2: Schema of SE-Inception and SE-ResNet modules. We set r=16 in all our models.
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-implementation" class="anchor" aria-hidden="true" href="#implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementation&lt;/h2&gt;
&lt;p&gt;In this repository, Squeeze-and-Excitation Networks are implemented by &lt;a href="https://github.com/BVLC/caffe"&gt;Caffe&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-augmentation" class="anchor" aria-hidden="true" href="#augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Augmentation&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Method&lt;/th&gt;
&lt;th align="center"&gt;Settings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Mirror&lt;/td&gt;
&lt;td align="center"&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Crop&lt;/td&gt;
&lt;td align="center"&gt;8% ~ 100%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Aspect Ratio&lt;/td&gt;
&lt;td align="center"&gt;3/4 ~ 4/3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Rotation&lt;/td&gt;
&lt;td align="center"&gt;-10° ~ 10°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Pixel Jitter&lt;/td&gt;
&lt;td align="center"&gt;-20 ~ 20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-note" class="anchor" aria-hidden="true" href="#note"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To achieve efficient training and testing, we combine the consecutive operations &lt;em&gt;&lt;strong&gt;channel-wise scale&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;element-wise summation&lt;/strong&gt;&lt;/em&gt; into a single layer &lt;strong&gt;"Axpy"&lt;/strong&gt; in the architectures with skip-connections, resulting in a considerable reduction in memory cost and computational burden.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, we found that the implementation for &lt;em&gt;&lt;strong&gt;global average pooling&lt;/strong&gt;&lt;/em&gt; on GPU supported by cuDNN and BVLC/caffe is less efficient. In this regard, we re-implement the operation which achieves significant acceleration.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-trained-models" class="anchor" aria-hidden="true" href="#trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trained Models&lt;/h2&gt;
&lt;p&gt;Table 1. Single crop validation error on ImageNet-1k (center 224x224 crop from resized image with shorter side = 256). The SENet-154 is one of our superior models used in &lt;a href="http://image-net.org/challenges/LSVRC/2017/index" rel="nofollow"&gt;ILSVRC 2017 Image Classification Challenge&lt;/a&gt; where we won the 1st place (Team name: &lt;a href="http://image-net.org/challenges/LSVRC/2017/results" rel="nofollow"&gt;WMW&lt;/a&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Top-1&lt;/th&gt;
&lt;th align="center"&gt;Top-5&lt;/th&gt;
&lt;th align="center"&gt;Size&lt;/th&gt;
&lt;th align="center"&gt;Caffe Model&lt;/th&gt;
&lt;th align="center"&gt;Caffe Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-BN-Inception&lt;/td&gt;
&lt;td align="center"&gt;23.62&lt;/td&gt;
&lt;td align="center"&gt;7.04&lt;/td&gt;
&lt;td align="center"&gt;46 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlTWRRbDZYbVB2WWc/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1qYoPdak" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-50&lt;/td&gt;
&lt;td align="center"&gt;22.37&lt;/td&gt;
&lt;td align="center"&gt;6.36&lt;/td&gt;
&lt;td align="center"&gt;107 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlS2QwZHFzM3RjNzg/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1gf5wsLl" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-101&lt;/td&gt;
&lt;td align="center"&gt;21.75&lt;/td&gt;
&lt;td align="center"&gt;5.72&lt;/td&gt;
&lt;td align="center"&gt;189 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlTEg4YmcwQ0FoZFU/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1c1FvCWg" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-152&lt;/td&gt;
&lt;td align="center"&gt;21.34&lt;/td&gt;
&lt;td align="center"&gt;5.54&lt;/td&gt;
&lt;td align="center"&gt;256 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlcFE0Q2NTcWl3WUE/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1dFEnSzR" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNeXt-50 (32 x 4d)&lt;/td&gt;
&lt;td align="center"&gt;20.97&lt;/td&gt;
&lt;td align="center"&gt;5.54&lt;/td&gt;
&lt;td align="center"&gt;105 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlQ2Z0Q204V1RITjA/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1dFbEmbv" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNeXt-101 (32 x 4d)&lt;/td&gt;
&lt;td align="center"&gt;19.81&lt;/td&gt;
&lt;td align="center"&gt;4.96&lt;/td&gt;
&lt;td align="center"&gt;187 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWleklsNzBiZlprblk/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1qY2wjt6" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SENet-154&lt;/td&gt;
&lt;td align="center"&gt;18.68&lt;/td&gt;
&lt;td align="center"&gt;4.47&lt;/td&gt;
&lt;td align="center"&gt;440 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlbTFZbzFTSXBUTUE/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1o7HdfAE" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here we obtain better performance than those reported in the paper.
We re-train the SENets described in the paper on a single GPU server with 8 NVIDIA Titan X cards, using a mini-batch of 256 and a initial learning rate of 0.1 with more epoches.
In contrast, the results reported in the paper were obtained by training the networks with a larger batch size (1024) and learning rate (0.6) across 4 servers.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-third-party-re-implementations" class="anchor" aria-hidden="true" href="#third-party-re-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Third-party re-implementations&lt;/h2&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Caffe. SE-mudolues are integrated with a modificated ResNet-50 using a stride 2 in the 3x3 convolution instead of the first 1x1 convolution which obtains better performance: &lt;a href="https://github.com/shicai/SENet-Caffe"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;TensorFlow. SE-modules are integrated with a pre-activation ResNet-50 which follows the setup in &lt;a href="https://github.com/facebook/fb.resnet.torch"&gt;fb.resnet.torch&lt;/a&gt;: &lt;a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;TensorFlow. Simple Tensorflow implementation of SENets using Cifar10: &lt;a href="https://github.com/taki0112/SENet-Tensorflow"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MatConvNet. All the released SENets are imported into &lt;a href="https://github.com/vlfeat/matconvnet"&gt;MatConvNet&lt;/a&gt;: &lt;a href="https://github.com/albanie/mcnSENets"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MXNet. SE-modules are integrated with the ResNeXt and more architectures are coming soon: &lt;a href="https://github.com/bruinxiong/SENet.mxnet"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PyTorch. Implementation of SENets by PyTorch: &lt;a href="https://github.com/moskomule/senet.pytorch"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Chainer. Implementation of SENets by Chainer: &lt;a href="https://github.com/nutszebra/SENets"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use Squeeze-and-Excitation Networks in your research, please cite the paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{hu2018senet,
  title={Squeeze-and-Excitation Networks},
  author={Jie Hu and Li Shen and Gang Sun},
  journal={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hujie-frank</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item></channel></rss>