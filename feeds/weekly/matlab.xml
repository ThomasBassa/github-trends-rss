<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, This week</title><link>https://github.com/trending/matlab?since=weekly</link><description>The top repositories on GitHub for matlab, measured weekly</description><pubDate>Tue, 05 Nov 2019 01:06:19 GMT</pubDate><lastBuildDate>Tue, 05 Nov 2019 01:06:19 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>Borye/machine-learning-coursera-1 #1 in MATLAB, This week</title><link>https://github.com/Borye/machine-learning-coursera-1</link><description>&lt;p&gt;&lt;i&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-coursera&lt;/h1&gt;
&lt;p&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Borye</author><guid isPermaLink="false">https://github.com/Borye/machine-learning-coursera-1</guid><pubDate>Tue, 05 Nov 2019 00:01:00 GMT</pubDate></item><item><title>atinesh-s/Coursera-Machine-Learning-Stanford #2 in MATLAB, This week</title><link>https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</link><description>&lt;p&gt;&lt;i&gt;Machine learning-Stanford University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning (Coursera)&lt;/h1&gt;
&lt;p&gt;This is my solution to all the programming assignments and quizzes of Machine-Learning (Coursera) taught by Andrew Ng. After completing this course you will get a broad idea of Machine learning algorithms. Try to solve all the assignments by yourself first, but if you get stuck somewhere then feel free to browse the code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lectures Slides&lt;/li&gt;
&lt;li&gt;Solution to programming assignment&lt;/li&gt;
&lt;li&gt;Solution to Quizzes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-certificate" class="anchor" aria-hidden="true" href="#certificate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Certificate&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/account/accomplishments/certificate/GDDBFB572MUQ" rel="nofollow"&gt;Verified Certificate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;[1] Machine Learning - Stanford University&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>atinesh-s</author><guid isPermaLink="false">https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</guid><pubDate>Tue, 05 Nov 2019 00:02:00 GMT</pubDate></item><item><title>ShaoqingRen/faster_rcnn #3 in MATLAB, This week</title><link>https://github.com/ShaoqingRen/faster_rcnn</link><description>&lt;p&gt;&lt;i&gt;Faster R-CNN&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-this-repo-has-been-deprecated-please-see-detectron-which-includes-an-implementation-of-mask-r-cnn" class="anchor" aria-hidden="true" href="#this-repo-has-been-deprecated-please-see-detectron-which-includes-an-implementation-of-mask-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;This repo has been deprecated. Please see &lt;a href="https://github.com/facebookresearch/Detectron"&gt;Detectron&lt;/a&gt;, which includes an implementation of &lt;a href="https://arxiv.org/abs/1703.06870" rel="nofollow"&gt;Mask R-CNN&lt;/a&gt;.&lt;/h2&gt;
&lt;h1&gt;&lt;a id="user-content-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks" class="anchor" aria-hidden="true" href="#faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Faster&lt;/em&gt; R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/h1&gt;
&lt;p&gt;By Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun at Microsoft Research&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Faster&lt;/strong&gt; R-CNN is an object detection framework based on deep convolutional networks, which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing.&lt;/p&gt;
&lt;p&gt;Faster R-CNN was initially described in an &lt;a href="http://arxiv.org/abs/1506.01497" rel="nofollow"&gt;arXiv tech report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: &lt;a href="https://github.com/rbgirshick/fast-rcnn"&gt;rbgirshick/fast-rcnn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This code has been tested on Windows 7/8 64-bit, Windows Server 2012 R2, and Linux, and on MATLAB 2014a.&lt;/p&gt;
&lt;p&gt;Python version is available at &lt;a href="https://github.com/rbgirshick/py-faster-rcnn"&gt;py-faster-rcnn&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-citing-faster-r-cnn" class="anchor" aria-hidden="true" href="#citing-faster-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing Faster R-CNN&lt;/h3&gt;
&lt;p&gt;If you find Faster R-CNN useful in your research, please consider citing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{ren15fasterrcnn,
    Author = {Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun},
    Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
    Journal = {arXiv preprint arXiv:1506.01497},
    Year = {2015}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-main-results" class="anchor" aria-hidden="true" href="#main-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;training data&lt;/th&gt;
&lt;th align="center"&gt;test data&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th align="center"&gt;time/img&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 test&lt;/td&gt;
&lt;td align="center"&gt;69.9%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 trainval + 2012 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 test&lt;/td&gt;
&lt;td align="center"&gt;73.2%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2012 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2012 test&lt;/td&gt;
&lt;td align="center"&gt;67.0%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 trainval&amp;amp;test + 2012 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2012 test&lt;/td&gt;
&lt;td align="center"&gt;70.4%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The mAP results are subject to random variations. We have run 5 times independently for ZF net, and the mAPs are 59.9 (as in the paper), 60.4, 59.5, 60.1, and 59.5, with a mean of 59.88 and std 0.39.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;&lt;a href="#requirements-software"&gt;Requirements: software&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements-hardware"&gt;Requirements: hardware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#preparation-for-testing"&gt;Preparation for Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing-demo"&gt;Testing Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#preparation-for-training"&gt;Preparation for Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training"&gt;Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-software" class="anchor" aria-hidden="true" href="#requirements-software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements: software&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;&lt;code&gt;Caffe&lt;/code&gt; build for Faster R-CNN (included in this repository, see &lt;code&gt;external/caffe&lt;/code&gt;)
&lt;ul&gt;
&lt;li&gt;If you are using Windows, you may download a compiled mex file by running &lt;code&gt;fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you are using Linux or you want to compile for Windows, please follow the &lt;a href="https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN"&gt;instructions&lt;/a&gt; on our Caffe branch.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MATLAB&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-hardware" class="anchor" aria-hidden="true" href="#requirements-hardware"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements: hardware&lt;/h3&gt;
&lt;p&gt;GPU: Titan, Titan Black, Titan X, K20, K40, K80.&lt;/p&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Region Proposal Network (RPN)
&lt;ul&gt;
&lt;li&gt;2GB GPU memory for ZF net&lt;/li&gt;
&lt;li&gt;5GB GPU memory for VGG-16 net&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Object Detection Network (Fast R-CNN)
&lt;ul&gt;
&lt;li&gt;3GB GPU memory for ZF net&lt;/li&gt;
&lt;li&gt;8GB GPU memory for VGG-16 net&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-preparation-for-testing" class="anchor" aria-hidden="true" href="#preparation-for-testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preparation for Testing:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Run &lt;code&gt;fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m&lt;/code&gt; to download a compiled Caffe mex (for Windows only).&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;faster_rcnn_build.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;startup.m&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-testing-demo" class="anchor" aria-hidden="true" href="#testing-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing Demo:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;fetch_data/fetch_faster_rcnn_final_model.m&lt;/code&gt; to download our trained models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;experiments/script_faster_rcnn_demo.m&lt;/code&gt; to test a single demo image.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;001763.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.201s (resize+conv+proposal: 0.150s, nms+regionwise: 0.052s)
004545.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.201s (resize+conv+proposal: 0.151s, nms+regionwise: 0.050s)
000542.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.192s (resize+conv+proposal: 0.151s, nms+regionwise: 0.041s)
000456.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.202s (resize+conv+proposal: 0.152s, nms+regionwise: 0.050s)
001150.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.194s (resize+conv+proposal: 0.151s, nms+regionwise: 0.043s)
mean time: 0.198s&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and with ZF net:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;001763.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.061s (resize+conv+proposal: 0.032s, nms+regionwise: 0.029s)
004545.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.063s (resize+conv+proposal: 0.034s, nms+regionwise: 0.029s)
000542.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.052s (resize+conv+proposal: 0.034s, nms+regionwise: 0.018s)
000456.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.062s (resize+conv+proposal: 0.034s, nms+regionwise: 0.028s)
001150.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.058s (resize+conv+proposal: 0.034s, nms+regionwise: 0.023s)
mean time: 0.059s&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;The visual results might be different from those in the paper due to numerical variations.&lt;/li&gt;
&lt;li&gt;Running time on other GPUs&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;GPU / mean time&lt;/th&gt;
&lt;th align="center"&gt;VGG-16&lt;/th&gt;
&lt;th align="center"&gt;ZF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;K40&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;td align="center"&gt;59ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Titan Black&lt;/td&gt;
&lt;td align="center"&gt;174ms&lt;/td&gt;
&lt;td align="center"&gt;56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Titan X&lt;/td&gt;
&lt;td align="center"&gt;151ms&lt;/td&gt;
&lt;td align="center"&gt;59ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-preparation-for-training" class="anchor" aria-hidden="true" href="#preparation-for-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preparation for Training:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Run &lt;code&gt;fetch_data/fetch_model_ZF.m&lt;/code&gt; to download an ImageNet-pre-trained ZF net.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;fetch_data/fetch_model_VGG16.m&lt;/code&gt; to download an ImageNet-pre-trained VGG-16 net.&lt;/li&gt;
&lt;li&gt;Download VOC 2007 and 2012 data to ./datasets&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Run &lt;code&gt;experiments/script_faster_rcnn_VOC2007_ZF.m&lt;/code&gt; to train a model with ZF net. It runs four steps as follows:
&lt;ul&gt;
&lt;li&gt;Train RPN with conv layers tuned; compute RPN results on the train/test sets.&lt;/li&gt;
&lt;li&gt;Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.&lt;/li&gt;
&lt;li&gt;Train RPN with conv layers fixed; compute RPN results on the train/test sets.&lt;/li&gt;
&lt;li&gt;Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: the entire training time is ~12 hours on K40.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;experiments/script_faster_rcnn_VOC2007_VGG16.m&lt;/code&gt; to train a model with VGG net.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: the entire training time is ~2 days on K40.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Check other scripts in &lt;code&gt;./experiments&lt;/code&gt; for more settings.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This documentation may contain links to third party websites, which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation, guarantee or assurance regarding any third party website, content, service or product. Third party websites may be subject to the third party’s terms, conditions, and privacy statements.&lt;/p&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Experiment logs: &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;amp;authkey=!ACpgYZR2MmfklwI&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1ntJ3dLv" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Regions proposals of our trained RPN:
&lt;ul&gt;
&lt;li&gt;ZF net trained on VOC 07 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;amp;authkey=!AJJMrFJHKLXIg5c&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1pKGBDyz" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ZF net trained on VOC 07/12 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;amp;authkey=!AJiy5F6Cum1iosI&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1jGAgkZW" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VGG net trained on VOC 07 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;amp;authkey=!AH4Zi_KAaun7MhQ&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1qWHv4JU" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VGG net trained on VOC 07/12 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;amp;authkey=!AB_lKk3dbGyr1-I&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1c0fQpqg" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: the proposals are in the format of [left, top, right, bottom, confidence]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the automatic "fetch_data" fails, you may manually download resouces from:&lt;/p&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Pre-complied caffe mex:
&lt;ul&gt;
&lt;li&gt;Windows-based mex complied with VS2013 and Cuda6.5: &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;amp;authkey=!AFVWFGTbViiX5tg&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1i3m0i0H" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ImageNet-pretrained networks:
&lt;ul&gt;
&lt;li&gt;Zeiler &amp;amp; Fergus (ZF) net &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;amp;authkey=!AIzdm0sD_SmhUQ4&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1o6zipPS" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VGG-16 net &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;amp;authkey=!AE8uV9B07dREbhM&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1mgzSnI4" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Final RPN+FastRCNN models: &lt;a href="https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;amp;authkey=!AERHoxZ-iAx_j34&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1hsFKmeK" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ShaoqingRen</author><guid isPermaLink="false">https://github.com/ShaoqingRen/faster_rcnn</guid><pubDate>Tue, 05 Nov 2019 00:03:00 GMT</pubDate></item><item><title>kpzhang93/MTCNN_face_detection_alignment #4 in MATLAB, This week</title><link>https://github.com/kpzhang93/MTCNN_face_detection_alignment</link><description>&lt;p&gt;&lt;i&gt;Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mtcnn_face_detection_alignment" class="anchor" aria-hidden="true" href="#mtcnn_face_detection_alignment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MTCNN_face_detection_alignment&lt;/h1&gt;
&lt;p&gt;Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-requirement" class="anchor" aria-hidden="true" href="#requirement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirement&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Caffe: Linux OS: &lt;a href="https://github.com/BVLC/caffe"&gt;https://github.com/BVLC/caffe&lt;/a&gt;. Windows OS: &lt;a href="https://github.com/BVLC/caffe/tree/windows"&gt;https://github.com/BVLC/caffe/tree/windows&lt;/a&gt; or &lt;a href="https://github.com/happynear/caffe-windows"&gt;https://github.com/happynear/caffe-windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pdollar toolbox: &lt;a href="https://github.com/pdollar/toolbox"&gt;https://github.com/pdollar/toolbox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Matlab 2014b or later&lt;/li&gt;
&lt;li&gt;Cuda (if use nvidia gpu)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/208be02edd9ecf8cfc02c7bf00f2d879c662f84e/68747470733a2f2f6b707a68616e6739332e6769746875622e696f2f4d54434e4e5f666163655f646574656374696f6e5f616c69676e6d656e742f70617065722f6578616d706c65732e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/208be02edd9ecf8cfc02c7bf00f2d879c662f84e/68747470733a2f2f6b707a68616e6739332e6769746875622e696f2f4d54434e4e5f666163655f646574656374696f6e5f616c69676e6d656e742f70617065722f6578616d706c65732e706e67" alt="image" data-canonical-src="https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/examples.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/6df79feec086599240cce920195b85aa4ce6e0b5/68747470733a2f2f6b707a68616e6739332e6769746875622e696f2f4d54434e4e5f666163655f646574656374696f6e5f616c69676e6d656e742f70617065722f726573756c742e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/6df79feec086599240cce920195b85aa4ce6e0b5/68747470733a2f2f6b707a68616e6739332e6769746875622e696f2f4d54434e4e5f666163655f646574656374696f6e5f616c69676e6d656e742f70617065722f726573756c742e706e67" alt="image" data-canonical-src="https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/result.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-other-implementation" class="anchor" aria-hidden="true" href="#other-implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other implementation&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/happynear/MTCNN_face_detection_alignment"&gt;C++ &amp;amp; caffe &lt;/a&gt; (strongly recommend)&lt;br&gt;
&lt;a href="https://github.com/pangyupo/mxnet_mtcnn_face_detection"&gt;Python &amp;amp; mxnet&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/DuinoDu/mtcnn"&gt;Python &amp;amp; caffe&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-face-recognition" class="anchor" aria-hidden="true" href="#face-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Face Recognition&lt;/h3&gt;
&lt;p&gt;Here we strongly recommend &lt;a href="https://github.com/ydwen/caffe-face"&gt;Center Face&lt;/a&gt;, which is an effective and efficient open-source tool for face recognition.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;@article{7553523,
    author={K. Zhang and Z. Zhang and Z. Li and Y. Qiao}, 
    journal={IEEE Signal Processing Letters}, 
    title={Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks}, 
    year={2016}, 
    volume={23}, 
    number={10}, 
    pages={1499-1503}, 
    keywords={Benchmark testing;Computer architecture;Convolution;Detectors;Face;Face detection;Training;Cascaded convolutional neural network (CNN);face alignment;face detection}, 
    doi={10.1109/LSP.2016.2603342}, 
    ISSN={1070-9908}, 
    month={Oct}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;This code is distributed under MIT LICENSE&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h3&gt;
&lt;p&gt;Yu Qiao
&lt;a href="mailto:yu.qiao@siat.ac.cn"&gt;yu.qiao@siat.ac.cn&lt;/a&gt;&lt;br&gt;
Kaipeng Zhang
&lt;a href="mailto:kpzhang@cmlab.csie.ntu.edu.tw"&gt;kpzhang@cmlab.csie.ntu.edu.tw&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>kpzhang93</author><guid isPermaLink="false">https://github.com/kpzhang93/MTCNN_face_detection_alignment</guid><pubDate>Tue, 05 Nov 2019 00:04:00 GMT</pubDate></item></channel></rss>