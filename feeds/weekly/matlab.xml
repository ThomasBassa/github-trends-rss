<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, This week</title><link>https://github.com/trending/matlab?since=weekly</link><description>The top repositories on GitHub for matlab, measured weekly</description><pubDate>Fri, 20 Dec 2019 01:08:19 GMT</pubDate><lastBuildDate>Fri, 20 Dec 2019 01:08:19 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>cszn/DnCNN #1 in MATLAB, This week</title><link>https://github.com/cszn/DnCNN</link><description>&lt;p&gt;&lt;i&gt;Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising (TIP, 2017)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dncnn" class="anchor" aria-hidden="true" href="#dncnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://www4.comp.polyu.edu.hk/~cslzhang/paper/DnCNN.pdf" rel="nofollow"&gt;DnCNN&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-beyond-a-gaussian-denoiser-residual-learning-of-deep-cnn-for-image-denoising" class="anchor" aria-hidden="true" href="#beyond-a-gaussian-denoiser-residual-learning-of-deep-cnn-for-image-denoising"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://ieeexplore.ieee.org/document/7839189/" rel="nofollow"&gt;Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-new-training-and-testing-codes-pytorch---18122019" class="anchor" aria-hidden="true" href="#new-training-and-testing-codes-pytorch---18122019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New training and testing codes (&lt;a href="https://github.com/cszn/KAIR"&gt;PyTorch&lt;/a&gt;) - 18/12/2019&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/KAIR/blob/master/main_train_dncnn.py"&gt;main_train_dncnn.py&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/KAIR/blob/master/main_test_dncnn.py"&gt;main_test_dncnn.py&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-training-and-testing-codes-keras-and-pytorch" class="anchor" aria-hidden="true" href="#training-and-testing-codes-keras-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Testing Codes (&lt;a href="https://keras.io/" rel="nofollow"&gt;Keras&lt;/a&gt; and &lt;a href="https://pytorch.org/" rel="nofollow"&gt;PyTorch&lt;/a&gt;)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/dncnn_keras"&gt;DnCNN-keras&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/dncnn_pytorch"&gt;DnCNN-pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-merge-batch-normalization-pytorch" class="anchor" aria-hidden="true" href="#merge-batch-normalization-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Merge batch normalization (PyTorch)&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;import&lt;/span&gt; torch.nn &lt;span class="pl-k"&gt;as&lt;/span&gt; nn


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;merge_bn&lt;/span&gt;(&lt;span class="pl-smi"&gt;model&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt; merge all 'Conv+BN' (or 'TConv+BN') into 'Conv' (or 'TConv')&lt;/span&gt;
&lt;span class="pl-s"&gt;    based on https://github.com/pytorch/pytorch/pull/901&lt;/span&gt;
&lt;span class="pl-s"&gt;    by Kai Zhang (cskaizhang@gmail.com) &lt;/span&gt;
&lt;span class="pl-s"&gt;    https://github.com/cszn/DnCNN&lt;/span&gt;
&lt;span class="pl-s"&gt;    01/01/2019&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;
    prev_m &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;
    &lt;span class="pl-k"&gt;for&lt;/span&gt; k, m &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(model.named_children()):
        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(m, nn.BatchNorm2d) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(m, nn.BatchNorm1d)) &lt;span class="pl-k"&gt;and&lt;/span&gt; (&lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.Conv2d) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.Linear) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.ConvTranspose2d)):

            w &lt;span class="pl-k"&gt;=&lt;/span&gt; prev_m.weight.data

            &lt;span class="pl-k"&gt;if&lt;/span&gt; prev_m.bias &lt;span class="pl-k"&gt;is&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;:
                zeros &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.Tensor(prev_m.out_channels).zero_().type(w.type())
                prev_m.bias &lt;span class="pl-k"&gt;=&lt;/span&gt; nn.Parameter(zeros)
            b &lt;span class="pl-k"&gt;=&lt;/span&gt; prev_m.bias.data

            invstd &lt;span class="pl-k"&gt;=&lt;/span&gt; m.running_var.clone().add_(m.eps).pow_(&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;0.5&lt;/span&gt;)
            &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.ConvTranspose2d):
                w.mul_(invstd.view(&lt;span class="pl-c1"&gt;1&lt;/span&gt;, w.size(&lt;span class="pl-c1"&gt;1&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
            &lt;span class="pl-k"&gt;else&lt;/span&gt;:
                w.mul_(invstd.view(w.size(&lt;span class="pl-c1"&gt;0&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
            b.add_(&lt;span class="pl-k"&gt;-&lt;/span&gt;m.running_mean).mul_(invstd)
            &lt;span class="pl-k"&gt;if&lt;/span&gt; m.affine:
                &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.ConvTranspose2d):
                    w.mul_(m.weight.data.view(&lt;span class="pl-c1"&gt;1&lt;/span&gt;, w.size(&lt;span class="pl-c1"&gt;1&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
                &lt;span class="pl-k"&gt;else&lt;/span&gt;:
                    w.mul_(m.weight.data.view(w.size(&lt;span class="pl-c1"&gt;0&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
                b.mul_(m.weight.data).add_(m.bias.data)

            &lt;span class="pl-k"&gt;del&lt;/span&gt; model._modules[k]
        prev_m &lt;span class="pl-k"&gt;=&lt;/span&gt; m
        merge_bn(m)


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;tidy_sequential&lt;/span&gt;(&lt;span class="pl-smi"&gt;model&lt;/span&gt;):
    &lt;span class="pl-k"&gt;for&lt;/span&gt; k, m &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(model.named_children()):
        &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(m, nn.Sequential):
            &lt;span class="pl-k"&gt;if&lt;/span&gt; m.&lt;span class="pl-c1"&gt;__len__&lt;/span&gt;() &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;:
                model._modules[k] &lt;span class="pl-k"&gt;=&lt;/span&gt; m.&lt;span class="pl-c1"&gt;__getitem__&lt;/span&gt;(&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
        tidy_sequential(m)
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training-matconvnet" class="anchor" aria-hidden="true" href="#training-matconvnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training (&lt;a href="http://www.vlfeat.org/matconvnet/" rel="nofollow"&gt;MatConvNet&lt;/a&gt;)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.vlfeat.org/matconvnet/wrappers/" rel="nofollow"&gt;Simplenn&lt;/a&gt; version&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/DnCNN_TrainingCodes_v1.1"&gt;DnCNN_TrainingCodes_v1.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.vlfeat.org/matconvnet/wrappers/" rel="nofollow"&gt;DagNN&lt;/a&gt; version&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/DnCNN_TrainingCodes_DagNN_v1.1"&gt;DnCNN_TrainingCodes_DagNN_v1.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-testing-matconvnet-or-matlab" class="anchor" aria-hidden="true" href="#testing-matconvnet-or-matlab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing (&lt;a href="http://www.vlfeat.org/matconvnet/" rel="nofollow"&gt;MatConvNet&lt;/a&gt; or Matlab)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[demos]  &lt;code&gt;Demo_test_DnCNN-.m&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[models]  including the trained models for Gaussian denoising; a single model for Gaussian denoising, single image super-resolution (SISR) and deblocking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[testsets]  BSD68 and Set10 for Gaussian denoising evaluation; Set5, Set14, BSD100 and Urban100 datasets for SISR evaluation; Classic5 and LIVE1 for JPEG image deblocking evaluation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-new-fdncnn-models" class="anchor" aria-hidden="true" href="#new-fdncnn-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New FDnCNN Models&lt;/h1&gt;
&lt;p&gt;I have trained new Flexible DnCNN (FDnCNN) models based on &lt;a href="https://github.com/cszn/FFDNet"&gt;FFDNet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;FDnCNN can handle noise level range of [0, 75] via a single model.&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Gray.m"&gt;Demo_FDnCNN_Gray.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Gray_Clip.m"&gt;Demo_FDnCNN_Gray_Clip.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Color.m"&gt;Demo_FDnCNN_Color.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Color_Clip.m"&gt;Demo_FDnCNN_Color_Clip.m&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-network-architecture-and-design-rationale" class="anchor" aria-hidden="true" href="#network-architecture-and-design-rationale"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network Architecture and Design Rationale&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Network Architecture&lt;/p&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="figs/dncnn.png"&gt;&lt;img src="figs/dncnn.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Batch normalization and residual learning are beneficial to Gaussian denoising (especially for a single noise level). The residual of a noisy image corrupted by additive white Gaussian noise (AWGN) follows a constant Gaussian distribution which stablizes batch normalization during training.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Histogram of noisy patches, clean patches, and residual (noise) patches from a batch of training. The noise level is 25, the patch size is 40x40, the batch size is 128.&lt;/li&gt;
&lt;/ul&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="figs/batch1.png"&gt;&lt;img src="figs/batch1.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Histogram of noisy patches, clean patches, and residual (noise) patches from another batch of training. The noise level is 25, the patch size is 40x40, the batch size is 128.&lt;/li&gt;
&lt;/ul&gt;
   &lt;a target="_blank" rel="noopener noreferrer" href="figs/batch2.png"&gt;&lt;img src="figs/batch2.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Noise-free image super-resolution does not have this property.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Predicting the residual can be interpreted as performing one gradient descent inference step at starting point (i.e., noisy image).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The parameters in DnCNN are mainly representing the image priors (task-independent), thus it is possible to learn a single model for different tasks, such as image denoising, image super-resolution and JPEG image deblocking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The left is the input image corrupted by different degradations, the right is the restored image by DnCNN-3.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/input.png"&gt;&lt;img src="figs/input.png" width="390px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/output.png"&gt;&lt;img src="figs/output.png" width="390px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-denoising" class="anchor" aria-hidden="true" href="#gaussian-denoising"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian Denoising&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The average PSNR(dB) results of different methods on the BSD68 dataset.&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Noise Level&lt;/th&gt;
&lt;th align="center"&gt;BM3D&lt;/th&gt;
&lt;th align="center"&gt;WNNM&lt;/th&gt;
&lt;th align="center"&gt;EPLL&lt;/th&gt;
&lt;th align="center"&gt;MLP&lt;/th&gt;
&lt;th align="center"&gt;CSF&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;DnCNN&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-B&lt;/th&gt;
&lt;th align="center"&gt;FDnCNN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;15&lt;/td&gt;
&lt;td align="center"&gt;31.07&lt;/td&gt;
&lt;td align="center"&gt;31.37&lt;/td&gt;
&lt;td align="center"&gt;31.21&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;31.24&lt;/td&gt;
&lt;td align="center"&gt;31.42&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;31.73&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;31.61&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;31.69&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;25&lt;/td&gt;
&lt;td align="center"&gt;28.57&lt;/td&gt;
&lt;td align="center"&gt;28.83&lt;/td&gt;
&lt;td align="center"&gt;28.68&lt;/td&gt;
&lt;td align="center"&gt;28.96&lt;/td&gt;
&lt;td align="center"&gt;28.74&lt;/td&gt;
&lt;td align="center"&gt;28.92&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.23&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.16&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.22&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;50&lt;/td&gt;
&lt;td align="center"&gt;25.62&lt;/td&gt;
&lt;td align="center"&gt;  25.87  &lt;/td&gt;
&lt;td align="center"&gt;25.67&lt;/td&gt;
&lt;td align="center"&gt;26.03&lt;/td&gt;
&lt;td align="center"&gt;   -  &lt;/td&gt;
&lt;td align="center"&gt;25.97&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.23&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.23&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.27&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Visual Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The left is the noisy image corrupted by AWGN, the middle is the denoised image by DnCNN, the right is the ground-truth.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/05_25_noisy.png"&gt;&lt;img src="figs/05_25_noisy.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/05_25.png"&gt;&lt;img src="figs/05_25.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="testsets/Set12/05.png"&gt;&lt;img src="testsets/Set12/05.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/02_25_noisy.png"&gt;&lt;img src="figs/02_25_noisy.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/02_25.png"&gt;&lt;img src="figs/02_25.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="testsets/Set12/02.png"&gt;&lt;img src="testsets/Set12/02.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/102061_noisy.png"&gt;&lt;img src="figs/102061_noisy.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/102061_dncnn.png"&gt;&lt;img src="figs/102061_dncnn.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/102061.png"&gt;&lt;img src="figs/102061.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-denoising-single-imagesuper-resolution-and-jpeg-image-deblocking-via-a-single-dncnn-3-model" class="anchor" aria-hidden="true" href="#gaussian-denoising-single-imagesuper-resolution-and-jpeg-image-deblocking-via-a-single-dncnn-3-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian Denoising, Single ImageSuper-Resolution and JPEG Image Deblocking via a Single (DnCNN-3) Model&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Average PSNR(dB)/SSIM results of different methods for Gaussian denoising with noise level 15, 25 and 50 on BSD68 dataset, single image super-resolution with
upscaling factors 2, 3 and 40 on Set5, Set14, BSD100 and Urban100 datasets, JPEG image deblocking with quality factors 10, 20, 30 and 40 on Classic5 and LIVE11 datasets.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gaussian-denoising-1" class="anchor" aria-hidden="true" href="#gaussian-denoising-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian Denoising&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Noise Level&lt;/th&gt;
&lt;th align="center"&gt;BM3D&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;15&lt;/td&gt;
&lt;td align="center"&gt;31.08 / 0.8722&lt;/td&gt;
&lt;td align="center"&gt;31.42 / 0.8826&lt;/td&gt;
&lt;td align="center"&gt;31.46 / 0.8826&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;BSD68&lt;/td&gt;
&lt;td align="center"&gt;25&lt;/td&gt;
&lt;td align="center"&gt;28.57 / 0.8017&lt;/td&gt;
&lt;td align="center"&gt;28.92 / 0.8157&lt;/td&gt;
&lt;td align="center"&gt;29.02 / 0.8190&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;50&lt;/td&gt;
&lt;td align="center"&gt;25.62 / 0.6869&lt;/td&gt;
&lt;td align="center"&gt;25.97 / 0.7029&lt;/td&gt;
&lt;td align="center"&gt;26.10 / 0.7076&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-single-image-super-resolution" class="anchor" aria-hidden="true" href="#single-image-super-resolution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Single Image Super-Resolution&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Upscaling Factor&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;VDSR&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;36.86 / 0.9556&lt;/td&gt;
&lt;td align="center"&gt;37.56 / 0.9591&lt;/td&gt;
&lt;td align="center"&gt;37.58 / 0.9590&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Set5&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;33.18 / 0.9152&lt;/td&gt;
&lt;td align="center"&gt;33.67 / 0.9220&lt;/td&gt;
&lt;td align="center"&gt;33.75 / 0.9222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;30.85 / 0.8732&lt;/td&gt;
&lt;td align="center"&gt;31.35 / 0.8845&lt;/td&gt;
&lt;td align="center"&gt;31.40 / 0.8845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;32.51 / 0.9069&lt;/td&gt;
&lt;td align="center"&gt;33.02 / 0.9128&lt;/td&gt;
&lt;td align="center"&gt;33.03 / 0.9128&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Set14&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;29.43 / 0.8232&lt;/td&gt;
&lt;td align="center"&gt;29.77 / 0.8318&lt;/td&gt;
&lt;td align="center"&gt;29.81 / 0.8321&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;27.66 / 0.7563&lt;/td&gt;
&lt;td align="center"&gt;27.99 / 0.7659&lt;/td&gt;
&lt;td align="center"&gt;28.04 / 0.7672&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;31.40 / 0.8878&lt;/td&gt;
&lt;td align="center"&gt;31.89 / 0.8961&lt;/td&gt;
&lt;td align="center"&gt;31.90 / 0.8961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;BSD100&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;28.50 / 0.7881&lt;/td&gt;
&lt;td align="center"&gt;28.82 / 0.7980&lt;/td&gt;
&lt;td align="center"&gt;28.85 / 0.7981&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;27.00 / 0.7140&lt;/td&gt;
&lt;td align="center"&gt;27.28 / 0.7256&lt;/td&gt;
&lt;td align="center"&gt;27.29 / 0.7253&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;29.70 / 0.8994&lt;/td&gt;
&lt;td align="center"&gt;30.76 / 0.9143&lt;/td&gt;
&lt;td align="center"&gt;30.74 / 0.9139&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Urban100&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;26.42 / 0.8076&lt;/td&gt;
&lt;td align="center"&gt;27.13 / 0.8283&lt;/td&gt;
&lt;td align="center"&gt;27.15 / 0.8276&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;24.61 / 0.7291&lt;/td&gt;
&lt;td align="center"&gt;25.17 / 0.7528&lt;/td&gt;
&lt;td align="center"&gt;25.20 / 0.7521&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-jpeg-image-deblocking" class="anchor" aria-hidden="true" href="#jpeg-image-deblocking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;JPEG Image Deblocking&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Quality Factor&lt;/th&gt;
&lt;th align="center"&gt;AR-CNN&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Classic5&lt;/td&gt;
&lt;td align="center"&gt;10&lt;/td&gt;
&lt;td align="center"&gt;29.03 / 0.7929&lt;/td&gt;
&lt;td align="center"&gt;29.28 / 0.7992&lt;/td&gt;
&lt;td align="center"&gt;29.40 / 0.8026&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;20&lt;/td&gt;
&lt;td align="center"&gt;31.15 / 0.8517&lt;/td&gt;
&lt;td align="center"&gt;31.47 / 0.8576&lt;/td&gt;
&lt;td align="center"&gt;31.63 / 0.8610&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;30&lt;/td&gt;
&lt;td align="center"&gt;32.51 / 0.8806&lt;/td&gt;
&lt;td align="center"&gt;32.78 / 0.8837&lt;/td&gt;
&lt;td align="center"&gt;32.91 / 0.8861&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;40&lt;/td&gt;
&lt;td align="center"&gt;33.34 / 0.8953&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;33.77 / 0.9003&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;LIVE1&lt;/td&gt;
&lt;td align="center"&gt;10&lt;/td&gt;
&lt;td align="center"&gt;28.96 / 0.8076&lt;/td&gt;
&lt;td align="center"&gt;29.15 / 0.8111&lt;/td&gt;
&lt;td align="center"&gt;29.19 / 0.8123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;20&lt;/td&gt;
&lt;td align="center"&gt;31.29 / 0.8733&lt;/td&gt;
&lt;td align="center"&gt;31.46 / 0.8769&lt;/td&gt;
&lt;td align="center"&gt;31.59 / 0.8802&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;30&lt;/td&gt;
&lt;td align="center"&gt;32.67 / 0.9043&lt;/td&gt;
&lt;td align="center"&gt;32.84 / 0.9059&lt;/td&gt;
&lt;td align="center"&gt;32.98 / 0.9090&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;40&lt;/td&gt;
&lt;td align="center"&gt;33.63 / 0.9198&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;33.96 / 0.9247&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-requirements-and-dependencies" class="anchor" aria-hidden="true" href="#requirements-and-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements and Dependencies&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;MATLAB R2015b&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/cuda-toolkit-archive" rel="nofollow"&gt;Cuda&lt;/a&gt;-8.0 &amp;amp; &lt;a href="https://developer.nvidia.com/cudnn" rel="nofollow"&gt;cuDNN&lt;/a&gt; v-5.1&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.vlfeat.org/matconvnet/" rel="nofollow"&gt;MatConvNet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or just MATLAB R2015b to test the model.
&lt;a href="https://github.com/cszn/DnCNN/blob/4a4b5b8bcac5a5ac23433874d4362329b25522ba/Demo_test_DnCNN.m#L64-L65"&gt;https://github.com/cszn/DnCNN/blob/4a4b5b8bcac5a5ac23433874d4362329b25522ba/Demo_test_DnCNN.m#L64-L65&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;@article{zhang2017beyond,
  title={Beyond a {Gaussian} denoiser: Residual learning of deep {CNN} for image denoising},
  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
  journal={IEEE Transactions on Image Processing},
  year={2017},
  volume={26}, 
  number={7}, 
  pages={3142-3155}, 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;====================================================================&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks-for-image-denoising-and-restoration" class="anchor" aria-hidden="true" href="#convolutional-neural-networks-for-image-denoising-and-restoration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-96029-6_4" rel="nofollow"&gt;Convolutional Neural Networks for Image Denoising and Restoration&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@Inbook{zuo2018convolutional,
author={Zuo, Wangmeng and Zhang, Kai and Zhang, Lei},
editor={Bertalm{\'i}o, Marcelo},
title={Convolutional Neural Networks for Image Denoising and Restoration},
bookTitle={Denoising of Photographic Images and Video: Fundamentals, Open Challenges and New Trends},
year={2018},
publisher={Springer International Publishing},
address={Cham},
pages={93--123},
isbn={978-3-319-96029-6},
doi={10.1007/978-3-319-96029-6_4},
url={https://doi.org/10.1007/978-3-319-96029-6_4}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-challenges-and-possible-solutions-from-the-above-book-chapter" class="anchor" aria-hidden="true" href="#challenges-and-possible-solutions-from-the-above-book-chapter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Challenges and Possible Solutions (from the above book chapter)&lt;/h3&gt;
&lt;p&gt;While the image denoising for AWGN removal has been well-studied, little work has been done on real image denoising.
The main difficulty arises from the fact that real noises are much more complex than AWGN and it is not an easy task to
thoroughly evaluate the performance of a denoiser. Fig. 4.15 shows four typical noise types in real world.
It can be seen that the characteristics of those noises are very different and a single noise level may be not enough to parameterize those noise types. In most cases, a denoiser can only work well under a certain noise model.
For example, a denoising model trained for AWGN removal is not effective for mixed Gaussian and Poisson noise removal.
This is intuitively reasonable because the CNN-based methods can be treated as general case of Eq. (4.3) and the important data fidelity term corresponds to the degradation process. In spite of this, the image denoising for AWGN removal is valuable due to the following reasons. First, it is an ideal test bed to evaluate the effectiveness of different CNN-based denoising methods.
Second, in the unrolled inference via variable splitting techniques, many image restoration problems can be addressed by sequentially solving a series of Gaussian denoising subproblems, which further broadens the application fields.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/noisetype.png"&gt;&lt;img src="figs/noisetype.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To improve the practicability of a CNN denoiser, perhaps the most straightforward way is to capture adequate amounts of real noisy-clean training pairs for training so that the real degradation space can be covered. This solution has advantage that there is no need to know the complex degradation process. However, deriving the corresponding clean image of a noisy one is not a trivial task due to the need of careful post-processing steps, such as spatial alignment and illumination correction. Alternatively, one can simulate the real degradation process to synthesize noisy images for a clean one. However, it is not easy to accurately model the complex degradation process. In particular, the noise model can be different across different cameras. Nevertheless, it is practically preferable to roughly model a certain noise type for training and then use the learned CNN model for type-specific denoising.&lt;/p&gt;
&lt;p&gt;Besides the training data, the robust architecture and robust training also play vital roles for the success of a CNN denoiser.
For the robust architecture, designing a deep multiscale CNN which involves a coarse-to-fine procedure is a promising direction.
Such a network is expected to inherit the merits of multiscale:
(i) the noise level decreases at larger scales;
(ii) the ubiquitous low-frequency noise can be alleviated by multiscale procedure;
and (iii) downsampling the image before denoising can effectively enlarge the receptive filed.
For the robust training, the effectiveness of the denoiser trained with generative adversarial networks (GAN) for real image denoising still remains further investigation. The main idea of GAN-based denoising is to introduce an adversarial loss to improve the perceptual quality of denoised image. Besides, a distinctive advantage of GAN is that it can do unsupervised learning. More specifically,
the noisy image without ground truth can be used in the training. So far, we have provided several possible solutions to improve the practicability of a CNN denoiser. We should note that those solutions can be combined to further improve the performance.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cszn</author><guid isPermaLink="false">https://github.com/cszn/DnCNN</guid><pubDate>Fri, 20 Dec 2019 00:01:00 GMT</pubDate></item><item><title>Borye/machine-learning-coursera-1 #2 in MATLAB, This week</title><link>https://github.com/Borye/machine-learning-coursera-1</link><description>&lt;p&gt;&lt;i&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-coursera&lt;/h1&gt;
&lt;p&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Borye</author><guid isPermaLink="false">https://github.com/Borye/machine-learning-coursera-1</guid><pubDate>Fri, 20 Dec 2019 00:02:00 GMT</pubDate></item><item><title>luanfujun/deep-photo-styletransfer #3 in MATLAB, This week</title><link>https://github.com/luanfujun/deep-photo-styletransfer</link><description>&lt;p&gt;&lt;i&gt;Code and data for paper "Deep Photo Style Transfer": https://arxiv.org/abs/1703.07511 &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-photo-styletransfer" class="anchor" aria-hidden="true" href="#deep-photo-styletransfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;deep-photo-styletransfer&lt;/h1&gt;
&lt;p&gt;Code and data for paper "&lt;a href="https://arxiv.org/abs/1703.07511" rel="nofollow"&gt;Deep Photo Style Transfer&lt;/a&gt;"&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This software is published for academic and non-commercial use only.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;p&gt;This code is based on torch. It has been tested on Ubuntu 14.04 LTS.&lt;/p&gt;
&lt;p&gt;Dependencies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/torch/torch7"&gt;Torch&lt;/a&gt; (with &lt;a href="https://github.com/soumith/matio-ffi.torch"&gt;matio-ffi&lt;/a&gt; and &lt;a href="https://github.com/szagoruyko/loadcaffe"&gt;loadcaffe&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mathworks.com/" rel="nofollow"&gt;Matlab&lt;/a&gt; or &lt;a href="https://www.gnu.org/software/octave/" rel="nofollow"&gt;Octave&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CUDA backend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow"&gt;CUDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/cudnn" rel="nofollow"&gt;cudnn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download VGG-19:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sh models/download_models.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile &lt;code&gt;cuda_utils.cu&lt;/code&gt; (Adjust &lt;code&gt;PREFIX&lt;/code&gt; and &lt;code&gt;NVCC_PREFIX&lt;/code&gt; in &lt;code&gt;makefile&lt;/code&gt; for your machine):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make clean &amp;amp;&amp;amp; make
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h3&gt;
&lt;p&gt;To generate all results (in &lt;code&gt;examples/&lt;/code&gt;) using the provided scripts, simply run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;run('gen_laplacian/gen_laplacian.m')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in Matlab or Octave and then&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python gen_all.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in Python. The final output will be in &lt;code&gt;examples/final_results/&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-basic-usage" class="anchor" aria-hidden="true" href="#basic-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic usage&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Given input and style images with semantic segmentation masks, put them in &lt;code&gt;examples/&lt;/code&gt; respectively. They will have the following filename form: &lt;code&gt;examples/input/in&amp;lt;id&amp;gt;.png&lt;/code&gt;, &lt;code&gt;examples/style/tar&amp;lt;id&amp;gt;.png&lt;/code&gt; and &lt;code&gt;examples/segmentation/in&amp;lt;id&amp;gt;.png&lt;/code&gt;, &lt;code&gt;examples/segmentation/tar&amp;lt;id&amp;gt;.png&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Compute the matting Laplacian matrix using &lt;code&gt;gen_laplacian/gen_laplacian.m&lt;/code&gt; in Matlab. The output matrix will have the following filename form: &lt;code&gt;gen_laplacian/Input_Laplacian_3x3_1e-7_CSR&amp;lt;id&amp;gt;.mat&lt;/code&gt;;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Note: Please make sure that the content image resolution is consistent for Matting Laplacian computation in Matlab and style transfer in Torch, otherwise the result won't be correct.&lt;/strong&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Run the following script to generate segmented intermediate result:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;th neuralstyle_seg.lua -content_image &amp;lt;input&amp;gt; -style_image &amp;lt;style&amp;gt; -content_seg &amp;lt;inputMask&amp;gt; -style_seg &amp;lt;styleMask&amp;gt; -index &amp;lt;id&amp;gt; -serial &amp;lt;intermediate_folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Run the following script to generate final result:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;th deepmatting_seg.lua -content_image &amp;lt;input&amp;gt; -style_image &amp;lt;style&amp;gt; -content_seg &amp;lt;inputMask&amp;gt; -style_seg &amp;lt;styleMask&amp;gt; -index &amp;lt;id&amp;gt; -init_image &amp;lt;intermediate_folder/out&amp;lt;id&amp;gt;_t_1000.png&amp;gt; -serial &amp;lt;final_folder&amp;gt; -f_radius 15 -f_edge 0.01
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can pass &lt;code&gt;-backend cudnn&lt;/code&gt; and &lt;code&gt;-cudnn_autotune&lt;/code&gt; to both Lua scripts (step 3.
and 4.) to potentially improve speed and memory usage. &lt;code&gt;libcudnn.so&lt;/code&gt; must be in
your &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt;. This requires &lt;a href="https://github.com/soumith/cudnn.torch"&gt;cudnn.torch&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-segmentation" class="anchor" aria-hidden="true" href="#image-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image segmentation&lt;/h3&gt;
&lt;p&gt;Note: In the main paper we generate all comparison results using automatic scene segmentation algorithm modified from &lt;a href="https://arxiv.org/abs/1606.00915" rel="nofollow"&gt;DilatedNet&lt;/a&gt;. Manual segmentation enables more diverse tasks hence we provide the masks in &lt;code&gt;examples/segmentation/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The mask colors we used (you could add more colors in &lt;code&gt;ExtractMask&lt;/code&gt; function in two &lt;code&gt;*.lua&lt;/code&gt; files):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Color variable&lt;/th&gt;
&lt;th&gt;RGB Value&lt;/th&gt;
&lt;th&gt;Hex Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;blue&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0 0 255&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0000ff&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;green&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0 255 0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;00ff00&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;black&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0 0 0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;000000&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;white&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;255 255 255&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ffffff&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;red&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;255 0 0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ff0000&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;yellow&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;255 255 0&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ffff00&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;grey&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;128 128 128&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;808080&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;lightblue&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0 255 255&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;00ffff&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;purple&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;255 0 255&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ff00ff &lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here are some automatic and manual tools for creating a segmentation mask for a photo image:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-automatic" class="anchor" aria-hidden="true" href="#automatic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automatic:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sceneparsing.csail.mit.edu/" rel="nofollow"&gt;MIT Scene Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.unc.edu/~jtighe/Papers/ECCV10/" rel="nofollow"&gt;SuperParsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.csail.mit.edu/celiu/LabelTransfer/" rel="nofollow"&gt;Nonparametric Scene Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html" rel="nofollow"&gt;Berkeley Contour Detection and Image Segmentation Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/torrvision/crfasrnn"&gt;CRF-RNN for Semantic Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/belltailjp/selective_search_py"&gt;Selective Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DrSleep/tensorflow-deeplab-lfov"&gt;DeepLab-TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-manual" class="anchor" aria-hidden="true" href="#manual"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://helpx.adobe.com/photoshop/using/making-quick-selections.html" rel="nofollow"&gt;Photoshop Quick Selection Tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.gimp.org/en/gimp-tools-selection.html" rel="nofollow"&gt;GIMP Selection Tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gmic.eu/gimp.shtml" rel="nofollow"&gt;GIMP G'MIC Interactive Foreground Extraction tool&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;Here are some results from our algorithm (from left to right are input, style and our output):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in3.png"&gt;&lt;img src="examples/input/in3.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar3.png"&gt;&lt;img src="examples/style/tar3.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_3.png"&gt;&lt;img src="examples/refine_posterization/refine_3.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in4.png"&gt;&lt;img src="examples/input/in4.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar4.png"&gt;&lt;img src="examples/style/tar4.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_4.png"&gt;&lt;img src="examples/refine_posterization/refine_4.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in13.png"&gt;&lt;img src="examples/input/in13.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar13.png"&gt;&lt;img src="examples/style/tar13.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_13.png"&gt;&lt;img src="examples/refine_posterization/refine_13.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in9.png"&gt;&lt;img src="examples/input/in9.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar9.png"&gt;&lt;img src="examples/style/tar9.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_9.png"&gt;&lt;img src="examples/refine_posterization/refine_9.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in20.png"&gt;&lt;img src="examples/input/in20.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar20.png"&gt;&lt;img src="examples/style/tar20.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_20.png"&gt;&lt;img src="examples/refine_posterization/refine_20.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in1.png"&gt;&lt;img src="examples/input/in1.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar1.png"&gt;&lt;img src="examples/style/tar1.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_1.png"&gt;&lt;img src="examples/refine_posterization/refine_1.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in39.png"&gt;&lt;img src="examples/input/in39.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar39.png"&gt;&lt;img src="examples/style/tar39.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_39.png"&gt;&lt;img src="examples/refine_posterization/refine_39.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in57.png"&gt;&lt;img src="examples/input/in57.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar57.png"&gt;&lt;img src="examples/style/tar57.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_57.png"&gt;&lt;img src="examples/refine_posterization/refine_57.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in47.png"&gt;&lt;img src="examples/input/in47.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar47.png"&gt;&lt;img src="examples/style/tar47.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_47.png"&gt;&lt;img src="examples/refine_posterization/refine_47.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in58.png"&gt;&lt;img src="examples/input/in58.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar58.png"&gt;&lt;img src="examples/style/tar58.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_58.png"&gt;&lt;img src="examples/refine_posterization/refine_58.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in51.png"&gt;&lt;img src="examples/input/in51.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar51.png"&gt;&lt;img src="examples/style/tar51.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_51.png"&gt;&lt;img src="examples/refine_posterization/refine_51.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in7.png"&gt;&lt;img src="examples/input/in7.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar7.png"&gt;&lt;img src="examples/style/tar7.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_7.png"&gt;&lt;img src="examples/refine_posterization/refine_7.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in23.png"&gt;&lt;img src="examples/input/in23.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in23.png"&gt;&lt;img src="examples/input/in23.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/final_results/best23_t_1000.png"&gt;&lt;img src="examples/final_results/best23_t_1000.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in16.png"&gt;&lt;img src="examples/input/in16.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar16.png"&gt;&lt;img src="examples/style/tar16.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_16.png"&gt;&lt;img src="examples/refine_posterization/refine_16.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in30.png"&gt;&lt;img src="examples/input/in30.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar30.png"&gt;&lt;img src="examples/style/tar30.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_30.png"&gt;&lt;img src="examples/refine_posterization/refine_30.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in2.png"&gt;&lt;img src="examples/input/in2.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar2.png"&gt;&lt;img src="examples/style/tar2.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/final_results/best2_t_1000.png"&gt;&lt;img src="examples/final_results/best2_t_1000.png" height="194" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/input/in11.png"&gt;&lt;img src="examples/input/in11.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/style/tar11.png"&gt;&lt;img src="examples/style/tar11.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="examples/refine_posterization/refine_11.png"&gt;&lt;img src="examples/refine_posterization/refine_11.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our torch implementation is based on Justin Johnson's &lt;a href="https://github.com/jcjohnson/neural-style"&gt;code&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;We use Anat Levin's Matlab &lt;a href="http://www.wisdom.weizmann.ac.il/~levina/matting.tar.gz" rel="nofollow"&gt;code&lt;/a&gt; to compute the matting Laplacian matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you find this work useful for your research, please cite:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{luan2017deep,
  title={Deep Photo Style Transfer},
  author={Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
  journal={arXiv preprint arXiv:1703.07511},
  year={2017}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;Feel free to contact me if there is any question (Fujun Luan &lt;a href="mailto:fl356@cornell.edu"&gt;fl356@cornell.edu&lt;/a&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>luanfujun</author><guid isPermaLink="false">https://github.com/luanfujun/deep-photo-styletransfer</guid><pubDate>Fri, 20 Dec 2019 00:03:00 GMT</pubDate></item><item><title>TadasBaltrusaitis/OpenFace #4 in MATLAB, This week</title><link>https://github.com/TadasBaltrusaitis/OpenFace</link><description>&lt;p&gt;&lt;i&gt;OpenFace – a state-of-the art tool intended for facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openface-220-a-facial-behavior-analysis-toolkit" class="anchor" aria-hidden="true" href="#openface-220-a-facial-behavior-analysis-toolkit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenFace 2.2.0: a facial behavior analysis toolkit&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/TadasBaltrusaitis/OpenFace" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4243d567083c3a651ecbadc77c77a00ba697fb40/68747470733a2f2f7472617669732d63692e6f72672f546164617342616c7472757361697469732f4f70656e466163652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/TadasBaltrusaitis/OpenFace.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/TadasBaltrusaitis/openface/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c6fb6db38385292d352f1b17205f42469d06fda/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f386d73696b6c786662686c6e736d78702f6272616e63682f6d61737465723f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/8msiklxfbhlnsmxp/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over the past few years, there has been an increased interest in automatic facial behavior analysis
and understanding. We present OpenFace – a tool intended for computer vision and machine learning
researchers, affective computing community and people interested in building interactive
applications based on facial behavior analysis. OpenFace is the ﬁrst toolkit capable of facial
landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation
with available source code for both running and training the models. The computer vision algorithms
which represent the core of OpenFace demonstrate state-of-the-art results in all of the above
mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a
simple webcam without any specialist hardware.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/muticomp_logo_black.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/muticomp_logo_black.png" alt="Multicomp logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenFace was originally developed by Tadas Baltrušaitis in collaboration with CMU MultiComp Lab led by Prof. Louis-Philippe Morency. Some of the original algorithms were created while at Rainbow Group, Cambridge University. The OpenFace library is still actively developed at the CMU MultiComp Lab in collaboration with Tadas Baltršaitis. Special thanks to researcher who helped developing, implementing and testing the algorithms present in OpenFace: Amir Zadeh and Yao Chong Lim on work on the CE-CLM model and Erroll Wood for the gaze estimation work.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WIKI&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;For instructions of how to install/compile/use the project please see &lt;a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki"&gt;WIKI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-functionality" class="anchor" aria-hidden="true" href="#functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Functionality&lt;/h2&gt;
&lt;p&gt;The system is capable of performing a number of facial analysis tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Landmark Detection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/multi_face_img.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/multi_face_img.png" alt="Sample facial landmark detection image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Landmark and head pose tracking (links to YouTube videos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=V7rV0uy7heQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/861e54570c553cb53c797dd8bc57a24f29152655/687474703a2f2f696d672e796f75747562652e636f6d2f76692f56377256307579376865512f302e6a7067" alt="Multiple Face Tracking" width="240" height="180" border="10" data-canonical-src="http://img.youtube.com/vi/V7rV0uy7heQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.youtube.com/watch?v=vYOa8Pif5lY" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a9137826a1740e8df2125ef9b76541445e32957e/687474703a2f2f696d672e796f75747562652e636f6d2f76692f76594f6138506966356c592f302e6a7067" alt="Multiple Face Tracking" width="240" height="180" border="10" data-canonical-src="http://img.youtube.com/vi/vYOa8Pif5lY/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Action Unit Recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/au_sample.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/au_sample.png" height="280" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaze tracking (image of it in action)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/gaze_ex.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/gaze_ex.png" height="182" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Feature Extraction (aligned faces and HOG features)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/appearance.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/appearance.png" alt="Sample aligned face and HOG image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use any of the resources provided on this page in any of your publications we ask you to cite the following work and the work for a relevant submodule you used.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-overall-system" class="anchor" aria-hidden="true" href="#overall-system"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overall system&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;OpenFace 2.0: Facial Behavior Analysis Toolkit&lt;/strong&gt;
Tadas Baltrušaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency,
&lt;em&gt;IEEE International Conference on Automatic Face and Gesture Recognition&lt;/em&gt;, 2018&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facial-landmark-detection-and-tracking" class="anchor" aria-hidden="true" href="#facial-landmark-detection-and-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facial landmark detection and tracking&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Convolutional experts constrained local model for facial landmark detection&lt;/strong&gt;
A. Zadeh, T. Baltrušaitis, and Louis-Philippe Morency.
&lt;em&gt;Computer Vision and Pattern Recognition Workshops&lt;/em&gt;, 2017&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Constrained Local Neural Fields for robust facial landmark detection in the wild&lt;/strong&gt;
Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency.
in IEEE Int. &lt;em&gt;Conference on Computer Vision Workshops, 300 Faces in-the-Wild Challenge&lt;/em&gt;, 2013.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-eye-gaze-tracking" class="anchor" aria-hidden="true" href="#eye-gaze-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eye gaze tracking&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Rendering of Eyes for Eye-Shape Registration and Gaze Estimation&lt;/strong&gt;
Erroll Wood, Tadas Baltrušaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, and Andreas Bulling
in &lt;em&gt;IEEE International Conference on Computer Vision (ICCV)&lt;/em&gt;, 2015&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facial-action-unit-detection" class="anchor" aria-hidden="true" href="#facial-action-unit-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facial Action Unit detection&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Cross-dataset learning and person-specific normalisation for automatic Action Unit detection&lt;/strong&gt;
Tadas Baltrušaitis, Marwa Mahmoud, and Peter Robinson
in &lt;em&gt;Facial Expression Recognition and Analysis Challenge&lt;/em&gt;,
&lt;em&gt;IEEE International Conference on Automatic Face and Gesture Recognition&lt;/em&gt;, 2015&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-commercial-license" class="anchor" aria-hidden="true" href="#commercial-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Commercial license&lt;/h1&gt;
&lt;p&gt;For inquiries about the commercial licensing of the OpenFace toolkit please visit &lt;a href="https://www.flintbox.com/public/project/50632/" rel="nofollow"&gt;https://www.flintbox.com/public/project/50632/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-final-remarks" class="anchor" aria-hidden="true" href="#final-remarks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Final remarks&lt;/h1&gt;
&lt;p&gt;I did my best to make sure that the code runs out of the box but there are always issues and I would be grateful for your understanding that this is research code and a research project. If you encounter any problems/bugs/issues please contact me on github or by emailing me at &lt;a href="mailto:tadyla@gmail.com"&gt;tadyla@gmail.com&lt;/a&gt; for any bug reports/questions/suggestions. I prefer questions and bug reports on github as that provides visibility to others who might be encountering same issues or who have the same questions.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h1&gt;
&lt;p&gt;Copyright can be found in the Copyright.txt&lt;/p&gt;
&lt;p&gt;You have to respect dlib, OpenBLAS, and OpenCV licenses.&lt;/p&gt;
&lt;p&gt;Furthermore you have to respect the licenses of the datasets used for model training - &lt;a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Datasets"&gt;https://github.com/TadasBaltrusaitis/OpenFace/wiki/Datasets&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>TadasBaltrusaitis</author><guid isPermaLink="false">https://github.com/TadasBaltrusaitis/OpenFace</guid><pubDate>Fri, 20 Dec 2019 00:04:00 GMT</pubDate></item><item><title>atinesh-s/Coursera-Machine-Learning-Stanford #5 in MATLAB, This week</title><link>https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</link><description>&lt;p&gt;&lt;i&gt;Machine learning-Stanford University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning (Coursera)&lt;/h1&gt;
&lt;p&gt;This is my solution to all the programming assignments and quizzes of Machine-Learning (Coursera) taught by Andrew Ng. After completing this course you will get a broad idea of Machine learning algorithms. Try to solve all the assignments by yourself first, but if you get stuck somewhere then feel free to browse the code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lectures Slides&lt;/li&gt;
&lt;li&gt;Solution to programming assignment&lt;/li&gt;
&lt;li&gt;Solution to Quizzes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-certificate" class="anchor" aria-hidden="true" href="#certificate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Certificate&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/account/accomplishments/certificate/GDDBFB572MUQ" rel="nofollow"&gt;Verified Certificate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;[1] Machine Learning - Stanford University&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>atinesh-s</author><guid isPermaLink="false">https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</guid><pubDate>Fri, 20 Dec 2019 00:05:00 GMT</pubDate></item></channel></rss>