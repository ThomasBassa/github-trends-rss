<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, This week</title><link>https://github.com/trending/python?since=weekly</link><description>The top repositories on GitHub for python, measured weekly</description><pubDate>Tue, 05 Nov 2019 01:06:21 GMT</pubDate><lastBuildDate>Tue, 05 Nov 2019 01:06:21 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>iGhibli/iOS-DeviceSupport #1 in Python, This week</title><link>https://github.com/iGhibli/iOS-DeviceSupport</link><description>&lt;p&gt;&lt;i&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ios-devicesupport" class="anchor" aria-hidden="true" href="#ios-devicesupport"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;iOS-DeviceSupport&lt;/h1&gt;
&lt;p&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;See docs: &lt;a href="https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/" rel="nofollow"&gt;https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Below command will try to unzip all new device support files to &lt;code&gt;/Applications/Xcode.app&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can use &lt;code&gt;-t&lt;/code&gt; if your Xcode is not in &lt;code&gt;/Applications/&lt;/code&gt; or has different name.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py -t /Applications/Xcode&lt;span class="pl-cce"&gt;\ &lt;/span&gt;9.app&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  -t TARGET   The path &lt;span class="pl-k"&gt;for&lt;/span&gt; Xcode&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-supported-versions" class="anchor" aria-hidden="true" href="#supported-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported versions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;iOS8
&lt;ul&gt;
&lt;li&gt;8.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.4 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS9
&lt;ul&gt;
&lt;li&gt;9.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS10
&lt;ul&gt;
&lt;li&gt;10.0 (14A345) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 (14B72) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 (14C92) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 (14E269) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS11
&lt;ul&gt;
&lt;li&gt;11.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 (15B87) &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 (15C107) &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5167d) &lt;code&gt;2018/01/30&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5201e) &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5037c) &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5061c) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F79) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS12
&lt;ul&gt;
&lt;li&gt;12.0 (16A5288q) &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5308d) &lt;code&gt;2018/06/19&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5318d) &lt;code&gt;2018/06/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5327d) &lt;code&gt;2018/07/20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5339e) &lt;code&gt;2018/07/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5354b) &lt;code&gt;2018/08/15&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A366) &lt;code&gt;2018/09/18&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5059d) &lt;code&gt;2018/09/21&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5068g) &lt;code&gt;2018/10/08&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B91) &lt;code&gt;2018/10/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5181e) &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5212e) &lt;code&gt;2019/03/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E226) &lt;code&gt;2019/03/27&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.3 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (16G73) &lt;code&gt;2019/07/22&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS13
&lt;ul&gt;
&lt;li&gt;13.0 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.0 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.1 &lt;code&gt;2019/08/28&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.2 &lt;code&gt;2019/10/02&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iGhibli</author><guid isPermaLink="false">https://github.com/iGhibli/iOS-DeviceSupport</guid><pubDate>Tue, 05 Nov 2019 00:01:00 GMT</pubDate></item><item><title>google-research/google-research #2 in Python, This week</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Tue, 05 Nov 2019 00:02:00 GMT</pubDate></item><item><title>bokeh/bokeh #3 in Python, This week</title><link>https://github.com/bokeh/bokeh</link><description>&lt;p&gt;&lt;i&gt;Interactive Data Visualization in the browser, from  Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a href="https://bokeh.org" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/23c4767f9deb6835e161e7bee64eeaa1125f0721/68747470733a2f2f7374617469632e626f6b65682e6f72672f6c6f676f732f6c6f676f747970652e737667" height="60" width="150" alt="Bokeh logotype" data-canonical-src="https://static.bokeh.org/logos/logotype.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Bokeh is a fiscally sponsored project of &lt;a href="https://numfocus.org" rel="nofollow"&gt;NumFOCUS&lt;/a&gt;, a nonprofit dedicated to supporting the open-source scientific computing community. If you like Bokeh and would like to support our mission, please consider &lt;a href="https://numfocus.org/donate-to-bokeh" rel="nofollow"&gt;making a donation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;Latest Release&lt;/td&gt;
  &lt;td&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/319e5c4665c805f31c4e03601804b746c5d74925/68747470733a2f2f62616467652e667572792e696f2f67682f626f6b6568253246626f6b65682e737667"&gt;&lt;img src="https://camo.githubusercontent.com/319e5c4665c805f31c4e03601804b746c5d74925/68747470733a2f2f62616467652e667572792e696f2f67682f626f6b6568253246626f6b65682e737667" alt="Latest release version" data-canonical-src="https://badge.fury.io/gh/bokeh%2Fbokeh.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://badge.fury.io/js/bokehjs" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/95363bfe2ac7fb7e5eb9885a50868c1dba3a9a70/68747470733a2f2f62616467652e667572792e696f2f6a732f626f6b65686a732e737667" alt="npm version" data-canonical-src="https://badge.fury.io/js/bokehjs.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Conda&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://docs.bokeh.org/en/latest/docs/installation.html" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d0117bc93858f82f1a8ea5ebf5c19cabd97df9fd/68747470733a2f2f707976697a2e6f72672f5f7374617469632f63616368652f626f6b65685f636f6e64615f646f776e6c6f6164735f62616467652e737667" alt="Conda downloads per month" data-canonical-src="https://pyviz.org/_static/cache/bokeh_conda_downloads_badge.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;License&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://github.com/bokeh/bokeh/blob/master/LICENSE.txt"&gt;
    &lt;img src="https://camo.githubusercontent.com/abf7fceca8e58b3842bfed01c2e3c2ee1612fb09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f626f6b65682f626f6b65682e737667" alt="Bokeh license (BSD 3-clause)" data-canonical-src="https://img.shields.io/github/license/bokeh/bokeh.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;PyPI&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://docs.bokeh.org/en/latest/docs/installation.html" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/5ec94c8c7308f7fb219ac302aa09a50b46b18a82/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626f6b65682e737667" alt="PyPI downloads per month" data-canonical-src="https://img.shields.io/pypi/dm/bokeh.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Sponsorship&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="http://numfocus.org" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/28d3beb4213a1bfc61313a5b5a0be78b06e96c05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706f776572656425323062792d4e756d464f4355532d626c61636b2e7376673f7374796c653d666c617426636f6c6f72413d35423542354226636f6c6f72423d303037443841" alt="Powered by NumFOCUS" data-canonical-src="https://img.shields.io/badge/powered%20by-NumFOCUS-black.svg?style=flat&amp;amp;colorA=5B5B5B&amp;amp;colorB=007D8A" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Live Tutorial&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Live Bokeh tutorial notebooks on MyBinder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Build Status&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://travis-ci.org/bokeh/bokeh" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/8335f2a93fe3e1293a3571623b8d23ba262b1d30/68747470733a2f2f7472617669732d63692e6f72672f626f6b65682f626f6b65682e7376673f6272616e63683d6d6173746572" alt="Current TravisCI build status" data-canonical-src="https://travis-ci.org/bokeh/bokeh.svg?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://ci.appveyor.com/project/bokeh-integrations/bokeh" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/91e2de256d6ef59085db8d6fa178f4850fbf6cb3/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f753469646632356468703231396d686f3f7376673d74727565" alt="Current Appveyor build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/u4idf25dhp219mho?svg=true" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Support&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://discourse.bokeh.org" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/4bc630e601229d0ec09ce27647039334bede13ed/68747470733a2f2f696d672e736869656c64732e696f2f646973636f757273652f68747470732f646973636f757273652e626f6b65682e6f72672f706f7374732e737667" alt="Community Support on discourse.bokeh.org" data-canonical-src="https://img.shields.io/discourse/https/discourse.bokeh.org/posts.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Static Analysis&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/0045ca709534292b0c5d2f8fc628b22ca3cbcf11/68747470733a2f2f626574746572636f64656875622e636f6d2f656467652f62616467652f626f6b65682f626f6b65683f6272616e63683d6d6173746572" alt="BetterCodeHub static analysis" data-canonical-src="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Twitter&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://twitter.com/BokehPlots" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/92ffc787e59932d1af96a7126fac375f84816b55/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f626f6b6568706c6f74732e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Follow BokehPlots on Twitter" data-canonical-src="https://img.shields.io/twitter/follow/bokehplots.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;a href="https://bokeh.org" rel="nofollow"&gt;Bokeh&lt;/a&gt; is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics, and affords high-performance interactivity over large or streaming datasets. Bokeh can help anyone who would like to quickly and easily make interactive plots, dashboards, and data applications.&lt;/p&gt;
&lt;p&gt;
&lt;/p&gt;&lt;table cellspacing="10"&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/image.html" rel="nofollow"&gt;
  &lt;img alt="colormapped image plot thumbnail" src="https://camo.githubusercontent.com/f1f586b237dff8683e0ceaa2bdcb1b7e35f13f93/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f696d6167655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/image_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/anscombe.html" rel="nofollow"&gt;
  &lt;img alt="anscombe plot thumbnail" src="https://camo.githubusercontent.com/073188ee921204d9f4741a91ab4562ca196711fe/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f616e73636f6d62655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/anscombe_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/stocks.html" rel="nofollow"&gt;
  &lt;img alt="stocks plot thumbnail" src="https://camo.githubusercontent.com/e616cb26a235f6354269b268ae07eae87bac0c29/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73746f636b735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/stocks_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/lorenz.html" rel="nofollow"&gt;
  &lt;img alt="lorenz attractor plot thumbnail" src="https://camo.githubusercontent.com/7dfd551f33c5b51a099938c4e07349b28ba86387/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f6c6f72656e7a5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/lorenz_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/candlestick.html" rel="nofollow"&gt;
  &lt;img alt="candlestick plot thumbnail" src="https://camo.githubusercontent.com/c44c6fc6939b0db3896efcb4902a68674f9af381/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63616e646c65737469636b5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/candlestick_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/color_scatter.html" rel="nofollow"&gt;
  &lt;img alt="scatter plot thumbnail" src="https://camo.githubusercontent.com/c7dce46ac91e2097b13979e697d42f469c3d3b9c/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f736361747465725f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/scatter_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/iris_splom.html" rel="nofollow"&gt;
  &lt;img alt="SPLOM plot thumbnail" src="https://camo.githubusercontent.com/85ebf47ee37f3ea5093426c73b87ef7122435ba0/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73706c6f6d5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/splom_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/iris.html" rel="nofollow"&gt;
  &lt;img alt="iris dataset plot thumbnail" src="https://camo.githubusercontent.com/0198838e25d20407fc590705827632199daa32a3/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f697269735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/iris_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/histogram.html" rel="nofollow"&gt;
  &lt;img alt="histogram plot thumbnail" src="https://camo.githubusercontent.com/5ca5fb847641949daa5da2c3ebb7afec7029eb96/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f686973746f6772616d5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/histogram_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/periodic.html" rel="nofollow"&gt;
  &lt;img alt="periodic table plot thumbnail" src="https://camo.githubusercontent.com/dc1fa06b214455fd268f67ac0a4ed47a38795b52/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f706572696f6469635f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/periodic_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/texas.html" rel="nofollow"&gt;
  &lt;img alt="choropleth plot thumbnail" src="https://camo.githubusercontent.com/1d1d55f394731332a63780514673c251136f2f63/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63686f726f706c6574685f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/choropleth_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/burtin.html" rel="nofollow"&gt;
  &lt;img alt="burtin antibiotic data plot thumbnail" src="https://camo.githubusercontent.com/f8329c228dabe62292ae4727cf911d783ebe9371/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f62757274696e5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/burtin_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/streamline.html" rel="nofollow"&gt;
  &lt;img alt="streamline plot thumbnail" src="https://camo.githubusercontent.com/2a39c0491917ad87e8c1ed266d85af408953152e/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73747265616d6c696e655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/streamline_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/image_rgba.html" rel="nofollow"&gt;
  &lt;img alt="RGBA image plot thumbnail" src="https://camo.githubusercontent.com/5a0021bc298920caf8f200c8661df4da2dbb908f/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f696d6167655f726762615f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/image_rgba_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/brewer.html" rel="nofollow"&gt;
  &lt;img alt="stacked bars plot thumbnail" src="https://camo.githubusercontent.com/8f4eef019e452e4e04ae69e54d3a603ae20ce861/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f737461636b65645f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/stacked_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/quiver.html" rel="nofollow"&gt;
  &lt;img alt="quiver plot thumbnail" src="https://camo.githubusercontent.com/4f762bcca73dddb3cc86e60b3d43f1521409a838/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f7175697665725f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/quiver_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/elements.html" rel="nofollow"&gt;
  &lt;img alt="elements data plot thumbnail" src="https://camo.githubusercontent.com/1b46c02fcad9fa523b1194f01b7c61821b68bcae/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f656c656d656e74735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/elements_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/boxplot.html" rel="nofollow"&gt;
  &lt;img alt="boxplot thumbnail" src="https://camo.githubusercontent.com/aee991248aed1c36ce01afabea75062560915d91/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f626f78706c6f745f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/boxplot_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/categorical.html" rel="nofollow"&gt;
  &lt;img alt="categorical plot thumbnail" src="https://camo.githubusercontent.com/ad3bf9a45ea612ea5fe36c9aebfd61cdd23f82da/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63617465676f726963616c5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/categorical_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/unemployment.html" rel="nofollow"&gt;
  &lt;img alt="unemployment data plot thumbnail" src="https://camo.githubusercontent.com/a3b8632276b3f5adedd036d1ad6f9e7a02d6c907/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f756e656d706c6f796d656e745f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/unemployment_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/les_mis.html" rel="nofollow"&gt;
  &lt;img alt="Les Mis co-occurrence plot thumbnail" src="https://camo.githubusercontent.com/d50bb70e41286e878e465a38684ace5fb9f8e74e/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f6c65735f6d69735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/les_mis_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;The easiest way to install Bokeh is using the &lt;a href="https://www.anaconda.com/what-is-anaconda/" rel="nofollow"&gt;Anaconda Python distribution&lt;/a&gt; and its included &lt;em&gt;Conda&lt;/em&gt; package management system. To install Bokeh and its required dependencies, enter the following command at a Bash or Windows command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install bokeh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To install using pip, enter the following command at a Bash or Windows command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install bokeh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information, refer to the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#quick-installation" rel="nofollow"&gt;installation documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;p&gt;Once Bokeh is installed, check out the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#getting-started" rel="nofollow"&gt;Getting Started&lt;/a&gt; section of the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html" rel="nofollow"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit the &lt;a href="https://docs.bokeh.org" rel="nofollow"&gt;full documentation site&lt;/a&gt; to view the &lt;a href="https://docs.bokeh.org/en/dev/docs/user_guide.html" rel="nofollow"&gt;User's Guide&lt;/a&gt; or &lt;a href="https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb" rel="nofollow"&gt;launch the Bokeh tutorial&lt;/a&gt; to learn about Bokeh in live Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;Community support is available on the &lt;a href="https://discourse.bokeh.org" rel="nofollow"&gt;Project Discourse&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you would like to contribute to Bokeh, please review the &lt;a href="https://docs.bokeh.org/en/latest/docs/dev_guide.html" rel="nofollow"&gt;Developer Guide&lt;/a&gt; and say hello on the &lt;a href="https://gitter.im/bokeh/bokeh-dev" rel="nofollow"&gt;bokeh-dev chat channel&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-follow-us" class="anchor" aria-hidden="true" href="#follow-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Follow us&lt;/h2&gt;
&lt;p&gt;Follow us on Twitter &lt;a href="https://twitter.com/BokehPlots" rel="nofollow"&gt;@bokehplots&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;The Bokeh project is grateful for &lt;a href="https://numfocus.org/donate-to-bokeh" rel="nofollow"&gt;individual contributions&lt;/a&gt; as well as sponsorship by the organizations and companies below:&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
    &lt;a href="https://www.numfocus.org/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d147c30d8a6d9ca4b5eb060a83c511d7c99e6948/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f6e756d666f6375732e737667" alt="NumFocus Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/numfocus.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.anaconda.com/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/64a2555013bed95305c66c8c823a419dcb1e2754/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f616e61636f6e64612e706e67" alt="Anaconda Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/anaconda.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.nvidia.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d2e3f41f97a9833e323294d46045dc8b0e7fc66a/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f6e76696469612e706e67" alt="NVidia Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/nvidia.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://developer.nvidia.com/rapids" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/16c8825f8ae78640242f57070aab1bf8e664fa9c/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7261706964732e706e67" alt="Rapids Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/rapids.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;table align="center"&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
    &lt;a href="https://www.quansight.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/8b37ec4276ae3506abd62b93028c9a3b5b86459a/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7175616e73696768742e706e67" alt="Quansight Logo" width="100" data-canonical-src="https://static.bokeh.org/sponsor/quansight.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.rexhomes.com/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/723b4a136c3f567e7ee1f9cb71767488eef13a30/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7265782e6a7067" alt="Rex Logo" width="100" data-canonical-src="https://static.bokeh.org/sponsor/rex.jpg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;If your company uses Bokeh and is able to sponsor the project, please contact &lt;a href="info@bokeh.org"&gt;&lt;/a&gt;&lt;a href="mailto:info@bokeh.org"&gt;info@bokeh.org&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bokeh</author><guid isPermaLink="false">https://github.com/bokeh/bokeh</guid><pubDate>Tue, 05 Nov 2019 00:03:00 GMT</pubDate></item><item><title>bitcoinbook/bitcoinbook #4 in Python, This week</title><link>https://github.com/bitcoinbook/bitcoinbook</link><description>&lt;p&gt;&lt;i&gt;Mastering Bitcoin 2nd Edition - Programming the Open Blockchain&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Code Examples: &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70"&gt;&lt;img src="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70" alt="travis_ci" data-canonical-src="https://travis-ci.org/bitcoinbook/bitcoinbook.svg?branch=develop" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-mastering-bitcoin" class="anchor" aria-hidden="true" href="#mastering-bitcoin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin&lt;/h1&gt;
&lt;p&gt;Mastering Bitcoin is a book for developers, although the first two chapters cover bitcoin at a level that is also approachable to non-programmers. Anyone with a basic understanding of technology can read the first two chapters to get a great understanding of bitcoin.&lt;/p&gt;
&lt;p&gt;This repository contains the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;first edition, second print&lt;/a&gt;, published in December 2014, and the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second edition, second print&lt;/a&gt;, published in July 2017, as published by O'Reilly Media in paperback and ebook formats.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-issues-errors-comments-contributions" class="anchor" aria-hidden="true" href="#issues-errors-comments-contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues, Errors, Comments, Contributions&lt;/h1&gt;
&lt;p&gt;If you know how to make a pull request to contribute a fix, please write the correction and use a pull request to submit it for consideration against the &lt;a href="https://github.com/bitcoinbook/bitcoinbook/tree/develop"&gt;develop branch&lt;/a&gt;. If you are making several changes, please use a separate commit for each to make it easier to cherry-pick or resolve conflicts. Otherwise, please submit an issue, explaining the error or comment. If you would like to contribute extensive changes or new material, please coordinate with the author first; contact information can be found on his website: &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;https://antonopoulos.com/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reading-this-book" class="anchor" aria-hidden="true" href="#reading-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reading this book&lt;/h1&gt;
&lt;p&gt;To read this book, see &lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/book.asciidoc"&gt;book.asciidoc&lt;/a&gt;. Click on each of the chapters to read in your browser. Other parties may choose to release PDFs of the book online.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-chapters" class="anchor" aria-hidden="true" href="#chapters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chapters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 1: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch01.asciidoc"&gt;Introduction&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 2: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch02.asciidoc"&gt;How Bitcoin Works&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 3: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch03.asciidoc"&gt;Bitcoin Core: The Reference Implementation&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 4: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch04.asciidoc"&gt;Keys, Addresses&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 5: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch05.asciidoc"&gt;Wallets&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 6: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch06.asciidoc"&gt;Transactions&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 7: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch07.asciidoc"&gt;Advanced Transactions and Scripting&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 8: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch08.asciidoc"&gt;The Bitcoin Network&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 9: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch09.asciidoc"&gt;The Blockchain&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 10: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch10.asciidoc"&gt;Mining and Consensus&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 11: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch11.asciidoc"&gt;Bitcoin Security&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 12: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch12.asciidoc"&gt;Blockchain Applications&lt;/a&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-published" class="anchor" aria-hidden="true" href="#published"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Published&lt;/h1&gt;
&lt;p&gt;"Mastering Bitcoin (Second Edition, Second Print): Programming the Open Blockchain" is now available in paperback and ebook formats by many booksellers worldwide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Mastering-Bitcoin-Programming-Open-Blockchain/dp/1491954388" rel="nofollow"&gt;Amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mastering Bitcoin (First Edition Second Print) is also published in Japanese, Korean, and Chinese (Simplified) by publishers in the respective countries.&lt;/p&gt;
&lt;p&gt;Mastering Bitcoin (Open Edition), based on the First Edition, has been translated by volunteers into more than a dozen languages. Translations are available for free under CC-BY-SA license at: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-source" class="anchor" aria-hidden="true" href="#source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source&lt;/h1&gt;
&lt;p&gt;The book's source code, found in this repository, is kept synchronized with the print and ebook editions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---first-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---first-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - First Edition&lt;/h2&gt;
&lt;p&gt;The tags &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print1"&gt;Edition1Print1&lt;/a&gt;, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;Edition1Print2&lt;/a&gt; correspond to the two existing prints of Mastering Bitcoin (First Edition) as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - First Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This "Free Culture" compliant license was approved by my publisher O'Reilly Media (&lt;a href="http://oreilly.com" rel="nofollow"&gt;http://oreilly.com&lt;/a&gt;), who understands the value of open source. O'Reilly Media is not just the world's best publisher of technical books, but is also a strong supporter of this open culture and the sharing of knowledge.&lt;/p&gt;
&lt;p&gt;Thank you O'Reilly Media!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---second-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---second-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - Second Edition&lt;/h2&gt;
&lt;p&gt;The tags, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print_1"&gt;second_edition_print_1&lt;/a&gt; and  &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second_edition_print2&lt;/a&gt;, correspond to the first (June 8th, 2017) and second (July 20th, 2017) print of Mastering Bitcoin (Second Edition), as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - Second Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;p&gt;If you are interested in translating this book, please join our team of volunteers at: &lt;a href="https://www.transifex.com/aantonop/mastering-bitcoin" rel="nofollow"&gt;https://www.transifex.com/aantonop/mastering-bitcoin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Free copies of "Mastering Bitcoin Open Edition," translated in many languages, can be downloaded from: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bitcoinbook</author><guid isPermaLink="false">https://github.com/bitcoinbook/bitcoinbook</guid><pubDate>Tue, 05 Nov 2019 00:04:00 GMT</pubDate></item><item><title>eriklindernoren/ML-From-Scratch #5 in Python, This week</title><link>https://github.com/eriklindernoren/ML-From-Scratch</link><description>&lt;p&gt;&lt;i&gt;Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-from-scratch" class="anchor" aria-hidden="true" href="#machine-learning-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning From Scratch&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.&lt;/p&gt;
&lt;p&gt;The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine-learning-from-scratch"&gt;Machine Learning From Scratch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about"&gt;About&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#polynomial-regression"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification-with-cnn"&gt;Classification With CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#density-based-clustering"&gt;Density-Based Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generating-handwritten-digits"&gt;Generating Handwritten Digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-reinforcement-learning"&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-reconstruction-with-rbm"&gt;Image Reconstruction With RBM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evolutionary-evolved-neural-network"&gt;Evolutionary Evolved Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genetic-algorithm"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#association-analysis"&gt;Association Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-polynomial-regression" class="anchor" aria-hidden="true" href="#polynomial-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Polynomial Regression&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/polynomial_regression.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/p_reg.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in Linkping, Sweden 2016.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-classification-with-cnn" class="anchor" aria-hidden="true" href="#classification-with-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification With CNN&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_cnn1.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-density-based-clustering" class="anchor" aria-hidden="true" href="#density-based-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Density-Based Clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/dbscan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dbscan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generating-handwritten-digits" class="anchor" aria-hidden="true" href="#generating-handwritten-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Handwritten Digits&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966"&gt;&lt;img src="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/gan_mnist5.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-reinforcement-learning" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Reinforcement Learning&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dql1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-reconstruction-with-rbm" class="anchor" aria-hidden="true" href="#image-reconstruction-with-rbm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Reconstruction With RBM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/restricted_boltzmann_machine.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/rbm_digits1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-evolutionary-evolved-neural-network" class="anchor" aria-hidden="true" href="#evolutionary-evolved-neural-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evolutionary Evolved Neural Network&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/evo_nn4.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-genetic-algorithm" class="anchor" aria-hidden="true" href="#genetic-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Genetic Algorithm&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-association-analysis" class="anchor" aria-hidden="true" href="#association-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Association Analysis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -&amp;gt; 2 (support: 0.43, confidence: 1.0)
    4 -&amp;gt; 2 (support: 0.57, confidence: 0.8)
    [1, 4] -&amp;gt; 2 (support: 0.29, confidence: 1.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/adaboost.py"&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/bayesian_regression.py"&gt;Bayesian Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/decision_tree.py"&gt;Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Elastic Net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/gradient_boosting.py"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/k_nearest_neighbors.py"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/linear_discriminant_analysis.py"&gt;Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/logistic_regression.py"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multi_class_lda.py"&gt;Multi-class Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/naive_bayes.py"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/neuroevolution.py"&gt;Neuroevolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/particle_swarm_optimization.py"&gt;Particle Swarm Optimization of Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/perceptron.py"&gt;Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/random_forest.py"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/support_vector_machine.py"&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/xgboost.py"&gt;XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/apriori.py"&gt;Apriori&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/dbscan.py"&gt;DBSCAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/fp_growth.py"&gt;FP-Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/gaussian_mixture_model.py"&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/generative_adversarial_network.py"&gt;Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/genetic_algorithm.py"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/k_means.py"&gt;K-Means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/partitioning_around_medoids.py"&gt;Partitioning Around Medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/principal_component_analysis.py"&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py"&gt;Restricted Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/reinforcement_learning/deep_q_network.py"&gt;Deep Q-Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/neural_network.py"&gt;Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/layers.py"&gt;Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Activation Layer&lt;/li&gt;
&lt;li&gt;Average Pooling Layer&lt;/li&gt;
&lt;li&gt;Batch Normalization Layer&lt;/li&gt;
&lt;li&gt;Constant Padding Layer&lt;/li&gt;
&lt;li&gt;Convolutional Layer&lt;/li&gt;
&lt;li&gt;Dropout Layer&lt;/li&gt;
&lt;li&gt;Flatten Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected (Dense) Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected RNN Layer&lt;/li&gt;
&lt;li&gt;Max Pooling Layer&lt;/li&gt;
&lt;li&gt;Reshape Layer&lt;/li&gt;
&lt;li&gt;Up Sampling Layer&lt;/li&gt;
&lt;li&gt;Zero Padding Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Types
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/convolutional_neural_network.py"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/recurrent_neural_network.py"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If there's some implementation you would like to see here or if you're just feeling social,
feel free to &lt;a href="mailto:eriklindernoren@gmail.com"&gt;email&lt;/a&gt; me or connect with me on &lt;a href="https://www.linkedin.com/in/eriklindernoren/" rel="nofollow"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/ML-From-Scratch</guid><pubDate>Tue, 05 Nov 2019 00:05:00 GMT</pubDate></item><item><title>521xueweihan/HelloGitHub #6 in Python, This week</title><link>https://github.com/521xueweihan/HelloGitHub</link><description>&lt;p&gt;&lt;i&gt;:octocat: Find pearls on open-source seashore  GitHub &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif"&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt; | &lt;a href="README_en.md"&gt;English&lt;/a&gt;
  &lt;br&gt;&lt;strong&gt;HelloGitHub&lt;/strong&gt;  GitHub &lt;br&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://hellogithub.com/weixin.png" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/61343b85520a4714ddb37eb300f8268cc881ae7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54616c6b2d2545352542452541452545342542462541312545372542452541342d627269676874677265656e2e7376673f7374796c653d706f706f75742d737175617265" alt="WeiXin" data-canonical-src="https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/stargazers"&gt;&lt;img src="https://camo.githubusercontent.com/0aec7fa1a5647255bbe8af37a82a007be69d8739/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/issues"&gt;&lt;img src="https://camo.githubusercontent.com/a8367e38e94eccf7e469023edfec05db15132454/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4627590b5d81a690c6c83abaf47f678d70d26e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362542302545362542352541412d576569626f2d7265642e7376673f7374796c653d706f706f75742d737175617265" alt="Sina Weibo" data-canonical-src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt; 28 &lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt; Cool&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="man_dancing" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f57a.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="video_game" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ae.png"&gt;&lt;/g-emoji&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;/strong&gt; &lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;/strong&gt; &lt;a href="https://github.com/HelloGitHub-Team/Article/blob/master/%E5%88%9B%E4%BD%9C%E9%A1%BB%E7%9F%A5.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt; 28 &lt;a href="/content/last.md"&gt;&lt;/a&gt; | &lt;a href="https://hellogithub.com" rel="nofollow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img class="emoji" title=":shipit:" alt=":shipit:" src="https://github.githubassets.com/images/icons/emoji/shipit.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="jack_o_lantern" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f383.png"&gt;&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="beer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37a.png"&gt;&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="fish_cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f365.png"&gt;&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/43/HelloGitHub43.md"&gt; 43 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/42/HelloGitHub42.md"&gt; 42 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/41/HelloGitHub41.md"&gt; 41 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/40/HelloGitHub40.md"&gt; 40 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/39/HelloGitHub39.md"&gt; 39 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/38/HelloGitHub38.md"&gt; 38 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/37/HelloGitHub37.md"&gt; 37 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/36/HelloGitHub36.md"&gt; 36 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/35/HelloGitHub35.md"&gt; 35 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/34/HelloGitHub34.md"&gt; 34 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/33/HelloGitHub33.md"&gt; 33 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/32/HelloGitHub32.md"&gt; 32 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/31/HelloGitHub31.md"&gt; 31 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/30/HelloGitHub30.md"&gt; 30 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/29/HelloGitHub29.md"&gt; 29 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/28/HelloGitHub28.md"&gt; 28 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/27/HelloGitHub27.md"&gt; 27 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/26/HelloGitHub26.md"&gt; 26 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/25/HelloGitHub25.md"&gt; 25 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/24/HelloGitHub24.md"&gt; 24 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/23/HelloGitHub23.md"&gt; 23 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/22/HelloGitHub22.md"&gt; 22 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/21/HelloGitHub21.md"&gt; 21 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/20/HelloGitHub20.md"&gt; 20 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/19/HelloGitHub19.md"&gt; 19 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/18/HelloGitHub18.md"&gt; 18 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/17/HelloGitHub17.md"&gt; 17 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/16/HelloGitHub16.md"&gt; 16 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/15/HelloGitHub15.md"&gt; 15 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/14/HelloGitHub14.md"&gt; 14 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/13/HelloGitHub13.md"&gt; 13 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/12/HelloGitHub12.md"&gt; 12 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/11/HelloGitHub11.md"&gt; 11 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/10/HelloGitHub10.md"&gt; 10 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/09/HelloGitHub09.md"&gt; 09 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/08/HelloGitHub08.md"&gt; 08 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/07/HelloGitHub07.md"&gt; 07 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/06/HelloGitHub06.md"&gt; 06 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/05/HelloGitHub05.md"&gt; 05 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/04/HelloGitHub04.md"&gt; 04 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/03/HelloGitHub03.md"&gt; 03 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/02/HelloGitHub02.md"&gt; 02 &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/01/HelloGitHub01.md"&gt; 01 &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href="https://github.com/521xueweihan/HelloGitHub/issues/new"&gt;&lt;/a&gt; &lt;strong&gt;HelloGitHub&lt;/strong&gt; &lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/8255800?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ming995"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/46031112?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FrontMage"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/17007026?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FrontMage&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/xibinyue"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/14122146?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;xibinyue&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/Eurus-Holmes"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/34226570?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Feiyang Chen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ChungZH"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/42088872?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;ChungZH&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/daixiang0"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/26538619?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;daixiang0&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/nivance"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/3291404?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;nivance&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/hellowHuaairen"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/19610305?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;hellowHuaairen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/17665302?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;&lt;a href="Mailto:595666367@qq.com"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FGDBTKD"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40509403?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FGDBTKD&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;AI/ML/DL/NLP&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/d2-projects"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40857578?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;D2 Projects&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Vue/JavaScript&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/doocs"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/43716716?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Doocs&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Technical Knowledge&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;&lt;img alt="" src="https://camo.githubusercontent.com/1ae74a56e22c4897b6fbfb9f301bd829c77429a7/68747470733a2f2f6c6963656e7365627574746f6e732e6e65742f6c2f62792d6e632d6e642f342e302f38387833312e706e67" data-canonical-src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;-- 4.0 &lt;/a&gt; &lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>521xueweihan</author><guid isPermaLink="false">https://github.com/521xueweihan/HelloGitHub</guid><pubDate>Tue, 05 Nov 2019 00:06:00 GMT</pubDate></item><item><title>KubeOperator/KubeOperator #7 in Python, This week</title><link>https://github.com/KubeOperator/KubeOperator</link><description>&lt;p&gt;&lt;i&gt;KubeOperator  Web UI  VMwareOpenStack Kubernetes &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kubeoperator----kubernetes-" class="anchor" aria-hidden="true" href="#kubeoperator----kubernetes-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KubeOperator -  Kubernetes &lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/KubeOperatpr/KubeOperatpr/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/7197a397ba1baf73679f3cf0edf68d821c35ae52/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d61706163686525323076322d626c75652e737667" alt="License" data-canonical-src="http://img.shields.io/badge/license-apache%20v2-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.python.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4c11ac2b538cba463dfd1e43d05fe4f30f2d33d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d677265656e2e7376673f7374796c653d706c6173746963" alt="Python3" data-canonical-src="https://img.shields.io/badge/python-3.6-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/35798c7a6bb116ad2e8d420db49766bce91239b1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646a616e676f2d322e312d627269676874677265656e2e7376673f7374796c653d706c6173746963" alt="Django" data-canonical-src="https://img.shields.io/badge/django-2.1-brightgreen.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.ansible.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dbfb9037d993ab109b0dd41252b2aabcd703e4a5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e7369626c652d322e362e352d626c75652e7376673f7374796c653d706c6173746963" alt="Ansible" data-canonical-src="https://img.shields.io/badge/ansible-2.6.5-blue.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.angular.cn/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9829fdfaae3736e19d738b29efaeec4aaf21c61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e67756c61722d372e302e342d7265642e7376673f7374796c653d706c6173746963" alt="Angular" data-canonical-src="https://img.shields.io/badge/angular-7.0.4-red.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator  Web UI  VMwareOpenstack  Kubernetes KubeOperator  &lt;a href="https://github.com/jumpserver/jumpserver"&gt;Jumpserver&lt;/a&gt;  Kubernetes &lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/overview.png?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/overview.png?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-web-ui-" class="anchor" aria-hidden="true" href="#web-ui-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web UI &lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot" rel="nofollow"&gt;https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;KubeOperator  Terraform  IaaS  Ansible  Kubernetes   Day 0  Day 1  Day 2 &lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/KubeOperator.jpeg?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/KubeOperator.jpeg?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt; API Kubernetes  ( Kubernetes as a Service)&lt;/li&gt;
&lt;li&gt; Kubernetes &lt;/li&gt;
&lt;li&gt; Kubernetes &lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt; Kubernetes &lt;/li&gt;
&lt;li&gt;Multi-AZ  Kubernetes  Master &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-demo-" class="anchor" aria-hidden="true" href="#demo-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo &lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubeoperator-1256577600.file.myqcloud.com/video/KubeOperator2.1.mp4" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="tv" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4fa.png"&gt;&lt;/g-emoji&gt;8 &lt;/a&gt; KubeOperator &lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.kubeoperator.io/" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;&lt;/g-emoji&gt;&lt;/a&gt; KubeOperator &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-kubernetes-" class="anchor" aria-hidden="true" href="#kubernetes-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kubernetes &lt;/h2&gt;
&lt;p&gt;KubeOperator  Kubernetes  KubernetesDockeretcdDashboardPromethusOS  KubeOperator &lt;a href="https://github.com/KubeOperator/k8s-package"&gt;k8s-package&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;v1.0 &lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes  RegistryPromethusDashboardTraefik IngressHelm &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  NFS &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Flannel &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes  NFS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.0 &lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  VMware vCenter API &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  VMware vSAN VMFS/NFS &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Multi AZ&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Calico &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Weave Scope&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  F5 BIG-IP Controller Nodeport mode, &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.1 &lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Openstack &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Openstack Cinder &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes  Day 2&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes Day 2&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes Day 2&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  Kubernetes Day 2&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt;  &lt;a href="https://github.com/webkubectl/webkubectl"&gt;webkubectl&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.2 2019.11.30 &lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; K8s &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; KubeOperator &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; TOP &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.3 &lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; KubeApps &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt;  Sonobuoy  Kubernetes &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; &lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt;  VMware NSX-T&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt; QQ 825046920&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;  wh_it0224-github, &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/hashicorp/terraform"&gt;Terraform&lt;/a&gt;: KubeOperator  Terraform &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vmware/clarity/"&gt;Clarity&lt;/a&gt;: KubeOperator  Clarity  Web &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ansible/ansible"&gt;Ansible&lt;/a&gt;: KubeOperator  Ansible &lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/easzlab/kubeasz"&gt;kubeasz&lt;/a&gt;:  Kubernetes Ansible &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2014-2019 FIT2CLOUD &lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.fit2cloud.com" rel="nofollow"&gt;https://www.fit2cloud.com&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator is licensed under the Apache License, Version 2.0.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>KubeOperator</author><guid isPermaLink="false">https://github.com/KubeOperator/KubeOperator</guid><pubDate>Tue, 05 Nov 2019 00:07:00 GMT</pubDate></item><item><title>google-research/bert #8 in Python, This week</title><link>https://github.com/google-research/bert</link><description>&lt;p&gt;&lt;i&gt;TensorFlow code and pre-trained models for BERT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bert" class="anchor" aria-hidden="true" href="#bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BERT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;***** New May 31st, 2019: Whole Word Masking Models *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a release of several new models which were the result of an improvement
the pre-processing code.&lt;/p&gt;
&lt;p&gt;In the original pre-processing code, we randomly select WordPiece tokens to
mask. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head&lt;/code&gt;
&lt;code&gt;Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The new technique is called Whole Word Masking. In this case, we always mask
&lt;em&gt;all&lt;/em&gt; of the the tokens corresponding to a word at once. The overall masking
rate remains the same.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The training is identical -- we still predict each masked WordPiece token
independently. The improvement comes from the fact that the original prediction
task was too 'easy' for words that had been split into multiple WordPieces.&lt;/p&gt;
&lt;p&gt;This can be enabled during data generation by passing the flag
&lt;code&gt;--do_whole_word_mask=True&lt;/code&gt; to &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pre-trained models with Whole Word Masking are linked below. The data and
training were otherwise identical, and the models have identical structure and
vocab to the original models. We only include BERT-Large models. When using
these models, please make it clear in the paper that you are using the Whole
Word Masking variant of BERT-Large.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;SQUAD 1.1 F1/EM&lt;/th&gt;
&lt;th align="center"&gt;Multi NLI Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.0/84.3&lt;/td&gt;
&lt;td align="center"&gt;86.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.8/86.7&lt;/td&gt;
&lt;td align="center"&gt;87.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.5/84.8&lt;/td&gt;
&lt;td align="center"&gt;86.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.9/86.7&lt;/td&gt;
&lt;td align="center"&gt;86.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;***** New February 7th, 2019: TfHub Module *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BERT has been uploaded to &lt;a href="https://tfhub.dev" rel="nofollow"&gt;TensorFlow Hub&lt;/a&gt;. See
&lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt; for an example of how to use the TF Hub module,
or run an example in the browser on
&lt;a href="https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="nofollow"&gt;Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 23rd, 2018: Un-normalized multilingual model + Thai +
Mongolian *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We uploaded a new multilingual model which does &lt;em&gt;not&lt;/em&gt; perform any normalization
on the input (no lower casing, accent stripping, or Unicode normalization), and
additionally inclues Thai and Mongolian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is recommended to use this version for developing multilingual models,
especially on languages with non-Latin alphabets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This does not require any code changes, and can be downloaded here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;***** New November 15th, 2018: SOTA SQuAD 2.0 System *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is
currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the
README for details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 5th, 2018: Third-party PyTorch and Chainer versions of
BERT available *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NLP researchers from HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. Sosuke Kobayashi also made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
(Thanks!) We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 3rd, 2018: Multilingual and Chinese models available
*****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have made two new BERT models available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use character-based tokenization for Chinese, and WordPiece tokenization for
all other languages. Both models should work out-of-the-box without any code
changes. We did update the implementation of &lt;code&gt;BasicTokenizer&lt;/code&gt; in
&lt;code&gt;tokenization.py&lt;/code&gt; to support Chinese character tokenization, so please update if
you forked it. However, we did not change the tokenization API.&lt;/p&gt;
&lt;p&gt;For more, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** End new information *****&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;, or &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from
&lt;strong&gt;T&lt;/strong&gt;ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.&lt;/p&gt;
&lt;p&gt;Our academic paper which describes BERT in detail and provides full results on a
number of tasks can be found here:
&lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a few numbers, here are the results on the
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD v1.1&lt;/a&gt; question answering
task:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQuAD v1.1 Leaderboard (Oct 8th 2018)&lt;/th&gt;
&lt;th align="center"&gt;Test EM&lt;/th&gt;
&lt;th align="center"&gt;Test F1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Ensemble - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;93.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Ensemble - nlnet&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;td align="center"&gt;91.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Single Model - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;85.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Single Model - nlnet&lt;/td&gt;
&lt;td align="center"&gt;83.5&lt;/td&gt;
&lt;td align="center"&gt;90.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And several natural language inference tasks:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th align="center"&gt;MultiNLI&lt;/th&gt;
&lt;th align="center"&gt;Question NLI&lt;/th&gt;
&lt;th align="center"&gt;SWAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI GPT (Prev. SOTA)&lt;/td&gt;
&lt;td align="center"&gt;82.2&lt;/td&gt;
&lt;td align="center"&gt;88.1&lt;/td&gt;
&lt;td align="center"&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plus many other tasks.&lt;/p&gt;
&lt;p&gt;Moreover, these results were all obtained with almost no task-specific neural
network architecture design.&lt;/p&gt;
&lt;p&gt;If you already know what BERT is and you just want to get started, you can
&lt;a href="#pre-trained-models"&gt;download the pre-trained models&lt;/a&gt; and
&lt;a href="#fine-tuning-with-bert"&gt;run a state-of-the-art fine-tuning&lt;/a&gt; in only a few
minutes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bert" class="anchor" aria-hidden="true" href="#what-is-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT is a method of pre-training language representations, meaning that we train
a general-purpose "language understanding" model on a large text corpus (like
Wikipedia), and then use that model for downstream NLP tasks that we care about
(like question answering). BERT outperforms previous methods because it is the
first &lt;em&gt;unsupervised&lt;/em&gt;, &lt;em&gt;deeply bidirectional&lt;/em&gt; system for pre-training NLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unsupervised&lt;/em&gt; means that BERT was trained using only a plain text corpus, which
is important because an enormous amount of plain text data is publicly available
on the web in many languages.&lt;/p&gt;
&lt;p&gt;Pre-trained representations can also either be &lt;em&gt;context-free&lt;/em&gt; or &lt;em&gt;contextual&lt;/em&gt;,
and contextual representations can further be &lt;em&gt;unidirectional&lt;/em&gt; or
&lt;em&gt;bidirectional&lt;/em&gt;. Context-free models such as
&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow"&gt;word2vec&lt;/a&gt; or
&lt;a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow"&gt;GloVe&lt;/a&gt; generate a single "word
embedding" representation for each word in the vocabulary, so &lt;code&gt;bank&lt;/code&gt; would have
the same representation in &lt;code&gt;bank deposit&lt;/code&gt; and &lt;code&gt;river bank&lt;/code&gt;. Contextual models
instead generate a representation of each word that is based on the other words
in the sentence.&lt;/p&gt;
&lt;p&gt;BERT was built upon recent work in pre-training contextual representations 
including &lt;a href="https://arxiv.org/abs/1511.01432" rel="nofollow"&gt;Semi-supervised Sequence Learning&lt;/a&gt;,
&lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Generative Pre-Training&lt;/a&gt;,
&lt;a href="https://allennlp.org/elmo" rel="nofollow"&gt;ELMo&lt;/a&gt;, and
&lt;a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="nofollow"&gt;ULMFit&lt;/a&gt;
 but crucially these models are all &lt;em&gt;unidirectional&lt;/em&gt; or &lt;em&gt;shallowly
bidirectional&lt;/em&gt;. This means that each word is only contextualized using the words
to its left (or right). For example, in the sentence &lt;code&gt;I made a bank deposit&lt;/code&gt; the
unidirectional representation of &lt;code&gt;bank&lt;/code&gt; is only based on &lt;code&gt;I made a&lt;/code&gt; but not
&lt;code&gt;deposit&lt;/code&gt;. Some previous work does combine the representations from separate
left-context and right-context models, but only in a "shallow" manner. BERT
represents "bank" using both its left and right context  &lt;code&gt;I made a ... deposit&lt;/code&gt;
 starting from the very bottom of a deep neural network, so it is &lt;em&gt;deeply
bidirectional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;BERT uses a simple approach for this: We mask out 15% of the words in the input,
run the entire sequence through a deep bidirectional
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; encoder, and then predict only
the masked words. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to learn relationships between sentences, we also train on a simple
task which can be generated from any monolingual corpus: Given two sentences &lt;code&gt;A&lt;/code&gt;
and &lt;code&gt;B&lt;/code&gt;, is &lt;code&gt;B&lt;/code&gt; the actual next sentence that comes after &lt;code&gt;A&lt;/code&gt;, or just a random
sentence from the corpus?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then train a large model (12-layer to 24-layer Transformer) on a large corpus
(Wikipedia + &lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt;) for a long time (1M
update steps), and that's BERT.&lt;/p&gt;
&lt;p&gt;Using BERT has two stages: &lt;em&gt;Pre-training&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a
one-time procedure for each language (current models are English-only, but
multilingual models will be released in the near future). We are releasing a
number of pre-trained models from the paper which were pre-trained at Google.
Most NLP researchers will never need to pre-train their own model from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; is inexpensive. All of the results in the paper can be
replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,
starting from the exact same pre-trained model. SQuAD, for example, can be
trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of
91.0%, which is the single system state-of-the-art.&lt;/p&gt;
&lt;p&gt;The other important aspect of BERT is that it can be adapted to many types of
NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on
sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level
(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific
modifications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-has-been-released-in-this-repository" class="anchor" aria-hidden="true" href="#what-has-been-released-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What has been released in this repository?&lt;/h2&gt;
&lt;p&gt;We are releasing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow code for the BERT model architecture (which is mostly a standard
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; architecture).&lt;/li&gt;
&lt;li&gt;Pre-trained checkpoints for both the lowercase and cased version of
&lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; from the paper.&lt;/li&gt;
&lt;li&gt;TensorFlow code for push-button replication of the most important
fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud
TPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained models&lt;/h2&gt;
&lt;p&gt;We are releasing the &lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; models from the paper.
&lt;code&gt;Uncased&lt;/code&gt; means that the text has been lowercased before WordPiece tokenization,
e.g., &lt;code&gt;John Smith&lt;/code&gt; becomes &lt;code&gt;john smith&lt;/code&gt;. The &lt;code&gt;Uncased&lt;/code&gt; model also strips out any
accent markers. &lt;code&gt;Cased&lt;/code&gt; means that the true case and accent markers are
preserved. Typically, the &lt;code&gt;Uncased&lt;/code&gt; model is better unless you know that case
information is important for your task (e.g., Named Entity Recognition or
Part-of-Speech tagging).&lt;/p&gt;
&lt;p&gt;These models are all released under the same license as the source code (Apache
2.0).&lt;/p&gt;
&lt;p&gt;For information about the Multilingual and Chinese model, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When using a cased model, make sure to pass &lt;code&gt;--do_lower=False&lt;/code&gt; to the training
scripts. (Or pass &lt;code&gt;do_lower_case=False&lt;/code&gt; directly to &lt;code&gt;FullTokenizer&lt;/code&gt; if you're
using your own script.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The links to the models are here (right-click, 'Save link as...' on the name):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads , 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased (New, recommended)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Uncased (Orig, not recommended)&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each .zip file contains three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained
weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of
the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fine-tuning-with-bert" class="anchor" aria-hidden="true" href="#fine-tuning-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with BERT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: All results on the paper were fine-tuned on a single Cloud TPU,
which has 64GB of RAM. It is currently not possible to re-produce most of the
&lt;code&gt;BERT-Large&lt;/code&gt; results on the paper using a GPU with 12GB - 16GB of RAM, because
the maximum batch size that can fit in memory is too small. We are working on
adding code to this repository which allows for much larger effective batch size
on the GPU. See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for
more details.&lt;/p&gt;
&lt;p&gt;This code was tested with TensorFlow 1.11.0. It was tested with Python2 and
Python3 (but more thoroughly with Python2, since this is what's used internally
in Google).&lt;/p&gt;
&lt;p&gt;The fine-tuning examples which use &lt;code&gt;BERT-Base&lt;/code&gt; should be able to run on a GPU
that has at least 12GB of RAM using the hyperparameters given.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-with-cloud-tpus" class="anchor" aria-hidden="true" href="#fine-tuning-with-cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with Cloud TPUs&lt;/h3&gt;
&lt;p&gt;Most of the examples below assumes that you will be running training/evaluation
on your local machine, using a GPU like a Titan X or GTX 1080.&lt;/p&gt;
&lt;p&gt;However, if you have access to a Cloud TPU that you want to train on, just add
the following flags to &lt;code&gt;run_classifier.py&lt;/code&gt; or &lt;code&gt;run_squad.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --use_tpu=True \
  --tpu_name=$TPU_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the
&lt;a href="https://cloud.google.com/tpu/docs/tutorials/mnist" rel="nofollow"&gt;Google Cloud TPU tutorial&lt;/a&gt;
for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;On Cloud TPUs, the pretrained model and the output directory will need to be on
Google Cloud Storage. For example, if you have a bucket named &lt;code&gt;some_bucket&lt;/code&gt;, you
might use the following flags instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --output_dir=gs://some_bucket/my_output_dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unzipped pre-trained model files can also be found in the Google Cloud
Storage folder &lt;code&gt;gs://bert_models/2018_10_18&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-sentence-and-sentence-pair-classification-tasks" class="anchor" aria-hidden="true" href="#sentence-and-sentence-pair-classification-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentence (and sentence-pair) classification tasks&lt;/h3&gt;
&lt;p&gt;Before running this example you must download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;. Next, download the &lt;code&gt;BERT-Base&lt;/code&gt;
checkpoint and unzip it to some directory &lt;code&gt;$BERT_BASE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This example code fine-tunes &lt;code&gt;BERT-Base&lt;/code&gt; on the Microsoft Research Paraphrase
Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a
few minutes on most GPUs.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a
high variance in the Dev set accuracy, even when starting from the same
pre-training checkpoint. If you re-run multiple times (making sure to point to
different &lt;code&gt;output_dir&lt;/code&gt;), you should see results between 84% and 88%.&lt;/p&gt;
&lt;p&gt;A few other pre-trained models are implemented off-the-shelf in
&lt;code&gt;run_classifier.py&lt;/code&gt;, so it should be straightforward to follow those examples to
use BERT for any single-sentence or sentence-pair classification task.&lt;/p&gt;
&lt;p&gt;Note: You might see a message &lt;code&gt;Running train on CPU&lt;/code&gt;. This really just means
that it's running on something other than a Cloud TPU, which includes a GPU.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-prediction-from-classifier" class="anchor" aria-hidden="true" href="#prediction-from-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prediction from classifier&lt;/h4&gt;
&lt;p&gt;Once you have trained your classifier you can use it in inference mode by using
the --do_predict=true command. You need to have a file named test.tsv in the
input folder. Output will be created in file called test_results.tsv in the
output folder. Each line will contain output for each sample, columns are the
class probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$TRAINED_CLASSIFIER&lt;/span&gt; \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-squad-11" class="anchor" aria-hidden="true" href="#squad-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 1.1&lt;/h3&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD) is a popular question answering
benchmark dataset. BERT (at the time of the release) obtains state-of-the-art
results on SQuAD with almost no task-specific network architecture modifications
or data augmentation. However, it does require semi-complex data pre-processing
and post-processing to deal with (a) the variable-length nature of SQuAD context
paragraphs, and (b) the character-level answer annotations which are used for
SQuAD training. This processing is implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD, you will first need to download the dataset. The
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD website&lt;/a&gt; does not seem to
link to the v1.1 datasets any longer, but the necessary files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json" rel="nofollow"&gt;train-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json" rel="nofollow"&gt;dev-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"&gt;evaluate-v1.1.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The state-of-the-art SQuAD results from the paper currently cannot be reproduced
on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does
not seem to fit on a 12GB GPU using &lt;code&gt;BERT-Large&lt;/code&gt;). However, a reasonably strong
&lt;code&gt;BERT-Base&lt;/code&gt; model can be trained on the GPU with these hyperparameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dev set predictions will be saved into a file called &lt;code&gt;predictions.json&lt;/code&gt; in
the &lt;code&gt;output_dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ./squad/predictions.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which should produce an output like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 88.41249612335034, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 81.2488174077578}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see a result similar to the 88.5% reported in the paper for
&lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you have access to a Cloud TPU, you can train with &lt;code&gt;BERT-Large&lt;/code&gt;. Here is a
set of hyperparameters (slightly different than the paper) which consistently
obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, one random run with these parameters produces the following Dev
scores:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 90.87081895814865, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 84.38978240302744}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you fine-tune for one epoch on
&lt;a href="http://nlp.cs.washington.edu/triviaqa/" rel="nofollow"&gt;TriviaQA&lt;/a&gt; before this the results will
be even better, but you will need to convert TriviaQA into the SQuAD json
format.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-squad-20" class="anchor" aria-hidden="true" href="#squad-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 2.0&lt;/h3&gt;
&lt;p&gt;This model is also implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD 2.0, you will first need to download the dataset. The necessary
files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json" rel="nofollow"&gt;train-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json" rel="nofollow"&gt;dev-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" rel="nofollow"&gt;evaluate-v2.0.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On Cloud TPU you can run with BERT-Large as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We assume you have copied everything from the output directory to a local
directory called ./squad/. The initial dev set predictions will be at
./squad/predictions.json and the differences between the score of no answer ("")
and the best non-null answer for each question will be in the file
./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Run this script to tune a threshold for predicting null versus non-null answers:&lt;/p&gt;
&lt;p&gt;python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json
./squad/predictions.json --na-prob-file ./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Assume the script outputs "best_f1_thresh" THRESH. (Typical values are between
-1.0 and -5.0). You can now re-run the model to generate predictions with the
derived threshold or alternatively you can extract the appropriate answers from
./squad/nbest_predictions.json.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=False \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True \
  --null_score_diff_threshold=&lt;span class="pl-smi"&gt;$THRESH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-out-of-memory-issues" class="anchor" aria-hidden="true" href="#out-of-memory-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Out-of-memory issues&lt;/h3&gt;
&lt;p&gt;All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of
device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely
to encounter out-of-memory issues if you use the same hyperparameters described
in the paper.&lt;/p&gt;
&lt;p&gt;The factors that affect memory usage are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;max_seq_length&lt;/code&gt;&lt;/strong&gt;: The released models were trained with sequence lengths
up to 512, but you can fine-tune with a shorter max sequence length to save
substantial memory. This is controlled by the &lt;code&gt;max_seq_length&lt;/code&gt; flag in our
example code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;train_batch_size&lt;/code&gt;&lt;/strong&gt;: The memory usage is also directly proportional to
the batch size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model type, &lt;code&gt;BERT-Base&lt;/code&gt; vs. &lt;code&gt;BERT-Large&lt;/code&gt;&lt;/strong&gt;: The &lt;code&gt;BERT-Large&lt;/code&gt; model
requires significantly more memory than &lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: The default optimizer for BERT is Adam, which requires a lot
of extra memory to store the &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; vectors. Switching to a more memory
efficient optimizer can reduce memory usage, but can also affect the
results. We have not experimented with other optimizers for fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the default training scripts (&lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;run_squad.py&lt;/code&gt;), we
benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with
TensorFlow 1.11.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, these max batch sizes for &lt;code&gt;BERT-Large&lt;/code&gt; are so small that they
will actually harm the model accuracy, regardless of the learning rate used. We
are working on adding code to this repository which will allow much larger
effective batch sizes to be used on the GPU. The code will be based on one (or
both) of the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient accumulation&lt;/strong&gt;: The samples in a minibatch are typically
independent with respect to gradient computation (excluding batch
normalization, which is not used here). This means that the gradients of
multiple smaller minibatches can be accumulated before performing the weight
update, and this will be exactly equivalent to a single larger update.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gradient-checkpointing"&gt;&lt;strong&gt;Gradient checkpointing&lt;/strong&gt;&lt;/a&gt;:
The major use of GPU/TPU memory during DNN training is caching the
intermediate activations in the forward pass that are necessary for
efficient computation in the backward pass. "Gradient checkpointing" trades
memory for compute time by re-computing the activations in an intelligent
way.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;However, this is not implemented in the current release.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo" class="anchor" aria-hidden="true" href="#using-bert-to-extract-fixed-feature-vectors-like-elmo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT to extract fixed feature vectors (like ELMo)&lt;/h2&gt;
&lt;p&gt;In certain cases, rather than fine-tuning the entire pre-trained model
end-to-end, it can be beneficial to obtained &lt;em&gt;pre-trained contextual
embeddings&lt;/em&gt;, which are fixed contextual representations of each input token
generated from the hidden layers of the pre-trained model. This should also
mitigate most of the out-of-memory issues.&lt;/p&gt;
&lt;p&gt;As an example, we include the script &lt;code&gt;extract_features.py&lt;/code&gt; which can be used
like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sentence A and Sentence B are separated by the ||| delimiter for sentence&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pair tasks like question answering and entailment.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For single sentence inputs, put one sentence per line and DON'T use the&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; delimiter.&lt;/span&gt;
&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Who was Jim Henson ? ||| Jim Henson was a puppeteer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will create a JSON file (one line per line of input) containing the BERT
activations from each Transformer layer specified by &lt;code&gt;layers&lt;/code&gt; (-1 is the final
hidden layer of the Transformer, etc.)&lt;/p&gt;
&lt;p&gt;Note that this script will produce very large output files (by default, around
15kb for every input token).&lt;/p&gt;
&lt;p&gt;If you need to maintain alignment between the original and tokenized words (for
projecting training labels), see the &lt;a href="#tokenization"&gt;Tokenization&lt;/a&gt; section
below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You may see a message like &lt;code&gt;Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.&lt;/code&gt; This message is expected, it
just means that we are using the &lt;code&gt;init_from_checkpoint()&lt;/code&gt; API rather than the
saved model API. If you don't specify a checkpoint or specify an invalid
checkpoint, this script will complain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.
Just follow the example code in &lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;extract_features.py&lt;/code&gt;.
The basic procedure for sentence-level tasks is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instantiate an instance of &lt;code&gt;tokenizer = tokenization.FullTokenizer&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenize the raw text with &lt;code&gt;tokens = tokenizer.tokenize(raw_text)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncate to the maximum sequence length. (You can use up to 512, but you
probably want to use shorter if possible for memory and speed reasons.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; tokens in the right place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since
you need to maintain alignment between your input text and output text so that
you can project your training labels. SQuAD is a particularly complex example
because the input labels are &lt;em&gt;character&lt;/em&gt;-based, and SQuAD paragraphs are often
longer than our maximum sequence length. See the code in &lt;code&gt;run_squad.py&lt;/code&gt; to show
how we handle this.&lt;/p&gt;
&lt;p&gt;Before we describe the general recipe for handling word-level tasks, it's
important to understand what exactly our tokenizer is doing. It has three main
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text normalization&lt;/strong&gt;: Convert all whitespace characters to spaces, and
(for the &lt;code&gt;Uncased&lt;/code&gt; model) lowercase the input and strip out accent markers.
E.g., &lt;code&gt;John Johanson's,  john johanson's,&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Punctuation splitting&lt;/strong&gt;: Split &lt;em&gt;all&lt;/em&gt; punctuation characters on both sides
(i.e., add whitespace around all punctuation characters). Punctuation
characters are defined as (a) Anything with a &lt;code&gt;P*&lt;/code&gt; Unicode class, (b) any
non-letter/number/space ASCII character (e.g., characters like &lt;code&gt;$&lt;/code&gt; which are
technically not punctuation). E.g., &lt;code&gt;john johanson's,  john johanson ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WordPiece tokenization&lt;/strong&gt;: Apply whitespace tokenization to the output of
the above procedure, and apply
&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py"&gt;WordPiece&lt;/a&gt;
tokenization to each token separately. (Our implementation is directly based
on the one from &lt;code&gt;tensor2tensor&lt;/code&gt;, which is linked). E.g., &lt;code&gt;john johanson ' s ,  john johan ##son ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this scheme is that it is "compatible" with most existing
English tokenizers. For example, imagine that you have a part-of-speech tagging
task which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tokenized output will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tokens: john johan ##son ' s house
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Crucially, this would be the same output as if the raw text were &lt;code&gt;John Johanson's house&lt;/code&gt; (with no space before the &lt;code&gt;'s&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have a pre-tokenized representation with word-level annotations, you can
simply tokenize each input word independently, and deterministically maintain an
original-to-tokenized alignment:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Input&lt;/span&gt;
orig_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;John&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Johanson&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;'s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;house&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
labels      &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;POS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Output&lt;/span&gt;
bert_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; []

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Token map will be an int -&amp;gt; int mapping between the `orig_tokens` index and&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the `bert_tokens` index.&lt;/span&gt;
orig_to_tok_map &lt;span class="pl-k"&gt;=&lt;/span&gt; []

tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenization.FullTokenizer(
    &lt;span class="pl-v"&gt;vocab_file&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;vocab_file, &lt;span class="pl-v"&gt;do_lower_case&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[CLS]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;for&lt;/span&gt; orig_token &lt;span class="pl-k"&gt;in&lt;/span&gt; orig_tokens:
  orig_to_tok_map.append(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[SEP]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; orig_to_tok_map == [1, 2, 4, 6]&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;code&gt;orig_to_tok_map&lt;/code&gt; can be used to project &lt;code&gt;labels&lt;/code&gt; to the tokenized
representation.&lt;/p&gt;
&lt;p&gt;There are common English tokenization schemes which will cause a slight mismatch
between how BERT was pre-trained. For example, if your input tokenization splits
off contractions like &lt;code&gt;do n't&lt;/code&gt;, this will cause a mismatch. If it is possible to
do so, you should pre-process your data to convert these back to raw-looking
text, but if it's not possible, this mismatch is likely not a big deal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-training-with-bert" class="anchor" aria-hidden="true" href="#pre-training-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training with BERT&lt;/h2&gt;
&lt;p&gt;We are releasing code to do "masked LM" and "next sentence prediction" on an
arbitrary text corpus. Note that this is &lt;em&gt;not&lt;/em&gt; the exact code that was used for
the paper (the original code was written in C++, and had some additional
complexity), but this code does generate pre-training data as described in the
paper.&lt;/p&gt;
&lt;p&gt;Here's how to run the data generation. The input is a plain text file, with one
sentence per line. (It is important that these be actual sentences for the "next
sentence prediction" task). Documents are delimited by empty lines. The output
is a set of &lt;code&gt;tf.train.Example&lt;/code&gt;s serialized into &lt;code&gt;TFRecord&lt;/code&gt; file format.&lt;/p&gt;
&lt;p&gt;You can perform sentence segmentation with an off-the-shelf NLP toolkit such as
&lt;a href="https://spacy.io/" rel="nofollow"&gt;spaCy&lt;/a&gt;. The &lt;code&gt;create_pretraining_data.py&lt;/code&gt; script will
concatenate segments until they reach the maximum sequence length to minimize
computational waste from padding (see the script for more details). However, you
may want to intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to non-sentential
input during fine-tuning.&lt;/p&gt;
&lt;p&gt;This script stores all of the examples for the entire input file in memory, so
for large data files you should shard the input file and call the script
multiple times. (You can pass in a file glob to &lt;code&gt;run_pretraining.py&lt;/code&gt;, e.g.,
&lt;code&gt;tf_examples.tf_record*&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;max_predictions_per_seq&lt;/code&gt; is the maximum number of masked LM predictions per
sequence. You should set this to around &lt;code&gt;max_seq_length&lt;/code&gt; * &lt;code&gt;masked_lm_prob&lt;/code&gt; (the
script doesn't do that automatically because the exact value needs to be passed
to both scripts).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how to run the pre-training. Do not include &lt;code&gt;init_checkpoint&lt;/code&gt; if you are
pre-training from scratch. The model configuration (including vocab size) is
specified in &lt;code&gt;bert_config_file&lt;/code&gt;. This demo code only pre-trains for a small
number of steps (20), but in practice you will probably want to set
&lt;code&gt;num_train_steps&lt;/code&gt; to 10000 steps or more. The &lt;code&gt;max_seq_length&lt;/code&gt; and
&lt;code&gt;max_predictions_per_seq&lt;/code&gt; parameters passed to &lt;code&gt;run_pretraining.py&lt;/code&gt; must be the
same as &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will produce an output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since our &lt;code&gt;sample_text.txt&lt;/code&gt; file is very small, this example training
will overfit that data in only a few steps and produce unrealistically high
accuracy numbers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-tips-and-caveats" class="anchor" aria-hidden="true" href="#pre-training-tips-and-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training tips and caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If using your own vocabulary, make sure to change &lt;code&gt;vocab_size&lt;/code&gt; in
&lt;code&gt;bert_config.json&lt;/code&gt;. If you use a larger vocabulary without changing this,
you will likely get NaNs when training on GPU or TPU due to unchecked
out-of-bounds access.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If your task has a large domain-specific corpus available (e.g., "movie
reviews" or "scientific papers"), it will likely be beneficial to run
additional steps of pre-training on your corpus, starting from the BERT
checkpoint.&lt;/li&gt;
&lt;li&gt;The learning rate we used in the paper was 1e-4. However, if you are doing
additional steps of pre-training starting from an existing BERT checkpoint,
you should use a smaller learning rate (e.g., 2e-5).&lt;/li&gt;
&lt;li&gt;Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages in the
near future (hopefully by the end of November 2018).&lt;/li&gt;
&lt;li&gt;Longer sequences are disproportionately expensive because attention is
quadratic to the sequence length. In other words, a batch of 64 sequences of
length 512 is much more expensive than a batch of 256 sequences of
length 128. The fully-connected/convolutional cost is the same, but the
attention cost is far greater for the 512-length sequences. Therefore, one
good recipe is to pre-train for, say, 90,000 steps with a sequence length of
128 and then for 10,000 additional steps with a sequence length of 512. The
very long sequences are mostly needed to learn positional embeddings, which
can be learned fairly quickly. Note that this does require generating the
data twice with different values of &lt;code&gt;max_seq_length&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are pre-training from scratch, be prepared that pre-training is
computationally expensive, especially on GPUs. If you are pre-training from
scratch, our recommended recipe is to pre-train a &lt;code&gt;BERT-Base&lt;/code&gt; on a single
&lt;a href="https://cloud.google.com/tpu/docs/pricing" rel="nofollow"&gt;preemptible Cloud TPU v2&lt;/a&gt;, which
takes about 2 weeks at a cost of about $500 USD (based on the pricing in
October 2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-data" class="anchor" aria-hidden="true" href="#pre-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training data&lt;/h3&gt;
&lt;p&gt;We will &lt;strong&gt;not&lt;/strong&gt; be able to release the pre-processed datasets used in the paper.
For Wikipedia, the recommended pre-processing is to download
&lt;a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="nofollow"&gt;the latest dump&lt;/a&gt;,
extract the text with
&lt;a href="https://github.com/attardi/wikiextractor"&gt;&lt;code&gt;WikiExtractor.py&lt;/code&gt;&lt;/a&gt;, and then apply
any necessary cleanup to convert it into plain text.&lt;/p&gt;
&lt;p&gt;Unfortunately the researchers who collected the
&lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt; no longer have it available for
public download. The
&lt;a href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html" rel="nofollow"&gt;Project Guttenberg Dataset&lt;/a&gt;
is a somewhat smaller (200M word) collection of older books that are public
domain.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://commoncrawl.org/" rel="nofollow"&gt;Common Crawl&lt;/a&gt; is another very large collection of
text, but you will likely have to do substantial pre-processing and cleanup to
extract a usable corpus for pre-training BERT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-learning-a-new-wordpiece-vocabulary" class="anchor" aria-hidden="true" href="#learning-a-new-wordpiece-vocabulary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning a new WordPiece vocabulary&lt;/h3&gt;
&lt;p&gt;This repository does not include code for &lt;em&gt;learning&lt;/em&gt; a new WordPiece vocabulary.
The reason is that the code used in the paper was implemented in C++ with
dependencies on Google's internal libraries. For English, it is almost always
better to just start with our vocabulary and pre-trained models. For learning
vocabularies of other languages, there are a number of open source options
available. However, keep in mind that these are not compatible with our
&lt;code&gt;tokenization.py&lt;/code&gt; library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;Google's SentencePiece library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py"&gt;tensor2tensor's WordPiece generation script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsennrich/subword-nmt"&gt;Rico Sennrich's Byte Pair Encoding library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-in-colab" class="anchor" aria-hidden="true" href="#using-bert-in-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT in Colab&lt;/h2&gt;
&lt;p&gt;If you want to use BERT with &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Colab&lt;/a&gt;, you can
get started with the notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".
&lt;strong&gt;At the time of this writing (October 31st, 2018), Colab users can access a
Cloud TPU completely for free.&lt;/strong&gt; Note: One per user, availability limited,
requires a Google Cloud Platform account with storage (although storage may be
purchased with free credit for signing up with GCP), and this capability may not
longer be available in the future. Click on the BERT Colab that was just linked
for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus" class="anchor" aria-hidden="true" href="#is-this-code-compatible-with-cloud-tpus-what-about-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is this code compatible with Cloud TPUs? What about GPUs?&lt;/h4&gt;
&lt;p&gt;Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and
Cloud TPU. However, GPU training is single-GPU only.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-i-am-getting-out-of-memory-errors-what-is-wrong" class="anchor" aria-hidden="true" href="#i-am-getting-out-of-memory-errors-what-is-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I am getting out-of-memory errors, what is wrong?&lt;/h4&gt;
&lt;p&gt;See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for more
information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-pytorch-version-available" class="anchor" aria-hidden="true" href="#is-there-a-pytorch-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a PyTorch version available?&lt;/h4&gt;
&lt;p&gt;There is no official PyTorch implementation. However, NLP researchers from
HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-chainer-version-available" class="anchor" aria-hidden="true" href="#is-there-a-chainer-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a Chainer version available?&lt;/h4&gt;
&lt;p&gt;There is no official Chainer implementation. However, Sosuke Kobayashi made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the Chainer
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-in-other-languages-be-released" class="anchor" aria-hidden="true" href="#will-models-in-other-languages-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models in other languages be released?&lt;/h4&gt;
&lt;p&gt;Yes, we plan to release a multi-lingual BERT model in the near future. We cannot
make promises about exactly which languages will be included, but it will likely
be a single model which includes &lt;em&gt;most&lt;/em&gt; of the languages which have a
significantly-sized Wikipedia.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-larger-than-bert-large-be-released" class="anchor" aria-hidden="true" href="#will-models-larger-than-bert-large-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models larger than &lt;code&gt;BERT-Large&lt;/code&gt; be released?&lt;/h4&gt;
&lt;p&gt;So far we have not attempted to train anything larger than &lt;code&gt;BERT-Large&lt;/code&gt;. It is
possible that we will release larger models if we are able to obtain significant
improvements.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-what-license-is-this-library-released-under" class="anchor" aria-hidden="true" href="#what-license-is-this-library-released-under"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What license is this library released under?&lt;/h4&gt;
&lt;p&gt;All code &lt;em&gt;and&lt;/em&gt; models are released under the Apache 2.0 license. See the
&lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how-do-i-cite-bert" class="anchor" aria-hidden="true" href="#how-do-i-cite-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I cite BERT?&lt;/h4&gt;
&lt;p&gt;For now, cite &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;the Arxiv paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we submit the paper to a conference or journal, we will update the BibTeX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This is not an official Google product.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact-information" class="anchor" aria-hidden="true" href="#contact-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact information&lt;/h2&gt;
&lt;p&gt;For help or issues using BERT, please submit a GitHub issue.&lt;/p&gt;
&lt;p&gt;For personal communication related to BERT, please contact Jacob Devlin
(&lt;code&gt;jacobdevlin@google.com&lt;/code&gt;), Ming-Wei Chang (&lt;code&gt;mingweichang@google.com&lt;/code&gt;), or
Kenton Lee (&lt;code&gt;kentonl@google.com&lt;/code&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/bert</guid><pubDate>Tue, 05 Nov 2019 00:08:00 GMT</pubDate></item><item><title>sebastianruder/NLP-progress #9 in Python, This week</title><link>https://github.com/sebastianruder/NLP-progress</link><description>&lt;p&gt;&lt;i&gt;Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tracking-progress-in-natural-language-processing" class="anchor" aria-hidden="true" href="#tracking-progress-in-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tracking Progress in Natural Language Processing&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-english" class="anchor" aria-hidden="true" href="#english"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;English&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="english/automatic_speech_recognition.md"&gt;Automatic speech recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/ccg.md"&gt;CCG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/common_sense.md"&gt;Common sense&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/constituency_parsing.md"&gt;Constituency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/coreference_resolution.md"&gt;Coreference resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/dependency_parsing.md"&gt;Dependency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/dialogue.md"&gt;Dialogue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/domain_adaptation.md"&gt;Domain adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/entity_linking.md"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/grammatical_error_correction.md"&gt;Grammatical error correction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/information_extraction.md"&gt;Information extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/language_modeling.md"&gt;Language modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/lexical_normalization.md"&gt;Lexical normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/machine_translation.md"&gt;Machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/missing_elements.md"&gt;Missing elements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/multi-task_learning.md"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/multimodal.md"&gt;Multi-modal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/named_entity_recognition.md"&gt;Named entity recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/natural_language_inference.md"&gt;Natural language inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/part-of-speech_tagging.md"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/question_answering.md"&gt;Question answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/relation_prediction.md"&gt;Relation prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/relationship_extraction.md"&gt;Relationship extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_textual_similarity.md"&gt;Semantic textual similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_parsing.md"&gt;Semantic parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_role_labeling.md"&gt;Semantic role labeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/sentiment_analysis.md"&gt;Sentiment analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/shallow_syntax.md"&gt;Shallow syntax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/simplification.md"&gt;Simplification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/stance_detection.md"&gt;Stance detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/summarization.md"&gt;Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/taxonomy_learning.md"&gt;Taxonomy learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/temporal_processing.md"&gt;Temporal processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/text_classification.md"&gt;Text classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/word_sense_disambiguation.md"&gt;Word sense disambiguation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-chinese" class="anchor" aria-hidden="true" href="#chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chinese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chinese/chinese.md#entity-linking"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chinese/chinese_word_segmentation.md"&gt;Chinese word segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-hindi" class="anchor" aria-hidden="true" href="#hindi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hindi&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#chunking"&gt;Chunking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#part-of-speech-tagging"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#machine-translation"&gt;Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-vietnamese" class="anchor" aria-hidden="true" href="#vietnamese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vietnamese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#dependency-parsing"&gt;Dependency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#machine-translation"&gt;Machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#named-entity-recognition"&gt;Named entity recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#part-of-speech-tagging"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#word-segmentation"&gt;Word segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-spanish" class="anchor" aria-hidden="true" href="#spanish"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spanish&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="spanish/entity_linking.md#entity-linking"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-portuguese" class="anchor" aria-hidden="true" href="#portuguese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Portuguese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="portuguese/question_answering.md"&gt;Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This document aims to track the progress in Natural Language Processing (NLP) and give an overview
of the state-of-the-art (SOTA) across the most common NLP tasks and their corresponding datasets.&lt;/p&gt;
&lt;p&gt;It aims to cover both traditional and core NLP tasks such as dependency parsing and part-of-speech tagging
as well as more recent ones such as reading comprehension and natural language inference. The main objective
is to provide the reader with a quick overview of benchmark datasets and the state-of-the-art for their
task of interest, which serves as a stepping stone for further research. To this end, if there is a
place where results for a task are already published and regularly maintained, such as a public leaderboard,
the reader will be pointed there.&lt;/p&gt;
&lt;p&gt;If you want to find this document again in the future, just go to &lt;a href="https://nlpprogress.com/" rel="nofollow"&gt;&lt;code&gt;nlpprogress.com&lt;/code&gt;&lt;/a&gt;
or &lt;a href="http://nlpsota.com/" rel="nofollow"&gt;&lt;code&gt;nlpsota.com&lt;/code&gt;&lt;/a&gt; in your browser.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-guidelines" class="anchor" aria-hidden="true" href="#guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Guidelines&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;  Results reported in published papers are preferred; an exception may be made for influential preprints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Datasets&lt;/strong&gt;  Datasets should have been used for evaluation in at least one published paper besides
the one that introduced the dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;  We recommend to add a link to an implementation
if available. You can add a &lt;code&gt;Code&lt;/code&gt; column (see below) to the table if it does not exist.
In the &lt;code&gt;Code&lt;/code&gt; column, indicate an official implementation with &lt;a href="http://link_to_implementation" rel="nofollow"&gt;Official&lt;/a&gt;.
If an unofficial implementation is available, use &lt;a href="http://link_to_implementation" rel="nofollow"&gt;Link&lt;/a&gt; (see below).
If no implementation is available, you can leave the cell empty.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-adding-a-new-result" class="anchor" aria-hidden="true" href="#adding-a-new-result"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding a new result&lt;/h4&gt;
&lt;p&gt;If you would like to add a new result, you can just click on the small edit button in the top-right
corner of the file for the respective task (see below).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/edit_file.png"&gt;&lt;img src="img/edit_file.png" alt="Click on the edit button to add a file" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This allows you to edit the file in Markdown. Simply add a row to the corresponding table in the
same format. Make sure that the table stays sorted (with the best result on top).
After you've made your change, make sure that the table still looks ok by clicking on the
"Preview changes" tab at the top of the page. If everything looks good, go to the bottom of the page,
where you see the below form.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/propose_file_change.png"&gt;&lt;img src="img/propose_file_change.png" alt="Fill out the file change information" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Add a name for your proposed change, an optional description, indicate that you would like to
"Create a new branch for this commit and start a pull request", and click on "Propose file change".&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-adding-a-new-dataset-or-task" class="anchor" aria-hidden="true" href="#adding-a-new-dataset-or-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding a new dataset or task&lt;/h4&gt;
&lt;p&gt;For adding a new dataset or task, you can also follow the steps above. Alternatively, you can fork the repository.
In both cases, follow the steps below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If your task is completely new, create a new file and link to it in the table of contents above.&lt;/li&gt;
&lt;li&gt;If not, add your task or dataset to the respective section of the corresponding file (in alphabetical order).&lt;/li&gt;
&lt;li&gt;Briefly describe the dataset/task and include relevant references.&lt;/li&gt;
&lt;li&gt;Describe the evaluation setting and evaluation metric.&lt;/li&gt;
&lt;li&gt;Show how an annotated example of the dataset/task looks like.&lt;/li&gt;
&lt;li&gt;Add a download link if available.&lt;/li&gt;
&lt;li&gt;Copy the below table and fill in at least two results (including the state-of-the-art)
for your dataset/task (change Score to the metric of your dataset). If your dataset/task
has multiple metrics, add them to the right of &lt;code&gt;Score&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Submit your change as a pull request.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Score&lt;/th&gt;
&lt;th&gt;Paper / Source&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-wish-list" class="anchor" aria-hidden="true" href="#wish-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wish list&lt;/h3&gt;
&lt;p&gt;These are tasks and datasets that are still missing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bilingual dictionary induction&lt;/li&gt;
&lt;li&gt;Discourse parsing&lt;/li&gt;
&lt;li&gt;Keyphrase extraction&lt;/li&gt;
&lt;li&gt;Knowledge base population (KBP)&lt;/li&gt;
&lt;li&gt;More dialogue tasks&lt;/li&gt;
&lt;li&gt;Semi-supervised learning&lt;/li&gt;
&lt;li&gt;Frame-semantic parsing (FrameNet full-sentence analysis)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-exporting-into-a-structured-format" class="anchor" aria-hidden="true" href="#exporting-into-a-structured-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Exporting into a structured format&lt;/h3&gt;
&lt;p&gt;You can extract all the data into a structured, machine-readable JSON format with parsed tasks, descriptions and SOTA tables.&lt;/p&gt;
&lt;p&gt;The instructions are in &lt;a href="structured/README.md"&gt;structured/README.md&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-instructions-for-building-the-site-locally" class="anchor" aria-hidden="true" href="#instructions-for-building-the-site-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Instructions for building the site locally&lt;/h3&gt;
&lt;p&gt;Instructions for building the website locally using Jekyll can be found &lt;a href="jekyll_instructions.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sebastianruder</author><guid isPermaLink="false">https://github.com/sebastianruder/NLP-progress</guid><pubDate>Tue, 05 Nov 2019 00:09:00 GMT</pubDate></item><item><title>NVIDIA/DeepLearningExamples #10 in Python, This week</title><link>https://github.com/NVIDIA/DeepLearningExamples</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nvidia-deep-learning-examples-for-tensor-cores" class="anchor" aria-hidden="true" href="#nvidia-deep-learning-examples-for-tensor-cores"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA Deep Learning Examples for Tensor Cores&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This repository provides the latest deep learning example networks for training.  These examples focus on achieving the best performance and convergence from NVIDIA Volta Tensor Cores.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-gpu-cloud-ngc-container-registry" class="anchor" aria-hidden="true" href="#nvidia-gpu-cloud-ngc-container-registry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA GPU Cloud (NGC) Container Registry&lt;/h2&gt;
&lt;p&gt;These examples, along with our NVIDIA deep learning software stack, are provided in a monthly updated Docker container on the NGC container registry (&lt;a href="https://ngc.nvidia.com" rel="nofollow"&gt;https://ngc.nvidia.com&lt;/a&gt;). These containers include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest NVIDIA examples from this repository&lt;/li&gt;
&lt;li&gt;The latest NVIDIA contributions shared upstream to the respective framework&lt;/li&gt;
&lt;li&gt;The latest NVIDIA Deep Learning software libraries, such as cuDNN, NCCL, cuBLAS, etc. which have all been through a rigorous monthly quality assurance process to ensure that they provide the best possible performance&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/dgx/index.html#nvidia-optimized-frameworks-release-notes" rel="nofollow"&gt;Monthly release notes&lt;/a&gt; for each of the NVIDIA optimized containers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-directory-structure" class="anchor" aria-hidden="true" href="#directory-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directory structure&lt;/h2&gt;
&lt;p&gt;The examples are organized first by framework, such as TensorFlow, PyTorch, etc. and second by use case, such as computer vision, natural language processing, etc. We hope this structure enables you to quickly locate the example networks that best suit your needs. Here are the currently supported models:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-computer-vision" class="anchor" aria-hidden="true" href="#computer-vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResNet-50&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/MxNet/Classification/RN50v1.5"&gt;MXNet&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/RN50v1.5"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Classification/RN50v1.5"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Detection/SSD"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(industrial)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Industrial"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(medical)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Medical"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GNMT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/GNMT"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Translation/GNMT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/Transformer"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT"&gt;PyTorch&lt;/a&gt;][&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recommender-systems" class="anchor" aria-hidden="true" href="#recommender-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommender Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NCF&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/NCF"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-text-to-speech" class="anchor" aria-hidden="true" href="#text-to-speech"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text to Speech&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tacotron &amp;amp; WaveGlow&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-speech-recognition" class="anchor" aria-hidden="true" href="#speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech Recognition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jasper&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-support" class="anchor" aria-hidden="true" href="#nvidia-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA support&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate the level of support that will be provided. The range is from ongoing updates and improvements to a point-in-time release for thought leadership.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-feedback--contributions" class="anchor" aria-hidden="true" href="#feedback--contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback / Contributions&lt;/h2&gt;
&lt;p&gt;We're posting these examples on GitHub to better support the community, facilitate feedback, as well as collect and implement contributions using GitHub Issues and pull requests. We welcome all contributions!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known issues&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate any known issues and encourage the community to provide feedback.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/DeepLearningExamples</guid><pubDate>Tue, 05 Nov 2019 00:10:00 GMT</pubDate></item><item><title>ddbourgin/numpy-ml #11 in Python, This week</title><link>https://github.com/ddbourgin/numpy-ml</link><description>&lt;p&gt;&lt;i&gt;Machine learning, in numpy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-numpy-ml" class="anchor" aria-hidden="true" href="#numpy-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;numpy-ml&lt;/h1&gt;
&lt;p&gt;Ever wish you had an inefficient but somewhat legible collection of machine
learning algorithms implemented exclusively in numpy? No?&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;To see all of the available models, take a look at the &lt;a href="https://numpy-ml.readthedocs.io/" rel="nofollow"&gt;project documentation&lt;/a&gt; or see &lt;a href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Am I missing your favorite model? Is there something that could be cleaner /
less confusing? Did I mess something up? Submit a PR! The only requirement is
that your models are written with just the &lt;a href="https://docs.python.org/3/library/" rel="nofollow"&gt;Python standard
library&lt;/a&gt; and &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt;. The
&lt;a href="https://scipy.github.io/devdocs/" rel="nofollow"&gt;SciPy library&lt;/a&gt; is also permitted under special
circumstances ;)&lt;/p&gt;
&lt;p&gt;See full contributing guidelines &lt;a href="./CONTRIBUTING.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ddbourgin</author><guid isPermaLink="false">https://github.com/ddbourgin/numpy-ml</guid><pubDate>Tue, 05 Nov 2019 00:11:00 GMT</pubDate></item><item><title>CorentinJ/Real-Time-Voice-Cloning #12 in Python, This week</title><link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link><description>&lt;p&gt;&lt;i&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-time-voice-cloning" class="anchor" aria-hidden="true" href="#real-time-voice-cloning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real-Time Voice Cloning&lt;/h1&gt;
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. Feel free to check &lt;a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow"&gt;my thesis&lt;/a&gt; if you're curious or if you're looking for info I haven't documented yet (don't hesitate to make an issue for that too). Mostly I would recommend giving a quick look to the figures beyond the introduction.&lt;/p&gt;
&lt;p&gt;SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9c33f78be8afe656503da974c478ea2ba2647db7/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-papers-implemented" class="anchor" aria-hidden="true" href="#papers-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers implemented&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Designation&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Implementation source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow"&gt;1802.08435&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;WaveRNN (vocoder)&lt;/td&gt;
&lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;1712.05884&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tacotron 2 (synthesizer)&lt;/td&gt;
&lt;td&gt;Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Rayhane-mamah/Tacotron-2"&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow"&gt;1710.10467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GE2E (encoder)&lt;/td&gt;
&lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;20/08/19:&lt;/strong&gt; I'm working on &lt;a href="https://github.com/resemble-ai/Resemblyzer"&gt;resemblyzer&lt;/a&gt;, an independent package for the voice encoder. You can use your trained encoder models from this repo with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;06/07/19:&lt;/strong&gt; Need to run within a docker container on a remote server? See &lt;a href="https://sean.lane.sh/posts/2019/07/Running-the-Real-Time-Voice-Cloning-project-in-Docker/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;25/06/19:&lt;/strong&gt; Experimental support for low-memory GPUs (~2gb) added for the synthesizer. Pass &lt;code&gt;--low_mem&lt;/code&gt; to &lt;code&gt;demo_cli.py&lt;/code&gt; or &lt;code&gt;demo_toolbox.py&lt;/code&gt; to enable it. It adds a big overhead, so it's not recommended if you have enough VRAM.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;You will need the following whether you plan to use the toolbox only or to retrain the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 3.7&lt;/strong&gt;. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the necessary packages. Additionally you will need &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch&lt;/a&gt; (&amp;gt;=1.0.1).&lt;/p&gt;
&lt;p&gt;A GPU is mandatory, but you don't necessarily need a high tier GPU if you only want to use the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained models&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary" class="anchor" aria-hidden="true" href="#preliminary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary&lt;/h3&gt;
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h3&gt;
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="http://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-toolbox" class="anchor" aria-hidden="true" href="#toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toolbox&lt;/h3&gt;
&lt;p&gt;You can then try the toolbox:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt;
or&lt;br&gt;
&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wiki&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it all works&lt;/strong&gt; (WIP - &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works"&gt;stub&lt;/a&gt;, you might be better off reading my thesis until it's done)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training"&gt;&lt;strong&gt;Training models yourself&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training with other data/languages&lt;/strong&gt; (WIP - see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-507864097"&gt;here&lt;/a&gt; for now)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/TODO-&amp;amp;-planned-features"&gt;&lt;strong&gt;TODO and planned features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;I'm working full-time as of June 2019. Replying to issues is very time-consuming, I can't always do it. I won't be making progress of my own on this repo, but I will still gladly merge PRs and accept contributions to the wiki. Don't hesitate to send me an email if you wish to contribute.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CorentinJ</author><guid isPermaLink="false">https://github.com/CorentinJ/Real-Time-Voice-Cloning</guid><pubDate>Tue, 05 Nov 2019 00:12:00 GMT</pubDate></item><item><title>geekcomputers/Python #13 in Python, This week</title><link>https://github.com/geekcomputers/Python</link><description>&lt;p&gt;&lt;i&gt;My Python Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-my-python-examples" class="anchor" aria-hidden="true" href="#my-python-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;My Python Examples&lt;/h1&gt;
&lt;p&gt;I do not consider myself a programmer. I create these little programs as experiments to play with the language, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me at &lt;a href="mailto:craig@geekcomputers.co.uk"&gt;craig@geekcomputers.co.uk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These scripts contain important functions which help reduce human workload.
Code documentation is aligned correctly when the files are viewed in &lt;a href="https://notepad-plus-plus.org/" rel="nofollow"&gt;Notepad++&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/batch_file_rename.py"&gt;batch_file_rename.py&lt;/a&gt; - This batch renames a group of files in a given directory, once you pass the current and the new extensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/create_dir_if_not_there.py"&gt;create_dir_if_not_there.py&lt;/a&gt; - Checks to see if a directory exists in the users home directory, if not then create it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube-downloader%20fast.py"&gt;Fast Youtube Downloader&lt;/a&gt; - Downloads YouTube videos quickly with parallel threads using aria2c&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/tree/master/Google_Image_Downloader"&gt;Google Image Downloader&lt;/a&gt; - Query a given term and retrieve images from the Google Image database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/dir_test.py"&gt;dir_test.py&lt;/a&gt; - Tests to see if the directory &lt;code&gt;testdir&lt;/code&gt; exists, if not it will create the directory for you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/env_check.py"&gt;env_check.py&lt;/a&gt; - This script will check to see if all of the environment variables required are set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Ratna04priya/Python/blob/master/BlackJack_game/blackjack.py"&gt;blackjack.py&lt;/a&gt; - This script contains the Casino BlackJack-21 Game in Python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/fileinfo.py"&gt;fileinfo.py&lt;/a&gt; - Shows file information for a given file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/folder_size.py"&gt;folder_size.py&lt;/a&gt; - Scans the current directory and all subdirectories and displays the size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/logs.py"&gt;logs.py&lt;/a&gt; - This script will search for all &lt;code&gt;*.log&lt;/code&gt; files in the given directory, zip them using the program you specify, and then date stamp them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/move_files_over_x_days.py"&gt;move_files_over_x_days.py&lt;/a&gt; - Moves all files over a specified age (in days) from the source directory to the destination directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/nslookup_check.py"&gt;nslookup_check.py&lt;/a&gt; - This simple script opens the file &lt;code&gt;server_list.txt&lt;/code&gt; and then does an nslookup for each one to check the DNS entry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/osinfo.py"&gt;osinfo.py&lt;/a&gt; - Displays some information about the OS on which you are running this script.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_servers.py"&gt;ping_servers.py&lt;/a&gt; - This script, depending on the arguments supplied, will ping the servers associated with that application group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_subnet.py"&gt;ping_subnet.py&lt;/a&gt; - After supplying the first 3 octets this file scans the final range for available addresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/powerdown_startup.py"&gt;powerdown_startup.py&lt;/a&gt; - This file goes through the server list and pings the machine, if it is up it will load the putty session, if it is not it will notify you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/puttylogs.py"&gt;puttylogs.py&lt;/a&gt; -  This file zips up all the logs in the given directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_count.py"&gt;script_count.py&lt;/a&gt; - This file scans the scripts directory and gives a count of the different types of scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[get_youtube_view.py] - This is very simple python script to get more views for your youtube videos.Some times I use for repeating my favorite songs by this scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_listing.py"&gt;script_listing.py&lt;/a&gt; - This file will list all the files in the given directory, and go through all the subdirectories as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/testlines.py"&gt;testlines.py&lt;/a&gt; - This simple script opens a file and prints out 100 lines of whatever is the set for the line variable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/tweeter.py"&gt;tweeter.py&lt;/a&gt; - Allows you to tweet text or a picture from the terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/serial_scanner.py"&gt;serial_scanner.py&lt;/a&gt; contains a method called ListAvailablePorts which returns a list with the names of the serial ports that are in use in the computer. This method works only on Linux and Windows (can be extended for mac osx). If no port is found, an empty list is returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/get_youtube_view.py"&gt;get_youtube_view.py&lt;/a&gt; - A simple python script to get more views for your YouTube videos. Useful for repeating songs on YouTube.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacter.py"&gt;CountMillionCharacter.py&lt;/a&gt; And &lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacters-2.0.py"&gt;CountMillionCharacter2.0&lt;/a&gt;.py - Gets character count of a text file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/xkcd_downloader.py"&gt;xkcd_downloader.py&lt;/a&gt; - Downloads the latest XKCD comic and places them in a new folder called "comics".&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/timymodule.py"&gt;timymodule.py&lt;/a&gt; - A great alternative to Pythons 'timeit' module and easier to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/calculator.py"&gt;calculator.py&lt;/a&gt; - Uses Python's eval() function to implement a calculator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Google_News.py"&gt;Google_News.py&lt;/a&gt; - Uses BeautifulSoup to provide Latest news headline along with news link.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Cricket_score.py"&gt;cricket_live_score&lt;/a&gt; - Uses BeautifulSoup to provide live cricket score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube.py"&gt;youtube.py&lt;/a&gt; - Takes a song name as input and fetches the YouTube URL of the best matching song and plays it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/site_health.py"&gt;site_health.py&lt;/a&gt; - Checks the health of a remote server&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/SimpleStopWatch.py"&gt;SimpleStopWatch.py&lt;/a&gt; - Simple Stop Watch implementation using Python's time module.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/changemac.py"&gt;Changemac.py&lt;/a&gt; - This script change your MAC address , generate random MAC address or enter input as new MAC address in your linux(Successfully Tested in Ubuntu 18.04).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/whatsapp-monitor.py"&gt;whatsapp-monitor.py&lt;/a&gt; - Uses Selenium to give online status about your contacts when your contacts become online in whatsapp you will get an update about it on terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/subahanii/whatsapp-Chat-Analyzer"&gt;whatsapp-chat-analyzer.py&lt;/a&gt; - This is whatsapp group/individual chat analyzer .
This script is able to analyse all activity happened in whatsapp group and visualize all thing through matplotlib library(In Graph form).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>geekcomputers</author><guid isPermaLink="false">https://github.com/geekcomputers/Python</guid><pubDate>Tue, 05 Nov 2019 00:13:00 GMT</pubDate></item><item><title>scikit-learn/scikit-learn #14 in Python, This week</title><link>https://github.com/scikit-learn/scikit-learn</link><description>&lt;p&gt;&lt;i&gt;scikit-learn: machine learning in Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img alt="Azure" src="https://camo.githubusercontent.com/bfe67a3604768c16e941f3331709bf55507a4b57/68747470733a2f2f6465762e617a7572652e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f5f617069732f6275696c642f7374617475732f7363696b69742d6c6561726e2e7363696b69742d6c6561726e3f6272616e63684e616d653d6d6173746572" data-canonical-src="https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/590475799489c962f111c9fc5c1432ecbc577578/68747470733a2f2f6170692e7472617669732d63692e6f72672f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/scikit-learn/scikit-learn?branch=master" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/58a0b06906ca5d106ec090fe8a1ac85a092b81c2/68747470733a2f2f636f6465636f762e696f2f6769746875622f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" data-canonical-src="https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&amp;amp;service=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d2913194913f85128f908483a265e64dcd6d31e4/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d3a636972636c652d746f6b656e" data-canonical-src="https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&amp;amp;circle-token=:circle-token" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://img.shields.io/pypi/pyversions/scikit-learn.svg" rel="nofollow"&gt;&lt;img alt="PythonVersion" src="https://camo.githubusercontent.com/45416807fdec5b0d83acca16b2b9f08fe7d32bf1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363696b69742d6c6561726e2e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/9f0ed32d05350afa18a801573e4da7f4a240e181/68747470733a2f2f62616467652e667572792e696f2f70792f7363696b69742d6c6561726e2e737667" data-canonical-src="https://badge.fury.io/py/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="DOI" src="https://camo.githubusercontent.com/73c63e44b8bee62df142664048c58f83ec8ad95c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f32313336392f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e737667" data-canonical-src="https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-scikit-learn"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-scikit-learn" class="anchor" aria-hidden="true" href="#scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;scikit-learn&lt;/h2&gt;
&lt;p&gt;scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;It is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;a name="user-content-dependencies"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;p&gt;scikit-learn requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python (&amp;gt;= 3.5)&lt;/li&gt;
&lt;li&gt;NumPy (&amp;gt;= 1.11.0)&lt;/li&gt;
&lt;li&gt;SciPy (&amp;gt;= 0.17.0)&lt;/li&gt;
&lt;li&gt;joblib (&amp;gt;= 0.11)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.&lt;/strong&gt;
scikit-learn 0.21 and later require Python 3.5 or newer.&lt;/p&gt;
&lt;p&gt;Scikit-learn plotting capabilities (i.e., functions start with "&lt;a href="#id2"&gt;&lt;span id="user-content-id3"&gt;plot_&lt;/span&gt;&lt;/a&gt;"
and classes end with "Display") require Matplotlib (&amp;gt;= 1.5.1). For running the
examples Matplotlib &amp;gt;= 1.5.1 is required. A few examples require
scikit-image &amp;gt;= 0.12.3, a few examples require pandas &amp;gt;= 0.18.0.&lt;/p&gt;
&lt;a name="user-content-user-installation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-user-installation" class="anchor" aria-hidden="true" href="#user-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User installation&lt;/h4&gt;
&lt;p&gt;If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;pip install -U scikit-learn
&lt;/pre&gt;
&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;conda install scikit-learn
&lt;/pre&gt;
&lt;p&gt;The documentation includes more detailed &lt;a href="http://scikit-learn.org/stable/install.html" rel="nofollow"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-changelog"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h3&gt;
&lt;p&gt;See the &lt;a href="http://scikit-learn.org/dev/whats_new.html" rel="nofollow"&gt;changelog&lt;/a&gt;
for a history of notable changes to scikit-learn.&lt;/p&gt;
&lt;a name="user-content-development"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
&lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;Development Guide&lt;/a&gt;
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.&lt;/p&gt;
&lt;a name="user-content-important-links"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-important-links" class="anchor" aria-hidden="true" href="#important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important links&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Official source code repo: &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download releases: &lt;a href="https://pypi.org/project/scikit-learn/" rel="nofollow"&gt;https://pypi.org/project/scikit-learn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/scikit-learn/scikit-learn/issues"&gt;https://github.com/scikit-learn/scikit-learn/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-source-code"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-source-code" class="anchor" aria-hidden="true" href="#source-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source code&lt;/h4&gt;
&lt;p&gt;You can check the latest sources with the command:&lt;/p&gt;
&lt;pre&gt;git clone https://github.com/scikit-learn/scikit-learn.git
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h4&gt;
&lt;p&gt;To learn more about making a contribution to scikit-learn, please see our
&lt;a href="https://scikit-learn.org/dev/developers/contributing.html" rel="nofollow"&gt;Contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-testing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h4&gt;
&lt;p&gt;After installation, you can launch the test suite from outside the
source directory (you will need to have &lt;code&gt;pytest&lt;/code&gt; &amp;gt;= 3.3.0 installed):&lt;/p&gt;
&lt;pre&gt;pytest sklearn
&lt;/pre&gt;
&lt;p&gt;See the web page &lt;a href="http://scikit-learn.org/dev/developers/advanced_installation.html#testing" rel="nofollow"&gt;http://scikit-learn.org/dev/developers/advanced_installation.html#testing&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;blockquote&gt;
Random number generation can be controlled during testing by setting
the &lt;code&gt;SKLEARN_SEED&lt;/code&gt; environment variable.&lt;/blockquote&gt;
&lt;a name="user-content-submitting-a-pull-request"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-submitting-a-pull-request" class="anchor" aria-hidden="true" href="#submitting-a-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Submitting a Pull Request&lt;/h4&gt;
&lt;p&gt;Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: &lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;http://scikit-learn.org/stable/developers/index.html&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-project-history"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-project-history" class="anchor" aria-hidden="true" href="#project-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project History&lt;/h3&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the  &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;The project is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: scikit-learn was previously referred to as scikits.learn.&lt;/p&gt;
&lt;a name="user-content-help-and-support"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-help-and-support" class="anchor" aria-hidden="true" href="#help-and-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help and Support&lt;/h3&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HTML documentation (stable release): &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HTML documentation (development version): &lt;a href="http://scikit-learn.org/dev/" rel="nofollow"&gt;http://scikit-learn.org/dev/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FAQ: &lt;a href="http://scikit-learn.org/stable/faq.html" rel="nofollow"&gt;http://scikit-learn.org/stable/faq.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-communication"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href="https://mail.python.org/mailman/listinfo/scikit-learn" rel="nofollow"&gt;https://mail.python.org/mailman/listinfo/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRC channel: &lt;code&gt;#scikit-learn&lt;/code&gt; at &lt;code&gt;webchat.freenode.net&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stack Overflow: &lt;a href="https://stackoverflow.com/questions/tagged/scikit-learn" rel="nofollow"&gt;https://stackoverflow.com/questions/tagged/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-citation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h4&gt;
&lt;p&gt;If you use scikit-learn in a scientific publication, we would appreciate citations: &lt;a href="http://scikit-learn.org/stable/about.html#citing-scikit-learn" rel="nofollow"&gt;http://scikit-learn.org/stable/about.html#citing-scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>scikit-learn</author><guid isPermaLink="false">https://github.com/scikit-learn/scikit-learn</guid><pubDate>Tue, 05 Nov 2019 00:14:00 GMT</pubDate></item><item><title>swisskyrepo/PayloadsAllTheThings #15 in Python, This week</title><link>https://github.com/swisskyrepo/PayloadsAllTheThings</link><description>&lt;p&gt;&lt;i&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-payloads-all-the-things" class="anchor" aria-hidden="true" href="#payloads-all-the-things"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Payloads All The Things&lt;/h1&gt;
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I &lt;g-emoji class="g-emoji" alias="heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png"&gt;&lt;/g-emoji&gt; pull requests :)&lt;/p&gt;
&lt;p&gt;You can also contribute with a &lt;g-emoji class="g-emoji" alias="beers" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37b.png"&gt;&lt;/g-emoji&gt; IRL&lt;/p&gt;
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;README.md - vulnerability description and how to exploit it&lt;/li&gt;
&lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt;
&lt;li&gt;Images - pictures for the README.md&lt;/li&gt;
&lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/"&gt;Methodology and Resources&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md"&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md"&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md"&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md"&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md"&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md"&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Discovery.md"&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md"&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md"&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md"&gt;Windows - Download and Execute.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Mimikatz.md"&gt;Windows - Mimikatz.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Persistence.md"&gt;Windows - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Post%20Exploitation%20Koadic.md"&gt;Windows - Post Exploitation Koadic.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Privilege%20Escalation.md"&gt;Windows - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Using%20credentials.md"&gt;Windows - Using credentials.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CVE%20Exploits"&gt;CVE Exploits&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Struts 2 CVE-2013-2251 CVE-2017-5638 CVE-2018-11776_.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2017-9805.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2018-11776.py&lt;/li&gt;
&lt;li&gt;Docker API RCE.py&lt;/li&gt;
&lt;li&gt;Drupalgeddon2 CVE-2018-7600.rb&lt;/li&gt;
&lt;li&gt;Heartbleed CVE-2014-0160.py&lt;/li&gt;
&lt;li&gt;JBoss CVE-2015-7501.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2015-8103.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2016-0792.py&lt;/li&gt;
&lt;li&gt;Rails CVE-2019-5420.rb&lt;/li&gt;
&lt;li&gt;Shellshock CVE-2014-6271.py&lt;/li&gt;
&lt;li&gt;Tomcat CVE-2017-12617.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2016-3510.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2017-10271.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2018-2894.py&lt;/li&gt;
&lt;li&gt;WebSphere CVE-2015-7450.py&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/YOUTUBE.md"&gt;Youtube videos&lt;/a&gt; selections.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>swisskyrepo</author><guid isPermaLink="false">https://github.com/swisskyrepo/PayloadsAllTheThings</guid><pubDate>Tue, 05 Nov 2019 00:15:00 GMT</pubDate></item><item><title>zhaoolee/ChromeAppHeroes #16 in Python, This week</title><link>https://github.com/zhaoolee/ChromeAppHeroes</link><description>&lt;p&gt;&lt;i&gt;-Chrome, Chrome, Chrome~  ChromePluginHeroes, Write a Chinese manual for the excellent Chrome plugin, let the Chrome plugin heroes benefit the human~&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/readme-en.html" rel="nofollow"&gt;English&lt;/a&gt; | &lt;a href="https://zhaoolee.gitbooks.io/chrome/content/" rel="nofollow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/996icu/996.ICU/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/41215df7ff78cefe41536bf897fe1c7e55b10bd2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d416e74692532303939362d626c75652e737667" alt="LICENSE" data-canonical-src="https://img.shields.io/badge/license-Anti%20996-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://996.icu" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13ac320a9a774e316fe72ffb1eaacf09b01b59a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e6b2d3939362e6963752d7265642e737667" alt="996.icu" data-canonical-src="https://img.shields.io/badge/link-996.icu-red.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content--chrome" class="anchor" aria-hidden="true" href="#-chrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;-Chrome&lt;/h1&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="rainbow" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f308.png"&gt;&lt;/g-emoji&gt;-Chrome, Chrome, Chrome~
ChromeAppHeroes, Write a Chinese manual for the excellent Chrome plugin, let the Chrome plugin heroes benefit the human~&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/5ecd2856f287477c89c20efb7de11a9b.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/5ecd2856f287477c89c20efb7de11a9b.png" alt="VI.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/LuoJiangYong"&gt;&lt;/a&gt;Logo | &lt;a href="https://zhaoolee.gitbooks.io/chrome/content/gu-li-qu-yi.html" rel="nofollow"&gt;()&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--chrome" class="anchor" aria-hidden="true" href="#-chrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;: &lt;a href="https://github.com/zhaoolee/ProgrammingWithChrome"&gt;Chrome&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Chrome(ChromeB), GifChrome, Chrome! &lt;a href="https://github.com/zhaoolee/ProgrammingWithChrome"&gt;https://github.com/zhaoolee/ProgrammingWithChrome&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://github.com/996icu/996.ICU/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/41215df7ff78cefe41536bf897fe1c7e55b10bd2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d416e74692532303939362d626c75652e737667" alt="LICENSE" data-canonical-src="https://img.shields.io/badge/license-Anti%20996-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://996.icu" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13ac320a9a774e316fe72ffb1eaacf09b01b59a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e6b2d3939362e6963752d7265642e737667" alt="996.icu" data-canonical-src="https://img.shields.io/badge/link-996.icu-red.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(, ~)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/060_tabagotchi.html" rel="nofollow"&gt;060Tabagotchi&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/059_page_speed_insight_and_check_list.html" rel="nofollow"&gt;059PageSpeed Insight and CheckList&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/058_ip_address.html" rel="nofollow"&gt;058IP-AddressIP&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/057_webp_save_as_png.html" rel="nofollow"&gt;057JPG/PNG/WebPWebPPNG&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/056_search.html" rel="nofollow"&gt;056SearchChrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/055_keylines.html" rel="nofollow"&gt;055Keylines &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/054_er_xiang_yi_tu_sou_tu.html" rel="nofollow"&gt;054 &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/053_shu_biao_dian_ji_te_xiao.html" rel="nofollow"&gt;053 (  )&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/052_site_palette.html" rel="nofollow"&gt;052Site Palette&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/051_custom_cursor_for_chrome.html" rel="nofollow"&gt;051Custom Cursor for ChromeChrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/050_google_results_previewer.html" rel="nofollow"&gt;050Google Results Previewer&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/049_web_server_for_chrome.html" rel="nofollow"&gt;049Web Server for ChromeWeb, &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/048_words_discoverer.html" rel="nofollow"&gt;048Words Discoverer,&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/047_go_to_tab.html" rel="nofollow"&gt;047Go to Tab&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/046_whatfont.html" rel="nofollow"&gt;046WhatFont&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/045_restlet_client.html" rel="nofollow"&gt;045Restlet ClientApi&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/044_gu_ge_fang_wen_zhu_shou.html" rel="nofollow"&gt;044Chrome Gmail &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/043_dream_afar_new_tab.html" rel="nofollow"&gt;043Dream Afar New Tab&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/042_edge.html" rel="nofollow"&gt;042 EdgeChrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/041_copy_all_urls.html" rel="nofollow"&gt;041Copy All Urls-&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/040_gitzip_for_github.html" rel="nofollow"&gt;040GitZip for githubGithub&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/039_simplify_gmail.html" rel="nofollow"&gt;039Simplify GmailGmail&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/038_alexa_traffic_rank.html" rel="nofollow"&gt;038Alexa Traffic Rank&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/037_saladict.html" rel="nofollow"&gt;037Saladict!!! , &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/036_screen_shader.html" rel="nofollow"&gt;036Screen Shader&lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;&lt;/g-emoji&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/035_print_friendly_and_pdf.html" rel="nofollow"&gt;035Print Friendly &amp;amp; PDF&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/034_astro_bot.html" rel="nofollow"&gt;034Astro Bot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/033_yi_ye.html" rel="nofollow"&gt;033  &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/032_smallpdf.html" rel="nofollow"&gt;032SmallpdfPDF&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/031_onetab.html" rel="nofollow"&gt;031OneTabTab&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/030_jue_jin.html" rel="nofollow"&gt;030&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/029_simread.html" rel="nofollow"&gt;029 SimpRead&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/028_adblock.html" rel="nofollow"&gt;028AdBlockAdblock&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/027_text.html" rel="nofollow"&gt;027TextChrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/026_quickey_launcher.html" rel="nofollow"&gt;026Quickey Launcher&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/025_console.html" rel="nofollow"&gt;025ConsoleChrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/024_dark_reader.html" rel="nofollow"&gt;024Dark Reader&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/023_fireshot.html" rel="nofollow"&gt;023FireShot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/022kuo_zhan_guan_li_qi.html" rel="nofollow"&gt;022Chrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/021_bi_li_bi_li_zhu_shou.html" rel="nofollow"&gt;021B&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/020_boxel_rebound.html" rel="nofollow"&gt;020Boxel Rebound()&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/019_mega.html" rel="nofollow"&gt;019MEGA? MEGA!&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/018_enhanced_github.html" rel="nofollow"&gt;018Enhanced Github,Github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/017_xin_lang_wei_bo_tu_chuang.html" rel="nofollow"&gt;017Markdown, &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/016_jie_chu_b_zhan_qu_yu_xian_zhi.html" rel="nofollow"&gt;016B&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/015_xpath_helper.html" rel="nofollow"&gt;015 XPath HelperBing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/014_chao_ji_ma_li_ao_you_xi.html" rel="nofollow"&gt;014Chrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/013_quick_qr.html" rel="nofollow"&gt;013Quick QR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/012_ourstickys.html" rel="nofollow"&gt;012OurStickysChrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/011_whatruns.html" rel="nofollow"&gt;011 whatruns&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/010_speedtest.html" rel="nofollow"&gt;010speedtestspeedtest&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/009_vimium.html" rel="nofollow"&gt;009vimiumChromevim&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/008_chrome_cleaner_pro.html" rel="nofollow"&gt;008Chrome Cleaner ProChrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/007_loom.html" rel="nofollow"&gt;007loom Chrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/006_similarsites.html" rel="nofollow"&gt;006SimilarSites  SimilarSites&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/005_video_speed_controller.html" rel="nofollow"&gt;005Video Speed Controller (16!)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/004_tampermonkey.html" rel="nofollow"&gt;004Tampermonkey ! &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/003_secure_shell_app.html" rel="nofollow"&gt;003Secure Shell App Chromessh&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/002_chrono.html" rel="nofollow"&gt;002chrono Chrome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/001_markdown_here.html" rel="nofollow"&gt;001markdown-here Markdown""&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;()&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/the-fucking-github/agajobpbaphiohkbkjigcalebbfmofdo" rel="nofollow"&gt;The Fucking Github&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/lvxianchao"&gt;lvxianchao&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/lvxianchao/the-fucking-github"&gt;Github&lt;/a&gt;&lt;/td&gt;
&lt;td&gt; Star  Github &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/hitup/eiokaohkigpbonodjcbjpecbnccijkjb" rel="nofollow"&gt;HitUP&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wonderbeyond"&gt;wonderbeyond&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wonderbeyond/HitUP"&gt;Github&lt;/a&gt;&lt;/td&gt;
&lt;td&gt; New Tab  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/gitako-github-file-tree/giljefjcheohhamkjphiebfjnlphnokk" rel="nofollow"&gt;Gitako - Github file tree&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/EnixCoda"&gt;EnixCoda&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/EnixCoda/Gitako"&gt;Github&lt;/a&gt;&lt;/td&gt;
&lt;td&gt; Octotree &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/githuber/janmcneaglgklfljjcpihkkomeghljnf" rel="nofollow"&gt;GITHUBER&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/zhuowenli"&gt;zhuowenli&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/zhuowenli/githuber"&gt;Github&lt;/a&gt;&lt;/td&gt;
&lt;td&gt; GitHub  Chrome &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/60c92f0de3d44bb7a612d08e2e1f3d18.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/60c92f0de3d44bb7a612d08e2e1f3d18.png" alt=".png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;?()&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c4fdea49e11241e392d6bcaa33855897.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c4fdea49e11241e392d6bcaa33855897.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;()&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20.00&lt;/td&gt;
&lt;td&gt;()&lt;/td&gt;
&lt;td&gt;201982&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20.00&lt;/td&gt;
&lt;td&gt;()&lt;/td&gt;
&lt;td&gt;2019711&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12.34&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2019820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;201995&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2019724&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;201962&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;Azuno&lt;/td&gt;
&lt;td&gt;201961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2019522&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;enjoy life&lt;/td&gt;
&lt;td&gt;2019920&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;L__hoo&lt;/td&gt;
&lt;td&gt;2019920&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;(:)&lt;/td&gt;
&lt;td&gt;2019914&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;1111&lt;/td&gt;
&lt;td&gt;2019727&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2019519&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;Lismg&lt;/td&gt;
&lt;td&gt;201965&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;small&lt;/td&gt;
&lt;td&gt;201979&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2019720&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.00&lt;/td&gt;
&lt;td&gt;@Coolstar&lt;/td&gt;
&lt;td&gt;201976&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;()&lt;/td&gt;
&lt;td&gt;2019926&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2019923&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;()&lt;/td&gt;
&lt;td&gt;2019726&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;2019712&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;()&lt;/td&gt;
&lt;td&gt;2019613&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Walter Wu&lt;/td&gt;
&lt;td&gt;201961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Joseph&lt;/td&gt;
&lt;td&gt;2019424&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;()&lt;/td&gt;
&lt;td&gt;2019412&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Edward&lt;/td&gt;
&lt;td&gt;2019412&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2019411&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Cloud 9&lt;/td&gt;
&lt;td&gt;201945&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.20&lt;/td&gt;
&lt;td&gt;()&lt;/td&gt;
&lt;td&gt;2019725&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;[]&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-060tabagotchi" class="anchor" aria-hidden="true" href="#060tabagotchi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/060_tabagotchi.html" rel="nofollow"&gt;060Tabagotchi&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63478935-7b1f7400-c4be-11e9-8679-5f4a6a56c89c.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63478935-7b1f7400-c4be-11e9-8679-5f4a6a56c89c.gif" alt="tabagotchi" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tabagotchi, , , ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-059pagespeed-insight-and-checklist" class="anchor" aria-hidden="true" href="#059pagespeed-insight-and-checklist"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/059_page_speed_insight_and_check_list.html" rel="nofollow"&gt;059PageSpeed Insight and CheckList&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63309328-f818e500-c328-11e9-8f1a-68fed13a4015.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63309328-f818e500-c328-11e9-8f1a-68fed13a4015.gif" alt="pag_speed" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63309327-f7804e80-c328-11e9-8eab-9055db8a5d2c.png"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63309327-f7804e80-c328-11e9-8eab-9055db8a5d2c.png" alt="001" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PageSpeed Insight and CheckList  Google Page Speed , ,,,,&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-058ip-addressip" class="anchor" aria-hidden="true" href="#058ip-addressip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/058_ip_address.html" rel="nofollow"&gt;058IP-AddressIP&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63222725-ee369b00-c1dd-11e9-986e-cbc002168db8.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63222725-ee369b00-c1dd-11e9-986e-cbc002168db8.gif" alt="ip_address" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;IP,,,IP-Address, &lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-057jpgpngwebpwebppngmd" class="anchor" aria-hidden="true" href="#057jpgpngwebpwebppngmd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/057_webp_save_as_png.html" rel="nofollow"&gt;057JPG/PNG/WebPWebPPNG.md&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63221240-ce48ac80-c1c8-11e9-9860-376fedc0845e.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63221240-ce48ac80-c1c8-11e9-9860-376fedc0845e.gif" alt="save_as_png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;WebP, Photoshop, png,, , , (WebPpng, pngWebP), ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-056searchchrome" class="anchor" aria-hidden="true" href="#056searchchrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/056_search.html" rel="nofollow"&gt;056SearchChrome&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/62503773-3c37c000-b828-11e9-9605-4ecce76830ec.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/62503773-3c37c000-b828-11e9-9605-4ecce76830ec.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;, (, ), Chrome, &lt;strong&gt;&lt;/strong&gt;, , &lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-055keylines-" class="anchor" aria-hidden="true" href="#055keylines-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/055_keylines.html" rel="nofollow"&gt;055Keylines &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61917657-dbcf9580-af80-11e9-87d3-528609ab85b0.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61917657-dbcf9580-af80-11e9-87d3-528609ab85b0.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Keylines(domoutline), , Keylines, , , ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-054" class="anchor" aria-hidden="true" href="#054"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/054_er_xiang_yi_tu_sou_tu.html" rel="nofollow"&gt;054+&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61757068-93ce3880-adf1-11e9-8903-ebf313fb6098.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61757068-93ce3880-adf1-11e9-8903-ebf313fb6098.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; , , ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-053---" class="anchor" aria-hidden="true" href="#053---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/053_shu_biao_dian_ji_te_xiao.htmll" rel="nofollow"&gt;053 (  )&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61600040-04921b00-ac61-11e9-8446-533752d71de1.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61600040-04921b00-ac61-11e9-8446-533752d71de1.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; (  ),,, , ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-052site-palette" class="anchor" aria-hidden="true" href="#052site-palette"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/052_site_palette.html" rel="nofollow"&gt;052Site Palette&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61169390-2f101400-a58f-11e9-8769-4d62b7b64f37.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61169390-2f101400-a58f-11e9-8769-4d62b7b64f37.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Site Palette, , , , , Chrome~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-051custom-cursor-for-chromechrome" class="anchor" aria-hidden="true" href="#051custom-cursor-for-chromechrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/051_custom_cursor_for_chrome.html" rel="nofollow"&gt;051Custom Cursor for ChromeChrome&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61166967-d0846f00-a569-11e9-9141-15cef4983098.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61166967-d0846f00-a569-11e9-9141-15cef4983098.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;QQ, , , , , , , Next,Next, , ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-050google-results-previewer" class="anchor" aria-hidden="true" href="#050google-results-previewer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/050_google_results_previewer.html" rel="nofollow"&gt;050Google Results Previewer&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/9219a092f0f4eb1c6f614c1667b316d1.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/9219a092f0f4eb1c6f614c1667b316d1.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Google Results Previewer, , &lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-049web-server-for-chromeweb-" class="anchor" aria-hidden="true" href="#049web-server-for-chromeweb-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/049_web_server_for_chrome.html" rel="nofollow"&gt;049Web Server for ChromeWeb, &lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/74d3eb882b103e0fb1e5e5dd651c052f.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/74d3eb882b103e0fb1e5e5dd651c052f.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Web Server for Chromehttp,, , , Web Server for Chrome~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-048words-discoverer" class="anchor" aria-hidden="true" href="#048words-discoverer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/048_words_discoverer.html" rel="nofollow"&gt;048Words Discoverer,&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/305439fdd84017da654e00f16aaee752.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/305439fdd84017da654e00f16aaee752.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Words Discoverer(: ),&lt;strong&gt;&lt;/strong&gt;,, , ,&lt;strong&gt; &lt;/strong&gt;, 15%16%,  , ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-047go-to-tab" class="anchor" aria-hidden="true" href="#047go-to-tab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/047_go_to_tab.html" rel="nofollow"&gt;047Go to Tab&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/59550928-2a623b00-8fa4-11e9-8525-8e830907463b.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/59550928-2a623b00-8fa4-11e9-8525-8e830907463b.gif" alt="2019-06-15-18 54 23" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Go to Tab, , &lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-046whatfont" class="anchor" aria-hidden="true" href="#046whatfont"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/046_whatfont.html" rel="nofollow"&gt;046WhatFont&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/59549312-4529b500-8f8e-11e9-8107-004486a02258.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/59549312-4529b500-8f8e-11e9-8107-004486a02258.gif" alt="font 2019-06-15 16_04_10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;WhatFont, , , &lt;a href="https://fonts.google.com/" rel="nofollow"&gt;https://fonts.google.com/&lt;/a&gt;, &lt;a href="https://www.myfonts.com/" rel="nofollow"&gt;https://www.myfonts.com/&lt;/a&gt;
,~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-045restlet-clientapi" class="anchor" aria-hidden="true" href="#045restlet-clientapi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/045_restlet_client.html" rel="nofollow"&gt;045Restlet ClientApi&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/89ea1e51dab48d5a84f089adf33eb274.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/89ea1e51dab48d5a84f089adf33eb274.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Restlet Client, Postmanapi&lt;/li&gt;
&lt;li&gt;, Postman70M, , ,PostmanChrome, macOS(, )&lt;/li&gt;
&lt;li&gt;Restlet ClientChrome, (~)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-044chrome-gmail-" class="anchor" aria-hidden="true" href="#044chrome-gmail-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/044_gu_ge_fang_wen_zhu_shou.html" rel="nofollow"&gt;044Chrome Gmail &lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/deff71a536ba4027a01fe3c7a558c277.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/deff71a536ba4027a01fe3c7a558c277.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Chrome, , Gmail
&lt;code&gt;,,,IP,&lt;/code&gt;, &lt;code&gt;https://2018.hao245.com/&lt;/code&gt;, , 360~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-043dream-afar-new-tab" class="anchor" aria-hidden="true" href="#043dream-afar-new-tab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/043_dream_afar_new_tab.html" rel="nofollow"&gt;043Dream Afar New Tab&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e40b7bec41ce4ac892578bc88a03d25c.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e40b7bec41ce4ac892578bc88a03d25c.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dream Afar New Tab, , , , , ~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-042-edgechrome" class="anchor" aria-hidden="true" href="#042-edgechrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/042_edge.html" rel="nofollow"&gt;042 EdgeChrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a131b9833d20424ab93cb258ab8542e8.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a131b9833d20424ab93cb258ab8542e8.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;EdgeChrome, ChromeApp, &lt;a href="https://chrome.google.com/webstore/detail/secure-shell-app/pnhechapfaindjhompbnflcldabbghjo" rel="nofollow"&gt;Secure Shell App&lt;/a&gt;, , EdgeChromeChromium, Chrome,Edge~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-041copy-all-urls-" class="anchor" aria-hidden="true" href="#041copy-all-urls-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/041_copy_all_urls.html" rel="nofollow"&gt;041Copy All Urls-&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/eac219ff189a4295bbf88974b045ba5b.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/eac219ff189a4295bbf88974b045ba5b.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Copy All Urls, Copy All Urls~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-040gitzip-for-githubgithub" class="anchor" aria-hidden="true" href="#040gitzip-for-githubgithub"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/040_gitzip_for_github.html" rel="nofollow"&gt;040GitZip for githubGithub&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/f5b923dc4a21437484e90859342ed366.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/f5b923dc4a21437484e90859342ed366.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Github&lt;a href="https://zhaoolee.gitbooks.io/chrome/content/018enhanced-github300b-cong-201c-bing-gui-201d-dao-201c-bing-gun-er-201d2c-xia-zai-github-dan-ge-wen-jian.html" rel="nofollow"&gt;Enhanced Github&lt;/a&gt; , Enhanced Github  GitZip for github , , github~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-039simplify-gmailgmail" class="anchor" aria-hidden="true" href="#039simplify-gmailgmail"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/039_simplify_gmail.html" rel="nofollow"&gt;039Simplify GmailGmail&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c9b1aa8201c24208b0e0aedfcdbdc992.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c9b1aa8201c24208b0e0aedfcdbdc992.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, , 
Gmail, , Gmail, , , &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-038alexa-traffic-rank" class="anchor" aria-hidden="true" href="#038alexa-traffic-rank"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/038_alexa_traffic_rank.html" rel="nofollow"&gt;038Alexa Traffic Rank&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcefd45a5cc74e4c824f567535f79c5c.webp"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcefd45a5cc74e4c824f567535f79c5c.webp" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Alexa, , , Alexa, , , ~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-037saladict--" class="anchor" aria-hidden="true" href="#037saladict--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/037_saladict.html" rel="nofollow"&gt;037Saladict!!! , &lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/07322f3c4b13484a8a048194558cec5c.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/07322f3c4b13484a8a048194558cec5c.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;(Saladict), , , &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-036screen-shader" class="anchor" aria-hidden="true" href="#036screen-shader"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/036_screen_shader.html" rel="nofollow"&gt;036Screen Shader&lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;&lt;/g-emoji&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/3a94a283267047c39114694706de7293.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/3a94a283267047c39114694706de7293.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, , , , ~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-035print-friendly--pdf" class="anchor" aria-hidden="true" href="#035print-friendly--pdf"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/035_print_friendly_and_pdf.html" rel="nofollow"&gt;035Print Friendly &amp;amp; PDF&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a71d2b280298482ba2408482c1537bf9.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a71d2b280298482ba2408482c1537bf9.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Print Friendly &amp;amp; PDFchrome, , Print Friendly &amp;amp; PDF~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-034astro-bot" class="anchor" aria-hidden="true" href="#034astro-bot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/034_astro_bot.html" rel="nofollow"&gt;034Astro Bot&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/671d39ca714f437fa1d287bfb988724e.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/671d39ca714f437fa1d287bfb988724e.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Astro Bot,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-033--" class="anchor" aria-hidden="true" href="#033--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/033_yi_ye.html" rel="nofollow"&gt;033  &lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6a328e8eb9984f5abea5816c681b8e4e.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6a328e8eb9984f5abea5816c681b8e4e.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;,, ,,pokemongo,,,,~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-032smallpdfpdf" class="anchor" aria-hidden="true" href="#032smallpdfpdf"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/032_smallpdf.html" rel="nofollow"&gt;032SmallpdfPDF&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/2c00d25291db4750963c60e78344d4cc.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/2c00d25291db4750963c60e78344d4cc.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;SmallpdfPDF,,, Smallpdfpdf, pdf, PDF, ~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-031onetabtab" class="anchor" aria-hidden="true" href="#031onetabtab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/031_onetab.htmll" rel="nofollow"&gt;031OneTabTab&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/93781d48870742e08dc68fa17e79169e.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/93781d48870742e08dc68fa17e79169e.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;,OneTab,,,OneTab,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-030" class="anchor" aria-hidden="true" href="#030"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/030_jue_jin.html" rel="nofollow"&gt;030&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcca47d65f2542808281c17ec379d7d9.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcca47d65f2542808281c17ec379d7d9.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt; , , , , ,  &lt;a href="https://juejin.im/" rel="nofollow"&gt;https://juejin.im/&lt;/a&gt; &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-029-simpread" class="anchor" aria-hidden="true" href="#029-simpread"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/029_simread.html" rel="nofollow"&gt;029 SimpRead&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0f9aa9ca332c4325806f92784af9f9ac.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0f9aa9ca332c4325806f92784af9f9ac.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
, , , SimpRead&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-028adblockadblock" class="anchor" aria-hidden="true" href="#028adblockadblock"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/028_adblock.html" rel="nofollow"&gt;028AdBlockAdblock&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e149c42ac1f343b88f50e522cba9ad64.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e149c42ac1f343b88f50e522cba9ad64.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Adblock, , Adblock, , ~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-027textchrome" class="anchor" aria-hidden="true" href="#027textchrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/027_text.html" rel="nofollow"&gt;027TextChrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6e287798ca1d4b939705447d4b8b2b3b.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6e287798ca1d4b939705447d4b8b2b3b.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;TextChrome, &lt;a href="https://github.com/GoogleChromeLabs/text-app"&gt;https://github.com/GoogleChromeLabs/text-app&lt;/a&gt; , Text, , , Chrome(Linux~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-026quickey-launcher" class="anchor" aria-hidden="true" href="#026quickey-launcher"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/026_quickey_launcher.html" rel="nofollow"&gt;026Quickey Launcher&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/322a82d214b34ff2ba70d9c1cd71d276.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/322a82d214b34ff2ba70d9c1cd71d276.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Quickey Launcher, , , ,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-025consolechrome" class="anchor" aria-hidden="true" href="#025consolechrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/025_console.html" rel="nofollow"&gt;025ConsoleChrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c7bc7cabd06a453dbed2bae0a2bf08d5.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c7bc7cabd06a453dbed2bae0a2bf08d5.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome: ,, , : Chrome()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-024dark-reader" class="anchor" aria-hidden="true" href="#024dark-reader"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/024_dark_reader.html" rel="nofollow"&gt;024Dark Reader&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/35e84f58945d4775a31154ea4dc51cac.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/35e84f58945d4775a31154ea4dc51cac.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, Dark Reader~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;&lt;a id="user-content-023fireshot" class="anchor" aria-hidden="true" href="#023fireshot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/023_fireshot.html" rel="nofollow"&gt;023FireShot&lt;/a&gt;&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/81ac43fe1d6e454b93dc7f3ae57d96cd.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/81ac43fe1d6e454b93dc7f3ae57d96cd.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
, FireShot, , &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-022chrome" class="anchor" aria-hidden="true" href="#022chrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/022kuo_zhan_guan_li_qi.html" rel="nofollow"&gt;022Chrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0480fffebb10437c8d5555f085de9006.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0480fffebb10437c8d5555f085de9006.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Chrome, , ,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-021b" class="anchor" aria-hidden="true" href="#021b"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/021_bi_li_bi_li_zhu_shou.html" rel="nofollow"&gt;021B&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6ccb9837b60d4d79814a8add20723d97.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6ccb9837b60d4d79814a8add20723d97.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, ,,~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-020boxel-rebound" class="anchor" aria-hidden="true" href="#020boxel-rebound"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/020_boxel_rebound.html" rel="nofollow"&gt;020Boxel Rebound()&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dbc83cc53c26492db8843ff3e35fc75d.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dbc83cc53c26492db8843ff3e35fc75d.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Boxel Rebound, , , , ; Mac,Windows,Linux, Chrome, Boxel Rebound&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-019mega-mega" class="anchor" aria-hidden="true" href="#019mega-mega"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/019_mega.html" rel="nofollow"&gt;019MEGA? MEGA!&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b5aea0b5e3c54f0a9a050a754a67093d.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b5aea0b5e3c54f0a9a050a754a67093d.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;()&lt;/li&gt;
&lt;li&gt;(google, , MEGAsync)&lt;/li&gt;
&lt;li&gt;, &lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-018enhanced-githubgithub" class="anchor" aria-hidden="true" href="#018enhanced-githubgithub"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/018_enhanced_github.html" rel="nofollow"&gt;018Enhanced Github,Github&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/769a22f995d74226ba4104aba7e8ab59.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/769a22f995d74226ba4104aba7e8ab59.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/00541b7bd6954f8ea2a6a1beaebbb79b.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/00541b7bd6954f8ea2a6a1beaebbb79b.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Github,Github... Enhanced Github, Github,   &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-017markdown-" class="anchor" aria-hidden="true" href="#017markdown-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/017_xin_lang_wei_bo_tu_chuang.html" rel="nofollow"&gt;017Markdown, &lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/48c12b3864f84e988e073209fd7cf8e4.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/48c12b3864f84e988e073209fd7cf8e4.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Markdown, , ,,, url, &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-016b" class="anchor" aria-hidden="true" href="#016b"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/016_jie_chu_b_zhan_qu_yu_xian_zhi.html" rel="nofollow"&gt;016B&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/34d50d4d15094ca08e1bbd76c477122a.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/34d50d4d15094ca08e1bbd76c477122a.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/99fd518796894945aa87225a5022c453.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/99fd518796894945aa87225a5022c453.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
B,B&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-015xpath-helperbing" class="anchor" aria-hidden="true" href="#015xpath-helperbing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/015_xpath_helper.html" rel="nofollow"&gt;015XPath HelperBing&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/308bec78f4674130b85a5852f0b25a88.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/308bec78f4674130b85a5852f0b25a88.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;XPath, XPathBing~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-014chrome" class="anchor" aria-hidden="true" href="#014chrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/014_chao_ji_ma_li_ao_you_xi.html" rel="nofollow"&gt;014Chrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/008f3bd3c8b8483b9d70be5d5ed4f9ee.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/008f3bd3c8b8483b9d70be5d5ed4f9ee.gif" alt=".gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome? , ! ,Chrome, , &lt;g-emoji class="g-emoji" alias="yum" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60b.png"&gt;&lt;/g-emoji&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-013quick-qr" class="anchor" aria-hidden="true" href="#013quick-qr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/013_quick_qr.html" rel="nofollow"&gt;013Quick QR&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b59f299316624e86aa7cdd379a02aac4.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b59f299316624e86aa7cdd379a02aac4.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Quick QR, ,,PC(~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-012ourstickyschrome" class="anchor" aria-hidden="true" href="#012ourstickyschrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/012_ourstickys.html" rel="nofollow"&gt;012OurStickysChrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/62597d60ffd6443396725c9677951221.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/62597d60ffd6443396725c9677951221.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;,,,,~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-011-whatruns" class="anchor" aria-hidden="true" href="#011-whatruns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/011_whatruns.html" rel="nofollow"&gt;011 whatruns&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/28cc002358c647878b54f9bcaaf67a0a.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/28cc002358c647878b54f9bcaaf67a0a.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, whatruns, facebook&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-010speedtestspeedtest" class="anchor" aria-hidden="true" href="#010speedtestspeedtest"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/010_speedtest.html" rel="nofollow"&gt;010speedtestspeedtest&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9aa1e5323a6a4cbcb96304b33a5261c8.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9aa1e5323a6a4cbcb96304b33a5261c8.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, ,window, 360, , , &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-009vimiumchromevim" class="anchor" aria-hidden="true" href="#009vimiumchromevim"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/009_vimium.html" rel="nofollow"&gt;009vimiumChromevim&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/7d6e9fadef3f48409c81a8c76d24e0cc.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/7d6e9fadef3f48409c81a8c76d24e0cc.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;vimium, vimium, ~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-008chrome-cleaner-prochrome" class="anchor" aria-hidden="true" href="#008chrome-cleaner-prochrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/008_chrome_cleaner_pro.html" rel="nofollow"&gt;008Chrome Cleaner ProChrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/30899ae22f644a9bb62eb8b24d75c884.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/30899ae22f644a9bb62eb8b24d75c884.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome, , Chrome OS, Windows, , Chrome, Chrome Cleaner Pro~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-007loom-chrome" class="anchor" aria-hidden="true" href="#007loom-chrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/007_loom.html" rel="nofollow"&gt;007loom Chrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/4058cf0008074c5f86b8eb1684e7a1a0.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/4058cf0008074c5f86b8eb1684e7a1a0.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Loom(), ,, , (~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-006similarsites--similarsites" class="anchor" aria-hidden="true" href="#006similarsites--similarsites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/006_similarsites.html" rel="nofollow"&gt;006SimilarSites  SimilarSites&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/26c6c604be1c41e88ebfe79c733173b0.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/26c6c604be1c41e88ebfe79c733173b0.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, , "", , , ""! SimilarSites, , !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-005video-speed-controller-16" class="anchor" aria-hidden="true" href="#005video-speed-controller-16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/005_video_speed_controller.html" rel="nofollow"&gt;005Video Speed Controller (16!)&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/083c51a1c32a4ad6931646bb005fd5a3.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/083c51a1c32a4ad6931646bb005fd5a3.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;, , 4, Video Speed Controller16~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-004tampermonkey--" class="anchor" aria-hidden="true" href="#004tampermonkey--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/004_tampermonkey.html" rel="nofollow"&gt;004Tampermonkey ! &lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e87601eb459549b3b8e33994fc3fdfb4.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e87601eb459549b3b8e33994fc3fdfb4.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome, , VIP, , ~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-003secure-shell-app-chromessh" class="anchor" aria-hidden="true" href="#003secure-shell-app-chromessh"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/003_secure_shell_app.html" rel="nofollow"&gt;003Secure Shell App Chromessh&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/87b66b4cbd12426bbab65a3443f1f1ec.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/87b66b4cbd12426bbab65a3443f1f1ec.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;VPN, , ssh, Windowsssh,puttyxshell ,Secure Shell Appchromessh&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-002-chrono-chrome" class="anchor" aria-hidden="true" href="#002-chrono-chrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/002_chrono.html" rel="nofollow"&gt;002 chrono Chrome&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b574ee1798984ff49396837b620f51ef.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b574ee1798984ff49396837b620f51ef.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;chrono, (!)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-001markdown-here-markdown" class="anchor" aria-hidden="true" href="#001markdown-here-markdown"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/001_markdown_here.html" rel="nofollow"&gt;001markdown-here Markdown""&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fc5de2eb22184a138c618728cfb40ede.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fc5de2eb22184a138c618728cfb40ede.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;markdown-here,  QQ, Gmail, , mardown,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content--chrome" class="anchor" aria-hidden="true" href="#-chrome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; Chrome()&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/88386634" rel="nofollow"&gt;Top20Chrome&lt;/a&gt; : &lt;a href="https://me.csdn.net/dQCFKyQDXYm3F8rB0" rel="nofollow"&gt;AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58636515" rel="nofollow"&gt;Chrome &lt;/a&gt; : &lt;a href="https://www.zhihu.com/people/loonggg/activities" rel="nofollow"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openingsource.org/6190/zh-tw/" rel="nofollow"&gt;363&lt;/a&gt; : &lt;a href="https://openingsource.org/" rel="nofollow"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/Y-9ht-E7-OdJOEDDb3yyWw" rel="nofollow"&gt;N&lt;/a&gt; : &lt;a href="https://github.com/LuoJiangYong"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Chrome&lt;/strong&gt;,&lt;strong&gt;&lt;/strong&gt;, , &lt;strong&gt;Chrome&lt;/strong&gt; ,@&lt;a href="https://github.com/hjthjthjt"&gt;hjthjthjt&lt;/a&gt; &lt;a href="https://github.com/zhaoolee/ChromeAppHeroes/issues/14"&gt;issue&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;:**&lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;Github&lt;/a&gt;**GithubGithub~ Github: &lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;https://github.com/zhaoolee/StarsAndClown&lt;/a&gt; ~&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;  &lt;a href="https://juejin.im/user/5b39bd7de51d4558d43ff06d" rel="nofollow"&gt;@&lt;/a&gt;  &lt;strong&gt;&lt;/strong&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.jianshu.com/" rel="nofollow"&gt;&lt;/a&gt;Markdown,&lt;strong&gt;Chrome&lt;/strong&gt;,&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;** emm... &lt;a href="https://zhaoolee.com/ChromeAppHeroes/download_the_chrome_extension_from_the_store.html" rel="nofollow"&gt;Chrome&lt;/a&gt;**&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chrome&lt;/strong&gt; Github: &lt;a href="https://github.com/zhaoolee/ChromeAppHeroes"&gt;https://github.com/zhaoolee/ChromeAppHeroes&lt;/a&gt;
,  &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;&lt;/g-emoji&gt;.
I need your support, I hope you can add a star &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;&lt;/g-emoji&gt; to this project.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-n" class="anchor" aria-hidden="true" href="#n"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/meaning_of_gu_li.html" rel="nofollow"&gt;N()&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg" alt="smartmockups_juunlhbe.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dc9ab48d958843c98f2a4c9336cff748.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dc9ab48d958843c98f2a4c9336cff748.png" alt="2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/chrome_extended_resources_site.html" rel="nofollow"&gt;Chrome&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zhaoolee</author><guid isPermaLink="false">https://github.com/zhaoolee/ChromeAppHeroes</guid><pubDate>Tue, 05 Nov 2019 00:16:00 GMT</pubDate></item><item><title>hindupuravinash/the-gan-zoo #17 in Python, This week</title><link>https://github.com/hindupuravinash/the-gan-zoo</link><description>&lt;p&gt;&lt;i&gt;A list of all named GANs!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-the-gan-zoo" class="anchor" aria-hidden="true" href="#the-gan-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The GAN Zoo&lt;/h1&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="The_GAN_Zoo.jpg"&gt;&lt;img width="40%" src="The_GAN_Zoo.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Every week, new GAN papers are coming out and it's hard to keep track of them all, not to mention the incredibly creative ways in which researchers are naming these GANs! So, here's a list of what started as a fun activity compiling all named GANs!&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="cumulative_gans.jpg"&gt;&lt;img width="50%" src="cumulative_gans.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also check out the same data in a tabular format with functionality to filter by year or do a quick search by title &lt;a href="https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contributions are welcome. Add links through pull requests in gans.tsv file in the same format or create an issue to lemme know something I missed or to start a discussion.&lt;/p&gt;
&lt;p&gt;Check out &lt;a href="https://deephunt.in" rel="nofollow"&gt;Deep Hunt&lt;/a&gt; - my weekly AI newsletter for this repo as &lt;a href="https://medium.com/deep-hunt/the-gan-zoo-79597dc8c347" rel="nofollow"&gt;blogpost&lt;/a&gt; and follow me on &lt;a href="https://www.twitter.com/hindupuravinash" rel="nofollow"&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3D-ED-GAN - &lt;a href="https://arxiv.org/abs/1711.06375" rel="nofollow"&gt;Shape Inpainting using 3D Generative Adversarial Network and Recurrent Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D-GAN - &lt;a href="https://arxiv.org/abs/1610.07584" rel="nofollow"&gt;Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling&lt;/a&gt; (&lt;a href="https://github.com/zck119/3dgan-release"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;3D-IWGAN - &lt;a href="https://arxiv.org/abs/1707.09557" rel="nofollow"&gt;Improved Adversarial Systems for 3D Object Generation and Reconstruction&lt;/a&gt; (&lt;a href="https://github.com/EdwardSmith1884/3D-IWGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;3D-PhysNet - &lt;a href="https://arxiv.org/abs/1805.00328" rel="nofollow"&gt;3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object Deformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D-RecGAN - &lt;a href="https://arxiv.org/abs/1708.07969" rel="nofollow"&gt;3D Object Reconstruction from a Single Depth View with Adversarial Learning&lt;/a&gt; (&lt;a href="https://github.com/Yang7879/3D-RecGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ABC-GAN - &lt;a href="https://drive.google.com/file/d/0B3wEP_lEl0laVTdGcHE2VnRiMlE/view" rel="nofollow"&gt;ABC-GAN: Adaptive Blur and Control for improved training stability of Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/IgorSusmelj/ABC-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ABC-GAN - &lt;a href="https://arxiv.org/abs/1711.11139" rel="nofollow"&gt;GANs for LIFE: Generative Adversarial Networks for Likelihood Free Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AC-GAN - &lt;a href="https://arxiv.org/abs/1610.09585" rel="nofollow"&gt;Conditional Image Synthesis With Auxiliary Classifier GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;acGAN - &lt;a href="https://arxiv.org/abs/1702.01983" rel="nofollow"&gt;Face Aging With Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ACGAN - &lt;a href="https://arxiv.org/abs/1712.06951" rel="nofollow"&gt;Coverless Information Hiding Based on Generative adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;acGAN - &lt;a href="https://arxiv.org/abs/1808.00020" rel="nofollow"&gt;On-line Adaptative Curriculum Learning for GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ACtuAL - &lt;a href="https://arxiv.org/abs/1711.04755" rel="nofollow"&gt;ACtuAL: Actor-Critic Under Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdaGAN - &lt;a href="https://arxiv.org/abs/1701.02386v1" rel="nofollow"&gt;AdaGAN: Boosting Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive GAN - &lt;a href="https://arxiv.org/abs/1806.10496" rel="nofollow"&gt;Customizing an Adversarial Example Generator with Class-Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdvEntuRe - &lt;a href="https://arxiv.org/abs/1805.04680" rel="nofollow"&gt;AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdvGAN - &lt;a href="https://arxiv.org/abs/1801.02610" rel="nofollow"&gt;Generating adversarial examples with adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AE-GAN - &lt;a href="https://arxiv.org/abs/1707.05474" rel="nofollow"&gt;AE-GAN: adversarial eliminating with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AE-OT - &lt;a href="https://arxiv.org/abs/1809.05964" rel="nofollow"&gt;Latent Space Optimal Transport for Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AEGAN - &lt;a href="https://arxiv.org/abs/1703.10094" rel="nofollow"&gt;Learning Inverse Mapping by Autoencoder based Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AF-DCGAN - &lt;a href="https://arxiv.org/abs/1804.05347" rel="nofollow"&gt;AF-DCGAN: Amplitude Feature Deep Convolutional GAN for Fingerprint Construction in Indoor Localization System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AffGAN - &lt;a href="https://arxiv.org/abs/1610.04490" rel="nofollow"&gt;Amortised MAP Inference for Image Super-resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AIM - &lt;a href="https://arxiv.org/abs/1809.05972" rel="nofollow"&gt;Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AL-CGAN - &lt;a href="https://arxiv.org/abs/1612.00215" rel="nofollow"&gt;Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ALI - &lt;a href="https://arxiv.org/abs/1606.00704" rel="nofollow"&gt;Adversarially Learned Inference&lt;/a&gt; (&lt;a href="https://github.com/IshmaelBelghazi/ALI"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AlignGAN - &lt;a href="https://arxiv.org/abs/1707.01400" rel="nofollow"&gt;AlignGAN: Learning to Align Cross-Domain Images with Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AlphaGAN - &lt;a href="https://arxiv.org/abs/1807.10088" rel="nofollow"&gt;AlphaGAN: Generative adversarial networks for natural image matting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AM-GAN - &lt;a href="https://arxiv.org/abs/1703.02000" rel="nofollow"&gt;Activation Maximization Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AmbientGAN - &lt;a href="https://openreview.net/forum?id=Hy7fDog0b" rel="nofollow"&gt;AmbientGAN: Generative models from lossy measurements&lt;/a&gt; (&lt;a href="https://github.com/AshishBora/ambient-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AMC-GAN - &lt;a href="https://arxiv.org/abs/1807.02635" rel="nofollow"&gt;Video Prediction with Appearance and Motion Conditions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AnoGAN - &lt;a href="https://arxiv.org/abs/1703.05921v1" rel="nofollow"&gt;Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;APD - &lt;a href="https://arxiv.org/abs/1806.10317" rel="nofollow"&gt;Adversarial Distillation of Bayesian Neural Network Posteriors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;APE-GAN - &lt;a href="https://arxiv.org/abs/1707.05474" rel="nofollow"&gt;APE-GAN: Adversarial Perturbation Elimination with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ARAE - &lt;a href="https://arxiv.org/abs/1706.04223" rel="nofollow"&gt;Adversarially Regularized Autoencoders for Generating Discrete Structures&lt;/a&gt; (&lt;a href="https://github.com/jakezhaojb/ARAE"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ARDA - &lt;a href="https://arxiv.org/abs/1707.01217" rel="nofollow"&gt;Adversarial Representation Learning for Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ARIGAN - &lt;a href="https://arxiv.org/abs/1709.00938" rel="nofollow"&gt;ARIGAN: Synthetic Arabidopsis Plants using Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ArtGAN - &lt;a href="https://arxiv.org/abs/1702.03410" rel="nofollow"&gt;ArtGAN: Artwork Synthesis with Conditional Categorial GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ASDL-GAN - &lt;a href="https://ieeexplore.ieee.org/document/8017430/" rel="nofollow"&gt;Automatic Steganographic Distortion Learning Using a Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ATA-GAN - &lt;a href="https://arxiv.org/abs/1802.09070" rel="nofollow"&gt;Attention-Aware Generative Adversarial Networks (ATA-GANs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Attention-GAN - &lt;a href="https://arxiv.org/abs/1803.06798" rel="nofollow"&gt;Attention-GAN for Object Transfiguration in Wild Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AttGAN - &lt;a href="https://arxiv.org/abs/1711.10678" rel="nofollow"&gt;Arbitrary Facial Attribute Editing: Only Change What You Want&lt;/a&gt; (&lt;a href="https://github.com/LynnHo/AttGAN-Tensorflow"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AttnGAN - &lt;a href="https://arxiv.org/abs/1711.10485" rel="nofollow"&gt;AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/taoxugit/AttnGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AVID - &lt;a href="https://arxiv.org/abs/1805.09521" rel="nofollow"&gt;AVID: Adversarial Visual Irregularity Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;B-DCGAN - &lt;a href="https://arxiv.org/abs/1803.10930" rel="nofollow"&gt;B-DCGAN:Evaluation of Binarized DCGAN for FPGA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;b-GAN - &lt;a href="https://arxiv.org/abs/1610.02920" rel="nofollow"&gt;Generative Adversarial Nets from a Density Ratio Estimation Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BAGAN - &lt;a href="https://arxiv.org/abs/1803.09655" rel="nofollow"&gt;BAGAN: Data Augmentation with Balancing GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bayesian GAN - &lt;a href="https://arxiv.org/abs/1702.08896" rel="nofollow"&gt;Deep and Hierarchical Implicit Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bayesian GAN - &lt;a href="https://arxiv.org/abs/1705.09558" rel="nofollow"&gt;Bayesian GAN&lt;/a&gt; (&lt;a href="https://github.com/andrewgordonwilson/bayesgan/"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;BCGAN - &lt;a href="https://arxiv.org/abs/1706.05477" rel="nofollow"&gt;Bayesian Conditional Generative Adverserial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BCGAN - &lt;a href="https://arxiv.org/abs/1711.07461" rel="nofollow"&gt;Bidirectional Conditional Generative Adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEAM - &lt;a href="https://arxiv.org/abs/1804.08682" rel="nofollow"&gt;Boltzmann Encoded Adversarial Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEGAN - &lt;a href="https://arxiv.org/abs/1703.10717" rel="nofollow"&gt;BEGAN: Boundary Equilibrium Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEGAN-CS - &lt;a href="https://arxiv.org/abs/1808.07258" rel="nofollow"&gt;Escaping from Collapsing Modes in a Constrained Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bellman GAN - &lt;a href="https://arxiv.org/abs/1808.01960" rel="nofollow"&gt;Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BGAN - &lt;a href="https://arxiv.org/abs/1708.04150" rel="nofollow"&gt;Binary Generative Adversarial Networks for Image Retrieval&lt;/a&gt; (&lt;a href="https://github.com/htconquer/BGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Bi-GAN - &lt;a href="https://arxiv.org/abs/1809.10244" rel="nofollow"&gt;Autonomously and Simultaneously Refining Deep Neural Network Parameters by a Bi-Generative Adversarial Network Aided Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BicycleGAN - &lt;a href="https://arxiv.org/abs/1711.11586" rel="nofollow"&gt;Toward Multimodal Image-to-Image Translation&lt;/a&gt; (&lt;a href="https://github.com/junyanz/BicycleGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;BiGAN - &lt;a href="https://arxiv.org/abs/1605.09782v7" rel="nofollow"&gt;Adversarial Feature Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BinGAN - &lt;a href="https://arxiv.org/abs/1806.06778" rel="nofollow"&gt;BinGAN: Learning Compact Binary Descriptors with a Regularized GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BourGAN - &lt;a href="https://arxiv.org/abs/1805.07674" rel="nofollow"&gt;BourGAN: Generative Networks with Metric Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BranchGAN - &lt;a href="https://arxiv.org/abs/1803.08467" rel="nofollow"&gt;Branched Generative Adversarial Networks for Multi-Scale Image Manifold Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BRE - &lt;a href="https://arxiv.org/abs/1805.03644" rel="nofollow"&gt;Improving GAN Training via Binarized Representation Entropy (BRE) Regularization&lt;/a&gt; (&lt;a href="https://github.com/BorealisAI/bre-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;BridgeGAN - &lt;a href="https://arxiv.org/abs/1808.00327" rel="nofollow"&gt;Generative Adversarial Frontal View to Bird View Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BS-GAN - &lt;a href="https://arxiv.org/abs/1702.08431v1" rel="nofollow"&gt;Boundary-Seeking Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BubGAN - &lt;a href="https://arxiv.org/abs/1809.02266" rel="nofollow"&gt;BubGAN: Bubble Generative Adversarial Networks for Synthesizing Realistic Bubbly Flow Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BWGAN - &lt;a href="https://arxiv.org/abs/1806.06621" rel="nofollow"&gt;Banach Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;C-GAN  - &lt;a href="https://arxiv.org/abs/1802.00237" rel="nofollow"&gt;Face Aging with Contextual Generative Adversarial Nets &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;C-RNN-GAN - &lt;a href="https://arxiv.org/abs/1611.09904" rel="nofollow"&gt;C-RNN-GAN: Continuous recurrent neural networks with adversarial training&lt;/a&gt; (&lt;a href="https://github.com/olofmogren/c-rnn-gan/"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;CA-GAN - &lt;a href="https://arxiv.org/abs/1712.00899" rel="nofollow"&gt;Composition-aided Sketch-realistic Portrait Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CaloGAN - &lt;a href="https://arxiv.org/abs/1705.02355" rel="nofollow"&gt;CaloGAN: Simulating 3D High Energy Particle Showers in Multi-Layer Electromagnetic Calorimeters with Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/hep-lbdl/CaloGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;CAN - &lt;a href="https://arxiv.org/abs/1706.07068" rel="nofollow"&gt;CAN: Creative Adversarial Networks, Generating Art by Learning About Styles and Deviating from Style Norms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CapsGAN - &lt;a href="https://arxiv.org/abs/1806.03968" rel="nofollow"&gt;CapsGAN: Using Dynamic Routing for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CapsuleGAN - &lt;a href="http://arxiv.org/abs/1802.06167" rel="nofollow"&gt;CapsuleGAN: Generative Adversarial Capsule Network &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CatGAN - &lt;a href="https://arxiv.org/abs/1511.06390v2" rel="nofollow"&gt;Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CatGAN - &lt;a href="https://arxiv.org/abs/1711.08904" rel="nofollow"&gt;CatGAN: Coupled Adversarial Transfer for Domain Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CausalGAN - &lt;a href="https://arxiv.org/abs/1709.02023" rel="nofollow"&gt;CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CC-GAN - &lt;a href="https://arxiv.org/abs/1611.06430" rel="nofollow"&gt;Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/edenton/cc-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;cd-GAN - &lt;a href="https://arxiv.org/abs/1805.00251" rel="nofollow"&gt;Conditional Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CDcGAN - &lt;a href="https://arxiv.org/abs/1708.09105" rel="nofollow"&gt;Simultaneously Color-Depth Super-Resolution with Conditional Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CE-GAN - &lt;a href="https://arxiv.org/abs/1807.04585" rel="nofollow"&gt;Deep Learning for Imbalance Data Classification using Class Expert Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CFG-GAN - &lt;a href="https://arxiv.org/abs/1801.06309" rel="nofollow"&gt;Composite Functional Gradient Learning of Generative Adversarial Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CGAN - &lt;a href="https://arxiv.org/abs/1411.1784" rel="nofollow"&gt;Conditional Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CGAN - &lt;a href="https://arxiv.org/abs/1708.00598" rel="nofollow"&gt;Controllable Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chekhov GAN - &lt;a href="https://arxiv.org/abs/1706.03269" rel="nofollow"&gt;An Online Learning Approach to Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ciGAN - &lt;a href="https://arxiv.org/abs/1807.08093" rel="nofollow"&gt;Conditional Infilling GANs for Data Augmentation in Mammogram Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CinCGAN - &lt;a href="https://arxiv.org/abs/1809.00437" rel="nofollow"&gt;Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CipherGAN - &lt;a href="https://arxiv.org/abs/1801.04883" rel="nofollow"&gt;Unsupervised Cipher Cracking Using Discrete GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ClusterGAN - &lt;a href="https://arxiv.org/abs/1809.03627" rel="nofollow"&gt;ClusterGAN : Latent Space Clustering in Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CM-GAN - &lt;a href="https://arxiv.org/abs/1710.05106" rel="nofollow"&gt;CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoAtt-GAN - &lt;a href="https://arxiv.org/abs/1711.07613" rel="nofollow"&gt;Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoGAN - &lt;a href="https://arxiv.org/abs/1606.07536v2" rel="nofollow"&gt;Coupled Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ComboGAN - &lt;a href="https://arxiv.org/abs/1712.06909" rel="nofollow"&gt;ComboGAN: Unrestrained Scalability for Image Domain Translation&lt;/a&gt; (&lt;a href="https://github.com/AAnoosheh/ComboGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ConceptGAN - &lt;a href="https://arxiv.org/abs/1711.06148" rel="nofollow"&gt;Learning Compositional Visual Concepts with Mutual Consistency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional cycleGAN - &lt;a href="https://arxiv.org/abs/1705.09966" rel="nofollow"&gt;Conditional CycleGAN for Attribute Guided Face Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;constrast-GAN - &lt;a href="https://arxiv.org/abs/1708.00315" rel="nofollow"&gt;Generative Semantic Manipulation with Contrasting GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Context-RNN-GAN - &lt;a href="https://arxiv.org/abs/1609.09444" rel="nofollow"&gt;Contextual RNN-GANs for Abstract Reasoning Diagram Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CorrGAN - &lt;a href="https://arxiv.org/abs/1804.00925" rel="nofollow"&gt;Correlated discrete data generation using adversarial training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Coulomb GAN - &lt;a href="https://arxiv.org/abs/1708.08819" rel="nofollow"&gt;Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cover-GAN - &lt;a href="https://arxiv.org/abs/1711.04916" rel="nofollow"&gt;Generative Steganography with Kerckhoffs' Principle based on Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cowboy - &lt;a href="https://arxiv.org/abs/1805.10652" rel="nofollow"&gt;Defending Against Adversarial Attacks by Leveraging an Entire GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CR-GAN - &lt;a href="https://arxiv.org/abs/1806.11191" rel="nofollow"&gt;CR-GAN: Learning Complete Representations for Multi-view Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cramr GAN  - &lt;a href="https://arxiv.org/abs/1705.10743" rel="nofollow"&gt;The Cramer Distance as a Solution to Biased Wasserstein Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cross-GAN - &lt;a href="https://arxiv.org/abs/1801.01760" rel="nofollow"&gt;Crossing Generative Adversarial Networks for Cross-View Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;crVAE-GAN - &lt;a href="https://arxiv.org/abs/1706.03729" rel="nofollow"&gt;Channel-Recurrent Variational Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CS-GAN - &lt;a href="https://arxiv.org/abs/1703.04887" rel="nofollow"&gt;Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CSG - &lt;a href="https://arxiv.org/abs/1806.00154" rel="nofollow"&gt;Speech-Driven Expressive Talking Lips with Conditional Sequential Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CT-GAN - &lt;a href="https://arxiv.org/abs/1807.04812" rel="nofollow"&gt;CT-GAN: Conditional Transformation Generative Adversarial Network for Image Attribute Modification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CVAE-GAN - &lt;a href="https://arxiv.org/abs/1703.10155" rel="nofollow"&gt;CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CycleGAN - &lt;a href="https://arxiv.org/abs/1703.10593" rel="nofollow"&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/junyanz/CycleGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;D-GAN - &lt;a href="https://arxiv.org/abs/1711.10267" rel="nofollow"&gt;Differential Generative Adversarial Networks: Synthesizing Non-linear Facial Variations with Limited Number of Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D-WCGAN - &lt;a href="https://arxiv.org/abs/1804.00290" rel="nofollow"&gt;I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D2GAN - &lt;a href="http://arxiv.org/abs/1709.03831" rel="nofollow"&gt;Dual Discriminator Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D2IA-GAN - &lt;a href="https://arxiv.org/abs/1804.00113" rel="nofollow"&gt;Tagging like Humans: Diverse and Distinct Image Annotation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DA-GAN  - &lt;a href="http://arxiv.org/abs/1802.06454" rel="nofollow"&gt;DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks (with Supplementary Materials)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DADA - &lt;a href="https://arxiv.org/abs/1809.00981" rel="nofollow"&gt;DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DAGAN - &lt;a href="https://arxiv.org/abs/1711.04340" rel="nofollow"&gt;Data Augmentation Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DAN - &lt;a href="https://arxiv.org/abs/1706.09549" rel="nofollow"&gt;Distributional Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DBLRGAN - &lt;a href="https://arxiv.org/abs/1804.00533" rel="nofollow"&gt;Adversarial Spatio-Temporal Learning for Video Deblurring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DCGAN - &lt;a href="https://arxiv.org/abs/1511.06434" rel="nofollow"&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/Newmu/dcgan_code"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;DE-GAN - &lt;a href="https://arxiv.org/abs/1807.03923" rel="nofollow"&gt;Generative Adversarial Networks with Decoder-Encoder Output Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DeblurGAN - &lt;a href="https://arxiv.org/abs/1711.07064" rel="nofollow"&gt;DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/KupynOrest/DeblurGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;DeepFD - &lt;a href="https://arxiv.org/abs/1809.08754" rel="nofollow"&gt;Learning to Detect Fake Face Images in the Wild&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Defense-GAN - &lt;a href="https://arxiv.org/abs/1805.06605" rel="nofollow"&gt;Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models &lt;/a&gt; (&lt;a href="https://github.com/kabkabm/defensegan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Defo-Net - &lt;a href="https://arxiv.org/abs/1804.05928" rel="nofollow"&gt;Defo-Net: Learning Body Deformation using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DeliGAN - &lt;a href="https://arxiv.org/abs/1706.02071" rel="nofollow"&gt;DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data&lt;/a&gt; (&lt;a href="https://github.com/val-iisc/deligan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;DF-GAN - &lt;a href="https://arxiv.org/abs/1712.04646" rel="nofollow"&gt;Learning Disentangling and Fusing Networks for Face Completion Under Structured Occlusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DialogWAE - &lt;a href="https://arxiv.org/abs/1805.12352" rel="nofollow"&gt;DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DiscoGAN - &lt;a href="https://arxiv.org/abs/1703.05192v1" rel="nofollow"&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DistanceGAN - &lt;a href="https://arxiv.org/abs/1706.00826" rel="nofollow"&gt;One-Sided Unsupervised Domain Mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DM-GAN - &lt;a href="https://arxiv.org/abs/1708.00284" rel="nofollow"&gt;Dual Motion GAN for Future-Flow Embedded Video Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DMGAN - &lt;a href="https://arxiv.org/abs/1806.00880" rel="nofollow"&gt;Disconnected Manifold Learning for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DNA-GAN - &lt;a href="https://arxiv.org/abs/1711.05415" rel="nofollow"&gt;DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DOPING - &lt;a href="https://arxiv.org/abs/1808.07632" rel="nofollow"&gt;DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;dp-GAN - &lt;a href="https://arxiv.org/abs/1801.01594" rel="nofollow"&gt;Differentially Private Releasing via Deep Generative Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DP-GAN - &lt;a href="https://arxiv.org/abs/1802.01345" rel="nofollow"&gt;DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DPGAN  - &lt;a href="http://arxiv.org/abs/1802.06739" rel="nofollow"&gt;Differentially Private Generative Adversarial Network &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DR-GAN - &lt;a href="https://arxiv.org/abs/1705.11136" rel="nofollow"&gt;Representation Learning by Rotating Your Faces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DRAGAN - &lt;a href="https://arxiv.org/abs/1705.07215" rel="nofollow"&gt;How to Train Your DRAGAN&lt;/a&gt; (&lt;a href="https://github.com/kodalinaveen3/DRAGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Dropout-GAN - &lt;a href="https://arxiv.org/abs/1807.11346" rel="nofollow"&gt;Dropout-GAN: Learning from a Dynamic Ensemble of Discriminators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DRPAN - &lt;a href="https://arxiv.org/abs/1711.09554" rel="nofollow"&gt;Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DSH-GAN - &lt;a href="https://arxiv.org/abs/1804.08275" rel="nofollow"&gt;Deep Semantic Hashing with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DSP-GAN - &lt;a href="https://arxiv.org/abs/1706.00212" rel="nofollow"&gt;Depth Structure Preserving Scene Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DTLC-GAN - &lt;a href="https://arxiv.org/abs/1805.10603" rel="nofollow"&gt;Generative Adversarial Image Synthesis with Decision Tree Latent Controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DTN - &lt;a href="https://arxiv.org/abs/1611.02200" rel="nofollow"&gt;Unsupervised Cross-Domain Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DTR-GAN - &lt;a href="https://arxiv.org/abs/1804.11228" rel="nofollow"&gt;DTR-GAN: Dilated Temporal Relational Adversarial Network for Video Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DualGAN - &lt;a href="https://arxiv.org/abs/1704.02510v1" rel="nofollow"&gt;DualGAN: Unsupervised Dual Learning for Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dualing GAN - &lt;a href="https://arxiv.org/abs/1706.06216" rel="nofollow"&gt;Dualing GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DVGAN - &lt;a href="https://arxiv.org/abs/1804.10652" rel="nofollow"&gt;Human Motion Modeling using DVGANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dynamics Transfer GAN - &lt;a href="https://arxiv.org/abs/1712.03534" rel="nofollow"&gt;Dynamics Transfer GAN: Generating Video by Transferring Arbitrary Temporal Dynamics from a Source Video to a Single Target Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;E-GAN - &lt;a href="https://arxiv.org/abs/1803.00657" rel="nofollow"&gt;Evolutionary Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EAR - &lt;a href="https://arxiv.org/abs/1804.09858" rel="nofollow"&gt;Generative Model for Heterogeneous Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EBGAN - &lt;a href="https://arxiv.org/abs/1609.03126v4" rel="nofollow"&gt;Energy-based Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ecGAN - &lt;a href="https://arxiv.org/abs/1801.03244" rel="nofollow"&gt;eCommerceGAN : A Generative Adversarial Network for E-commerce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ED//GAN - &lt;a href="https://arxiv.org/abs/1705.09367" rel="nofollow"&gt;Stabilizing Training of Generative Adversarial Networks through Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Editable GAN - &lt;a href="https://arxiv.org/abs/1807.07700" rel="nofollow"&gt;Editable Generative Adversarial Networks: Generating and Editing Faces Simultaneously&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EGAN - &lt;a href="https://arxiv.org/abs/1705.08245" rel="nofollow"&gt;Enhanced Experience Replay Generation for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EL-GAN - &lt;a href="https://arxiv.org/abs/1806.05525" rel="nofollow"&gt;EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ELEGANT - &lt;a href="https://arxiv.org/abs/1803.10562" rel="nofollow"&gt;ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EnergyWGAN - &lt;a href="https://arxiv.org/abs/1712.01026" rel="nofollow"&gt;Energy-relaxed Wassertein GANs (EnergyWGAN): Towards More Stable and High Resolution Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ESRGAN - &lt;a href="https://arxiv.org/abs/1809.00219" rel="nofollow"&gt;ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ExGAN - &lt;a href="https://arxiv.org/abs/1712.03999" rel="nofollow"&gt;Eye In-Painting with Exemplar Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ExposureGAN - &lt;a href="https://arxiv.org/abs/1709.09602" rel="nofollow"&gt;Exposure: A White-Box Photo Post-Processing Framework&lt;/a&gt; (&lt;a href="https://github.com/yuanming-hu/exposure"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ExprGAN - &lt;a href="https://arxiv.org/abs/1709.03842" rel="nofollow"&gt;ExprGAN: Facial Expression Editing with Controllable Expression Intensity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;f-CLSWGAN - &lt;a href="https://arxiv.org/abs/1712.00981" rel="nofollow"&gt;Feature Generating Networks for Zero-Shot Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;f-GAN - &lt;a href="https://arxiv.org/abs/1606.00709" rel="nofollow"&gt;f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FairGAN - &lt;a href="https://arxiv.org/abs/1805.11202" rel="nofollow"&gt;FairGAN: Fairness-aware Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fairness GAN - &lt;a href="https://arxiv.org/abs/1805.09910" rel="nofollow"&gt;Fairness GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FakeGAN - &lt;a href="https://arxiv.org/abs/1805.10364" rel="nofollow"&gt;Detecting Deceptive Reviews using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FBGAN - &lt;a href="https://arxiv.org/abs/1804.01694" rel="nofollow"&gt;Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for Optimizing Protein Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FBGAN - &lt;a href="https://arxiv.org/abs/1805.07862" rel="nofollow"&gt;Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FC-GAN - &lt;a href="https://arxiv.org/abs/1805.01972" rel="nofollow"&gt;Fast-converging Conditional Generative Adversarial Networks for Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FF-GAN - &lt;a href="https://arxiv.org/abs/1704.06244" rel="nofollow"&gt;Towards Large-Pose Face Frontalization in the Wild&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FGGAN - &lt;a href="https://arxiv.org/abs/1807.02247" rel="nofollow"&gt;Adversarial Learning for Fine-grained Image Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fictitious GAN - &lt;a href="https://arxiv.org/abs/1803.08647" rel="nofollow"&gt;Fictitious GAN: Training GANs with Historical Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FIGAN - &lt;a href="https://arxiv.org/abs/1711.06045" rel="nofollow"&gt;Frame Interpolation with Multi-Scale Deep Loss Functions and Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fila-GAN - &lt;a href="https://arxiv.org/abs/1706.02185" rel="nofollow"&gt;Synthesizing Filamentary Structured Images with GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;First Order GAN  - &lt;a href="https://arxiv.org/abs/1802.04591" rel="nofollow"&gt;First Order Generative Adversarial Networks &lt;/a&gt; (&lt;a href="https://github.com/zalandoresearch/first_order_gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Fisher GAN - &lt;a href="https://arxiv.org/abs/1705.09675" rel="nofollow"&gt;Fisher GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Flow-GAN - &lt;a href="https://arxiv.org/abs/1705.08868" rel="nofollow"&gt;Flow-GAN: Bridging implicit and prescribed learning in generative models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FrankenGAN - &lt;a href="https://arxiv.org/abs/1806.07179" rel="nofollow"&gt;rankenGAN: Guided Detail Synthesis for Building Mass-Models Using Style-Synchonized GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FSEGAN - &lt;a href="https://arxiv.org/abs/1711.05747" rel="nofollow"&gt;Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FTGAN - &lt;a href="https://arxiv.org/abs/1711.09618" rel="nofollow"&gt;Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FusedGAN - &lt;a href="https://arxiv.org/abs/1801.05551" rel="nofollow"&gt;Semi-supervised FusedGAN for Conditional Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FusionGAN - &lt;a href="https://arxiv.org/abs/1712.01456" rel="nofollow"&gt;Learning to Fuse Music Genres with Generative Adversarial Dual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FusionGAN - &lt;a href="https://arxiv.org/abs/1804.07455" rel="nofollow"&gt;Generating a Fusion Image: One's Identity and Another's Shape&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;G2-GAN - &lt;a href="https://arxiv.org/abs/1712.03474" rel="nofollow"&gt;Geometry Guided Adversarial Facial Expression Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAAN - &lt;a href="https://arxiv.org/abs/1803.08887" rel="nofollow"&gt;Generative Adversarial Autoencoder Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAF - &lt;a href="https://arxiv.org/abs/1805.05185" rel="nofollow"&gt;Generative Adversarial Forests for Better Conditioned Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAGAN - &lt;a href="https://arxiv.org/abs/1712.00684" rel="nofollow"&gt;GAGAN: Geometry-Aware Generative Adverserial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAIA - &lt;a href="https://arxiv.org/abs/1807.06650" rel="nofollow"&gt;Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAIN  - &lt;a href="https://arxiv.org/abs/1806.02920" rel="nofollow"&gt;GAIN: Missing Data Imputation using Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAMN - &lt;a href="https://arxiv.org/abs/1709.09820" rel="nofollow"&gt;Generative Adversarial Mapping Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN - &lt;a href="https://arxiv.org/abs/1406.2661" rel="nofollow"&gt;Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/goodfeli/adversarial"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GAN Lab - &lt;a href="https://arxiv.org/abs/1809.01587" rel="nofollow"&gt;GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN Q-learning - &lt;a href="https://arxiv.org/abs/1805.04874" rel="nofollow"&gt;GAN Q-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-AD - &lt;a href="https://arxiv.org/abs/1809.04758" rel="nofollow"&gt;Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-ATV - &lt;a href="https://arxiv.org/abs/1710.10553" rel="nofollow"&gt;A Novel Approach to Artistic Textual Visualization via GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-CLS - &lt;a href="https://arxiv.org/abs/1605.05396" rel="nofollow"&gt;Generative Adversarial Text to Image Synthesis&lt;/a&gt; (&lt;a href="https://github.com/reedscot/icml2016"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GAN-RS - &lt;a href="https://arxiv.org/abs/1712.00736" rel="nofollow"&gt;Towards Qualitative Advancement of Underwater Machine Vision with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-SD - &lt;a href="https://arxiv.org/abs/1805.10000" rel="nofollow"&gt;Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-sep - &lt;a href="https://arxiv.org/abs/1708.04692" rel="nofollow"&gt;GANs for Biological Image Synthesis&lt;/a&gt; (&lt;a href="https://github.com/aosokin/biogans"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GAN-VFS - &lt;a href="https://arxiv.org/abs/1708.02681" rel="nofollow"&gt;Generative Adversarial Network-based Synthesis of Visible Faces from Polarimetric Thermal Faces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-Word2Vec - &lt;a href="https://arxiv.org/abs/1805.08720" rel="nofollow"&gt;Adversarial Training of Word2Vec for Basket Completion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANAX - &lt;a href="https://arxiv.org/abs/1806.01107" rel="nofollow"&gt;GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANCS - &lt;a href="https://arxiv.org/abs/1706.00051" rel="nofollow"&gt;Deep Generative Adversarial Networks for Compressed Sensing Automates MRI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANDI - &lt;a href="https://arxiv.org/abs/1711.01391" rel="nofollow"&gt;Guiding the search in continuous state-action spaces by learning an action sampling distribution from off-target samples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANG - &lt;a href="https://arxiv.org/abs/1712.00679" rel="nofollow"&gt;GANGs: Generative Adversarial Network Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANG - &lt;a href="https://arxiv.org/abs/1806.07268" rel="nofollow"&gt;Beyond Local Nash Equilibria for Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANosaic - &lt;a href="https://arxiv.org/abs/1712.00269" rel="nofollow"&gt;GANosaic: Mosaic Creation with Generative Texture Manifolds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANVO - &lt;a href="https://arxiv.org/abs/1809.05786" rel="nofollow"&gt;GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAP - &lt;a href="https://arxiv.org/abs/1710.09549" rel="nofollow"&gt;Context-Aware Generative Adversarial Privacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAP - &lt;a href="https://arxiv.org/abs/1807.05306" rel="nofollow"&gt;Generative Adversarial Privacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GATS - &lt;a href="https://arxiv.org/abs/1806.05780" rel="nofollow"&gt;Sample-Efficient Deep RL with Generative Adversarial Tree Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAWWN - &lt;a href="https://arxiv.org/abs/1610.02454" rel="nofollow"&gt;Learning What and Where to Draw&lt;/a&gt; (&lt;a href="https://github.com/reedscot/nips2016"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GC-GAN - &lt;a href="https://arxiv.org/abs/1802.01822" rel="nofollow"&gt;Geometry-Contrastive Generative Adversarial Network for Facial Expression Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GcGAN - &lt;a href="https://arxiv.org/abs/1809.05852" rel="nofollow"&gt;Geometry-Consistent Adversarial Networks for One-Sided Unsupervised Domain Mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GeneGAN - &lt;a href="https://arxiv.org/abs/1705.04932" rel="nofollow"&gt;GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data&lt;/a&gt; (&lt;a href="https://github.com/Prinsphield/GeneGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GeoGAN - &lt;a href="https://arxiv.org/abs/1801.08839" rel="nofollow"&gt;Generating Instance Segmentation Annotation by Geometry-guided GAN &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Geometric GAN - &lt;a href="https://arxiv.org/abs/1705.02894" rel="nofollow"&gt;Geometric GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GIN - &lt;a href="https://arxiv.org/abs/1808.04495" rel="nofollow"&gt;Generative Invertible Networks (GIN): Pathophysiology-Interpretable Feature Mapping and Virtual Patient Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GLCA-GAN - &lt;a href="https://arxiv.org/abs/1801.08390" rel="nofollow"&gt;Global and Local Consistent Age Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GM-GAN - &lt;a href="https://arxiv.org/abs/1808.10356" rel="nofollow"&gt;Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GMAN - &lt;a href="http://arxiv.org/abs/1611.01673" rel="nofollow"&gt;Generative Multi-Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GMM-GAN - &lt;a href="https://arxiv.org/abs/1706.09884" rel="nofollow"&gt;Towards Understanding the Dynamics of Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GoGAN - &lt;a href="https://arxiv.org/abs/1704.04865" rel="nofollow"&gt;Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GONet - &lt;a href="https://arxiv.org/abs/1803.03254" rel="nofollow"&gt;GONet: A Semi-Supervised Deep Learning Approach For Traversability Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GP-GAN - &lt;a href="https://arxiv.org/abs/1703.07195" rel="nofollow"&gt;GP-GAN: Towards Realistic High-Resolution Image Blending&lt;/a&gt; (&lt;a href="https://github.com/wuhuikai/GP-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GP-GAN - &lt;a href="https://arxiv.org/abs/1710.00962" rel="nofollow"&gt;GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GPU - &lt;a href="https://arxiv.org/abs/1711.08054" rel="nofollow"&gt;A generative adversarial framework for positive-unlabeled classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GRAN - &lt;a href="https://arxiv.org/abs/1602.05110" rel="nofollow"&gt;Generating images with recurrent adversarial networks&lt;/a&gt; (&lt;a href="https://github.com/jiwoongim/GRAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Graphical-GAN - &lt;a href="https://arxiv.org/abs/1804.03429" rel="nofollow"&gt;Graphical Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GraphSGAN - &lt;a href="https://arxiv.org/abs/1809.00130" rel="nofollow"&gt;Semi-supervised Learning on Graphs with Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GraspGAN - &lt;a href="https://arxiv.org/abs/1709.07857" rel="nofollow"&gt;Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GT-GAN - &lt;a href="https://arxiv.org/abs/1805.09980" rel="nofollow"&gt;Deep Graph Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HAN - &lt;a href="https://arxiv.org/abs/1711.06448" rel="nofollow"&gt;Chinese Typeface Transformation with Hierarchical Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HAN - &lt;a href="https://arxiv.org/abs/1805.08006" rel="nofollow"&gt;Bidirectional Learning for Robust Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HiGAN - &lt;a href="https://arxiv.org/abs/1805.04384" rel="nofollow"&gt;Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HP-GAN - &lt;a href="https://arxiv.org/abs/1711.09561" rel="nofollow"&gt;HP-GAN: Probabilistic 3D human motion prediction via GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HR-DCGAN - &lt;a href="https://arxiv.org/abs/1711.06491" rel="nofollow"&gt;High-Resolution Deep Convolutional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hredGAN - &lt;a href="https://arxiv.org/abs/1805.11752" rel="nofollow"&gt;Multi-turn Dialogue Response Generation in an Adversarial Learning framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IAN - &lt;a href="https://arxiv.org/abs/1609.07093" rel="nofollow"&gt;Neural Photo Editing with Introspective Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/ajbrock/Neural-Photo-Editor"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IcGAN - &lt;a href="https://arxiv.org/abs/1611.06355" rel="nofollow"&gt;Invertible Conditional GANs for image editing&lt;/a&gt; (&lt;a href="https://github.com/Guim3/IcGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ID-CGAN - &lt;a href="https://arxiv.org/abs/1701.05957v3" rel="nofollow"&gt;Image De-raining Using a Conditional Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IdCycleGAN - &lt;a href="https://arxiv.org/abs/1712.00971" rel="nofollow"&gt;Face Translation between Images and Videos using Identity-aware CycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IFcVAEGAN - &lt;a href="https://arxiv.org/abs/1711.05175" rel="nofollow"&gt;Conditional Autoencoders with Adversarial Information Factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;iGAN - &lt;a href="https://arxiv.org/abs/1609.03552v2" rel="nofollow"&gt;Generative Visual Manipulation on the Natural Image Manifold&lt;/a&gt; (&lt;a href="https://github.com/junyanz/iGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IGMM-GAN - &lt;a href="https://arxiv.org/abs/1809.02728" rel="nofollow"&gt;Coupled IGMM-GANs for deep multimodal anomaly detection in human mobility data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved GAN - &lt;a href="https://arxiv.org/abs/1606.03498" rel="nofollow"&gt;Improved Techniques for Training GANs&lt;/a&gt; (&lt;a href="https://github.com/openai/improved-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;In2I - &lt;a href="https://arxiv.org/abs/1711.09334" rel="nofollow"&gt;In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;InfoGAN - &lt;a href="https://arxiv.org/abs/1606.03657v1" rel="nofollow"&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/a&gt; (&lt;a href="https://github.com/openai/InfoGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IntroVAE - &lt;a href="https://arxiv.org/abs/1807.06358" rel="nofollow"&gt;IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IR2VI - &lt;a href="https://arxiv.org/abs/1806.09565" rel="nofollow"&gt;IR2VI: Enhanced Night Environmental Perception by Unsupervised Thermal Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRGAN - &lt;a href="https://arxiv.org/abs/1705.10513v1" rel="nofollow"&gt;IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRGAN - &lt;a href="https://arxiv.org/abs/1806.03577" rel="nofollow"&gt;Generative Adversarial Nets for Information Retrieval: Fundamentals and Advances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ISGAN - &lt;a href="https://arxiv.org/abs/1807.08571" rel="nofollow"&gt;Invisible Steganography via Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ISP-GPM - &lt;a href="https://arxiv.org/abs/1808.02104" rel="nofollow"&gt;Inner Space Preserving Generative Pose Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Iterative-GAN - &lt;a href="https://arxiv.org/abs/1711.06078" rel="nofollow"&gt;Two Birds with One Stone: Iteratively Learn Facial Attributes with GANs&lt;/a&gt; (&lt;a href="https://github.com/punkcure/Iterative-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IterGAN - &lt;a href="https://arxiv.org/abs/1804.05651" rel="nofollow"&gt;IterGANs: Iterative GANs to Learn and Control 3D Object Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IVE-GAN - &lt;a href="https://arxiv.org/abs/1711.08646" rel="nofollow"&gt;IVE-GAN: Invariant Encoding Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;iVGAN - &lt;a href="https://arxiv.org/abs/1711.11453" rel="nofollow"&gt;Towards an Understanding of Our World by GANing Videos in the Wild&lt;/a&gt; (&lt;a href="https://github.com/bernhard2202/improved-video-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IWGAN - &lt;a href="https://arxiv.org/abs/1706.00550" rel="nofollow"&gt;On Unifying Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;JointGAN - &lt;a href="https://arxiv.org/abs/1806.02978" rel="nofollow"&gt;JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;JR-GAN - &lt;a href="https://arxiv.org/abs/1806.09235" rel="nofollow"&gt;JR-GAN: Jacobian Regularization for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KBGAN - &lt;a href="https://arxiv.org/abs/1711.04071" rel="nofollow"&gt;KBGAN: Adversarial Learning for Knowledge Graph Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KGAN - &lt;a href="https://arxiv.org/abs/1711.01744" rel="nofollow"&gt;KGAN: How to Break The Minimax Game in GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;l-GAN - &lt;a href="https://arxiv.org/abs/1707.02392" rel="nofollow"&gt;Representation Learning and Adversarial Generation of 3D Point Clouds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LAC-GAN - &lt;a href="https://arxiv.org/abs/1801.05096" rel="nofollow"&gt;Grounded Language Understanding for Manipulation Instructions Using GAN-Based Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LAGAN - &lt;a href="https://arxiv.org/abs/1701.05927" rel="nofollow"&gt;Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LAPGAN - &lt;a href="https://arxiv.org/abs/1506.05751" rel="nofollow"&gt;Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/facebook/eyescream"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;LB-GAN - &lt;a href="http://arxiv.org/abs/1802.07447" rel="nofollow"&gt;Load Balanced GANs for Multi-view Face Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LBT - &lt;a href="https://arxiv.org/abs/1807.03870" rel="nofollow"&gt;Learning Implicit Generative Models by Teaching Explicit Ones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LCC-GAN - &lt;a href="https://arxiv.org/abs/1806.04895" rel="nofollow"&gt;Adversarial Learning with Local Coordinate Coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LD-GAN - &lt;a href="https://arxiv.org/abs/1707.07831" rel="nofollow"&gt;Linear Discriminant Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LDAN - &lt;a href="https://arxiv.org/abs/1709.01993" rel="nofollow"&gt;Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LeakGAN - &lt;a href="https://arxiv.org/abs/1709.08624" rel="nofollow"&gt;Long Text Generation via Adversarial Training with Leaked Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LeGAN - &lt;a href="https://arxiv.org/abs/1707.07530" rel="nofollow"&gt;Likelihood Estimation for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LGAN - &lt;a href="https://arxiv.org/abs/1711.06020" rel="nofollow"&gt;Global versus Localized Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lipizzaner - &lt;a href="https://arxiv.org/abs/1807.08194" rel="nofollow"&gt;Towards Distributed Coevolutionary GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LR-GAN - &lt;a href="https://arxiv.org/abs/1703.01560v1" rel="nofollow"&gt;LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LS-GAN - &lt;a href="https://arxiv.org/abs/1701.06264" rel="nofollow"&gt;Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSGAN - &lt;a href="https://arxiv.org/abs/1611.04076v3" rel="nofollow"&gt;Least Squares Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;M-AAE - &lt;a href="https://arxiv.org/abs/1804.08882" rel="nofollow"&gt;Mask-aware Photorealistic Face Attribute Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MAD-GAN - &lt;a href="https://arxiv.org/abs/1704.02906" rel="nofollow"&gt;Multi-Agent Diverse Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MAGAN - &lt;a href="https://arxiv.org/abs/1704.03817v1" rel="nofollow"&gt;MAGAN: Margin Adaptation for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MAGAN - &lt;a href="https://arxiv.org/abs/1803.00385" rel="nofollow"&gt;MAGAN: Aligning Biological Manifolds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MalGAN - &lt;a href="https://arxiv.org/abs/1702.05983v1" rel="nofollow"&gt;Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MaliGAN - &lt;a href="https://arxiv.org/abs/1702.07983" rel="nofollow"&gt;Maximum-Likelihood Augmented Discrete Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;manifold-WGAN - &lt;a href="https://arxiv.org/abs/1712.01551" rel="nofollow"&gt;Manifold-valued Image Generation with Wasserstein Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MARTA-GAN - &lt;a href="https://arxiv.org/abs/1612.08879" rel="nofollow"&gt;Deep Unsupervised Representation Learning for Remote Sensing Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MaskGAN - &lt;a href="https://arxiv.org/abs/1801.07736" rel="nofollow"&gt;MaskGAN: Better Text Generation via Filling in the ______ &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MC-GAN - &lt;a href="https://arxiv.org/abs/1712.00516" rel="nofollow"&gt;Multi-Content GAN for Few-Shot Font Style Transfer&lt;/a&gt; (&lt;a href="https://github.com/azadis/MC-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;MC-GAN - &lt;a href="https://arxiv.org/abs/1805.01123" rel="nofollow"&gt;MC-GAN: Multi-conditional Generative Adversarial Network for Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;McGAN - &lt;a href="https://arxiv.org/abs/1702.08398v1" rel="nofollow"&gt;McGan: Mean and Covariance Feature Matching GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MD-GAN - &lt;a href="https://arxiv.org/abs/1709.07592" rel="nofollow"&gt;Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MDGAN - &lt;a href="https://arxiv.org/abs/1612.02136" rel="nofollow"&gt;Mode Regularized Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MedGAN - &lt;a href="https://arxiv.org/abs/1703.06490v1" rel="nofollow"&gt;Generating Multi-label Discrete Electronic Health Records using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MedGAN - &lt;a href="https://arxiv.org/abs/1806.06397" rel="nofollow"&gt;MedGAN: Medical Image Translation using GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MEGAN - &lt;a href="https://arxiv.org/abs/1805.02481" rel="nofollow"&gt;MEGAN: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MelanoGAN - &lt;a href="https://arxiv.org/abs/1804.04338" rel="nofollow"&gt;MelanoGANs: High Resolution Skin Lesion Synthesis with GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;memoryGAN - &lt;a href="https://arxiv.org/abs/1803.01500" rel="nofollow"&gt;Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MeRGAN - &lt;a href="https://arxiv.org/abs/1809.02058" rel="nofollow"&gt;Memory Replay GANs: learning to generate images from new categories without forgetting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MGAN - &lt;a href="https://arxiv.org/abs/1604.04382" rel="nofollow"&gt;Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/chuanli11/MGANs"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;MGGAN - &lt;a href="https://arxiv.org/abs/1708.02556" rel="nofollow"&gt;Multi-Generator Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MGGAN - &lt;a href="https://arxiv.org/abs/1804.04391" rel="nofollow"&gt;MGGAN: Solving Mode Collapse using Manifold Guided Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MIL-GAN - &lt;a href="https://arxiv.org/abs/1712.01455" rel="nofollow"&gt;Multimodal Storytelling via Generative Adversarial Imitation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MinLGAN - &lt;a href="https://arxiv.org/abs/1808.00200" rel="nofollow"&gt;Anomaly Detection via Minimum Likelihood Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MIX+GAN - &lt;a href="https://arxiv.org/abs/1703.00573v3" rel="nofollow"&gt;Generalization and Equilibrium in Generative Adversarial Nets (GANs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MIXGAN - &lt;a href="https://arxiv.org/abs/1807.01659" rel="nofollow"&gt;MIXGAN: Learning Concepts from Different Domains for Mixture Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MLGAN - &lt;a href="https://arxiv.org/abs/1711.02792" rel="nofollow"&gt;Metric Learning-based Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MMC-GAN - &lt;a href="https://arxiv.org/abs/1806.03847" rel="nofollow"&gt;A Multimodal Classifier Generative Adversarial Network for Carry and Place Tasks from Ambiguous Language Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MMD-GAN - &lt;a href="https://arxiv.org/abs/1705.08584" rel="nofollow"&gt;MMD GAN: Towards Deeper Understanding of Moment Matching Network&lt;/a&gt; (&lt;a href="https://github.com/dougalsutherland/opt-mmd"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;MMGAN - &lt;a href="https://arxiv.org/abs/1707.08273" rel="nofollow"&gt;MMGAN: Manifold Matching Generative Adversarial Network for Generating Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MoCoGAN - &lt;a href="https://arxiv.org/abs/1707.04993" rel="nofollow"&gt;MoCoGAN: Decomposing Motion and Content for Video Generation&lt;/a&gt; (&lt;a href="https://github.com/sergeytulyakov/mocogan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Modified GAN-CLS - &lt;a href="https://arxiv.org/abs/1806.11302" rel="nofollow"&gt;Generate the corresponding Image from Text Description using Modified GAN-CLS Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ModularGAN - &lt;a href="https://arxiv.org/abs/1804.03343" rel="nofollow"&gt;Modular Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MolGAN - &lt;a href="https://arxiv.org/abs/1805.11973" rel="nofollow"&gt;MolGAN: An implicit generative model for small molecular graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MPM-GAN - &lt;a href="https://arxiv.org/abs/1612.01294" rel="nofollow"&gt;Message Passing Multi-Agent GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MS-GAN - &lt;a href="http://papers.nips.cc/paper/7014-temporal-coherency-based-criteria-for-predicting-video-frames-using-deep-multi-stage-generative-adversarial-networks" rel="nofollow"&gt;Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MTGAN - &lt;a href="https://arxiv.org/abs/1803.09059" rel="nofollow"&gt;MTGAN: Speaker Verification through Multitasking Triplet Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MuseGAN - &lt;a href="https://arxiv.org/abs/1709.06298" rel="nofollow"&gt;MuseGAN: Symbolic-domain Music Generation and Accompaniment with Multi-track Sequential Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MV-BiGAN - &lt;a href="https://arxiv.org/abs/1611.02019v1" rel="nofollow"&gt;Multi-view Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;N2RPP - &lt;a href="https://arxiv.org/abs/1805.02825" rel="nofollow"&gt;N2RPP: An Adversarial Network to Rebuild Plantar Pressure for ACLD Patients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NAN - &lt;a href="https://arxiv.org/abs/1804.03287" rel="nofollow"&gt;Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NCE-GAN - &lt;a href="https://arxiv.org/abs/1803.10996" rel="nofollow"&gt;Dihedral angle prediction using generative adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ND-GAN - &lt;a href="https://arxiv.org/abs/1802.10560" rel="nofollow"&gt;Novelty Detection with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NetGAN - &lt;a href="https://arxiv.org/abs/1803.00816" rel="nofollow"&gt;NetGAN: Generating Graphs via Random Walks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OCAN - &lt;a href="https://arxiv.org/abs/1803.01798" rel="nofollow"&gt;One-Class Adversarial Nets for Fraud Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OptionGAN - &lt;a href="https://arxiv.org/abs/1709.06683" rel="nofollow"&gt;OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ORGAN - &lt;a href="https://arxiv.org/abs/1705.10843" rel="nofollow"&gt;Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ORGAN - &lt;a href="https://arxiv.org/abs/1711.06363" rel="nofollow"&gt;3D Reconstruction of Incomplete Archaeological Objects Using a Generative Adversary Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OT-GAN - &lt;a href="https://arxiv.org/abs/1803.05573" rel="nofollow"&gt;Improving GANs Using Optimal Transport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PacGAN - &lt;a href="https://arxiv.org/abs/1712.04086" rel="nofollow"&gt;PacGAN: The power of two samples in generative adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PAN - &lt;a href="https://arxiv.org/abs/1706.09138" rel="nofollow"&gt;Perceptual Adversarial Networks for Image-to-Image Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PassGAN - &lt;a href="https://arxiv.org/abs/1709.00440" rel="nofollow"&gt;PassGAN: A Deep Learning Approach for Password Guessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PD-WGAN - &lt;a href="https://arxiv.org/abs/1805.09575" rel="nofollow"&gt;Primal-Dual Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceptual GAN - &lt;a href="https://arxiv.org/abs/1706.05274" rel="nofollow"&gt;Perceptual Generative Adversarial Networks for Small Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PGAN - &lt;a href="https://arxiv.org/abs/1708.01886" rel="nofollow"&gt;Probabilistic Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PGD-GAN - &lt;a href="https://arxiv.org/abs/1802.08406" rel="nofollow"&gt;Solving Linear Inverse Problems Using GAN Priors: An Algorithm with Provable Guarantees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PGGAN - &lt;a href="https://arxiv.org/abs/1803.07422" rel="nofollow"&gt;Patch-Based Image Inpainting with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PIONEER - &lt;a href="https://arxiv.org/abs/1807.03026" rel="nofollow"&gt;Pioneer Networks: Progressively Growing Generative Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pip-GAN - &lt;a href="https://arxiv.org/abs/1711.10742" rel="nofollow"&gt;Pipeline Generative Adversarial Networks for Facial Images Generation with Multiple Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pix2pix - &lt;a href="https://arxiv.org/abs/1611.07004" rel="nofollow"&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/phillipi/pix2pix"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;pix2pixHD - &lt;a href="https://arxiv.org/abs/1711.11585" rel="nofollow"&gt;High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs&lt;/a&gt; (&lt;a href="https://github.com/NVIDIA/pix2pixHD"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;PixelGAN - &lt;a href="https://arxiv.org/abs/1706.00531" rel="nofollow"&gt;PixelGAN Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PM-GAN - &lt;a href="https://arxiv.org/abs/1804.06248" rel="nofollow"&gt;PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-modalities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PN-GAN - &lt;a href="https://arxiv.org/abs/1712.02225" rel="nofollow"&gt;Pose-Normalized Image Generation for Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;POGAN - &lt;a href="https://arxiv.org/abs/1805.01084" rel="nofollow"&gt;Perceptually Optimized Generative Adversarial Network for Single Image Dehazing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pose-GAN - &lt;a href="https://arxiv.org/abs/1705.00053" rel="nofollow"&gt;The Pose Knows: Video Forecasting by Generating Pose Futures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PP-GAN - &lt;a href="https://arxiv.org/abs/1806.08906" rel="nofollow"&gt;Privacy-Protective-GAN for Face De-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PPAN - &lt;a href="https://arxiv.org/abs/1712.07008" rel="nofollow"&gt;Privacy-Preserving Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PPGN - &lt;a href="https://arxiv.org/abs/1612.00005" rel="nofollow"&gt;Plug &amp;amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PrGAN - &lt;a href="https://arxiv.org/abs/1612.05872" rel="nofollow"&gt;3D Shape Induction from 2D Views of Multiple Objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ProGanSR - &lt;a href="https://arxiv.org/abs/1804.02900" rel="nofollow"&gt;A Fully Progressive Approach to Single-Image Super-Resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Progressive GAN - &lt;a href="https://arxiv.org/abs/1710.10196" rel="nofollow"&gt;Progressive Growing of GANs for Improved Quality, Stability, and Variation&lt;/a&gt; (&lt;a href="https://github.com/tkarras/progressive_growing_of_gans"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;PS-GAN - &lt;a href="https://arxiv.org/abs/1804.02047" rel="nofollow"&gt;Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PSGAN - &lt;a href="http://arxiv.org/abs/1705.06566" rel="nofollow"&gt;Learning Texture Manifolds with the Periodic Spatial GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PSGAN - &lt;a href="https://arxiv.org/abs/1805.03371" rel="nofollow"&gt;PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PS-GAN - &lt;a href="https://arxiv.org/abs/1710.10182" rel="nofollow"&gt;High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RadialGAN - &lt;a href="http://arxiv.org/abs/1802.06403" rel="nofollow"&gt;RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RaGAN - &lt;a href="https://arxiv.org/abs/1807.00734" rel="nofollow"&gt;The relativistic discriminator: a key element missing from standard GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RAN - &lt;a href="https://arxiv.org/abs/1712.05444" rel="nofollow"&gt;RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment&lt;/a&gt; (&lt;a href=""&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RankGAN - &lt;a href="https://arxiv.org/abs/1705.11001" rel="nofollow"&gt;Adversarial Ranking for Language Generation &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RCGAN - &lt;a href="https://arxiv.org/abs/1706.02633" rel="nofollow"&gt;Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ReConNN - &lt;a href="https://arxiv.org/abs/1805.00528" rel="nofollow"&gt;Reconstruction of Simulation-Based Physical Field with Limited Samples by Reconstruction Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recycle-GAN - &lt;a href="https://arxiv.org/abs/1808.05174" rel="nofollow"&gt;Recycle-GAN: Unsupervised Video Retargeting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RefineGAN - &lt;a href="https://arxiv.org/abs/1709.00753" rel="nofollow"&gt;Compressed Sensing MRI Reconstruction with Cyclic Loss in Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ReGAN - &lt;a href="https://arxiv.org/abs/1805.02788" rel="nofollow"&gt;ReGAN: RE[LAX|BAR|INFORCE] based Sequence Generation using GANs&lt;/a&gt; (&lt;a href="https://github.com/TalkToTheGAN/REGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RegCGAN - &lt;a href="https://arxiv.org/abs/1805.02456" rel="nofollow"&gt;Unpaired Multi-Domain Image Generation via Regularized Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RenderGAN - &lt;a href="https://arxiv.org/abs/1611.01331" rel="nofollow"&gt;RenderGAN: Generating Realistic Labeled Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Resembled GAN - &lt;a href="https://arxiv.org/abs/1807.00947" rel="nofollow"&gt;Resembled Generative Adversarial Networks: Two Domains with Similar Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ResGAN - &lt;a href="https://arxiv.org/abs/1707.04881" rel="nofollow"&gt;Generative Adversarial Network based on Resnet for Conditional Image Restoration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RNN-WGAN - &lt;a href="https://arxiv.org/abs/1706.01399" rel="nofollow"&gt;Language Generation with Recurrent Generative Adversarial Networks without Pre-training&lt;/a&gt; (&lt;a href="https://github.com/amirbar/rnn.wgan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RoCGAN - &lt;a href="https://arxiv.org/abs/1805.08657" rel="nofollow"&gt;Robust Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RPGAN - &lt;a href="https://arxiv.org/abs/1705.07831" rel="nofollow"&gt;Stabilizing GAN Training with Multiple Random Projections&lt;/a&gt; (&lt;a href="https://github.com/ayanc/rpgan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RTT-GAN - &lt;a href="https://arxiv.org/abs/1703.07022v2" rel="nofollow"&gt;Recurrent Topic-Transition GAN for Visual Paragraph Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RWGAN - &lt;a href="https://arxiv.org/abs/1705.07164" rel="nofollow"&gt;Relaxed Wasserstein with Applications to GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAD-GAN - &lt;a href="https://arxiv.org/abs/1611.08788v1" rel="nofollow"&gt;SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAGA - &lt;a href="https://arxiv.org/abs/1804.00709" rel="nofollow"&gt;Generative Adversarial Learning for Spectrum Sensing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAGAN - &lt;a href="https://arxiv.org/abs/1805.08318" rel="nofollow"&gt;Self-Attention Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SalGAN - &lt;a href="https://arxiv.org/abs/1701.01081" rel="nofollow"&gt;SalGAN: Visual Saliency Prediction with Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/imatge-upc/saliency-salgan-2017"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SAM - &lt;a href="https://arxiv.org/abs/1809.02064" rel="nofollow"&gt;Sample-Efficient Imitation Learning via Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sAOG - &lt;a href="https://arxiv.org/abs/1807.03877" rel="nofollow"&gt;Deep Structured Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAR-GAN - &lt;a href="https://arxiv.org/abs/1802.10036" rel="nofollow"&gt;Generating High Quality Visible Images from SAR Images Using CNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SBADA-GAN - &lt;a href="https://arxiv.org/abs/1705.08824" rel="nofollow"&gt;From source to target and back: symmetric bi-directional adaptive GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ScarGAN - &lt;a href="https://arxiv.org/abs/1808.04500" rel="nofollow"&gt;ScarGAN: Chained Generative Adversarial Networks to Simulate Pathological Tissue on Cardiovascular MR Scans&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SCH-GAN - &lt;a href="https://arxiv.org/abs/1802.02488" rel="nofollow"&gt;SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SD-GAN - &lt;a href="https://arxiv.org/abs/1705.07904" rel="nofollow"&gt;Semantically Decomposing the Latent Spaces of Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sdf-GAN - &lt;a href="https://arxiv.org/abs/1803.06657" rel="nofollow"&gt;Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SEGAN - &lt;a href="https://arxiv.org/abs/1703.09452v1" rel="nofollow"&gt;SEGAN: Speech Enhancement Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeGAN - &lt;a href="https://arxiv.org/abs/1703.10239" rel="nofollow"&gt;SeGAN: Segmenting and Generating the Invisible&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SegAN - &lt;a href="https://arxiv.org/abs/1706.01805" rel="nofollow"&gt;SegAN: Adversarial Network with Multi-scale L1 Loss for Medical Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sem-GAN - &lt;a href="https://arxiv.org/abs/1807.04409" rel="nofollow"&gt;Sem-GAN: Semantically-Consistent Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeqGAN - &lt;a href="https://arxiv.org/abs/1609.05473v5" rel="nofollow"&gt;SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient&lt;/a&gt; (&lt;a href="https://github.com/LantaoYu/SeqGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SeUDA - &lt;a href="https://arxiv.org/abs/1806.00600" rel="nofollow"&gt;Semantic-Aware Generative Adversarial Nets for Unsupervised Domain Adaptation in Chest X-ray Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SG-GAN - &lt;a href="https://arxiv.org/abs/1801.01726" rel="nofollow"&gt;Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption&lt;/a&gt; (&lt;a href="https://github.com/Peilun-Li/SG-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SG-GAN - &lt;a href="https://arxiv.org/abs/1805.07509" rel="nofollow"&gt;Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1611.08207" rel="nofollow"&gt;Texture Synthesis with Spatial Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1612.04357v4" rel="nofollow"&gt;Stacked Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/xunhuang1995/SGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1703.05502" rel="nofollow"&gt;Steganographic Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1712.02330" rel="nofollow"&gt;SGAN: An Alternative Training of Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1807.07144" rel="nofollow"&gt;CT Image Enhancement Using Stacked Generative Adversarial Networks and Transfer Learning for Lesion Segmentation Improvement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sGAN  - &lt;a href="https://arxiv.org/abs/1804.04366" rel="nofollow"&gt;Generative Adversarial Training for MRA Image Synthesis Using Multi-Contrast MRI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SiftingGAN - &lt;a href="https://arxiv.org/abs/1809.04985" rel="nofollow"&gt;SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline in vitro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SiGAN - &lt;a href="https://arxiv.org/abs/1807.08370" rel="nofollow"&gt;SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SimGAN - &lt;a href="https://arxiv.org/abs/1612.07828" rel="nofollow"&gt;Learning from Simulated and Unsupervised Images through Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SisGAN - &lt;a href="https://arxiv.org/abs/1707.06873" rel="nofollow"&gt;Semantic Image Synthesis via Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sketcher-Refiner GAN - &lt;a href="https://arxiv.org/abs/1804.08039" rel="nofollow"&gt;Learning Myelin Content in Multiple Sclerosis from Multimodal MRI through Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SketchGAN - &lt;a href="https://arxiv.org/abs/1607.02748" rel="nofollow"&gt;Adversarial Training For Sketch Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SketchyGAN - &lt;a href="https://arxiv.org/abs/1801.02753" rel="nofollow"&gt;SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Skip-Thought GAN - &lt;a href="https://arxiv.org/abs/1808.08703" rel="nofollow"&gt;Generating Text through Adversarial Training using Skip-Thought Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SL-GAN - &lt;a href="https://arxiv.org/abs/1704.02166" rel="nofollow"&gt;Semi-Latent GAN: Learning to generate and modify facial images from attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SLSR - &lt;a href="https://arxiv.org/abs/1809.04976" rel="nofollow"&gt;Sparse Label Smoothing for Semi-supervised Person Re-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SN-DCGAN - &lt;a href="https://arxiv.org/abs/1806.00236" rel="nofollow"&gt;Generative Adversarial Networks for Unsupervised Object Co-localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SN-GAN - &lt;a href="https://drive.google.com/file/d/0B8HZ50DPgR3eSVV6YlF3XzQxSjQ/view" rel="nofollow"&gt;Spectral Normalization for Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/pfnet-research/chainer-gan-lib"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SN-PatchGAN - &lt;a href="https://arxiv.org/abs/1806.03589" rel="nofollow"&gt;Free-Form Image Inpainting with Gated Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sobolev GAN - &lt;a href="https://arxiv.org/abs/1711.04894" rel="nofollow"&gt;Sobolev GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Social GAN - &lt;a href="https://arxiv.org/abs/1803.10892" rel="nofollow"&gt;Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Softmax GAN - &lt;a href="https://arxiv.org/abs/1704.06191" rel="nofollow"&gt;Softmax GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SoPhie - &lt;a href="https://arxiv.org/abs/1806.01482" rel="nofollow"&gt;SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;speech-driven animation GAN - &lt;a href="https://arxiv.org/abs/1805.09313" rel="nofollow"&gt;End-to-End Speech-Driven Facial Animation with Temporal GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spike-GAN - &lt;a href="https://arxiv.org/abs/1803.00338" rel="nofollow"&gt;Synthesizing realistic neural population activity patterns using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Splitting GAN - &lt;a href="https://arxiv.org/abs/1709.07359" rel="nofollow"&gt;Class-Splitting Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SR-CNN-VAE-GAN - &lt;a href="https://arxiv.org/abs/1806.00509" rel="nofollow"&gt;Semi-Recurrent CNN-based VAE-GAN for Sequential Data Generation&lt;/a&gt; (&lt;a href="https://github.com/makbari7/SR-CNN-VAE-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SRGAN - &lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SRPGAN - &lt;a href="https://arxiv.org/abs/1712.05927" rel="nofollow"&gt;SRPGAN: Perceptual Generative Adversarial Network for Single Image Super Resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SS-GAN - &lt;a href="https://arxiv.org/abs/1708.05789" rel="nofollow"&gt;Semi-supervised Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ss-InfoGAN - &lt;a href="https://arxiv.org/abs/1707.04487" rel="nofollow"&gt;Guiding InfoGAN with Semi-Supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SSGAN - &lt;a href="https://arxiv.org/abs/1707.01613" rel="nofollow"&gt;SSGAN: Secure Steganography Based on Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SSL-GAN - &lt;a href="https://arxiv.org/abs/1611.06430v1" rel="nofollow"&gt;Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ST-CGAN - &lt;a href="https://arxiv.org/abs/1712.02478" rel="nofollow"&gt;Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ST-GAN - &lt;a href="https://arxiv.org/abs/1702.06762" rel="nofollow"&gt;Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ST-GAN - &lt;a href="https://arxiv.org/abs/1803.01837" rel="nofollow"&gt;ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;StackGAN - &lt;a href="https://arxiv.org/abs/1612.03242v1" rel="nofollow"&gt;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/hanzhanggit/StackGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;StainGAN - &lt;a href="https://arxiv.org/abs/1804.01601" rel="nofollow"&gt;StainGAN: Stain Style Transfer for Digital Histological Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;StarGAN - &lt;a href="https://arxiv.org/abs/1711.09020" rel="nofollow"&gt;StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/a&gt; (&lt;a href="https://github.com/yunjey/StarGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;StarGAN-VC - &lt;a href="https://arxiv.org/abs/1806.02169" rel="nofollow"&gt;StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SteinGAN - &lt;a href="https://arxiv.org/abs/1707.00797" rel="nofollow"&gt;Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;StepGAN - &lt;a href="https://arxiv.org/abs/1808.05599" rel="nofollow"&gt;Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Super-FAN - &lt;a href="https://arxiv.org/abs/1712.02765" rel="nofollow"&gt;Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SVSGAN - &lt;a href="https://arxiv.org/abs/1710.11428" rel="nofollow"&gt;SVSGAN: Singing Voice Separation via Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SWGAN - &lt;a href="https://arxiv.org/abs/1802.08249" rel="nofollow"&gt;Solving Approximate Wasserstein GANs to Stationarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SyncGAN - &lt;a href="https://arxiv.org/abs/1804.00410" rel="nofollow"&gt;SyncGAN: Synchronize the Latent Space of Cross-modal Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;S^2GAN - &lt;a href="https://arxiv.org/abs/1603.05631v2" rel="nofollow"&gt;Generative Image Modeling using Style and Structure Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;T2Net - &lt;a href="https://arxiv.org/abs/1808.01454" rel="nofollow"&gt;T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;table-GAN - &lt;a href="https://arxiv.org/abs/1806.03384" rel="nofollow"&gt;Data Synthesis based on Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TAC-GAN - &lt;a href="https://arxiv.org/abs/1703.06412v2" rel="nofollow"&gt;TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network&lt;/a&gt; (&lt;a href="https://github.com/dashayushman/TAC-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;TAN - &lt;a href="https://arxiv.org/abs/1704.08834" rel="nofollow"&gt;Outline Colorization through Tandem Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tcGAN - &lt;a href="https://arxiv.org/abs/1806.05147" rel="nofollow"&gt;Cross-modal Hallucination for Few-shot Fine-grained Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TD-GAN - &lt;a href="https://arxiv.org/abs/1806.07201" rel="nofollow"&gt;Task Driven Generative Modeling for Unsupervised Domain Adaptation: Application to X-ray Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tempCycleGAN - &lt;a href="https://arxiv.org/abs/1806.03627" rel="nofollow"&gt;Improving Surgical Training Phantoms by Hyperrealism: Deep Unpaired Image-to-Image Translation from Real Surgeries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tempoGAN - &lt;a href="https://arxiv.org/abs/1801.09710" rel="nofollow"&gt;tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TequilaGAN - &lt;a href="https://arxiv.org/abs/1807.04919" rel="nofollow"&gt;TequilaGAN: How to easily identify GAN samples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Text2Shape - &lt;a href="https://arxiv.org/abs/1803.08495" rel="nofollow"&gt;Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;textGAN - &lt;a href="https://zhegan27.github.io/Papers/textGAN_nips2016_workshop.pdf" rel="nofollow"&gt;Generating Text via Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TextureGAN - &lt;a href="https://arxiv.org/abs/1706.02823" rel="nofollow"&gt;TextureGAN: Controlling Deep Image Synthesis with Texture Patches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGAN - &lt;a href="https://arxiv.org/abs/1611.06624v1" rel="nofollow"&gt;Temporal Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGAN - &lt;a href="https://arxiv.org/abs/1710.10772" rel="nofollow"&gt;Tensorizing Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGAN - &lt;a href="https://arxiv.org/abs/1711.02666" rel="nofollow"&gt;Tensor-Generative Adversarial Network with Two-dimensional Sparse Coding: Application to Real-time Indoor Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGANs-C - &lt;a href="https://arxiv.org/abs/1804.08264" rel="nofollow"&gt;To Create What You Tell: Generating Videos from Captions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tiny-GAN - &lt;a href="https://arxiv.org/abs/1803.05045" rel="nofollow"&gt;Analysis of Nonautonomous Adversarial Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TP-GAN - &lt;a href="https://arxiv.org/abs/1704.04086" rel="nofollow"&gt;Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TreeGAN - &lt;a href="https://arxiv.org/abs/1808.07582" rel="nofollow"&gt;TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Triple-GAN - &lt;a href="https://arxiv.org/abs/1703.02291v2" rel="nofollow"&gt;Triple Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tripletGAN - &lt;a href="https://arxiv.org/abs/1711.05084" rel="nofollow"&gt;TripletGAN: Training Generative Model with Triplet Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TV-GAN - &lt;a href="https://arxiv.org/abs/1712.02514" rel="nofollow"&gt;TV-GAN: Generative Adversarial Network Based Thermal to Visible Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Twin-GAN - &lt;a href="https://arxiv.org/abs/1809.00946" rel="nofollow"&gt;Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UGACH - &lt;a href="https://arxiv.org/abs/1712.00358" rel="nofollow"&gt;Unsupervised Generative Adversarial Cross-modal Hashing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UGAN - &lt;a href="https://arxiv.org/abs/1801.04011" rel="nofollow"&gt;Enhancing Underwater Imagery using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unim2im - &lt;a href="https://arxiv.org/abs/1701.02676" rel="nofollow"&gt;Unsupervised Image-to-Image Translation with Generative Adversarial Networks &lt;/a&gt; (&lt;a href="http://github.com/zsdonghao/Unsup-Im2Im"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;UNIT - &lt;a href="https://arxiv.org/abs/1703.00848" rel="nofollow"&gt;Unsupervised Image-to-image Translation Networks&lt;/a&gt; (&lt;a href="https://github.com/mingyuliutw/UNIT"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Unrolled GAN - &lt;a href="https://arxiv.org/abs/1611.02163" rel="nofollow"&gt;Unrolled Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/poolio/unrolled_gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;UT-SCA-GAN - &lt;a href="https://arxiv.org/abs/1804.07939" rel="nofollow"&gt;Spatial Image Steganography Based on Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UV-GAN - &lt;a href="https://arxiv.org/abs/1712.04695" rel="nofollow"&gt;UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VA-GAN - &lt;a href="https://arxiv.org/abs/1711.08998" rel="nofollow"&gt;Visual Feature Attribution using Wasserstein GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VAC+GAN  - &lt;a href="https://arxiv.org/abs/1806.07751" rel="nofollow"&gt;Versatile Auxiliary Classifier with Generative Adversarial Network (VAC+GAN), Multi Class Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VAE-GAN - &lt;a href="https://arxiv.org/abs/1512.09300" rel="nofollow"&gt;Autoencoding beyond pixels using a learned similarity metric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VariGAN - &lt;a href="https://arxiv.org/abs/1704.04886" rel="nofollow"&gt;Multi-View Image Generation from a Single-View&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VAW-GAN - &lt;a href="https://arxiv.org/abs/1704.00849" rel="nofollow"&gt;Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VEEGAN - &lt;a href="https://arxiv.org/abs/1705.07761" rel="nofollow"&gt;VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning&lt;/a&gt; (&lt;a href="https://github.com/akashgit/VEEGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;VGAN - &lt;a href="https://arxiv.org/abs/1609.02612" rel="nofollow"&gt;Generating Videos with Scene Dynamics&lt;/a&gt; (&lt;a href="https://github.com/cvondrick/videogan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;VGAN - &lt;a href="https://arxiv.org/abs/1611.01799" rel="nofollow"&gt;Generative Adversarial Networks as Variational Training of Energy Based Models&lt;/a&gt; (&lt;a href="https://github.com/Shuangfei/vgan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;VGAN - &lt;a href="https://arxiv.org/abs/1712.00170" rel="nofollow"&gt;Text Generation Based on Generative Adversarial Nets with Latent Variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ViGAN - &lt;a href="https://arxiv.org/abs/1701.04568v1" rel="nofollow"&gt;Image Generation and Editing with Variational Info Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VIGAN - &lt;a href="https://arxiv.org/abs/1708.06724" rel="nofollow"&gt;VIGAN: Missing View Imputation with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VoiceGAN - &lt;a href="http://arxiv.org/abs/1802.06840" rel="nofollow"&gt;Voice Impersonation using Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VOS-GAN - &lt;a href="https://arxiv.org/abs/1803.09092" rel="nofollow"&gt;VOS-GAN: Adversarial Learning of Visual-Temporal Dynamics for Unsupervised Dense Prediction in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VRAL - &lt;a href="https://arxiv.org/abs/1707.00309" rel="nofollow"&gt;Variance Regularizing Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaterGAN - &lt;a href="https://arxiv.org/abs/1702.07392v1" rel="nofollow"&gt;WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaveGAN - &lt;a href="https://arxiv.org/abs/1802.04208" rel="nofollow"&gt;Synthesizing Audio with Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaveletGLCA-GAN - &lt;a href="https://arxiv.org/abs/1809.07764" rel="nofollow"&gt;Global and Local Consistent Wavelet-domain Age Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;weGAN - &lt;a href="https://arxiv.org/abs/1712.09127" rel="nofollow"&gt;Generative Adversarial Nets for Multiple Text Corpora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WGAN - &lt;a href="https://arxiv.org/abs/1701.07875v2" rel="nofollow"&gt;Wasserstein GAN&lt;/a&gt; (&lt;a href="https://github.com/martinarjovsky/WassersteinGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;WGAN-CLS - &lt;a href="https://arxiv.org/abs/1805.00676" rel="nofollow"&gt;Text to Image Synthesis Using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WGAN-GP - &lt;a href="https://arxiv.org/abs/1704.00028" rel="nofollow"&gt;Improved Training of Wasserstein GANs&lt;/a&gt; (&lt;a href="https://github.com/igul222/improved_wgan_training"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;WGAN-L1 - &lt;a href="https://arxiv.org/abs/1807.04418" rel="nofollow"&gt;Subsampled Turbulence Removal Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WS-GAN - &lt;a href="https://arxiv.org/abs/1705.10904" rel="nofollow"&gt;Weakly Supervised Generative Adversarial Networks for 3D Reconstruction &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;X-GANs - &lt;a href="https://arxiv.org/abs/1808.04432" rel="nofollow"&gt;X-GANs: Image Reconstruction Made Easy for Extreme Cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;XGAN - &lt;a href="https://arxiv.org/abs/1711.05139" rel="nofollow"&gt;XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ZipNet-GAN - &lt;a href="https://arxiv.org/abs/1711.02413" rel="nofollow"&gt;ZipNet-GAN: Inferring Fine-grained Mobile Traffic Patterns via a Generative Adversarial Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;-GAN - &lt;a href="https://arxiv.org/abs/1706.04987" rel="nofollow"&gt;Variational Approaches for Auto-Encoding Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/victor-shepardson/alpha-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;-GAN - &lt;a href="https://arxiv.org/abs/1705.07505" rel="nofollow"&gt;Annealed Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;-GAN - &lt;a href="https://arxiv.org/abs/1709.06548" rel="nofollow"&gt;Triangle Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hindupuravinash</author><guid isPermaLink="false">https://github.com/hindupuravinash/the-gan-zoo</guid><pubDate>Tue, 05 Nov 2019 00:17:00 GMT</pubDate></item><item><title>hanxiao/bert-as-service #18 in Python, This week</title><link>https://github.com/hanxiao/bert-as-service</link><description>&lt;p&gt;&lt;i&gt;Mapping a variable-length sentence to a fixed-length vector using BERT model&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-bert-as-service" class="anchor" aria-hidden="true" href="#bert-as-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bert-as-service&lt;/h1&gt;
&lt;p align="center"&gt;Using BERT model as a sentence encoding service, i.e. mapping a variable-length sentence to a fixed-length vector.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/stargazers"&gt;
    &lt;img src="https://camo.githubusercontent.com/827fc64cf3b84a82a3057b15bd67bd110c3f094f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616e7869616f2f626572742d61732d736572766963652e7376673f636f6c6f72413d6f72616e676526636f6c6f72423d6f72616e6765266c6f676f3d676974687562" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/hanxiao/bert-as-service.svg?colorA=orange&amp;amp;colorB=orange&amp;amp;logo=github" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://pypi.org/search/?q=bert-serving" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/17c79028fcd99fcb6ffd09d9078a9e90ca72dabe/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626572742d73657276696e672d7365727665722e7376673f636f6c6f72423d627269676874677265656e" alt="Pypi package" data-canonical-src="https://img.shields.io/pypi/v/bert-serving-server.svg?colorB=brightgreen" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;a href="https://bert-as-service.readthedocs.io/" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/97f0af3aadd65722bd4510764170eb12e26c20a2/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;a href="https://pypi.org/search/?q=bert-serving" rel="nofollow"&gt;
      &lt;img alt="PyPI - Downloads" src="https://camo.githubusercontent.com/f0c139454d4564bf4abdcf294e009d886e53e1f5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626572742d73657276696e672d736572766572" data-canonical-src="https://img.shields.io/pypi/dm/bert-serving-server" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/issues"&gt;
        &lt;img src="https://camo.githubusercontent.com/37c917311b11a54f44462c0271a9e62fbd82dc03/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f68616e7869616f2f626572742d61732d736572766963652e737667" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/hanxiao/bert-as-service.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/blob/master/LICENSE"&gt;
        &lt;img src="https://camo.githubusercontent.com/82e75359cfc65c373073242222565e1d21cd5979/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68616e7869616f2f626572742d61732d736572766963652e737667" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/hanxiao/bert-as-service.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://twitter.com/intent/tweet?text=Wow:&amp;amp;url=https%3A%2F%2Fgithub.com%2Fhanxiao%2Fbert-as-service" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/c03e98ee22b873659d1c89f929e35fc8eafbeada/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f6769746875622e636f6d2f68616e7869616f2f626572742d61732d736572766963652e7376673f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/https/github.com/hanxiao/bert-as-service.svg?style=social" style="max-width:100%;"&gt;
  &lt;/a&gt;      
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#highlights"&gt;Highlights&lt;/a&gt; 
  &lt;a href="#what-is-it"&gt;What is it&lt;/a&gt; 
  &lt;a href="#install"&gt;Install&lt;/a&gt; 
  &lt;a href="#getting-started"&gt;Getting Started&lt;/a&gt; 
  &lt;a href="#server-and-client-api"&gt;API&lt;/a&gt; 
  &lt;a href="#book-tutorial"&gt;Tutorials&lt;/a&gt; 
  &lt;a href="#speech_balloon-faq"&gt;FAQ&lt;/a&gt; 
  &lt;a href="#zap-benchmark"&gt;Benchmark&lt;/a&gt; 
  &lt;a href="https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" rel="nofollow"&gt;Blog&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href=".github/demo.gif?raw=true"&gt;&lt;img src=".github/demo.gif?raw=true" width="700" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-made-by-han-xiao--globe_with_meridians-httpshanxiaogithubio" class="anchor" aria-hidden="true" href="#made-by-han-xiao--globe_with_meridians-httpshanxiaogithubio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Made by Han Xiao  &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;&lt;/g-emoji&gt; &lt;a href="https://hanxiao.github.io" rel="nofollow"&gt;https://hanxiao.github.io&lt;/a&gt;&lt;/h6&gt;

&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
  &lt;td width="25%"&gt;&lt;a href="https://github.com/gnes-ai/gnes"&gt;
      &lt;img src=".github/gnes-logo-tight.svg" alt="GNES is Generic Neural Elastic Search (logo made by Han Xiao)" style="max-width:100%;"&gt;
      &lt;/a&gt;&lt;/td&gt;
  &lt;td&gt;
  &lt;b&gt;&lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;&lt;/g-emoji&gt;Looking for X-as-service? Or more generic and cloud-native solution?&lt;/b&gt;
  &lt;p&gt;&lt;br&gt;Checkout my new project &lt;a href="https://github.com/gnes-ai/gnes"&gt;GNES&lt;/a&gt;! GNES is Generic Neural Elastic Search, a cloud-native semantic search system based on deep neural network. GNES enables large-scale index and semantic search for text-to-text, image-to-image, video-to-video and any-to-any content form.&lt;/p&gt;
&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2 align="center"&gt;&lt;a id="user-content-what-is-it" class="anchor" aria-hidden="true" href="#what-is-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is it&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt; is a NLP model &lt;a href="https://github.com/google-research/bert"&gt;developed by Google&lt;/a&gt; for pre-training language representations. It leverages an enormous amount of plain text data publicly available on the web and is trained in an unsupervised manner. Pre-training a BERT model is a fairly expensive yet one-time procedure for each language. Fortunately, Google released several pre-trained models where &lt;a href="https://github.com/google-research/bert#pre-trained-models"&gt;you can download from here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sentence Encoding/Embedding&lt;/strong&gt; is a upstream task required in many NLP applications, e.g. sentiment analysis, text classification. The goal is to represent a variable length sentence into a fixed length vector, e.g. &lt;code&gt;hello world&lt;/code&gt; to &lt;code&gt;[0.1, 0.3, 0.9]&lt;/code&gt;. Each element of the vector should "encode" some semantics of the original sentence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finally, &lt;code&gt;bert-as-service&lt;/code&gt;&lt;/strong&gt; uses BERT as a sentence encoder and hosts it as a service via ZeroMQ, allowing you to map sentences into fixed-length representations in just two lines of code.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-highlights" class="anchor" aria-hidden="true" href="#highlights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="telescope" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52d.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;State-of-the-art&lt;/strong&gt;: build on pretrained 12/24-layer BERT models released by Google AI, which is considered as a milestone in the NLP community.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="hatching_chick" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f423.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;Easy-to-use&lt;/strong&gt;: require only two lines of code to get sentence/token-level encodes.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="zap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;Fast&lt;/strong&gt;: 900 sentences/s on a single Tesla M40 24GB. Low latency, optimized for speed. See &lt;a href="#zap-benchmark"&gt;benchmark&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="octopus" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f419.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;Scalable&lt;/strong&gt;: scale nicely and smoothly on multiple GPUs and multiple clients without worrying about concurrency. See &lt;a href="#speed-wrt-num_client"&gt;benchmark&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gem" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f48e.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;Reliable&lt;/strong&gt;: tested on multi-billion sentences; days of running without a break or OOM or any nasty exceptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More features: &lt;a href="#speed-wrt--fp16-and--xla"&gt;XLA &amp;amp; FP16 support&lt;/a&gt;; mix GPU-CPU workloads; optimized graph; &lt;code&gt;tf.data&lt;/code&gt; friendly; customized tokenizer; flexible pooling strategy; &lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;build-in HTTP server&lt;/a&gt; and dashboard; &lt;a href="#asynchronous-encoding"&gt;async encoding&lt;/a&gt;; &lt;a href="#broadcasting-to-multiple-clients"&gt;multicasting&lt;/a&gt;; etc.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;Install the server and client via &lt;code&gt;pip&lt;/code&gt;. They can be installed separately or even on &lt;em&gt;different&lt;/em&gt; machines:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install bert-serving-server  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; server&lt;/span&gt;
pip install bert-serving-client  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; client, independent of `bert-serving-server`&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the server MUST be running on &lt;strong&gt;Python &amp;gt;= 3.5&lt;/strong&gt; with &lt;strong&gt;Tensorflow &amp;gt;= 1.10&lt;/strong&gt; (&lt;em&gt;one-point-ten&lt;/em&gt;). Again, the server does not support Python 2!&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png"&gt;&lt;/g-emoji&gt; The client can be running on both Python 2 and 3 &lt;a href="#q-can-i-run-it-in-python-2"&gt;for the following consideration&lt;/a&gt;.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-1-download-a-pre-trained-bert-model" class="anchor" aria-hidden="true" href="#1-download-a-pre-trained-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Download a Pre-trained BERT Model&lt;/h4&gt;
&lt;p&gt;Download a model listed below, then uncompress the zip file into some folder, say &lt;code&gt;/tmp/english_L-12_H-768_A-12/&lt;/code&gt;&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;List of released pretrained BERT models (click to expand...)&lt;/summary&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Uncased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;BERT-Large, Uncased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Cased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;12-layer, 768-hidden, 12-heads , 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;BERT-Large, Cased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Multilingual Cased (New)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Multilingual Cased (Old)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Chinese&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/details&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt; fine-tuning the model on your downstream task. &lt;a href="#q-are-you-suggesting-using-bert-without-fine-tuning"&gt;Why is it optional?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-2-start-the-bert-service" class="anchor" aria-hidden="true" href="#2-start-the-bert-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Start the BERT service&lt;/h4&gt;
&lt;p&gt;After installing the server, you should be able to use &lt;code&gt;bert-serving-start&lt;/code&gt; CLI as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will start a service with four workers, meaning that it can handle up to four &lt;strong&gt;concurrent&lt;/strong&gt; requests. More concurrent requests will be queued in a load balancer. Details can be found in our &lt;a href="#q-what-is-the-parallel-processing-model-behind-the-scene"&gt;FAQ&lt;/a&gt; and &lt;a href="#speed-wrt-num_client"&gt;the benchmark on number of clients&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below shows what the server looks like when starting correctly:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/server-demo.gif?raw=true"&gt;&lt;img src=".github/server-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Alternatively, one can start the BERT Service in a Docker Container (click to expand...)&lt;/summary&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker build -t bert-as-service -f ./docker/Dockerfile &lt;span class="pl-c1"&gt;.&lt;/span&gt;
NUM_WORKER=1
PATH_MODEL=/PATH_TO/_YOUR_MODEL/
docker run --runtime nvidia -dit -p 5555:5555 -p 5556:5556 -v &lt;span class="pl-smi"&gt;$PATH_MODEL&lt;/span&gt;:/model -t bert-as-service &lt;span class="pl-smi"&gt;$NUM_WORKER&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h4&gt;&lt;a id="user-content-3-use-client-to-get-sentence-encodes" class="anchor" aria-hidden="true" href="#3-use-client-to-get-sentence-encodes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Use Client to Get Sentence Encodes&lt;/h4&gt;
&lt;p&gt;Now you can encode sentences simply as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.client &lt;span class="pl-k"&gt;import&lt;/span&gt; BertClient
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it better&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It will return a &lt;code&gt;ndarray&lt;/code&gt; (or &lt;code&gt;List[List[float]]&lt;/code&gt; if you wish), in which each row is a fixed-length vector representing a sentence. Having thousands of sentences? Just &lt;code&gt;encode&lt;/code&gt;! &lt;em&gt;Don't even bother to batch&lt;/em&gt;, the server will take care of it.&lt;/p&gt;
&lt;p&gt;As a feature of BERT, you may get encodes of a pair of sentences by concatenating them with &lt;code&gt;|||&lt;/code&gt; (with whitespace before and after), e.g.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it ||| then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Below shows what the server looks like while encoding:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/server-run-demo.gif?raw=true"&gt;&lt;img src=".github/server-run-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-use-bert-service-remotely" class="anchor" aria-hidden="true" href="#use-bert-service-remotely"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use BERT Service Remotely&lt;/h4&gt;
&lt;p&gt;One may also start the service on one (GPU) machine and call it from another (CPU) machine as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; on another CPU machine&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.client &lt;span class="pl-k"&gt;import&lt;/span&gt; BertClient
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;ip&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xx.xx.xx.xx&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ip address of the GPU machine&lt;/span&gt;
bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it better&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that you only need &lt;code&gt;pip install -U bert-serving-client&lt;/code&gt; in this case, the server side is not required. You may also &lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;call the service via HTTP requests.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png"&gt;&lt;/g-emoji&gt; &lt;strong&gt;Want to learn more? Checkout our tutorials:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;Building a QA semantic search engine in 3 min.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serving-a-fine-tuned-bert-model"&gt;Serving a fine-tuned BERT model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-elmo-like-contextual-word-embedding"&gt;Getting ELMo-like contextual word embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-your-own-tokenizer"&gt;Using your own tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bertclient-with-tfdata-api"&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;Training a text classifier using BERT features and tf.estimator API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#saving-and-loading-with-tfrecord-data"&gt;Saving and loading with TFRecord data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asynchronous-encoding"&gt;Asynchronous encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#broadcasting-to-multiple-clients"&gt;Broadcasting to multiple clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#monitoring-the-service-status-in-a-dashboard"&gt;Monitoring the service status in a dashboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#starting-bertserver-from-python"&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-server-and-client-api" class="anchor" aria-hidden="true" href="#server-and-client-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server and Client API&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt; Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bert-as-service.readthedocs.io" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The best way to learn &lt;code&gt;bert-as-service&lt;/code&gt; &lt;strong&gt;latest API&lt;/strong&gt; is &lt;a href="http://bert-as-service.readthedocs.io" rel="nofollow"&gt;reading the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-server-api" class="anchor" aria-hidden="true" href="#server-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server API&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/server.html#server-side-api" rel="nofollow"&gt;Please always refer to the latest server-side API documented here.&lt;/a&gt;, you may get the latest usage via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start --help
bert-serving-terminate --help
bert-serving-benchmark --help&lt;/pre&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Required&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;folder path of the pre-trained BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tuned_model_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;(Optional)&lt;/td&gt;
&lt;td&gt;folder path of a fine-tuned BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ckpt_name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_model.ckpt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;filename of the checkpoint file.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;config_name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;filename of the JSON config file for BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;graph_tmp_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;path to graph temp file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_seq_len&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;maximum length of sequence, longer sequence will be trimmed on the right side. Set it to NONE for dynamically using the longest sequence in a (mini)batch.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cased_tokenization&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;Whether tokenizer should skip the default lowercasing and accent removal. Should be used for e.g. the multilingual cased pretrained BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mask_cls_sep&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;masking the embedding on [CLS] and [SEP] with zero.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;num_worker&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;number of (GPU/CPU) worker runs BERT model, each works in a separate process.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_batch_size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;maximum number of sequences handled by each worker, larger batch will be partitioned into small batches.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;priority_batch_size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;batch smaller than this size will be labeled as high priority, and jumps forward in the job queue to get result faster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5555&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for pushing data from client to server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port_out&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5556&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for publishing results from server to client&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;http_port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;server port for receiving HTTP requests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cors&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;*&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;setting "Access-Control-Allow-Origin" for HTTP requests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pooling_strategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the pooling strategy for generating encoding vectors, valid values are &lt;code&gt;NONE&lt;/code&gt;, &lt;code&gt;REDUCE_MEAN&lt;/code&gt;, &lt;code&gt;REDUCE_MAX&lt;/code&gt;, &lt;code&gt;REDUCE_MEAN_MAX&lt;/code&gt;, &lt;code&gt;CLS_TOKEN&lt;/code&gt;, &lt;code&gt;FIRST_TOKEN&lt;/code&gt;, &lt;code&gt;SEP_TOKEN&lt;/code&gt;, &lt;code&gt;LAST_TOKEN&lt;/code&gt;. Explanation of these strategies &lt;a href="#q-what-are-the-available-pooling-strategies"&gt;can be found here&lt;/a&gt;. To get encoding for each token in the sequence, please set this to &lt;code&gt;NONE&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pooling_layer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[-2]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the encoding layer that pooling operates on, where &lt;code&gt;-1&lt;/code&gt; means the last layer, &lt;code&gt;-2&lt;/code&gt; means the second-to-last, &lt;code&gt;[-1, -2]&lt;/code&gt; means concatenating the result of last two layers, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gpu_memory_fraction&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the fraction of the overall amount of memory that each GPU should be allocated per worker&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cpu&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;run on CPU instead of GPU&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xla&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;enable &lt;a href="https://www.tensorflow.org/xla/jit" rel="nofollow"&gt;XLA compiler&lt;/a&gt; for graph optimization (&lt;em&gt;experimental!&lt;/em&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fp16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;use float16 precision (experimental)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;device_map&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;specify the list of GPU device ids that will be used (id starts from 0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;show_tokens_to_client&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;sending tokenization results to client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-client-api" class="anchor" aria-hidden="true" href="#client-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client API&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/client.html#module-client" rel="nofollow"&gt;Please always refer to the latest client-side API documented here.&lt;/a&gt; Client-side provides a Python class called &lt;code&gt;BertClient&lt;/code&gt;, which accepts arguments as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ip&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address of the server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5555&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for pushing data from client to server, &lt;em&gt;must be consistent with the server side config&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port_out&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5556&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for publishing results from server to client, &lt;em&gt;must be consistent with the server side config&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;output_fmt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ndarray&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the output format of the sentence encodes, either in numpy array or python List[List[float]] (&lt;code&gt;ndarray&lt;/code&gt;/&lt;code&gt;list&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;show_server_config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;whether to show server configs when first connected&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;check_version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;whether to force client and server to have the same version&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;identity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;a UUID that identifies the client, useful in multi-casting&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;timeout&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;set the timeout (milliseconds) for receive operation on the client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A &lt;code&gt;BertClient&lt;/code&gt; implements the following methods and properties:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.encode()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Encode a list of strings to a list of vectors&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.encode_async()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Asynchronous encode batches from a generator&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.fetch()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch all encoded vectors from server and return them in a generator, use it with &lt;code&gt;.encode_async()&lt;/code&gt; or &lt;code&gt;.encode(blocking=False)&lt;/code&gt;. Sending order is NOT preserved.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.fetch_all()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch all encoded vectors from server and return them in a list, use it with &lt;code&gt;.encode_async()&lt;/code&gt; or &lt;code&gt;.encode(blocking=False)&lt;/code&gt;. Sending order is preserved.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.close()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Gracefully close the connection between the client and the server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Get the client status in JSON format&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.server_status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Get the server status in JSON format&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-book-tutorial" class="anchor" aria-hidden="true" href="#book-tutorial"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;&lt;/g-emoji&gt; Tutorial&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt; Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/faq.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The full list of examples can be found in &lt;a href="example"&gt;&lt;code&gt;example/&lt;/code&gt;&lt;/a&gt;. You can run each via &lt;code&gt;python example/example-k.py&lt;/code&gt;. Most of examples require you to start a BertServer first, please follow &lt;a href="#2-start-the-bert-service"&gt;the instruction here&lt;/a&gt;. Note that although &lt;code&gt;BertClient&lt;/code&gt; works universally on both Python 2.x and 3.x, examples are only tested on Python 3.6.&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Table of contents (click to expand...)&lt;/summary&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;Building a QA semantic search engine in 3 min.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serving-a-fine-tuned-bert-model"&gt;Serving a fine-tuned BERT model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-elmo-like-contextual-word-embedding"&gt;Getting ELMo-like contextual word embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-your-own-tokenizer"&gt;Using your own tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bertclient-with-tfdata-api"&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;Training a text classifier using BERT features and tf.estimator API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#saving-and-loading-with-tfrecord-data"&gt;Saving and loading with TFRecord data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asynchronous-encoding"&gt;Asynchronous encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#broadcasting-to-multiple-clients"&gt;Broadcasting to multiple clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#monitoring-the-service-status-in-a-dashboard"&gt;Monitoring the service status in a dashboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#starting-bertserver-from-python"&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-building-a-qa-semantic-search-engine-in-3-minutes" class="anchor" aria-hidden="true" href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building a QA semantic search engine in 3 minutes&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example8.py"&gt;be found example8.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As the first example, we will implement a simple QA search engine using &lt;code&gt;bert-as-service&lt;/code&gt; in just three minutes. No kidding! The goal is to find similar questions to user's input and return the corresponding answer. To start, we need a list of question-answer pairs. Fortunately, this README file already contains &lt;a href="#speech_balloon-faq"&gt;a list of FAQ&lt;/a&gt;, so I will just use that to make this example perfectly self-contained. Let's first load all questions and show some statistics.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;prefix_q &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;##### **Q:** &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;README.md&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; fp:
    questions &lt;span class="pl-k"&gt;=&lt;/span&gt; [v.replace(prefix_q, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).strip() &lt;span class="pl-k"&gt;for&lt;/span&gt; v &lt;span class="pl-k"&gt;in&lt;/span&gt; fp &lt;span class="pl-k"&gt;if&lt;/span&gt; v.strip() &lt;span class="pl-k"&gt;and&lt;/span&gt; v.startswith(prefix_q)]
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-c1"&gt;%d&lt;/span&gt; questions loaded, avg. len of &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (&lt;span class="pl-c1"&gt;len&lt;/span&gt;(questions), np.mean([&lt;span class="pl-c1"&gt;len&lt;/span&gt;(d.split()) &lt;span class="pl-k"&gt;for&lt;/span&gt; d &lt;span class="pl-k"&gt;in&lt;/span&gt; questions])))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives &lt;code&gt;33 questions loaded, avg. len of 9&lt;/code&gt;. So looks like we have enough questions. Now start a BertServer with &lt;code&gt;uncased_L-12_H-768_A-12&lt;/code&gt; pretrained BERT model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -num_worker=1 -model_dir=/data/cips/data/lab/data/model/uncased_L-12_H-768_A-12&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we need to encode our questions into vectors:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;port&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4000&lt;/span&gt;, &lt;span class="pl-v"&gt;port_out&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4001&lt;/span&gt;)
doc_vecs &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(questions)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we are ready to receive new query and perform a simple "fuzzy" search against the existing questions. To do that, every time a new query is coming, we encode it as a vector and compute its dot product with &lt;code&gt;doc_vecs&lt;/code&gt;; sort the result descendingly; and return the top-k similar questions as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
    query &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;input&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;your question: &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    query_vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode([query])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; compute normalized dot product as score&lt;/span&gt;
    score &lt;span class="pl-k"&gt;=&lt;/span&gt; np.sum(query_vec &lt;span class="pl-k"&gt;*&lt;/span&gt; doc_vecs, &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;) &lt;span class="pl-k"&gt;/&lt;/span&gt; np.linalg.norm(doc_vecs, &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)
    topk_idx &lt;span class="pl-k"&gt;=&lt;/span&gt; np.argsort(score)[::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;][:topk]
    &lt;span class="pl-k"&gt;for&lt;/span&gt; idx &lt;span class="pl-k"&gt;in&lt;/span&gt; topk_idx:
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;gt; &lt;span class="pl-c1"&gt;%s&lt;/span&gt;&lt;span class="pl-cce"&gt;\t&lt;/span&gt;&lt;span class="pl-c1"&gt;%s&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (score[idx], questions[idx]))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! Now run the code and type your query, see how this search engine handles fuzzy match:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/qasearch-demo.gif?raw=true"&gt;&lt;img src=".github/qasearch-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serving-a-fine-tuned-bert-model" class="anchor" aria-hidden="true" href="#serving-a-fine-tuned-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serving a fine-tuned BERT model&lt;/h3&gt;
&lt;p&gt;Pretrained BERT models often show quite "okayish" performance on many tasks. However, to release the true power of BERT a fine-tuning on the downstream task (or on domain-specific data) is necessary. In this example, I will show you how to serve a fine-tuned BERT model.&lt;/p&gt;
&lt;p&gt;We follow the instruction in &lt;a href="https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks"&gt;"Sentence (and sentence-pair) classification tasks"&lt;/a&gt; and use &lt;code&gt;run_classifier.py&lt;/code&gt; to fine tune &lt;code&gt;uncased_L-12_H-768_A-12&lt;/code&gt; model on MRPC task. The fine-tuned model is stored at &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;, which can be changed by specifying &lt;code&gt;--output_dir&lt;/code&gt; of &lt;code&gt;run_classifier.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you look into &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;, it contains something like:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;checkpoint                                        128
&lt;span class="pl-c1"&gt;eval&lt;/span&gt;                                              4.0K
eval_results.txt                                  86
eval.tf_record                                    219K
events.out.tfevents.1545202214.TENCENT64.site     6.1M
events.out.tfevents.1545203242.TENCENT64.site     14M
graph.pbtxt                                       9.0M
model.ckpt-0.data-00000-of-00001                  1.3G
model.ckpt-0.index                                23K
model.ckpt-0.meta                                 3.9M
model.ckpt-343.data-00000-of-00001                1.3G
model.ckpt-343.index                              23K
model.ckpt-343.meta                               3.9M
train.tf_record                                   2.0M&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Don't be afraid of those mysterious files, as the only important one to us is &lt;code&gt;model.ckpt-343.data-00000-of-00001&lt;/code&gt; (looks like my training stops at the 343 step. One may get &lt;code&gt;model.ckpt-123.data-00000-of-00001&lt;/code&gt; or &lt;code&gt;model.ckpt-9876.data-00000-of-00001&lt;/code&gt; depending on the total training steps). Now we have collected all three pieces of information that are needed for serving this fine-tuned model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pretrained model is downloaded to &lt;code&gt;/path/to/bert/uncased_L-12_H-768_A-12&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Our fine-tuned model is stored at &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Our fine-tuned model checkpoint is named as &lt;code&gt;model.ckpt-343&lt;/code&gt; something something.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now start a BertServer by putting three pieces together:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir=/pretrained/uncased_L-12_H-768_A-12 -tuned_model_dir=/tmp/mrpc_output/ -ckpt_name=model.ckpt-343&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the server started, you should find this line in the log:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;I:GRAPHOPT:[gra:opt: 50]:checkpoint (override by fine-tuned model): /tmp/mrpc_output/model.ckpt-343
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which means the BERT parameters is overrode and successfully loaded from our fine-tuned &lt;code&gt;/tmp/mrpc_output/model.ckpt-343&lt;/code&gt;. Done!&lt;/p&gt;
&lt;p&gt;In short, find your fine-tuned model path and checkpoint name, then feed them to &lt;code&gt;-tuned_model_dir&lt;/code&gt; and &lt;code&gt;-ckpt_name&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-elmo-like-contextual-word-embedding" class="anchor" aria-hidden="true" href="#getting-elmo-like-contextual-word-embedding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting ELMo-like contextual word embedding&lt;/h3&gt;
&lt;p&gt;Start the server with &lt;code&gt;pooling_strategy&lt;/code&gt; set to NONE.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -pooling_strategy NONE -model_dir /tmp/english_L-12_H-768_A-12/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To get the word embedding corresponds to every token, you can simply use slice index as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; max_seq_len = 25&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pooling_strategy = NONE&lt;/span&gt;

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hey you&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;whats up?&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])

vec  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [2, 25, 768]&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 25, 768], sentence embeddings for `hey you`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `[CLS]`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;1&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `hey`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;2&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `you`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;3&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `[SEP]`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;4&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for padding symbol&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;25&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; error, out of index!&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that no matter how long your original sequence is, the service will always return a &lt;code&gt;[max_seq_len, 768]&lt;/code&gt; matrix for every sequence. When using slice index to get the word embedding, beware of the special tokens padded to the sequence, i.e. &lt;code&gt;[CLS]&lt;/code&gt;, &lt;code&gt;[SEP]&lt;/code&gt;, &lt;code&gt;0_PAD&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-your-own-tokenizer" class="anchor" aria-hidden="true" href="#using-your-own-tokenizer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using your own tokenizer&lt;/h3&gt;
&lt;p&gt;Often you want to use your own tokenizer to segment sentences instead of the default one from BERT. Simply call &lt;code&gt;encode(is_tokenized=True)&lt;/code&gt; on the client slide as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;texts &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;good day&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; a naive whitespace tokenizer&lt;/span&gt;
texts2 &lt;span class="pl-k"&gt;=&lt;/span&gt; [s.split() &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; texts]

vecs &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(texts2, &lt;span class="pl-v"&gt;is_tokenized&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives &lt;code&gt;[2, 25, 768]&lt;/code&gt; tensor where the first &lt;code&gt;[1, 25, 768]&lt;/code&gt; corresponds to the token-level encoding of "hello world!". If you look into its values, you will find that only the first four elements, i.e. &lt;code&gt;[1, 0:3, 768]&lt;/code&gt; have values, all the others are zeros. This is due to the fact that BERT considers "hello world!" as four tokens: &lt;code&gt;[CLS]&lt;/code&gt; &lt;code&gt;hello&lt;/code&gt; &lt;code&gt;world!&lt;/code&gt; &lt;code&gt;[SEP]&lt;/code&gt;, the rest are padding symbols and are masked out before output.&lt;/p&gt;
&lt;p&gt;Note that there is no need to start a separate server for handling tokenized/untokenized sentences. The server can tell and handle both cases automatically.&lt;/p&gt;
&lt;p&gt;Sometimes you want to know explicitly the tokenization performed on the server side to have better understanding of the embedding result. One such case is asking word embedding from the server (with &lt;code&gt;-pooling_strategy NONE&lt;/code&gt;), one wants to tell which word is tokenized and which is unrecognized. You can get such information with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;enabling &lt;code&gt;-show_tokens_to_client&lt;/code&gt; on the server side;&lt;/li&gt;
&lt;li&gt;calling the server via &lt;code&gt;encode(..., show_tokens=True)&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, a basic usage like&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thisis it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;show_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;returns a tuple, where the first element is the embedding and the second is the tokenization result from the server:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;(array([[[ 0.        , -0.        ,  0.        , ...,  0.        , -0.        , -0.        ],
        [ 1.1100919 , -0.20474958,  0.9895898 , ...,  0.3873255  , -1.4093989 , -0.47620595],
        ..., -0.        , -0.        ]],

       [[ 0.        , -0.        ,  0.        , ...,  0.        , 0.        ,  0.        ],
        [ 0.6293478 , -0.4088499 ,  0.6022662 , ...,  0.41740108, 1.214456  ,  1.2532915 ],
        ..., 0.        ,  0.        ]]], dtype=float32),
         
          [['[CLS]', 'hello', 'world', '!', '[SEP]'], ['[CLS]', 'this', '##is', 'it', '[SEP]']])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When using your own tokenization, you may still want to check if the server respects your tokens. For example,&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thisis&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]], &lt;span class="pl-v"&gt;show_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;is_tokenized&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;returns:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;(array([[[ 0.        , -0.        ,  0.        , ...,  0.       ,  -0.        ,  0.        ],
        [ 1.1111546 , -0.56572634,  0.37183186, ...,  0.02397121,  -0.5445367 ,  1.1009651 ],
        ..., -0.        ,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,  -0.        ,  0.        ],
        [ 0.39262453,  0.3782491 ,  0.27096173, ...,  0.7122045 ,  -0.9874849 ,  0.9318679 ],
        ..., -0.        ,  0.        ]]], dtype=float32),
         
         [['[CLS]', 'hello', '[UNK]', '[SEP]'], ['[CLS]', '[UNK]', 'it', '[SEP]']])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One can observe that &lt;code&gt;world!&lt;/code&gt; and &lt;code&gt;thisis&lt;/code&gt; are not recognized on the server, hence they are set to &lt;code&gt;[UNK]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, beware that the pretrained BERT Chinese from Google is character-based, i.e. its vocabulary is made of single Chinese characters. Therefore it makes no sense if you use word-level segmentation algorithm to pre-process the data and feed to such model.&lt;/p&gt;
&lt;p&gt;Extremely curious readers may notice that the first row in the above example is all-zero even though the tokenization result includes &lt;code&gt;[CLS]&lt;/code&gt; (well done, detective!). The reason is that the tokenization result will &lt;strong&gt;always&lt;/strong&gt; includes &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[UNK]&lt;/code&gt; regardless the setting of &lt;code&gt;-mask_cls_sep&lt;/code&gt;. This could be useful when you want to align the tokens afterwards. Remember, &lt;code&gt;-mask_cls_sep&lt;/code&gt; only masks &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; out of the computation. It doesn't affect the tokenization algorithm.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-bertclient-with-tfdata-api" class="anchor" aria-hidden="true" href="#using-bertclient-with-tfdata-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example4.py"&gt;be found example4.py&lt;/a&gt;. There is also &lt;a href="https://github.com/hanxiao/bert-as-service/issues/29#issuecomment-442362241"&gt;an example in Keras&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;a href="https://www.tensorflow.org/guide/datasets" rel="nofollow"&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt; API enables you to build complex input pipelines from simple, reusable pieces. One can also use &lt;code&gt;BertClient&lt;/code&gt; to encode sentences on-the-fly and use the vectors in a downstream model. Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;batch_size &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;256&lt;/span&gt;
num_parallel_calls &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;4&lt;/span&gt;
num_clients &lt;span class="pl-k"&gt;=&lt;/span&gt; num_parallel_calls &lt;span class="pl-k"&gt;*&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; should be at least greater than `num_parallel_calls`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; start a pool of clients&lt;/span&gt;
bc_clients &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertClient(&lt;span class="pl-v"&gt;show_server_config&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;) &lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(num_clients)]


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;get_encodes&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;):
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; x is `batch_size` of lines, each of which is a json object&lt;/span&gt;
    samples &lt;span class="pl-k"&gt;=&lt;/span&gt; [json.loads(l) &lt;span class="pl-k"&gt;for&lt;/span&gt; l &lt;span class="pl-k"&gt;in&lt;/span&gt; x]
    text &lt;span class="pl-k"&gt;=&lt;/span&gt; [s[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;raw_text&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; samples]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; List[List[str]]&lt;/span&gt;
    labels &lt;span class="pl-k"&gt;=&lt;/span&gt; [s[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;label&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; samples]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; List[str]&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get a client from available clients&lt;/span&gt;
    bc_client &lt;span class="pl-k"&gt;=&lt;/span&gt; bc_clients.pop()
    features &lt;span class="pl-k"&gt;=&lt;/span&gt; bc_client.encode(text)
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; after use, put it back&lt;/span&gt;
    bc_clients.append(bc_client)
    &lt;span class="pl-k"&gt;return&lt;/span&gt; features, labels


ds &lt;span class="pl-k"&gt;=&lt;/span&gt; (tf.data.TextLineDataset(train_fp).batch(batch_size)
        .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: tf.py_func(get_encodes, [x], [tf.float32, tf.string]),  &lt;span class="pl-v"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_parallel_calls)
        .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: x, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;label&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: y})
        .make_one_shot_iterator().get_next())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The trick here is to start a pool of &lt;code&gt;BertClient&lt;/code&gt; and reuse them one by one. In this way, we can fully harness the power of &lt;code&gt;num_parallel_calls&lt;/code&gt; of &lt;code&gt;Dataset.map()&lt;/code&gt; API.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training-a-text-classifier-using-bert-features-and-tfestimator-api" class="anchor" aria-hidden="true" href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training a text classifier using BERT features and &lt;code&gt;tf.estimator&lt;/code&gt; API&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example5.py"&gt;be found example5.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Following the last example, we can easily extend it to a full classifier using &lt;code&gt;tf.estimator&lt;/code&gt; API. One only need minor change on the input function as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;estimator &lt;span class="pl-k"&gt;=&lt;/span&gt; DNNClassifier(
    &lt;span class="pl-v"&gt;hidden_units&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[&lt;span class="pl-c1"&gt;512&lt;/span&gt;],
    &lt;span class="pl-v"&gt;feature_columns&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[tf.feature_column.numeric_column(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;shape&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(&lt;span class="pl-c1"&gt;768&lt;/span&gt;,))],
    &lt;span class="pl-v"&gt;n_classes&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;len&lt;/span&gt;(laws),
    &lt;span class="pl-v"&gt;config&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;run_config,
    &lt;span class="pl-v"&gt;label_vocabulary&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;laws_str,
    &lt;span class="pl-v"&gt;dropout&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.1&lt;/span&gt;)

input_fn &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;fp&lt;/span&gt;: (tf.data.TextLineDataset(fp)
                       .apply(tf.contrib.data.shuffle_and_repeat(&lt;span class="pl-v"&gt;buffer_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10000&lt;/span&gt;))
                       .batch(batch_size)
                       .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: tf.py_func(get_encodes, [x], [tf.float32, tf.string]), &lt;span class="pl-v"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_parallel_calls)
                       .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: ({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: x}, y))
                       .prefetch(&lt;span class="pl-c1"&gt;20&lt;/span&gt;))

train_spec &lt;span class="pl-k"&gt;=&lt;/span&gt; TrainSpec(&lt;span class="pl-v"&gt;input_fn&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;lambda&lt;/span&gt;: input_fn(train_fp))
eval_spec &lt;span class="pl-k"&gt;=&lt;/span&gt; EvalSpec(&lt;span class="pl-v"&gt;input_fn&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;lambda&lt;/span&gt;: input_fn(eval_fp), &lt;span class="pl-v"&gt;throttle_secs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
train_and_evaluate(estimator, train_spec, eval_spec)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complete example can &lt;a href="example/example5.py"&gt;be found example5.py&lt;/a&gt;, in which a simple MLP is built on BERT features for predicting the relevant articles according to the fact description in the law documents. The problem is a part of the &lt;a href="https://github.com/thunlp/CAIL/blob/master/README_en.md"&gt;Chinese AI and Law Challenge Competition&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-saving-and-loading-with-tfrecord-data" class="anchor" aria-hidden="true" href="#saving-and-loading-with-tfrecord-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Saving and loading with TFRecord data&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example6.py"&gt;be found example6.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. You can also pre-encode all your sequences and store their encodings to a TFRecord file, then later load it to build a &lt;code&gt;tf.Dataset&lt;/code&gt;. For example, to write encoding into a TFRecord file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
list_vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(lst_str)
list_label &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; lst_str]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; a dummy list of all-zero labels&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; write to tfrecord&lt;/span&gt;
&lt;span class="pl-k"&gt;with&lt;/span&gt; tf.python_io.TFRecordWriter(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tmp.tfrecord&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; writer:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_float_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;float_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.FloatList(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;values))

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_int_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;int64_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.Int64List(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;list&lt;/span&gt;(values)))

    &lt;span class="pl-k"&gt;for&lt;/span&gt; (vec, label) &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;zip&lt;/span&gt;(list_vec, list_label):
        features &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;features&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: create_float_feature(vec), &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;labels&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: create_int_feature([label])}
        tf_example &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.train.Example(&lt;span class="pl-v"&gt;features&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.Features(&lt;span class="pl-v"&gt;feature&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;features))
        writer.write(tf_example.SerializeToString())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can load from it and build a &lt;code&gt;tf.Dataset&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_decode_record&lt;/span&gt;(&lt;span class="pl-smi"&gt;record&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Decodes a record to a TensorFlow example.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.parse_single_example(record, {
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;features&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([&lt;span class="pl-c1"&gt;768&lt;/span&gt;], tf.float32),
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;labels&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([], tf.int64),
    })

ds &lt;span class="pl-k"&gt;=&lt;/span&gt; (tf.data.TFRecordDataset(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tmp.tfrecord&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).repeat().shuffle(&lt;span class="pl-v"&gt;buffer_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;).apply(
    tf.contrib.data.map_and_batch(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;record&lt;/span&gt;: _decode_record(record), &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;))
      .make_one_shot_iterator().get_next())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To save word/token-level embedding to TFRecord, one needs to first flatten &lt;code&gt;[max_seq_len, num_hidden]&lt;/code&gt; tensor into an 1D array as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_float_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;float_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.FloatList(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;values.reshape(&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And later reconstruct the shape when loading it:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;name_to_features &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;feature&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([max_seq_length &lt;span class="pl-k"&gt;*&lt;/span&gt; num_hidden], tf.float32),
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;label_ids&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([], tf.int64),
}
    
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_decode_record&lt;/span&gt;(&lt;span class="pl-smi"&gt;record&lt;/span&gt;, &lt;span class="pl-smi"&gt;name_to_features&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Decodes a record to a TensorFlow example.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    example &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.parse_single_example(record, name_to_features)
    example[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.reshape(example[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [max_seq_length, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;])
    &lt;span class="pl-k"&gt;return&lt;/span&gt; example&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be careful, this will generate a huge TFRecord file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-asynchronous-encoding" class="anchor" aria-hidden="true" href="#asynchronous-encoding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Asynchronous encoding&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example2.py"&gt;be found example2.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;BertClient.encode()&lt;/code&gt; offers a nice synchronous way to get sentence encodes. However,   sometimes we want to do it in an asynchronous manner by feeding all textual data to the server first, fetching the encoded results later. This can be easily done by:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; an endless data stream, generating data in an extremely fast speed&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;text_gen&lt;/span&gt;():
    &lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
        &lt;span class="pl-k"&gt;yield&lt;/span&gt; lst_str  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; yield a batch of text lines&lt;/span&gt;

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get encoded vectors&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; bc.encode_async(text_gen(), &lt;span class="pl-v"&gt;max_num_batch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;received &lt;span class="pl-c1"&gt;%d&lt;/span&gt; x &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (j.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], j.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]))&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-broadcasting-to-multiple-clients" class="anchor" aria-hidden="true" href="#broadcasting-to-multiple-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Broadcasting to multiple clients&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example3.py"&gt;be found in example3.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The encoded result is routed to the client according to its identity. If you have multiple clients with same identity, then they all receive the results! You can use this &lt;em&gt;multicast&lt;/em&gt; feature to do some cool things, e.g. training multiple different models (some using &lt;code&gt;scikit-learn&lt;/code&gt; some using &lt;code&gt;tensorflow&lt;/code&gt;) in multiple separated processes while only call &lt;code&gt;BertServer&lt;/code&gt; once. In the example below, &lt;code&gt;bc&lt;/code&gt; and its two clones will all receive encoded vector.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; clone a client by reusing the identity &lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;client_clone&lt;/span&gt;(&lt;span class="pl-smi"&gt;id&lt;/span&gt;, &lt;span class="pl-smi"&gt;idx&lt;/span&gt;):
    bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;identity&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;id&lt;/span&gt;)
    &lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; bc.listen():
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;clone-client-&lt;span class="pl-c1"&gt;%d&lt;/span&gt;: received &lt;span class="pl-c1"&gt;%d&lt;/span&gt; x &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (idx, j.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], j.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]))

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; start two cloned clients sharing the same identity as bc&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;2&lt;/span&gt;):
    threading.Thread(&lt;span class="pl-v"&gt;target&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;client_clone, &lt;span class="pl-v"&gt;args&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(bc.identity, j)).start()

&lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;3&lt;/span&gt;):
    bc.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-monitoring-the-service-status-in-a-dashboard" class="anchor" aria-hidden="true" href="#monitoring-the-service-status-in-a-dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monitoring the service status in a dashboard&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="plugin/dashboard"&gt;be found in plugin/dashboard/&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a part of the infrastructure, one may also want to monitor the service status and show it in a dashboard. To do that, we can use:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;ip&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;server_ip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

json.dumps(bc.server_status, &lt;span class="pl-v"&gt;ensure_ascii&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives the current status of the server including number of requests, number of clients etc. in JSON format. The only thing remained is to start a HTTP server for returning this JSON to the frontend that renders it.&lt;/p&gt;
&lt;p&gt;Alternatively, one may simply expose an HTTP port when starting a server via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -http_port 8001 -model_dir ...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will allow one to use javascript or &lt;code&gt;curl&lt;/code&gt; to fetch the server status at port 8001.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plugin/dashboard/index.html&lt;/code&gt; shows a simple dashboard based on Bootstrap and Vue.js.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/dashboard.png?raw=true"&gt;&lt;img src=".github/dashboard.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-bert-as-service-to-serve-http-requests-in-json" class="anchor" aria-hidden="true" href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/h3&gt;
&lt;p&gt;Besides calling &lt;code&gt;bert-as-service&lt;/code&gt; from Python, one can also call it via HTTP request in JSON. It is quite useful especially when low transport layer is prohibited. Behind the scene, &lt;code&gt;bert-as-service&lt;/code&gt; spawns a Flask server in a separate process and then reuse a &lt;code&gt;BertClient&lt;/code&gt; instance as a proxy to communicate with the ventilator.&lt;/p&gt;
&lt;p&gt;To enable the build-in HTTP server, we need to first (re)install the server with some extra Python dependencies:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -U bert-serving-server[http]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then simply start the server with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir=/YOUR_MODEL -http_port 8125&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Done! Your server is now listening HTTP and TCP requests at port &lt;code&gt;8125&lt;/code&gt; simultaneously!&lt;/p&gt;
&lt;p&gt;To send a HTTP request, first prepare the payload in JSON as following:&lt;/p&gt;
&lt;div class="highlight highlight-source-json"&gt;&lt;pre&gt;{
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;123&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;texts&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;hello world&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;good day!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;is_tokenized&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;false&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;, where &lt;code&gt;id&lt;/code&gt; is a unique identifier helping you to synchronize the results; &lt;code&gt;is_tokenized&lt;/code&gt; follows the meaning in &lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/client.html#client.BertClient.encode_async" rel="nofollow"&gt;&lt;code&gt;BertClient&lt;/code&gt; API&lt;/a&gt; and &lt;code&gt;false&lt;/code&gt; by default.&lt;/p&gt;
&lt;p&gt;Then simply call the server at &lt;code&gt;/encode&lt;/code&gt; via HTTP POST request. You can use javascript or whatever, here is an example using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;curl -X POST http://xx.xx.xx.xx:8125/encode \
  -H &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;content-type: application/json&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; \
  -d &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;{"id": 123,"texts": ["hello world"], "is_tokenized": false}&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;, which returns a JSON:&lt;/p&gt;
&lt;div class="highlight highlight-source-json"&gt;&lt;pre&gt;{
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;123&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;results&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [[&lt;span class="pl-c1"&gt;768&lt;/span&gt; &lt;span class="pl-ii"&gt;float-list&lt;/span&gt;], [&lt;span class="pl-c1"&gt;768&lt;/span&gt; &lt;span class="pl-ii"&gt;float-list&lt;/span&gt;]],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;status&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;200&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To get the server's status and client's status, you can send GET requests at &lt;code&gt;/status/server&lt;/code&gt; and &lt;code&gt;/status/client&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Finally, one may also config CORS to restrict the public access of the server by specifying &lt;code&gt;-cors&lt;/code&gt; when starting &lt;code&gt;bert-serving-start&lt;/code&gt;. By default &lt;code&gt;-cors=*&lt;/code&gt;, meaning the server is public accessible.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-starting-bertserver-from-python" class="anchor" aria-hidden="true" href="#starting-bertserver-from-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/h3&gt;
&lt;p&gt;Besides shell, one can also start a &lt;code&gt;BertServer&lt;/code&gt; from python. Simply do&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.server.helper &lt;span class="pl-k"&gt;import&lt;/span&gt; get_args_parser
&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.server &lt;span class="pl-k"&gt;import&lt;/span&gt; BertServer
args &lt;span class="pl-k"&gt;=&lt;/span&gt; get_args_parser().parse_args([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-model_dir&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;YOUR_MODEL_PATH_HERE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-port&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;5555&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-port_out&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;5556&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-max_seq_len&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NONE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-mask_cls_sep&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-cpu&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
server &lt;span class="pl-k"&gt;=&lt;/span&gt; BertServer(args)
server.start()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that it's basically mirroring the arg-parsing behavior in CLI, so everything in that &lt;code&gt;.parse_args([])&lt;/code&gt; list should be string, e.g. &lt;code&gt;['-port', '5555']&lt;/code&gt; not &lt;code&gt;['-port', 5555]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To shutdown the server, you may call the static method in &lt;code&gt;BertServer&lt;/code&gt; class via:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;BertServer.shutdown(&lt;span class="pl-v"&gt;port&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;5555&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or via shell CLI:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-terminate -port 5555&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will terminate the server running on localhost at port 5555. You may also use it to terminate a remote server, see &lt;code&gt;bert-serving-terminate --help&lt;/code&gt; for details.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-speech_balloon-faq" class="anchor" aria-hidden="true" href="#speech_balloon-faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;&lt;/g-emoji&gt; FAQ&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt; Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/faq.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-you-have-a-paper-or-other-written-explanation-to-introduce-your-models-details" class="anchor" aria-hidden="true" href="#q-do-you-have-a-paper-or-other-written-explanation-to-introduce-your-models-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do you have a paper or other written explanation to introduce your model's details?&lt;/h5&gt;
&lt;p&gt;The design philosophy and technical details can be found &lt;a href="https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" rel="nofollow"&gt;in my blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-where-is-the-bert-code-come-from" class="anchor" aria-hidden="true" href="#q-where-is-the-bert-code-come-from"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is the BERT code come from?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; &lt;a href="server/bert_serving/server/bert/"&gt;BERT code of this repo&lt;/a&gt; is forked from the &lt;a href="https://github.com/google-research/bert"&gt;original BERT repo&lt;/a&gt; with necessary modification, &lt;a href="server/bert_serving/server/bert/extract_features.py"&gt;especially in extract_features.py&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-large-is-a-sentence-vector" class="anchor" aria-hidden="true" href="#q-how-large-is-a-sentence-vector"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How large is a sentence vector?&lt;/h5&gt;
&lt;p&gt;In general, each sentence is translated to a 768-dimensional vector. Depending on the pretrained BERT you are using, &lt;code&gt;pooling_strategy&lt;/code&gt; and &lt;code&gt;pooling_layer&lt;/code&gt; the dimensions of the output vector could be different.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-do-you-get-the-fixed-representation-did-you-do-pooling-or-something" class="anchor" aria-hidden="true" href="#q-how-do-you-get-the-fixed-representation-did-you-do-pooling-or-something"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How do you get the fixed representation? Did you do pooling or something?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, pooling is required to get a fixed representation of a sentence. In the default strategy &lt;code&gt;REDUCE_MEAN&lt;/code&gt;, I take the second-to-last hidden layer of all of the tokens in the sentence and do average pooling.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-are-you-suggesting-using-bert-without-fine-tuning" class="anchor" aria-hidden="true" href="#q-are-you-suggesting-using-bert-without-fine-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Are you suggesting using BERT without fine-tuning?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes and no. On the one hand, Google pretrained BERT on Wikipedia data, thus should encode enough prior knowledge of the language into the model. Having such feature is not a bad idea. On the other hand, these prior knowledge is not specific to any particular domain. It should be totally reasonable if the performance is not ideal if you are using it on, for example, classifying legal cases. Nonetheless, you can always first fine-tune your own BERT on the downstream task and then use &lt;code&gt;bert-as-service&lt;/code&gt; to extract the feature vectors efficiently. Keep in mind that &lt;code&gt;bert-as-service&lt;/code&gt; is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-get-a-concatenation-of-several-layers-instead-of-a-single-layer-" class="anchor" aria-hidden="true" href="#q-can-i-get-a-concatenation-of-several-layers-instead-of-a-single-layer-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I get a concatenation of several layers instead of a single layer ?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Sure! Just use a list of the layer you want to concatenate when calling the server. Example:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -pooling_layer -4 -3 -2 -1 -model_dir /tmp/english_L-12_H-768_A-12/&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-are-the-available-pooling-strategies" class="anchor" aria-hidden="true" href="#q-what-are-the-available-pooling-strategies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What are the available pooling strategies?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Here is a table summarizes all pooling strategies I implemented. Choose your favorite one by specifying &lt;code&gt;bert-serving-start -pooling_strategy&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Strategy&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;NONE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;no pooling at all, useful when you want to use word embedding instead of sentence embedding. This will results in a &lt;code&gt;[max_seq_len, 768]&lt;/code&gt; encode matrix for a sequence.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;take the average of the hidden state of encoding layer on the time axis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MAX&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;take the maximum of the hidden state of encoding layer on the time axis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN_MAX&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;do &lt;code&gt;REDUCE_MEAN&lt;/code&gt; and &lt;code&gt;REDUCE_MAX&lt;/code&gt; separately and then concat them together on the last axis, resulting in 1536-dim sentence encodes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;CLS_TOKEN&lt;/code&gt; or &lt;code&gt;FIRST_TOKEN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;get the hidden state corresponding to &lt;code&gt;[CLS]&lt;/code&gt;, i.e. the first token&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;SEP_TOKEN&lt;/code&gt; or &lt;code&gt;LAST_TOKEN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;get the hidden state corresponding to &lt;code&gt;[SEP]&lt;/code&gt;, i.e. the last token&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls" class="anchor" aria-hidden="true" href="#q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why not use the hidden state of the first token as default strategy, i.e. the &lt;code&gt;[CLS]&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Because a pre-trained model is not fine-tuned on any downstream tasks yet. In this case, the hidden state of &lt;code&gt;[CLS]&lt;/code&gt; is not a good sentence representation. If later you fine-tune the model, you may use &lt;code&gt;[CLS]&lt;/code&gt; as well.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-bert-has-1224-layers-so-which-layer-are-you-talking-about" class="anchor" aria-hidden="true" href="#q-bert-has-1224-layers-so-which-layer-are-you-talking-about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; BERT has 12/24 layers, so which layer are you talking about?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; By default this service works on the second last layer, i.e. &lt;code&gt;pooling_layer=-2&lt;/code&gt;. You can change it by setting &lt;code&gt;pooling_layer&lt;/code&gt; to other negative values, e.g. -1 corresponds to the last layer.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-not-the-last-hidden-layer-why-second-to-last" class="anchor" aria-hidden="true" href="#q-why-not-the-last-hidden-layer-why-second-to-last"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why not the last hidden layer? Why second-to-last?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets. If you question about this argument and want to use the last hidden layer anyway, please feel free to set &lt;code&gt;pooling_layer=-1&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-so-which-layer-and-which-pooling-strategy-is-the-best" class="anchor" aria-hidden="true" href="#q-so-which-layer-and-which-pooling-strategy-is-the-best"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; So which layer and which pooling strategy is the best?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It depends. Keep in mind that different BERT layers capture different information. To see that more clearly, here is a visualization on &lt;a href="https://www.kaggle.com/uciml/news-aggregator-dataset" rel="nofollow"&gt;UCI-News Aggregator Dataset&lt;/a&gt;, where I randomly sample 20K news titles; get sentence encodes from different layers and with different pooling strategies, finally reduce it to 2D via PCA (one can of course do t-SNE as well, but that's not my point). There are only four classes of the data, illustrated in red, blue, yellow and green. To reproduce the result, please run &lt;a href="example/example7.py"&gt;example7.py&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pool_mean.png?raw=true"&gt;&lt;img src=".github/pool_mean.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pool_max.png?raw=true"&gt;&lt;img src=".github/pool_max.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Intuitively, &lt;code&gt;pooling_layer=-1&lt;/code&gt; is close to the training output, so it may be biased to the training targets. If you don't fine tune the model, then this could lead to a bad representation. &lt;code&gt;pooling_layer=-12&lt;/code&gt; is close to the word embedding, may preserve the very original word information (with no fancy self-attention etc.). On the other hand, you may achieve the very same performance by simply using a word-embedding only. That said, anything in-between [-1, -12] is then a trade-off.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-could-i-use-other-pooling-techniques" class="anchor" aria-hidden="true" href="#q-could-i-use-other-pooling-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Could I use other pooling techniques?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; For sure. But if you introduce new &lt;code&gt;tf.variables&lt;/code&gt; to the graph, then you need to train those variables before using the model. You may also want to check &lt;a href="https://hanxiao.github.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/#pooling-block" rel="nofollow"&gt;some pooling techniques I mentioned in my blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-to-batch-the-data-before-encode" class="anchor" aria-hidden="true" href="#q-do-i-need-to-batch-the-data-before-encode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need to batch the data before &lt;code&gt;encode()&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;No, not at all. Just do &lt;code&gt;encode&lt;/code&gt; and let the server handles the rest. If the batch is too large, the server will do batching automatically and it is more efficient than doing it by yourself. No matter how many sentences you have, 10K or 100K, as long as you can hold it in client's memory, just send it to the server. Please also read &lt;a href="https://github.com/hanxiao/bert-as-service#speed-wrt-client_batch_size"&gt;the benchmark on the client batch size&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-start-multiple-clients-and-send-requests-to-one-server-simultaneously" class="anchor" aria-hidden="true" href="#q-can-i-start-multiple-clients-and-send-requests-to-one-server-simultaneously"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I start multiple clients and send requests to one server simultaneously?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes! That's the purpose of this repo. In fact you can start as many clients as you want. One server can handle all of them (given enough time).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-many-requests-can-one-service-handle-concurrently" class="anchor" aria-hidden="true" href="#q-how-many-requests-can-one-service-handle-concurrently"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How many requests can one service handle concurrently?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The maximum number of concurrent requests is determined by &lt;code&gt;num_worker&lt;/code&gt; in &lt;code&gt;bert-serving-start&lt;/code&gt;. If you a sending more than &lt;code&gt;num_worker&lt;/code&gt; requests concurrently, the new requests will be temporally stored in a queue until a free worker becomes available.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-so-one-request-means-one-sentence" class="anchor" aria-hidden="true" href="#q-so-one-request-means-one-sentence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; So one request means one sentence?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. One request means a list of sentences sent from a client. Think the size of a request as the batch size. A request may contain 256, 512 or 1024 sentences. The optimal size of a request is often determined empirically. One large request can certainly improve the GPU utilization, yet it also increases the overhead of transmission. You may run &lt;code&gt;python example/example1.py&lt;/code&gt; for a simple benchmark.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-about-the-speed-is-it-fast-enough-for-production" class="anchor" aria-hidden="true" href="#q-how-about-the-speed-is-it-fast-enough-for-production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How about the speed? Is it fast enough for production?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It highly depends on the &lt;code&gt;max_seq_len&lt;/code&gt; and the size of a request. On a single Tesla M40 24GB with &lt;code&gt;max_seq_len=40&lt;/code&gt;, you should get about 470 samples per second using a 12-layer BERT. In general, I'd suggest smaller &lt;code&gt;max_seq_len&lt;/code&gt; (25) and larger request size (512/1024).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-did-you-benchmark-the-efficiency" class="anchor" aria-hidden="true" href="#q-did-you-benchmark-the-efficiency"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Did you benchmark the efficiency?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. See &lt;a href="#zap-benchmark"&gt;Benchmark&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To reproduce the results, please run &lt;code&gt;bert-serving-benchmark&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-is-backend-based-on" class="anchor" aria-hidden="true" href="#q-what-is-backend-based-on"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is backend based on?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; &lt;a href="http://zeromq.org/" rel="nofollow"&gt;ZeroMQ&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-is-the-parallel-processing-model-behind-the-scene" class="anchor" aria-hidden="true" href="#q-what-is-the-parallel-processing-model-behind-the-scene"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is the parallel processing model behind the scene?&lt;/h5&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/bert-parallel-pipeline.png?raw=true"&gt;&lt;img src=".github/bert-parallel-pipeline.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-does-the-server-need-two-ports" class="anchor" aria-hidden="true" href="#q-why-does-the-server-need-two-ports"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why does the server need two ports?&lt;/h5&gt;
&lt;p&gt;One port is for pushing text data into the server, the other port is for publishing the encoded result to the client(s). In this way, we get rid of back-chatter, meaning that at every level recipients never talk back to senders. The overall message flow is strictly one-way, as depicted in the above figure. Killing back-chatter is essential to real scalability, allowing us to use &lt;code&gt;BertClient&lt;/code&gt; in an asynchronous way.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-tensorflow-on-the-client-side" class="anchor" aria-hidden="true" href="#q-do-i-need-tensorflow-on-the-client-side"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need Tensorflow on the client side?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. Think of &lt;code&gt;BertClient&lt;/code&gt; as a general feature extractor, whose output can be fed to &lt;em&gt;any&lt;/em&gt; ML models, e.g. &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;pytorch&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt;. The only file that client need is &lt;a href="service/client.py"&gt;&lt;code&gt;client.py&lt;/code&gt;&lt;/a&gt;. Copy this file to your project and import it, then you are ready to go.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-multilingual-bert-model-provided-by-google" class="anchor" aria-hidden="true" href="#q-can-i-use-multilingual-bert-model-provided-by-google"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use multilingual BERT model provided by Google?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-my-own-fine-tuned-bert-model" class="anchor" aria-hidden="true" href="#q-can-i-use-my-own-fine-tuned-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use my own fine-tuned BERT model?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. In fact, this is suggested. Make sure you have the following three items in &lt;code&gt;model_dir&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-run-it-in-python-2" class="anchor" aria-hidden="true" href="#q-can-i-run-it-in-python-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I run it in python 2?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Server side no, client side yes. This is based on the consideration that python 2.x might still be a major piece in some tech stack. Migrating the whole downstream stack to python 3 for supporting &lt;code&gt;bert-as-service&lt;/code&gt; can take quite some effort. On the other hand, setting up &lt;code&gt;BertServer&lt;/code&gt; is just a one-time thing, which can be even &lt;a href="#run-bert-service-on-nvidia-docker"&gt;run in a docker container&lt;/a&gt;. To ease the integration, we support python 2 on the client side so that you can directly use &lt;code&gt;BertClient&lt;/code&gt; as a part of your python 2 project, whereas the server side should always be hosted with python 3.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-to-do-segmentation-for-chinese" class="anchor" aria-hidden="true" href="#q-do-i-need-to-do-segmentation-for-chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need to do segmentation for Chinese?&lt;/h5&gt;
&lt;p&gt;No, if you are using &lt;a href="https://github.com/google-research/bert#pre-trained-models"&gt;the pretrained Chinese BERT released by Google&lt;/a&gt; you don't need word segmentation. As this Chinese BERT is character-based model. It won't recognize word/phrase even if you intentionally add space in-between. To see that more clearly, this is what the BERT model actually receives after tokenization:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hey you&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;whats up?&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;  &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;tokens: [CLS] hey you [SEP]
input_ids: 101 13153 8357 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] what ##s up ? [SEP]
input_ids: 101 9100 8118 8644 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS]     [SEP]
input_ids: 101 872 1962 720 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS]     [SEP]
input_ids: 101 2769 6820 1377 809 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That means the word embedding is actually the character embedding for Chinese-BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-my-english-word-is-tokenized-to-something" class="anchor" aria-hidden="true" href="#q-why-my-english-word-is-tokenized-to-something"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why my (English) word is tokenized to &lt;code&gt;##something&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;Because your word is out-of-vocabulary (OOV). The tokenizer from Google uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;input&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;unaffable&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
tokenizer_output &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;un&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;##aff&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;##able&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-my-own-tokenizer" class="anchor" aria-hidden="true" href="#q-can-i-use-my-own-tokenizer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use my own tokenizer?&lt;/h5&gt;
&lt;p&gt;Yes. If you already tokenize the sentence on your own, simply send use &lt;code&gt;encode&lt;/code&gt; with &lt;code&gt;List[List[Str]]&lt;/code&gt; as input and turn on &lt;code&gt;is_tokenized&lt;/code&gt;, i.e. &lt;code&gt;bc.encode(texts, is_tokenized=True)&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-i-encounter-zmqerrorzmqerror-operation-cannot-be-accomplished-in-current-state-when-using-bertclient-what-should-i-do" class="anchor" aria-hidden="true" href="#q-i-encounter-zmqerrorzmqerror-operation-cannot-be-accomplished-in-current-state-when-using-bertclient-what-should-i-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; I encounter &lt;code&gt;zmq.error.ZMQError: Operation cannot be accomplished in current state&lt;/code&gt; when using &lt;code&gt;BertClient&lt;/code&gt;, what should I do?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is often due to the misuse of &lt;code&gt;BertClient&lt;/code&gt; in multi-thread/process environment. Note that you cant reuse one &lt;code&gt;BertClient&lt;/code&gt; among multiple threads/processes, you have to make a separate instance for each thread/process. For example, the following won't work at all:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; BAD example&lt;/span&gt;
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc1/Thread1 scope:&lt;/span&gt;
bc.encode(lst_str)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc2/Thread2 scope:&lt;/span&gt;
bc.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, please do:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc1/Thread1 scope:&lt;/span&gt;
bc1 &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc1.encode(lst_str)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc2/Thread2 scope:&lt;/span&gt;
bc2 &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc2.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-" class="anchor" aria-hidden="true" href="#q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; After running the server, I have several garbage &lt;code&gt;tmpXXXX&lt;/code&gt; folders. How can I change this behavior ?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; These folders are used by ZeroMQ to store sockets. You can choose a different location by setting the environment variable &lt;code&gt;ZEROMQ_SOCK_TMP_DIR&lt;/code&gt; :
&lt;code&gt;export ZEROMQ_SOCK_TMP_DIR=/tmp/&lt;/code&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong" class="anchor" aria-hidden="true" href="#q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; The cosine similarity of two sentence vectors is unreasonably high (e.g. always &amp;gt; 0.8), what's wrong?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; A decent representation for a downstream task doesn't mean that it will be meaningful in terms of cosine distance. Since cosine distance is a linear space where all dimensions are weighted equally. if you want to use cosine distance anyway, then please focus on the rank not the absolute value. Namely, do not use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if cosine(A, B) &amp;gt; 0.9, then A and B are similar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please consider the following instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if cosine(A, B) &amp;gt; cosine(A, C), then A is more similar to B than C.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The graph below illustrates the pairwise similarity of 3000 Chinese sentences randomly sampled from web (char. length &amp;lt; 25). We compute cosine similarity based on the sentence vectors and &lt;a href="https://en.wikipedia.org/wiki/ROUGE_(metric)" rel="nofollow"&gt;Rouge-L&lt;/a&gt; based on the raw text. The diagonal (self-correlation) is removed for the sake of clarity. As one can see, there is some positive correlation between these two metrics.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/cosine-vs-rougel.png?raw=true"&gt;&lt;img src=".github/cosine-vs-rougel.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;   
&lt;h5&gt;&lt;a id="user-content-q-im-getting-bad-performance-what-should-i-do" class="anchor" aria-hidden="true" href="#q-im-getting-bad-performance-what-should-i-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; I'm getting bad performance, what should I do?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This often suggests that the pretrained BERT could not generate a descent representation of your downstream task. Thus, you can fine-tune the model on the downstream task and then use &lt;code&gt;bert-as-service&lt;/code&gt; to serve the fine-tuned BERT. Note that, &lt;code&gt;bert-as-service&lt;/code&gt; is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-run-the-server-side-on-cpu-only-machine" class="anchor" aria-hidden="true" href="#q-can-i-run-the-server-side-on-cpu-only-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I run the server side on CPU-only machine?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, please run &lt;code&gt;bert-serving-start -cpu -max_batch_size 16&lt;/code&gt;. Note that, CPU does not scale as good as GPU on large batches, therefore the &lt;code&gt;max_batch_size&lt;/code&gt; on the server side needs to be smaller, e.g. 16 or 32.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-can-i-choose-num_worker" class="anchor" aria-hidden="true" href="#q-how-can-i-choose-num_worker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How can I choose &lt;code&gt;num_worker&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Generally, the number of workers should be less than or equal to the number of GPU/CPU you have. Otherwise, multiple workers will be allocated to one GPU/CPU, which may not scale well (and may cause out-of-memory on GPU).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-specify-which-gpu-to-use" class="anchor" aria-hidden="true" href="#q-can-i-specify-which-gpu-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I specify which GPU to use?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, you can specifying &lt;code&gt;-device_map&lt;/code&gt; as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -device_map 0 1 4 -num_worker 4 -model_dir ...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will start four workers and allocate them to GPU0, GPU1, GPU4 and again GPU0, respectively. In general, if &lt;code&gt;num_worker&lt;/code&gt; &amp;gt; &lt;code&gt;device_map&lt;/code&gt;, then devices will be reused and shared by the workers (may scale suboptimally or cause OOM); if &lt;code&gt;num_worker&lt;/code&gt; &amp;lt; &lt;code&gt;device_map&lt;/code&gt;, only &lt;code&gt;device_map[:num_worker]&lt;/code&gt; will be used.&lt;/p&gt;
&lt;p&gt;Note, &lt;code&gt;device_map&lt;/code&gt; is ignored when running on CPU.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-zap-benchmark" class="anchor" aria-hidden="true" href="#zap-benchmark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="zap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png"&gt;&lt;/g-emoji&gt; Benchmark&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt; Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/benchmark.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The primary goal of benchmarking is to test the scalability and the speed of this service, which is crucial for using it in a dev/prod environment. Benchmark was done on Tesla M40 24GB, experiments were repeated 10 times and the average value is reported.&lt;/p&gt;
&lt;p&gt;To reproduce the results, please run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-benchmark --help&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Common arguments across all experiments are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;num_worker&lt;/td&gt;
&lt;td&gt;1,2,4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max_seq_len&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;client_batch_size&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max_batch_size&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;num_client&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-max_seq_len" class="anchor" aria-hidden="true" href="#speed-wrt-max_seq_len"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;max_seq_len&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;max_seq_len&lt;/code&gt; is a parameter on the server side, which controls the maximum length of a sequence that a BERT model can handle. Sequences larger than &lt;code&gt;max_seq_len&lt;/code&gt; will be truncated on the left side. Thus, if your client want to send long sequences to the model, please make sure the server can handle them correctly.&lt;/p&gt;
&lt;p&gt;Performance-wise, longer sequences means slower speed and  more chance of OOM, as the multi-head self-attention (the core unit of BERT) needs to do dot products and matrix multiplications between every two symbols in the sequence.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/max_seq_len.png?raw=true"&gt;&lt;img src=".github/max_seq_len.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;max_seq_len&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;903&lt;/td&gt;
&lt;td&gt;1774&lt;/td&gt;
&lt;td&gt;3254&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;td&gt;231&lt;/td&gt;
&lt;td&gt;435&lt;/td&gt;
&lt;td&gt;768&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;td&gt;119&lt;/td&gt;
&lt;td&gt;237&lt;/td&gt;
&lt;td&gt;464&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;108&lt;/td&gt;
&lt;td&gt;212&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-client_batch_size" class="anchor" aria-hidden="true" href="#speed-wrt-client_batch_size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;client_batch_size&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;client_batch_size&lt;/code&gt; is the number of sequences from a client when invoking &lt;code&gt;encode()&lt;/code&gt;. For performance reason, please consider encoding sequences in batch rather than encoding them one by one.&lt;/p&gt;
&lt;p&gt;For example, do:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; prepare your sent in advance&lt;/span&gt;
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
my_sentences &lt;span class="pl-k"&gt;=&lt;/span&gt; [s &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; my_corpus.iter()]
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; doing encoding in one-shot&lt;/span&gt;
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(my_sentences)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;DON'T:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; []
&lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; my_corpus.iter():
    vec.append(bc.encode(s))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It's even worse if you put &lt;code&gt;BertClient()&lt;/code&gt; inside the loop. Don't do that.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/client_batch_size.png?raw=true"&gt;&lt;img src=".github/client_batch_size.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;client_batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;74&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;206&lt;/td&gt;
&lt;td&gt;205&lt;/td&gt;
&lt;td&gt;201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;274&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;267&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;332&lt;/td&gt;
&lt;td&gt;329&lt;/td&gt;
&lt;td&gt;330&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;382&lt;/td&gt;
&lt;td&gt;383&lt;/td&gt;
&lt;td&gt;383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;432&lt;/td&gt;
&lt;td&gt;766&lt;/td&gt;
&lt;td&gt;762&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;862&lt;/td&gt;
&lt;td&gt;1517&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;917&lt;/td&gt;
&lt;td&gt;1681&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4096&lt;/td&gt;
&lt;td&gt;481&lt;/td&gt;
&lt;td&gt;943&lt;/td&gt;
&lt;td&gt;1809&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-num_client" class="anchor" aria-hidden="true" href="#speed-wrt-num_client"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;num_client&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;num_client&lt;/code&gt; represents the number of concurrent clients connected to the server at the same time.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/num_clients.png?raw=true"&gt;&lt;img src=".github/num_clients.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;num_client&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1759&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;261&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;1028&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;133&lt;/td&gt;
&lt;td&gt;267&lt;/td&gt;
&lt;td&gt;533&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As one can observe, 1 clients 1 GPU = 381 seqs/s, 2 clients 2 GPU 402 seqs/s, 4 clients 4 GPU 413 seqs/s. This shows the efficiency of our parallel pipeline and job scheduling, as the service can leverage the GPU time  more exhaustively as concurrent requests increase.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-max_batch_size" class="anchor" aria-hidden="true" href="#speed-wrt-max_batch_size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;max_batch_size&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;max_batch_size&lt;/code&gt; is a parameter on the server side, which controls the maximum number of samples per batch per worker. If a incoming batch from client is larger than &lt;code&gt;max_batch_size&lt;/code&gt;, the server will split it into small batches so that each of them is less or equal than &lt;code&gt;max_batch_size&lt;/code&gt; before sending it to workers.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/max_batch_size.png?raw=true"&gt;&lt;img src=".github/max_batch_size.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;max_batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;450&lt;/td&gt;
&lt;td&gt;887&lt;/td&gt;
&lt;td&gt;1726&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;897&lt;/td&gt;
&lt;td&gt;1759&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;931&lt;/td&gt;
&lt;td&gt;1816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1688&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;464&lt;/td&gt;
&lt;td&gt;866&lt;/td&gt;
&lt;td&gt;1483&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-pooling_layer" class="anchor" aria-hidden="true" href="#speed-wrt-pooling_layer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;pooling_layer&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;pooling_layer&lt;/code&gt; determines the encoding layer that pooling operates on. For example, in a 12-layer BERT model, &lt;code&gt;-1&lt;/code&gt; represents the layer closed to the output, &lt;code&gt;-12&lt;/code&gt; represents the layer closed to the embedding layer. As one can observe below, the depth of the pooling layer affects the speed.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pooling_layer.png?raw=true"&gt;&lt;img src=".github/pooling_layer.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;pooling_layer&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[-1]&lt;/td&gt;
&lt;td&gt;438&lt;/td&gt;
&lt;td&gt;844&lt;/td&gt;
&lt;td&gt;1568&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-2]&lt;/td&gt;
&lt;td&gt;475&lt;/td&gt;
&lt;td&gt;916&lt;/td&gt;
&lt;td&gt;1686&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-3]&lt;/td&gt;
&lt;td&gt;516&lt;/td&gt;
&lt;td&gt;995&lt;/td&gt;
&lt;td&gt;1823&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-4]&lt;/td&gt;
&lt;td&gt;569&lt;/td&gt;
&lt;td&gt;1076&lt;/td&gt;
&lt;td&gt;1986&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-5]&lt;/td&gt;
&lt;td&gt;633&lt;/td&gt;
&lt;td&gt;1193&lt;/td&gt;
&lt;td&gt;2184&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-6]&lt;/td&gt;
&lt;td&gt;711&lt;/td&gt;
&lt;td&gt;1340&lt;/td&gt;
&lt;td&gt;2430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-7]&lt;/td&gt;
&lt;td&gt;820&lt;/td&gt;
&lt;td&gt;1528&lt;/td&gt;
&lt;td&gt;2729&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-8]&lt;/td&gt;
&lt;td&gt;945&lt;/td&gt;
&lt;td&gt;1772&lt;/td&gt;
&lt;td&gt;3104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-9]&lt;/td&gt;
&lt;td&gt;1128&lt;/td&gt;
&lt;td&gt;2047&lt;/td&gt;
&lt;td&gt;3622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-10]&lt;/td&gt;
&lt;td&gt;1392&lt;/td&gt;
&lt;td&gt;2542&lt;/td&gt;
&lt;td&gt;4241&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-11]&lt;/td&gt;
&lt;td&gt;1523&lt;/td&gt;
&lt;td&gt;2737&lt;/td&gt;
&lt;td&gt;4752&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-12]&lt;/td&gt;
&lt;td&gt;1568&lt;/td&gt;
&lt;td&gt;2985&lt;/td&gt;
&lt;td&gt;5303&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt--fp16-and--xla" class="anchor" aria-hidden="true" href="#speed-wrt--fp16-and--xla"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;-fp16&lt;/code&gt; and &lt;code&gt;-xla&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;bert-as-service&lt;/code&gt; supports two additional optimizations: half-precision and XLA, which can be turned on by adding &lt;code&gt;-fp16&lt;/code&gt; and &lt;code&gt;-xla&lt;/code&gt; to &lt;code&gt;bert-serving-start&lt;/code&gt;, respectively. To enable these two options, you have to meet the following requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;your GPU supports FP16 instructions;&lt;/li&gt;
&lt;li&gt;your Tensorflow is self-compiled with XLA and &lt;code&gt;-march=native&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;your CUDA and cudnn are not too old.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On Tesla V100 with &lt;code&gt;tensorflow=1.13.0-rc0&lt;/code&gt; it gives:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/fp16-xla.svg"&gt;&lt;img src=".github/fp16-xla.svg" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FP16 achieves ~1.4x speedup (round-trip) comparing to the FP32 counterpart. To reproduce the result, please run &lt;code&gt;python example/example1.py&lt;/code&gt;.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt; Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use bert-as-service in a scientific publication, we would appreciate references to the following BibTex entry:&lt;/p&gt;
&lt;div class="highlight highlight-text-tex-latex"&gt;&lt;pre&gt;@misc{xiao2018bertservice,
  title={bert-as-service},
  author={Xiao, Han},
  howpublished={&lt;span class="pl-c1"&gt;\url&lt;/span&gt;{https://github.com/hanxiao/bert-as-service}},
  year={2018}
}&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hanxiao</author><guid isPermaLink="false">https://github.com/hanxiao/bert-as-service</guid><pubDate>Tue, 05 Nov 2019 00:18:00 GMT</pubDate></item><item><title>ansible/ansible #19 in Python, This week</title><link>https://github.com/ansible/ansible</link><description>&lt;p&gt;&lt;i&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications  automate in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com/ansible/&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible" rel="nofollow"&gt;&lt;img alt="PyPI version" src="https://camo.githubusercontent.com/1700ed8e65665052f4e72ba6ae9e1f1d7fddc6c6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f616e7369626c652e737667" data-canonical-src="https://img.shields.io/pypi/v/ansible.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/" rel="nofollow"&gt;&lt;img alt="Docs badge" src="https://camo.githubusercontent.com/dc37b81ae5ef1245837ee1f1547892e8345ccd4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/docs-latest-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;&lt;img alt="Chat badge" src="https://camo.githubusercontent.com/a36ab54aea33fa40f9d063c8804b9bbf1b6fbd47/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d4952432d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/chat-IRC-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://app.shippable.com/projects/573f79d02a8192902e20e34b" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/c4dd185960fb101604717a4c8965ac9ba2725e69/68747470733a2f2f6170692e736869707061626c652e636f6d2f70726f6a656374732f3537336637396430326138313932393032653230653334622f62616467653f6272616e63683d646576656c" data-canonical-src="https://api.shippable.com/projects/573f79d02a8192902e20e34b/badge?branch=devel" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/code_of_conduct.html" rel="nofollow"&gt;&lt;img alt="Ansible Code of Conduct" src="https://camo.githubusercontent.com/412f4c0b8d7289e25a69d8568bd02c1bf976f9fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532306f66253230636f6e647563742d416e7369626c652d73696c7665722e737667" data-canonical-src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information" rel="nofollow"&gt;&lt;img alt="Ansible mailing lists" src="https://camo.githubusercontent.com/74dd4958c493abf9d3105cbcd020e55aa5df90c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696c696e672532306c697374732d416e7369626c652d6f72616e67652e737667" data-canonical-src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="COPYING"&gt;&lt;img alt="Repository License" src="https://camo.githubusercontent.com/0ac7552afd56fbe0c2ce6722d54f68857aa92b82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c25323076332e302d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-ansible"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-ansible" class="anchor" aria-hidden="true" href="#ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ansible&lt;/h2&gt;
&lt;p&gt;Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on &lt;a href="https://ansible.com/" rel="nofollow"&gt;the Ansible website&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-design-principles"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-design-principles" class="anchor" aria-hidden="true" href="#design-principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Design Principles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Have a dead simple setup process and a minimal learning curve.&lt;/li&gt;
&lt;li&gt;Manage machines very quickly and in parallel.&lt;/li&gt;
&lt;li&gt;Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.&lt;/li&gt;
&lt;li&gt;Describe infrastructure in a language that is both machine and human
friendly.&lt;/li&gt;
&lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt;
&lt;li&gt;Manage new remote machines instantly, without bootstrapping any
software.&lt;/li&gt;
&lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt;
&lt;li&gt;Be usable as non-root.&lt;/li&gt;
&lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-use-ansible"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-use-ansible" class="anchor" aria-hidden="true" href="#use-ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use Ansible&lt;/h3&gt;
&lt;p&gt;You can install a released version of Ansible via &lt;code&gt;pip&lt;/code&gt;, a package manager, or
our &lt;a href="https://releases.ansible.com/ansible/" rel="nofollow"&gt;release repository&lt;/a&gt;. See our
&lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html" rel="nofollow"&gt;installation guide&lt;/a&gt; for details on installing Ansible
on a variety of platforms.&lt;/p&gt;
&lt;p&gt;Red Hat offers supported builds of &lt;a href="https://www.ansible.com/ansible-engine" rel="nofollow"&gt;Ansible Engine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved
in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt;
&lt;a name="user-content-get-involved"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-get-involved" class="anchor" aria-hidden="true" href="#get-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Involved&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/latest/community" rel="nofollow"&gt;Community
Information&lt;/a&gt; for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.&lt;/li&gt;
&lt;li&gt;Join a &lt;a href="https://github.com/ansible/community/wiki"&gt;Working Group&lt;/a&gt;, an organized community devoted to a specific technology domain or platform.&lt;/li&gt;
&lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt;
&lt;li&gt;Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.&lt;/li&gt;
&lt;li&gt;For a list of email lists, IRC channels and Working Groups, see the
&lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;Communication page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-branch-info"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-branch-info" class="anchor" aria-hidden="true" href="#branch-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Branch Info&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt;
&lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.html#common-environment-setup" rel="nofollow"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt;
&lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/release_and_maintenance.html" rel="nofollow"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-roadmap"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-roadmap" class="anchor" aria-hidden="true" href="#roadmap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Roadmap&lt;/h3&gt;
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/" rel="nofollow"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt;
&lt;a name="user-content-authors"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h3&gt;
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt;
and has contributions from over 4600 users (and growing). Thanks everyone!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ansible.com" rel="nofollow"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com" rel="nofollow"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-license"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;GNU General Public License v3.0&lt;/p&gt;
&lt;p&gt;See &lt;a href="COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>ansible</author><guid isPermaLink="false">https://github.com/ansible/ansible</guid><pubDate>Tue, 05 Nov 2019 00:19:00 GMT</pubDate></item><item><title>psf/black #20 in Python, This week</title><link>https://github.com/psf/black</link><description>&lt;p&gt;&lt;i&gt;The uncompromising Python code formatter&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png"&gt;&lt;img src="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png" alt="Black Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-the-uncompromising-code-formatter" class="anchor" aria-hidden="true" href="#the-uncompromising-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Uncompromising Code Formatter&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;a href="https://travis-ci.com/psf/black" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/0d62c6ce125db151bb0bc13cbc834c0d0522ed88/68747470733a2f2f7472617669732d63692e636f6d2f7073662f626c61636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/psf/black.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://black.readthedocs.io/en/stable/?badge=stable" rel="nofollow"&gt;&lt;img alt="Documentation Status" src="https://camo.githubusercontent.com/ab197284ad0cd9cae552157b75f815b640eba827/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626c61636b2f62616467652f3f76657273696f6e3d737461626c65" data-canonical-src="https://readthedocs.org/projects/black/badge/?version=stable" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/psf/black?branch=master" rel="nofollow"&gt;&lt;img alt="Coverage Status" src="https://camo.githubusercontent.com/b77842b75f031fbf9bbf690eb55585ae55a8321d/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7073662f626c61636b2f62616467652e7376673f6272616e63683d6d6173746572" data-canonical-src="https://coveralls.io/repos/github/psf/black/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black/blob/master/LICENSE"&gt;&lt;img alt="License: MIT" src="https://camo.githubusercontent.com/14a9abb7e83098f2949f26d2190e04fb1bd52c06/68747470733a2f2f626c61636b2e72656164746865646f63732e696f2f656e2f737461626c652f5f7374617469632f6c6963656e73652e737667" data-canonical-src="https://black.readthedocs.io/en/stable/_static/license.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/black/" rel="nofollow"&gt;&lt;img alt="PyPI" src="https://camo.githubusercontent.com/5f6551edb3716d9b0a5659caf441d987bb1527a6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626c61636b" data-canonical-src="https://img.shields.io/pypi/v/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/black" rel="nofollow"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/7b3026c6e1fecc9574cb4b93a3925e392bee087d/68747470733a2f2f706570792e746563682f62616467652f626c61636b" data-canonical-src="https://pepy.tech/badge/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Any color you like.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed,
determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time
and mental energy for more important matters.&lt;/p&gt;
&lt;p&gt;Blackened code looks the same regardless of the project you're reading. Formatting
becomes transparent after a while and you can focus on the content instead.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt;
&lt;p&gt;Try it out now using the &lt;a href="https://black.now.sh" rel="nofollow"&gt;Black Playground&lt;/a&gt;. Watch the
&lt;a href="https://youtu.be/esZLCuWs_2Y" rel="nofollow"&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Contents:&lt;/em&gt; &lt;strong&gt;&lt;a href="#installation-and-usage"&gt;Installation and usage&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#the-black-code-style"&gt;Code style&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#pyprojecttoml"&gt;pyproject.toml&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#editor-integration"&gt;Editor integration&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#blackd"&gt;blackd&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#version-control-integration"&gt;Version control integration&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#ignoring-unmodified-files"&gt;Ignoring unmodified files&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#used-by"&gt;Used by&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#testimonials"&gt;Testimonials&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#show-your-style"&gt;Show your style&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#contributing-to-black"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#change-log"&gt;Change Log&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.6.0+ to
run but you can reformat Python 2 code with it, too.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-command-line-options" class="anchor" aria-hidden="true" href="#command-line-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command line options&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; doesn't provide many options. You can list them by running &lt;code&gt;black --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;black [OPTIONS] [SRC]...

Options:
  -c, --code TEXT                 Format the code passed in as a string.
  -l, --line-length INTEGER       How many characters per line to allow.
                                  [default: 88]
  -t, --target-version [py27|py33|py34|py35|py36|py37|py38]
                                  Python versions that should be supported by
                                  Black's output. [default: per-file auto-
                                  detection]
  --py36                          Allow using Python 3.6-only syntax on all
                                  input files.  This will put trailing commas
                                  in function signatures and calls also after
                                  *args and **kwargs. Deprecated; use
                                  --target-version instead. [default: per-file
                                  auto-detection]
  --pyi                           Format all input files like typing stubs
                                  regardless of file extension (useful when
                                  piping source on standard input).
  -S, --skip-string-normalization
                                  Don't normalize string quotes or prefixes.
  --check                         Don't write the files back, just return the
                                  status.  Return code 0 means nothing would
                                  change.  Return code 1 means some files
                                  would be reformatted.  Return code 123 means
                                  there was an internal error.
  --diff                          Don't write the files back, just output a
                                  diff for each file on stdout.
  --fast / --safe                 If --fast given, skip temporary sanity
                                  checks. [default: --safe]
  --include TEXT                  A regular expression that matches files and
                                  directories that should be included on
                                  recursive searches.  An empty value means
                                  all files are included regardless of the
                                  name.  Use forward slashes for directories
                                  on all platforms (Windows, too).  Exclusions
                                  are calculated first, inclusions later.
                                  [default: \.pyi?$]
  --exclude TEXT                  A regular expression that matches files and
                                  directories that should be excluded on
                                  recursive searches.  An empty value means no
                                  paths are excluded. Use forward slashes for
                                  directories on all platforms (Windows, too).
                                  Exclusions are calculated first, inclusions
                                  later.  [default: /(\.eggs|\.git|\.hg|\.mypy
                                  _cache|\.nox|\.tox|\.venv|_build|buck-
                                  out|build|dist)/]
  -q, --quiet                     Don't emit non-error messages to stderr.
                                  Errors are still emitted, silence those with
                                  2&amp;gt;/dev/null.
  -v, --verbose                   Also emit messages to stderr about files
                                  that were not changed or were ignored due to
                                  --exclude=.
  --version                       Show the version and exit.
  --config PATH                   Read configuration from PATH.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a well-behaved Unix-style command-line tool:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it does nothing if no sources are passed to it;&lt;/li&gt;
&lt;li&gt;it will read from standard input and write to standard output if &lt;code&gt;-&lt;/code&gt; is used as the
filename;&lt;/li&gt;
&lt;li&gt;it only outputs messages to users on standard error;&lt;/li&gt;
&lt;li&gt;exits with code 0 unless an internal error occurred (or &lt;code&gt;--check&lt;/code&gt; was used).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-note-this-is-a-beta-product" class="anchor" aria-hidden="true" href="#note-this-is-a-beta-product"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NOTE: This is a beta product&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href="#used-by"&gt;successfully used&lt;/a&gt; by many projects, small and big. It
also sports a decent test suite. However, it is still very new. Things will probably be
wonky for a while. This is made explicit by the "Beta" trove classifier, as well as by
the "b" in the version number. What this means for you is that &lt;strong&gt;until the formatter
becomes stable, you should expect some formatting to change in the future&lt;/strong&gt;. That being
said, no drastic stylistic changes are planned, mostly responses to bug reports.&lt;/p&gt;
&lt;p&gt;Also, as a temporary safety measure, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still
produces a valid AST that is equivalent to the original. This slows it down. If you're
feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-black-code-style" class="anchor" aria-hidden="true" href="#the-black-code-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; reformats entire files in place. It is not configurable. It doesn't take
previous formatting into account. It doesn't reformat blocks that start with
&lt;code&gt;# fmt: off&lt;/code&gt; and end with &lt;code&gt;# fmt: on&lt;/code&gt;. &lt;code&gt;# fmt: on/off&lt;/code&gt; have to be on the same level of
indentation. It also recognizes &lt;a href="https://github.com/google/yapf"&gt;YAPF&lt;/a&gt;'s block comments
to the same effect, as a courtesy for straddling code.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-black-wraps-lines" class="anchor" aria-hidden="true" href="#how-black-wraps-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How &lt;em&gt;Black&lt;/em&gt; wraps lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; ignores previous formatting and applies uniform horizontal and vertical
whitespace to your code. The rules for horizontal whitespace can be summarized as: do
whatever makes &lt;code&gt;pycodestyle&lt;/code&gt; happy. The coding style used by &lt;em&gt;Black&lt;/em&gt; can be viewed as a
strict subset of PEP 8.&lt;/p&gt;
&lt;p&gt;As for vertical whitespace, &lt;em&gt;Black&lt;/em&gt; tries to render one full expression or simple
statement per line. If this fits the allotted line length, great.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;2&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;3&lt;/span&gt;,
]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If not, &lt;em&gt;Black&lt;/em&gt; will look at the contents of the first outer matching brackets and put
that in a separate indented line.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

ImportantClass.important_method(exc, limit, lookup_lines, capture_locals, extra_argument)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

ImportantClass.important_method(
    exc, limit, lookup_lines, capture_locals, extra_argument
)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If that still doesn't fit the bill, it will decompose the internal expression further
using the same rule, indenting matching brackets every time. If the contents of the
matching brackets pair are comma-separated (like an argument list, or a dict literal,
and so on) then &lt;em&gt;Black&lt;/em&gt; will first try to keep them on the same line with the matching
brackets. If that doesn't work, it will put all of them in separate lines.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(&lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;, &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike, &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;w&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(
    &lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike,
    &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;,
):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;w&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You might have noticed that closing brackets are always dedented and that a trailing
comma is always added. Such formatting produces smaller diffs; when you add or remove an
element, it's always just one line. Also, having the closing bracket dedented provides a
clear delimiter between two distinct sections of the code that otherwise share the same
indentation level (like the arguments list and the docstring in the example above).&lt;/p&gt;
&lt;p&gt;If a data structure literal (tuple, list, set, dict) or a line of "from" imports cannot
fit in the allotted length, it's always split into one element per line. This minimizes
diffs as well as enables readers of code to find which commit introduced a particular
entry. This also makes &lt;em&gt;Black&lt;/em&gt; compatible with &lt;a href="https://pypi.org/p/isort/" rel="nofollow"&gt;isort&lt;/a&gt; with
the following configuration.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;A compatible `.isort.cfg`&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;[settings]
multi_line_output=3
include_trailing_comma=True
force_grid_wrap=0
use_parentheses=True
line_length=88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equivalent command line is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ isort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88 [ file.py ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-line-length" class="anchor" aria-hidden="true" href="#line-length"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line length&lt;/h3&gt;
&lt;p&gt;You probably noticed the peculiar default line length. &lt;em&gt;Black&lt;/em&gt; defaults to 88 characters
per line, which happens to be 10% over 80. This number was found to produce
significantly shorter files than sticking with 80 (the most popular), or even 79 (used
by the standard library). In general,
&lt;a href="https://youtu.be/wf-BqAjZb8M?t=260" rel="nofollow"&gt;90-ish seems like the wise choice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're paid by the line of code you write, you can pass &lt;code&gt;--line-length&lt;/code&gt; with a lower
number. &lt;em&gt;Black&lt;/em&gt; will try to respect that. However, sometimes it won't be able to without
breaking other rules. In those rare cases, auto-formatted code will exceed your allotted
limit.&lt;/p&gt;
&lt;p&gt;You can also increase it, but remember that people with sight disabilities find it
harder to work with line lengths exceeding 100 characters. It also adversely affects
side-by-side diff review on typical screen resolutions. Long lines also make it harder
to present code neatly in documentation or talk slides.&lt;/p&gt;
&lt;p&gt;If you're using Flake8, you can bump &lt;code&gt;max-line-length&lt;/code&gt; to 88 and forget about it.
Alternatively, use &lt;a href="https://github.com/PyCQA/flake8-bugbear"&gt;Bugbear&lt;/a&gt;'s B950 warning
instead of E501 and keep the max line length at 80 which you are probably already using.
You'd do it like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-ini"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;[flake8]&lt;/span&gt;
&lt;span class="pl-k"&gt;max-line-length&lt;/span&gt; = 80
...
&lt;span class="pl-k"&gt;select&lt;/span&gt; = C,E,F,W,B,B950
&lt;span class="pl-k"&gt;ignore&lt;/span&gt; = E203, E501, W503&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll find &lt;em&gt;Black&lt;/em&gt;'s own .flake8 config file is configured like this. Explanation of
why W503 and E203 are disabled can be found further in this documentation. And if you're
curious about the reasoning behind B950,
&lt;a href="https://github.com/PyCQA/flake8-bugbear#opinionated-warnings"&gt;Bugbear's documentation&lt;/a&gt;
explains it. The tl;dr is "it's like highway speed limits, we won't bother you if you
overdo it by a few km/h".&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-empty-lines" class="anchor" aria-hidden="true" href="#empty-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Empty lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; avoids spurious vertical whitespace. This is in the spirit of PEP 8 which says
that in-function vertical whitespace should only be used sparingly.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will allow single empty lines inside functions, and single and double empty
lines on module level left by the original editors, except when they're within
parenthesized expressions. Since such expressions are always reformatted to fit minimal
space, this whitespace is lost.&lt;/p&gt;
&lt;p&gt;It will also insert proper spacing before and after function definitions. It's one line
before and after inner functions and two lines before and after module-level functions
and classes. &lt;em&gt;Black&lt;/em&gt; will not put empty lines between function/class definitions and
standalone comments that immediately precede the given function/class.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will enforce single empty lines between a class-level docstring and the first
following field or method. This conforms to
&lt;a href="https://www.python.org/dev/peps/pep-0257/#multi-line-docstrings" rel="nofollow"&gt;PEP 257&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; won't insert empty lines after function docstrings unless that empty line is
required due to an inner function starting immediately after.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-trailing-commas" class="anchor" aria-hidden="true" href="#trailing-commas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trailing commas&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will add trailing commas to expressions that are split by comma where each
element is on its own line. This includes function signatures.&lt;/p&gt;
&lt;p&gt;Unnecessary trailing commas are removed if an expression fits in one line. This makes it
1% more likely that your line won't exceed the allotted line length limit. Moreover, in
this scenario, if you added another argument to your call, you'd probably fit it in the
same line anyway. That doesn't make diffs any larger.&lt;/p&gt;
&lt;p&gt;One exception to removing trailing commas is tuple expressions with just one element. In
this case &lt;em&gt;Black&lt;/em&gt; won't touch the single trailing comma as this would unexpectedly
change the underlying data type. Note that this is also the case when commas are used
while indexing. This is a tuple in disguise: &lt;code&gt;numpy_array[3, ]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One exception to adding trailing commas is function signatures containing &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;*args&lt;/code&gt;,
or &lt;code&gt;**kwargs&lt;/code&gt;. In this case a trailing comma is only safe to use on Python 3.6. &lt;em&gt;Black&lt;/em&gt;
will detect if your file is already 3.6+ only and use trailing commas in this situation.
If you wonder how it knows, it looks for f-strings and existing use of trailing commas
in function signatures that have stars in them. In other words, if you'd like a trailing
comma in this situation and &lt;em&gt;Black&lt;/em&gt; didn't recognize it was safe to do so, put it there
manually and &lt;em&gt;Black&lt;/em&gt; will keep it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-strings" class="anchor" aria-hidden="true" href="#strings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Strings&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; prefers double quotes (&lt;code&gt;"&lt;/code&gt; and &lt;code&gt;"""&lt;/code&gt;) over single quotes (&lt;code&gt;'&lt;/code&gt; and &lt;code&gt;'''&lt;/code&gt;). It
will replace the latter with the former as long as it does not result in more backslash
escapes than before.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; also standardizes string prefixes, making them always lowercase. On top of that,
if your code is already Python 3.6+ only or it's using the &lt;code&gt;unicode_literals&lt;/code&gt; future
import, &lt;em&gt;Black&lt;/em&gt; will remove &lt;code&gt;u&lt;/code&gt; from the string prefix as it is meaningless in those
scenarios.&lt;/p&gt;
&lt;p&gt;The main reason to standardize on a single form of quotes is aesthetics. Having one kind
of quotes everywhere reduces reader distraction. It will also enable a future version of
&lt;em&gt;Black&lt;/em&gt; to merge consecutive string literals that ended up on the same line (see
&lt;a href="https://github.com/psf/black/issues/26"&gt;#26&lt;/a&gt; for details).&lt;/p&gt;
&lt;p&gt;Why settle on double quotes? They anticipate apostrophes in English text. They match the
docstring standard described in
&lt;a href="https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring" rel="nofollow"&gt;PEP 257&lt;/a&gt;. An empty
string in double quotes (&lt;code&gt;""&lt;/code&gt;) is impossible to confuse with a one double-quote
regardless of fonts and syntax highlighting used. On top of this, double quotes for
strings are consistent with C which Python interacts a lot with.&lt;/p&gt;
&lt;p&gt;On certain keyboard layouts like US English, typing single quotes is a bit easier than
double quotes. The latter requires use of the Shift key. My recommendation here is to
keep using whatever is faster to type and let &lt;em&gt;Black&lt;/em&gt; handle the transformation.&lt;/p&gt;
&lt;p&gt;If you are adopting &lt;em&gt;Black&lt;/em&gt; in a large project with pre-existing string conventions
(like the popular
&lt;a href="https://stackoverflow.com/a/56190" rel="nofollow"&gt;"single quotes for data, double quotes for human-readable strings"&lt;/a&gt;),
you can pass &lt;code&gt;--skip-string-normalization&lt;/code&gt; on the command line. This is meant as an
adoption helper, avoid using this for new projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-numeric-literals" class="anchor" aria-hidden="true" href="#numeric-literals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numeric literals&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; standardizes most numeric literals to use lowercase letters for the syntactic
parts and uppercase letters for the digits themselves: &lt;code&gt;0xAB&lt;/code&gt; instead of &lt;code&gt;0XAB&lt;/code&gt; and
&lt;code&gt;1e10&lt;/code&gt; instead of &lt;code&gt;1E10&lt;/code&gt;. Python 2 long literals are styled as &lt;code&gt;2L&lt;/code&gt; instead of &lt;code&gt;2l&lt;/code&gt; to
avoid confusion between &lt;code&gt;l&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-line-breaks--binary-operators" class="anchor" aria-hidden="true" href="#line-breaks--binary-operators"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line breaks &amp;amp; binary operators&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will break a line before a binary operator when splitting a block of code over
multiple lines. This is so that &lt;em&gt;Black&lt;/em&gt; is compliant with the recent changes in the
&lt;a href="https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator" rel="nofollow"&gt;PEP 8&lt;/a&gt;
style guide, which emphasizes that this approach improves readability.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;W503 line break before binary operator&lt;/code&gt; warnings in style
guide enforcement tools like Flake8. Since &lt;code&gt;W503&lt;/code&gt; is not PEP 8 compliant, you should
tell Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-slices" class="anchor" aria-hidden="true" href="#slices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slices&lt;/h3&gt;
&lt;p&gt;PEP 8
&lt;a href="https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements" rel="nofollow"&gt;recommends&lt;/a&gt;
to treat &lt;code&gt;:&lt;/code&gt; in slices as a binary operator with the lowest priority, and to leave an
equal amount of space on either side, except if a parameter is omitted (e.g.
&lt;code&gt;ham[1 + 1 :]&lt;/code&gt;). It also states that for extended slices, both &lt;code&gt;:&lt;/code&gt; operators have to
have the same amount of spacing, except if a parameter is omitted (&lt;code&gt;ham[1 + 1 ::]&lt;/code&gt;).
&lt;em&gt;Black&lt;/em&gt; enforces these rules consistently.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;E203 whitespace before ':'&lt;/code&gt; warnings in style guide
enforcement tools like Flake8. Since &lt;code&gt;E203&lt;/code&gt; is not PEP 8 compliant, you should tell
Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-parentheses" class="anchor" aria-hidden="true" href="#parentheses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parentheses&lt;/h3&gt;
&lt;p&gt;Some parentheses are optional in the Python grammar. Any expression can be wrapped in a
pair of parentheses to form an atom. There are a few interesting cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;while (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;for (...) in (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;assert (...), (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from X import (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;assignments like:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;target = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;target: type = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;some, *un, packing = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;augmented += (...)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In those cases, parentheses are removed when the entire statement fits in one line, or
if the inner expression doesn't have any delimiters to further split on. If there is
only a single delimiter and the expression starts or ends with a bracket, the
parenthesis can also be successfully omitted since the existing bracket pair will
organize the expression neatly anyway. Otherwise, the parentheses are added.&lt;/p&gt;
&lt;p&gt;Please note that &lt;em&gt;Black&lt;/em&gt; does not add or remove any additional nested parentheses that
you might want to have for clarity or further code organization. For example those
parentheses are not going to be removed:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; (this &lt;span class="pl-k"&gt;or&lt;/span&gt; that)
decision &lt;span class="pl-k"&gt;=&lt;/span&gt; (maybe.this() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; (maybe.that() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-call-chains" class="anchor" aria-hidden="true" href="#call-chains"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Call chains&lt;/h3&gt;
&lt;p&gt;Some popular APIs, like ORMs, use call chaining. This API style is known as a
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interface&lt;/a&gt;. &lt;em&gt;Black&lt;/em&gt; formats
those by treating dots that follow a call or an indexing operation like a very low
priority delimiter. It's easier to show the behavior than to explain it. Look at the
example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;example&lt;/span&gt;(&lt;span class="pl-smi"&gt;session&lt;/span&gt;):
    result &lt;span class="pl-k"&gt;=&lt;/span&gt; (
        session.query(models.Customer.id)
        .filter(
            models.Customer.account_id &lt;span class="pl-k"&gt;==&lt;/span&gt; account_id,
            models.Customer.email &lt;span class="pl-k"&gt;==&lt;/span&gt; email_address,
        )
        .order_by(models.Customer.id.asc())
        .all()
    )&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typing-stub-files" class="anchor" aria-hidden="true" href="#typing-stub-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typing stub files&lt;/h3&gt;
&lt;p&gt;PEP 484 describes the syntax for type hints in Python. One of the use cases for typing
is providing type annotations for modules which cannot contain them directly (they might
be written in C, or they might be third-party, or their implementation may be overly
dynamic, and so on).&lt;/p&gt;
&lt;p&gt;To solve this,
&lt;a href="https://www.python.org/dev/peps/pep-0484/#stub-files" rel="nofollow"&gt;stub files with the &lt;code&gt;.pyi&lt;/code&gt; file extension&lt;/a&gt;
can be used to describe typing information for an external module. Those stub files omit
the implementation of classes and functions they describe, instead they only contain the
structure of the file (listing globals, functions, and classes with their members). The
recommended code style for those files is more terse than PEP 8:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; on the same line as the class/function signature;&lt;/li&gt;
&lt;li&gt;avoid vertical whitespace between consecutive module-level functions, names, or
methods and fields within a single class;&lt;/li&gt;
&lt;li&gt;use a single blank line between top-level class definitions, or none if the classes
are very small.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; enforces the above rules. There are additional guidelines for formatting &lt;code&gt;.pyi&lt;/code&gt;
file that are not enforced yet but might be in a future version of the formatter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all function bodies should be empty (contain &lt;code&gt;...&lt;/code&gt; instead of the body);&lt;/li&gt;
&lt;li&gt;do not use docstrings;&lt;/li&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; over &lt;code&gt;pass&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;for arguments with a default, use &lt;code&gt;...&lt;/code&gt; instead of the actual default;&lt;/li&gt;
&lt;li&gt;avoid using string literals in type annotations, stub files support forward references
natively (like Python 3.7 code with &lt;code&gt;from __future__ import annotations&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;use variable annotations instead of type comments, even for stubs that target older
versions of Python;&lt;/li&gt;
&lt;li&gt;for arguments that default to &lt;code&gt;None&lt;/code&gt;, use &lt;code&gt;Optional[]&lt;/code&gt; explicitly;&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;float&lt;/code&gt; instead of &lt;code&gt;Union[int, float]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pyprojecttoml" class="anchor" aria-hidden="true" href="#pyprojecttoml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyproject.toml&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options
from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom
&lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; patterns for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you're asking yourself "Do I need to configure anything?" the answer is
"No". &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-on-earth-is-a-pyprojecttoml-file" class="anchor" aria-hidden="true" href="#what-on-earth-is-a-pyprojecttoml-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What on Earth is a &lt;code&gt;pyproject.toml&lt;/code&gt; file?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.python.org/dev/peps/pep-0518/" rel="nofollow"&gt;PEP 518&lt;/a&gt; defines &lt;code&gt;pyproject.toml&lt;/code&gt; as a
configuration file to store build system requirements for Python projects. With the help
of tools like &lt;a href="https://poetry.eustace.io/" rel="nofollow"&gt;Poetry&lt;/a&gt; or
&lt;a href="https://flit.readthedocs.io/en/latest/" rel="nofollow"&gt;Flit&lt;/a&gt; it can fully replace the need for
&lt;code&gt;setup.py&lt;/code&gt; and &lt;code&gt;setup.cfg&lt;/code&gt; files.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-where-black-looks-for-the-file" class="anchor" aria-hidden="true" href="#where-black-looks-for-the-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where &lt;em&gt;Black&lt;/em&gt; looks for the file&lt;/h3&gt;
&lt;p&gt;By default &lt;em&gt;Black&lt;/em&gt; looks for &lt;code&gt;pyproject.toml&lt;/code&gt; starting from the common base directory of
all files and directories passed on the command line. If it's not there, it looks in
parent directories. It stops looking when it finds the file, or a &lt;code&gt;.git&lt;/code&gt; directory, or a
&lt;code&gt;.hg&lt;/code&gt; directory, or the root of the file system, whichever comes first.&lt;/p&gt;
&lt;p&gt;If you're formatting standard input, &lt;em&gt;Black&lt;/em&gt; will look for configuration starting from
the current working directory.&lt;/p&gt;
&lt;p&gt;You can also explicitly specify the path to a particular file that you want with
&lt;code&gt;--config&lt;/code&gt;. In this situation &lt;em&gt;Black&lt;/em&gt; will not look for any other file.&lt;/p&gt;
&lt;p&gt;If you're running with &lt;code&gt;--verbose&lt;/code&gt;, you will see a blue message if a file was found and
used.&lt;/p&gt;
&lt;p&gt;Please note &lt;code&gt;blackd&lt;/code&gt; will not use &lt;code&gt;pyproject.toml&lt;/code&gt; configuration.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-configuration-format" class="anchor" aria-hidden="true" href="#configuration-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration format&lt;/h3&gt;
&lt;p&gt;As the file extension suggests, &lt;code&gt;pyproject.toml&lt;/code&gt; is a
&lt;a href="https://github.com/toml-lang/toml"&gt;TOML&lt;/a&gt; file. It contains separate sections for
different tools. &lt;em&gt;Black&lt;/em&gt; is using the &lt;code&gt;[tool.black]&lt;/code&gt; section. The option keys are the
same as long names of options on the command line.&lt;/p&gt;
&lt;p&gt;Note that you have to use single-quoted strings in TOML for regular expressions. It's
the equivalent of r-strings in Python. Multiline strings are treated as verbose regular
expressions by Black. Use &lt;code&gt;[ ]&lt;/code&gt; to denote a significant space character.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example `pyproject.toml`&lt;/summary&gt;
&lt;div class="highlight highlight-source-toml"&gt;&lt;pre&gt;[&lt;span class="pl-en"&gt;tool&lt;/span&gt;.&lt;span class="pl-en"&gt;black&lt;/span&gt;]
&lt;span class="pl-smi"&gt;line-length&lt;/span&gt; = &lt;span class="pl-c1"&gt;88&lt;/span&gt;
&lt;span class="pl-smi"&gt;target-version&lt;/span&gt; = [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;py37&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
&lt;span class="pl-smi"&gt;include&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;\.pyi?$&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-smi"&gt;exclude&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;(&lt;/span&gt;
&lt;span class="pl-s"&gt;  /(&lt;/span&gt;
&lt;span class="pl-s"&gt;      \.eggs         # exclude a few common directories in the&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.git          # root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.hg&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.mypy_cache&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.tox&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.venv&lt;/span&gt;
&lt;span class="pl-s"&gt;    | _build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | buck-out&lt;/span&gt;
&lt;span class="pl-s"&gt;    | build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | dist&lt;/span&gt;
&lt;span class="pl-s"&gt;  )/&lt;/span&gt;
&lt;span class="pl-s"&gt;  | foo.py           # also separately exclude a file named foo.py in&lt;/span&gt;
&lt;span class="pl-s"&gt;                     # the root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;)&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-lookup-hierarchy" class="anchor" aria-hidden="true" href="#lookup-hierarchy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lookup hierarchy&lt;/h3&gt;
&lt;p&gt;Command-line options have defaults that you can see in &lt;code&gt;--help&lt;/code&gt;. A &lt;code&gt;pyproject.toml&lt;/code&gt; can
override those defaults. Finally, options provided by the user on the command line
override both.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will only ever use one &lt;code&gt;pyproject.toml&lt;/code&gt; file during an entire run. It doesn't
look for multiple files, and doesn't compose configuration from different levels of the
file hierarchy.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-editor-integration" class="anchor" aria-hidden="true" href="#editor-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Editor integration&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-emacs" class="anchor" aria-hidden="true" href="#emacs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Emacs&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/proofit404/blacken"&gt;proofit404/blacken&lt;/a&gt; or
&lt;a href="https://github.com/jorgenschaefer/elpy"&gt;Elpy&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pycharmintellij-idea" class="anchor" aria-hidden="true" href="#pycharmintellij-idea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyCharm/IntelliJ IDEA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Locate your &lt;code&gt;black&lt;/code&gt; installation folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS / Linux / BSD:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;which black&lt;/span&gt;
&lt;span class="pl-c1"&gt;/usr/local/bin/black  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Windows:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;where black&lt;/span&gt;
&lt;span class="pl-c1"&gt;%LocalAppData%\Programs\Python\Python36-32\Scripts\black.exe  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Open External tools in PyCharm/IntelliJ IDEA&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PyCharm -&amp;gt; Preferences -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;On Windows / Linux / BSD:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;File -&amp;gt; Settings -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;p&gt;Click the + icon to add a new external tool with the following values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;Description: Black is the uncompromising Python code formatter.&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;"$FilePath$"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Format the currently opened file by selecting &lt;code&gt;Tools -&amp;gt; External Tools -&amp;gt; black&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, you can set a keyboard shortcut by navigating to
&lt;code&gt;Preferences or Settings -&amp;gt; Keymap -&amp;gt; External Tools -&amp;gt; External Tools - Black&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optionally, run &lt;em&gt;Black&lt;/em&gt; on every file save:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have the
&lt;a href="https://plugins.jetbrains.com/plugin/7177-file-watchers" rel="nofollow"&gt;File Watcher&lt;/a&gt; plugin
installed.&lt;/li&gt;
&lt;li&gt;Go to &lt;code&gt;Preferences or Settings -&amp;gt; Tools -&amp;gt; File Watchers&lt;/code&gt; and click &lt;code&gt;+&lt;/code&gt; to add a
new watcher:
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;File type: Python&lt;/li&gt;
&lt;li&gt;Scope: Project Files&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Output paths to refresh: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Working directory: &lt;code&gt;$ProjectFileDir$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Uncheck "Auto-save edited files to trigger the watcher"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-wing-ide" class="anchor" aria-hidden="true" href="#wing-ide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wing IDE&lt;/h3&gt;
&lt;p&gt;Wing supports black via the OS Commands tool, as explained in the Wing documentation on
&lt;a href="https://wingware.com/doc/edit/pep8" rel="nofollow"&gt;pep8 formatting&lt;/a&gt;. The detailed procedure is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Make sure it runs from the command line, e.g.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;black --help&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;In Wing IDE, activate the &lt;strong&gt;OS Commands&lt;/strong&gt; panel and define the command &lt;strong&gt;black&lt;/strong&gt; to
execute black on the currently selected file:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use the Tools -&amp;gt; OS Commands menu selection&lt;/li&gt;
&lt;li&gt;click on &lt;strong&gt;+&lt;/strong&gt; in &lt;strong&gt;OS Commands&lt;/strong&gt; -&amp;gt; New: Command line..
&lt;ul class="contains-task-list"&gt;
&lt;li&gt;Title: black&lt;/li&gt;
&lt;li&gt;Command Line: black %s&lt;/li&gt;
&lt;li&gt;I/O Encoding: Use Default&lt;/li&gt;
&lt;li&gt;Key Binding: F1&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Raise OS Commands when executed&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Auto-save files before execution&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Line mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Select a file in the editor and press &lt;strong&gt;F1&lt;/strong&gt; , or whatever key binding you selected
in step 3, to reformat the file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-vim" class="anchor" aria-hidden="true" href="#vim"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vim&lt;/h3&gt;
&lt;p&gt;Commands and shortcuts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;:Black&lt;/code&gt; to format the entire file (ranges not supported);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackUpgrade&lt;/code&gt; to upgrade &lt;em&gt;Black&lt;/em&gt; inside the virtualenv;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackVersion&lt;/code&gt; to get the current version of &lt;em&gt;Black&lt;/em&gt; inside the virtualenv.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;g:black_fast&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_linelength&lt;/code&gt; (defaults to &lt;code&gt;88&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_skip_string_normalization&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_virtualenv&lt;/code&gt; (defaults to &lt;code&gt;~/.vim/black&lt;/code&gt; or &lt;code&gt;~/.local/share/nvim/black&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install with &lt;a href="https://github.com/junegunn/vim-plug"&gt;vim-plug&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plug 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with &lt;a href="https://github.com/VundleVim/Vundle.vim"&gt;Vundle&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plugin 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can copy the plugin from
&lt;a href="https://github.com/psf/black/tree/master/plugin/black.vim"&gt;plugin/black.vim&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/.vim/pack/python/start/black/plugin
curl https://raw.githubusercontent.com/psf/black/master/plugin/black.vim -o ~/.vim/pack/python/start/black/plugin/black.vim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me know if this requires any changes to work with Vim 8's builtin &lt;code&gt;packadd&lt;/code&gt;, or
Pathogen, and so on.&lt;/p&gt;
&lt;p&gt;This plugin &lt;strong&gt;requires Vim 7.0+ built with Python 3.6+ support&lt;/strong&gt;. It needs Python 3.6 to
be able to run &lt;em&gt;Black&lt;/em&gt; inside the Vim process which is much faster than calling an
external command.&lt;/p&gt;
&lt;p&gt;On first run, the plugin creates its own virtualenv using the right Python version and
automatically installs &lt;em&gt;Black&lt;/em&gt;. You can upgrade it later by calling &lt;code&gt;:BlackUpgrade&lt;/code&gt; and
restarting Vim.&lt;/p&gt;
&lt;p&gt;If you need to do anything special to make your virtualenv work and install &lt;em&gt;Black&lt;/em&gt; (for
example you want to run a version from master), create a virtualenv manually and point
&lt;code&gt;g:black_virtualenv&lt;/code&gt; to it. The plugin will use it.&lt;/p&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on save, add the following line to &lt;code&gt;.vimrc&lt;/code&gt; or &lt;code&gt;init.vim&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autocmd BufWritePre *.py execute ':Black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on a key press (e.g. F9 below), add this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nnoremap &amp;lt;F9&amp;gt; :Black&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;How to get Vim with Python 3.6?&lt;/strong&gt; On Ubuntu 17.10 Vim comes with Python 3.6 by
default. On macOS with Homebrew run: &lt;code&gt;brew install vim --with-python3&lt;/code&gt;. When building
Vim from source, use: &lt;code&gt;./configure --enable-python3interp=yes&lt;/code&gt;. There's many guides
online how to do this.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h3&gt;
&lt;p&gt;Use the
&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python" rel="nofollow"&gt;Python extension&lt;/a&gt;
(&lt;a href="https://code.visualstudio.com/docs/python/editing#_formatting" rel="nofollow"&gt;instructions&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sublimetext-3" class="anchor" aria-hidden="true" href="#sublimetext-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SublimeText 3&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/jgirardet/sublack"&gt;sublack plugin&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jupyter-notebook-magic" class="anchor" aria-hidden="true" href="#jupyter-notebook-magic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook Magic&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/csurfer/blackcellmagic"&gt;blackcellmagic&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-language-server" class="anchor" aria-hidden="true" href="#python-language-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Language Server&lt;/h3&gt;
&lt;p&gt;If your editor supports the &lt;a href="https://langserver.org/" rel="nofollow"&gt;Language Server Protocol&lt;/a&gt; (Atom,
Sublime Text, Visual Studio Code and many more), you can use the
&lt;a href="https://github.com/palantir/python-language-server"&gt;Python Language Server&lt;/a&gt; with the
&lt;a href="https://github.com/rupert/pyls-black"&gt;pyls-black&lt;/a&gt; plugin.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-atomnuclide" class="anchor" aria-hidden="true" href="#atomnuclide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Atom/Nuclide&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://atom.io/packages/python-black" rel="nofollow"&gt;python-black&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-kakoune" class="anchor" aria-hidden="true" href="#kakoune"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kakoune&lt;/h3&gt;
&lt;p&gt;Add the following hook to your kakrc, then run black with &lt;code&gt;:format&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hook global WinSetOption filetype=python %{
    set-option window formatcmd 'black -q  -'
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-other-editors" class="anchor" aria-hidden="true" href="#other-editors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other editors&lt;/h3&gt;
&lt;p&gt;Other editors will require external contributions.&lt;/p&gt;
&lt;p&gt;Patches welcome! &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;Any tool that can pipe code through &lt;em&gt;Black&lt;/em&gt; using its stdio mode (just
&lt;a href="https://www.tldp.org/LDP/abs/html/special-chars.html#DASHREF2" rel="nofollow"&gt;use &lt;code&gt;-&lt;/code&gt; as the file name&lt;/a&gt;).
The formatted code will be returned on stdout (unless &lt;code&gt;--check&lt;/code&gt; was passed). &lt;em&gt;Black&lt;/em&gt;
will still emit messages on stderr but that shouldn't affect your use case.&lt;/p&gt;
&lt;p&gt;This can be used for example with PyCharm's or IntelliJ's
&lt;a href="https://www.jetbrains.com/help/pycharm/file-watchers.html" rel="nofollow"&gt;File Watchers&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-blackd" class="anchor" aria-hidden="true" href="#blackd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;blackd&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is a small HTTP server that exposes &lt;em&gt;Black&lt;/em&gt;'s functionality over a simple
protocol. The main benefit of using it is to avoid paying the cost of starting up a new
&lt;em&gt;Black&lt;/em&gt; process every time you want to blacken a file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage-1" class="anchor" aria-hidden="true" href="#usage-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is not packaged alongside &lt;em&gt;Black&lt;/em&gt; by default because it has additional
dependencies. You will need to do &lt;code&gt;pip install black[d]&lt;/code&gt; to install it.&lt;/p&gt;
&lt;p&gt;You can start the server on the default port, binding only to the local interface by
running &lt;code&gt;blackd&lt;/code&gt;. You will see a single line mentioning the server's version, and the
host and port it's listening on. &lt;code&gt;blackd&lt;/code&gt; will then print an access log similar to most
web servers on standard output, merged with any exception traces caused by invalid
formatting requests.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; provides even less options than &lt;em&gt;Black&lt;/em&gt;. You can see them by running
&lt;code&gt;blackd --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;Usage: blackd [OPTIONS]

Options:
  --bind-host TEXT                Address to bind the server to.
  --bind-port INTEGER             Port to listen on
  --version                       Show the version and exit.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no official blackd client tool (yet!). You can test that blackd is working
using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blackd --bind-port 9090 &amp;amp;  # or let blackd choose a port
curl -s -XPOST "localhost:9090" -d "print('valid')"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-protocol" class="anchor" aria-hidden="true" href="#protocol"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Protocol&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; only accepts &lt;code&gt;POST&lt;/code&gt; requests at the &lt;code&gt;/&lt;/code&gt; path. The body of the request should
contain the python source code to be formatted, encoded according to the &lt;code&gt;charset&lt;/code&gt; field
in the &lt;code&gt;Content-Type&lt;/code&gt; request header. If no &lt;code&gt;charset&lt;/code&gt; is specified, &lt;code&gt;blackd&lt;/code&gt; assumes
&lt;code&gt;UTF-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are a few HTTP headers that control how the source is formatted. These correspond
to command line flags for &lt;em&gt;Black&lt;/em&gt;. There is one exception to this: &lt;code&gt;X-Protocol-Version&lt;/code&gt;
which if present, should have the value &lt;code&gt;1&lt;/code&gt;, otherwise the request is rejected with
&lt;code&gt;HTTP 501&lt;/code&gt; (Not Implemented).&lt;/p&gt;
&lt;p&gt;The headers controlling how code is formatted are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X-Line-Length&lt;/code&gt;: corresponds to the &lt;code&gt;--line-length&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Skip-String-Normalization&lt;/code&gt;: corresponds to the &lt;code&gt;--skip-string-normalization&lt;/code&gt;
command line flag. If present and its value is not the empty string, no string
normalization will be performed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Fast-Or-Safe&lt;/code&gt;: if set to &lt;code&gt;fast&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--fast&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Python-Variant&lt;/code&gt;: if set to &lt;code&gt;pyi&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--pyi&lt;/code&gt; command line flag. Otherwise, its value must correspond to a Python version or
a set of comma-separated Python versions, optionally prefixed with &lt;code&gt;py&lt;/code&gt;. For example,
to request code that is compatible with Python 3.5 and 3.6, set the header to
&lt;code&gt;py3.5,py3.6&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Diff&lt;/code&gt;: corresponds to the &lt;code&gt;--diff&lt;/code&gt; command line flag. If present, a diff of the
formats will be output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of these headers are set to invalid values, &lt;code&gt;blackd&lt;/code&gt; returns a &lt;code&gt;HTTP 400&lt;/code&gt; error
response, mentioning the name of the problematic header in the message body.&lt;/p&gt;
&lt;p&gt;Apart from the above, &lt;code&gt;blackd&lt;/code&gt; can produce the following response codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;HTTP 204&lt;/code&gt;: If the input is already well-formatted. The response body is empty.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 200&lt;/code&gt;: If formatting was needed on the input. The response body contains the
blackened Python code, and the &lt;code&gt;Content-Type&lt;/code&gt; header is set accordingly.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 400&lt;/code&gt;: If the input contains a syntax error. Details of the error are returned in
the response body.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 500&lt;/code&gt;: If there was any kind of error while trying to format the input. The
response body contains a textual representation of the error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The response headers include a &lt;code&gt;X-Black-Version&lt;/code&gt; header containing the version of
&lt;em&gt;Black&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-version-control-integration" class="anchor" aria-hidden="true" href="#version-control-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Version control integration&lt;/h2&gt;
&lt;p&gt;Use &lt;a href="https://pre-commit.com/" rel="nofollow"&gt;pre-commit&lt;/a&gt;. Once you
&lt;a href="https://pre-commit.com/#install" rel="nofollow"&gt;have it installed&lt;/a&gt;, add this to the
&lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; in your repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-yaml"&gt;&lt;pre&gt;&lt;span class="pl-ent"&gt;repos&lt;/span&gt;:
  - &lt;span class="pl-ent"&gt;repo&lt;/span&gt;: &lt;span class="pl-s"&gt;https://github.com/psf/black&lt;/span&gt;
    &lt;span class="pl-ent"&gt;rev&lt;/span&gt;: &lt;span class="pl-s"&gt;stable&lt;/span&gt;
    &lt;span class="pl-ent"&gt;hooks&lt;/span&gt;:
      - &lt;span class="pl-ent"&gt;id&lt;/span&gt;: &lt;span class="pl-s"&gt;black&lt;/span&gt;
        &lt;span class="pl-ent"&gt;language_version&lt;/span&gt;: &lt;span class="pl-s"&gt;python3.6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then run &lt;code&gt;pre-commit install&lt;/code&gt; and you're ready to go.&lt;/p&gt;
&lt;p&gt;Avoid using &lt;code&gt;args&lt;/code&gt; in the hook. Instead, store necessary configuration in
&lt;code&gt;pyproject.toml&lt;/code&gt; so that editors and command-line usage of Black all behave consistently
for your project. See &lt;em&gt;Black&lt;/em&gt;'s own &lt;a href="/pyproject.toml"&gt;pyproject.toml&lt;/a&gt; for an example.&lt;/p&gt;
&lt;p&gt;If you're already using Python 3.7, switch the &lt;code&gt;language_version&lt;/code&gt; accordingly. Finally,
&lt;code&gt;stable&lt;/code&gt; is a tag that is pinned to the latest release on PyPI. If you'd rather run on
master, this is also an option.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ignoring-unmodified-files" class="anchor" aria-hidden="true" href="#ignoring-unmodified-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ignoring unmodified files&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; remembers files it has already formatted, unless the &lt;code&gt;--diff&lt;/code&gt; flag is used or
code is passed via standard input. This information is stored per-user. The exact
location of the file depends on the &lt;em&gt;Black&lt;/em&gt; version and the system on which &lt;em&gt;Black&lt;/em&gt; is
run. The file is non-portable. The standard location on common operating systems is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows:
&lt;code&gt;C:\\Users\&amp;lt;username&amp;gt;\AppData\Local\black\black\Cache\&amp;lt;version&amp;gt;\cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;macOS:
&lt;code&gt;/Users/&amp;lt;username&amp;gt;/Library/Caches/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Linux:
&lt;code&gt;/home/&amp;lt;username&amp;gt;/.cache/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;file-mode&lt;/code&gt; is an int flag that determines whether the file was formatted as 3.6+ only,
as .pyi, and whether string normalization was omitted.&lt;/p&gt;
&lt;p&gt;To override the location of these files on macOS or Linux, set the environment variable
&lt;code&gt;XDG_CACHE_HOME&lt;/code&gt; to your preferred location. For example, if you want to put the cache
in the directory you're running &lt;em&gt;Black&lt;/em&gt; from, set &lt;code&gt;XDG_CACHE_HOME=.cache&lt;/code&gt;. &lt;em&gt;Black&lt;/em&gt; will
then write the above files to &lt;code&gt;.cache/black/&amp;lt;version&amp;gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-used-by" class="anchor" aria-hidden="true" href="#used-by"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Used by&lt;/h2&gt;
&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent
code style: pytest, tox, Pyramid, Django Channels, Hypothesis, attrs, SQLAlchemy,
Poetry, PyPA applications (Warehouse, Pipenv, virtualenv), pandas, Pillow, every Datadog
Agent Integration.&lt;/p&gt;
&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testimonials" class="anchor" aria-hidden="true" href="#testimonials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testimonials&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;,
&lt;a href="https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips" rel="nofollow"&gt;writer&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don't have to be.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href="https://www.attrs.org/" rel="nofollow"&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of
Twisted and CPython:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An auto-formatter that doesn't suck is all I want for Xmas!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;Django&lt;/a&gt; core developer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At least the name is good.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href="http://python-requests.org/" rel="nofollow"&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and
&lt;a href="https://docs.pipenv.org/" rel="nofollow"&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-show-your-style" class="anchor" aria-hidden="true" href="#show-your-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Show your style&lt;/h2&gt;
&lt;p&gt;Use the badge in your project's README.md:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;[![&lt;span class="pl-e"&gt;Code style: black&lt;/span&gt;](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like this:
&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-black" class="anchor" aria-hidden="true" href="#contributing-to-black"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to &lt;em&gt;Black&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;In terms of inspiration, &lt;em&gt;Black&lt;/em&gt; is about as configurable as &lt;em&gt;gofmt&lt;/em&gt;. This is
deliberate.&lt;/p&gt;
&lt;p&gt;Bug reports and fixes are always welcome! However, before you suggest a new feature or
configuration knob, ask yourself why you want it. If it enables better integration with
some workflow, fixes an inconsistency, speeds things up, and so on - go for it! On the
other hand, if your answer is "because I don't like a particular formatting" then you're
not ready to embrace &lt;em&gt;Black&lt;/em&gt; yet. Such changes are unlikely to get accepted. You can
still try but prepare to be disappointed.&lt;/p&gt;
&lt;p&gt;More details can be found in &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1910b0" class="anchor" aria-hidden="true" href="#1910b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.10b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 572 assignment expressions (#711)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 570 positional-only arguments (#943)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for async generators (#593)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for pre-splitting collections by putting an explicit trailing comma
inside (#826)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;black -c&lt;/code&gt; as a way to format code passed from the command line (#761)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;--safe now works with Python 2 code (#840)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed grammar selection for Python 2-specific code (#765)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed feature detection for trailing commas in function definitions and call sites
(#763)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt;/&lt;code&gt;# fmt: on&lt;/code&gt; comment pairs placed multiple times within the same block of
code now behave correctly (#1005)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Windows machines with more than 61 cores (#838)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on standalone comments prepended with a backslash (#767)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on &lt;code&gt;from&lt;/code&gt; ... &lt;code&gt;import&lt;/code&gt; blocks with comments (#829)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Python 3.7 on some platform configurations (#494)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails on comments in from-imports (#671)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails when the file starts with a backslash (#922)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer merges regular comments with type comments (#1027)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer splits long lines that contain type comments (#997)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;removed unnecessary parentheses around &lt;code&gt;yield&lt;/code&gt; expressions (#834)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around long tuples in unpacking assignments (#832)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around complex powers when they are prefixed by a unary operator
(#646)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed bug that led &lt;em&gt;Black&lt;/em&gt; format some code with a line length target of 1 (#762)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer introduces quotes in f-string subexpressions on string boundaries
(#863)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if &lt;em&gt;Black&lt;/em&gt; puts parenthesis around a single expression, it moves comments to the
wrapped expression instead of after the brackets (#872)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now returns the version of &lt;em&gt;Black&lt;/em&gt; in the response headers (#1013)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; can now output the diff of formats on source code when the &lt;code&gt;X-Diff&lt;/code&gt; header is
provided (#969)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-193b0" class="anchor" aria-hidden="true" href="#193b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.3b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;new option &lt;code&gt;--target-version&lt;/code&gt; to control which Python versions &lt;em&gt;Black&lt;/em&gt;-formatted code
should target (#618)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;deprecated &lt;code&gt;--py36&lt;/code&gt; (use &lt;code&gt;--target-version=py36&lt;/code&gt; instead) (#724)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer normalizes numeric literals to include &lt;code&gt;_&lt;/code&gt; separators (#696)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;long &lt;code&gt;del&lt;/code&gt; statements are now split into multiple lines (#698)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;type comments are no longer mangled in function signatures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;improved performance of formatting deeply nested data structures (#509)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now properly formats multiple files in parallel on Windows (#632)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now creates cache files atomically which allows it to be used in parallel
pipelines (like &lt;code&gt;xargs -P8&lt;/code&gt;) (#673)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now correctly indents comments in files that were previously formatted with
tabs (#262)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now supports CORS (#622)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-189b0" class="anchor" aria-hidden="true" href="#189b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.9b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are now formatted by &lt;em&gt;Black&lt;/em&gt; (#452, #461, #464, #469):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are normalized to include &lt;code&gt;_&lt;/code&gt; separators on Python 3.6+ code&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-numeric-underscore-normalization&lt;/code&gt; to disable the above behavior and
leave numeric underscores as they were in the input&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code with &lt;code&gt;_&lt;/code&gt; in numeric literals is recognized as Python 3.6+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;most letters in numeric literals are lowercased (e.g., in &lt;code&gt;1e10&lt;/code&gt;, &lt;code&gt;0x01&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hexadecimal digits are always uppercased (e.g. &lt;code&gt;0xBADC0DE&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;blackd&lt;/code&gt;, see &lt;a href="#blackd"&gt;its documentation&lt;/a&gt; for more info (#349)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjacent string literals are now correctly split into multiple lines (#463)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;trailing comma is now added to single imports that don't fit on a line (#250)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cache is now populated when &lt;code&gt;--check&lt;/code&gt; is successful for a file which speeds up
consecutive checks of properly formatted unmodified files (#448)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whitespace at the beginning of the file is now removed (#399)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed mangling &lt;a href="http://mpastell.com/pweave/" rel="nofollow"&gt;pweave&lt;/a&gt; and
&lt;a href="https://pythonhosted.org/spyder/" rel="nofollow"&gt;Spyder IDE&lt;/a&gt; special comments (#532)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when unpacking big tuples (#267)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of &lt;code&gt;__future__&lt;/code&gt; imports with renames (#389)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed scope of &lt;code&gt;# fmt: off&lt;/code&gt; when directly preceding &lt;code&gt;yield&lt;/code&gt; and other nodes (#385)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of lambda expressions with default arguments (#468)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;async for&lt;/code&gt; statements: &lt;em&gt;Black&lt;/em&gt; no longer breaks them into separate lines (#372)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;note: the Vim plugin stopped registering &lt;code&gt;,=&lt;/code&gt; as a default chord as it turned out to
be a bad idea (#415)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b4" class="anchor" aria-hidden="true" href="#186b4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hotfix: don't freeze when multiple comments directly precede &lt;code&gt;# fmt: off&lt;/code&gt; (#371)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b3" class="anchor" aria-hidden="true" href="#186b3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) now have blank lines added after constants (#340)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are now much more dependable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they now work also within bracket pairs (#329)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now correctly work across function/class boundaries (#335)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now work when an indentation block starts with empty lines or misaligned
comments (#334)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;made Click not fail on invalid environments; note that Click is right but the
likelihood we'll need to access non-ASCII file paths when dealing with Python source
code is low (#277)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper formatting of f-strings with quotes inside interpolated expressions
(#322)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown when long list literals where found in a file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown on AST nodes with very many siblings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed cannibalizing backslashes during string normalization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed a crash due to symbolic links pointing outside of the project directory (#338)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b2" class="anchor" aria-hidden="true" href="#186b2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--config&lt;/code&gt; (#65)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;-h&lt;/code&gt; equivalent to &lt;code&gt;--help&lt;/code&gt; (#316)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper unmodified file caching when &lt;code&gt;-S&lt;/code&gt; was used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra space in string unpacking (#305)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of empty triple quoted strings (#313)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown in comment placement calculation on lines without comments&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b1" class="anchor" aria-hidden="true" href="#186b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output human-facing information on stdout (#299)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output cake emoji on non-zero return code (#300)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b0" class="anchor" aria-hidden="true" href="#186b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; (#270)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-string-normalization&lt;/code&gt; (#118)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--verbose&lt;/code&gt; (#283)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the header output in &lt;code&gt;--diff&lt;/code&gt; now actually conforms to the unified diff spec&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed long trivial assignments being wrapped in unnecessary parentheses (#273)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary parentheses when a line contained multiline strings (#232)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed stdin handling not working correctly if an old version of Click was used (#276)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now preserves line endings when formatting a file in place (#258)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b1" class="anchor" aria-hidden="true" href="#185b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--pyi&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--py36&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python grammar pickle caches are stored with the formatting caches, making &lt;em&gt;Black&lt;/em&gt;
work in environments where site-packages is not user-writable (#192)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now enforces a PEP 257 empty line after a class-level docstring (and/or
fields) and the first method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when standalone comments were present in a trailer that
was omitted from line splitting on a large expression (#237)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed optional parentheses being removed within &lt;code&gt;# fmt: off&lt;/code&gt; sections (#224)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when stars in very long imports were incorrectly wrapped
in optional parentheses (#234)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when inline comments were moved around in a trailer that was
omitted from line splitting on a large expression (#238)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a class declaration and the first method if no class
docstring or fields are present (#219)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a function signature and an inner function or inner
class (#196)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b0" class="anchor" aria-hidden="true" href="#185b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;call chains are now formatted according to the
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interfaces&lt;/a&gt; style (#67)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;data structure literals (tuples, lists, dictionaries, and sets) are now also always
exploded like imports when they don't fit in a single line (#152)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;slices are now formatted according to PEP 8 (#178)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;parentheses are now also managed automatically on the right-hand side of assignments
and return statements (#140)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;math operators now use their respective priorities for delimiting multiline
expressions (#148)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;optional parentheses are now omitted on expressions that start or end with a bracket
and only contain a single operator (#177)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;empty parentheses in a class definition are now removed (#145, #180)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;string prefixes are now standardized to lowercase and &lt;code&gt;u&lt;/code&gt; is removed on Python 3.6+
only code and Python 2.7+ code with the &lt;code&gt;unicode_literals&lt;/code&gt; future import (#188, #198,
#199)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) are now formatted in a style that is consistent with PEP
484 (#207, #210)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;progress when reformatting many files is now reported incrementally&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed trailers (content with brackets) being unnecessarily exploded into their own
lines after a dedented closing bracket (#119)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed an invalid trailing comma sometimes left in imports (#185)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed non-deterministic formatting when multiple pairs of removable parentheses were
used (#183)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed multiline strings being unnecessarily wrapped in optional parentheses in long
assignments (#215)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed not splitting long from-imports with only a single name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed Python 3.6+ file discovery by also looking at function calls with unpacking.
This fixed non-deterministic formatting if trailing commas where used both in function
signatures with stars and function calls with stars but the former would be
reformatted to a single line.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash on dealing with optional parentheses (#193)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed "is", "is not", "in", and "not in" not considered operators for splitting
purposes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash when dead symlinks where encountered&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a4" class="anchor" aria-hidden="true" href="#184a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;don't populate the cache on &lt;code&gt;--check&lt;/code&gt; (#175)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a3" class="anchor" aria-hidden="true" href="#184a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added a "cache"; files already reformatted that haven't changed on disk won't be
reformatted again (#109)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;--check&lt;/code&gt; and &lt;code&gt;--diff&lt;/code&gt; are no longer mutually exclusive (#149)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generalized star expression handling, including double stars; this fixes
multiplication making expressions "unsafe" for trailing commas (#132)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer enforces putting empty lines behind control flow statements (#90)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now splits imports like "Mode 3 + trailing comma" of isort (#127)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed comment indentation when a standalone comment closes a block (#16, #32)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed standalone comments receiving extra empty lines if immediately preceding a
class, def, or decorator (#56, #154)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;--diff&lt;/code&gt; not showing entire path (#130)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of complex expressions after star and double stars in function calls
(#2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting on comma in lambda arguments (#133)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing splits of ternary expressions (#141)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a2" class="anchor" aria-hidden="true" href="#184a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of unaligned standalone comments (#99, #112)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed placement of dictionary unpacking inside dictionary literals (#111)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vim plugin now works on Windows, too&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when encountering unnecessarily escaped quotes in a string
(#120)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a1" class="anchor" aria-hidden="true" href="#184a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--quiet&lt;/code&gt; (#78)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added automatic parentheses management (#4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;a href="https://pre-commit.com" rel="nofollow"&gt;pre-commit&lt;/a&gt; integration (#103, #104)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed reporting on &lt;code&gt;--check&lt;/code&gt; with multiple files (#101, #102)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed removing backslash escapes from raw strings (#100, #105)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a0" class="anchor" aria-hidden="true" href="#184a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--diff&lt;/code&gt; (#87)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;add line breaks before all delimiters, except in cases like commas, to better comply
with PEP 8 (#73)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;standardize string literals to use double quotes (almost) everywhere (#75)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed handling of standalone comments within nested bracketed expressions; &lt;em&gt;Black&lt;/em&gt;
will no longer produce super long lines or put all standalone comments at the end of
the expression (#22)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: don't crash and burn on empty lines with trailing whitespace
(#80)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: &lt;code&gt;# yapf: disable&lt;/code&gt; usage as trailing comment would cause
&lt;em&gt;Black&lt;/em&gt; to not emit the rest of the file (#95)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when CTRL+C is pressed while formatting many files, &lt;em&gt;Black&lt;/em&gt; no longer freaks out with
a flurry of asyncio-related exceptions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only allow up to two empty lines on module level and only single empty lines within
functions (#74)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a4" class="anchor" aria-hidden="true" href="#183a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are implemented (#5)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;automatic detection of deprecated Python 2 forms of print statements and exec
statements in the formatted file (#49)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use proper spaces for complex expressions in default values of typed function
arguments (#60)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only return exit code 1 when --check is used (#50)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't remove single trailing commas from square bracket indexing (#59)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't omit whitespace if the previous factor leaf wasn't a math operator (#55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in kwarg unpacking if it's the first argument (#46)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in
&lt;a href="http://www.sphinx-doc.org/en/stable/ext/autodoc.html#directive-autoattribute" rel="nofollow"&gt;Sphinx auto-attribute comments&lt;/a&gt;
(#68)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a3" class="anchor" aria-hidden="true" href="#183a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;don't remove single empty lines outside of bracketed expressions (#19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added ability to pipe formatting from stdin to stdin (#25)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;restored ability to format code with legacy usage of &lt;code&gt;async&lt;/code&gt; as a name (#20, #42)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;even better handling of numpy-style array indexing (#33, again)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a2" class="anchor" aria-hidden="true" href="#183a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;changed positioning of binary operators to occur at beginning of lines instead of at
the end, following
&lt;a href="https://github.com/python/peps/commit/c59c4376ad233a62ca4b3a6060c81368bd21e85b"&gt;a recent change to PEP 8&lt;/a&gt;
(#21)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ignore empty bracket pairs while splitting. This avoids very weirdly looking
formattings (#34, #35)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;remove a trailing comma if there is a single argument to a call&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if top level functions were separated by a comment, don't put four empty lines after
the upper function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting of newlines with imports&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unintentional folding of post scriptum standalone comments into last statement
if it was a simple statement (#18, #28)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing space in numpy-style array indexing (#33)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after star-based unary expressions (#31)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a1" class="anchor" aria-hidden="true" href="#183a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--check&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only put trailing commas in function signatures and calls if it's safe to do so. If
the file is Python 3.6+ it's always safe, otherwise only safe if there are no &lt;code&gt;*args&lt;/code&gt;
or &lt;code&gt;**kwargs&lt;/code&gt; used in the signature or call. (#8)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid spacing of dots in relative imports (#6, #13)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting after comma on unpacked variables in for-loops (#23)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space in parenthesized set expressions (#7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after opening parentheses and in default arguments (#14, #17)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after unary operators when the operand was a complex expression
(#15)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a0" class="anchor" aria-hidden="true" href="#183a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;first published version, Happy &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;&lt;/g-emoji&gt; Day 2018!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;alpha quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date-versioned (see: &lt;a href="https://calver.org/" rel="nofollow"&gt;https://calver.org/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;Glued together by &lt;a href="mailto:lukasz@langa.pl"&gt;ukasz Langa&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Maintained with &lt;a href="mailto:carolcode@willingconsulting.com"&gt;Carol Willing&lt;/a&gt;,
&lt;a href="mailto:carl@oddbird.net"&gt;Carl Meyer&lt;/a&gt;,
&lt;a href="mailto:jelle.zijlstra@gmail.com"&gt;Jelle Zijlstra&lt;/a&gt;,
&lt;a href="mailto:mail@autophagy.io"&gt;Mika Naylor&lt;/a&gt;, and
&lt;a href="mailto:zsol.zsol@gmail.com"&gt;Zsolt Dollenstein&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Multiple contributions by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mailto:cryptolabour@gmail.com"&gt;Abdur-Rahmaan Janhangeer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@adamj.eu"&gt;Adam Johnson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@grande.coffee"&gt;Alexander Huynh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andrew.thorp.dev@gmail.com"&gt;Andrew Thorp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:dyuuus@yandex.ru"&gt;Andrey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andy@andyfreeland.net"&gt;Andy Freeland&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asottile@umich.edu"&gt;Anthony Sottile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:arjaan.buijk@gmail.com"&gt;Arjaan Buijk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:proofit404@gmail.com"&gt;Artem Malyshev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asgerdrewsen@gmail.com"&gt;Asger Hautop Drewsen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:raf@durin42.com"&gt;Augie Fackler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:aviskarkc10@gmail.com"&gt;Aviskar KC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@benjam.info"&gt;Benjamin Woodruff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:brandtbucher@gmail.com"&gt;Brandt Bucher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Charles Reid&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:christian@python.org"&gt;Christian Heimes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:chuck.wooters@microsoft.com"&gt;Chuck Wooters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@thequod.de"&gt;Daniel Hahler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:polycitizen@gmail.com"&gt;Daniel M. Capella&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Daniele Esposti&lt;/li&gt;
&lt;li&gt;dylanjblack&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:eli@treuherz.com"&gt;Eli Treuherz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:fthiery@gmail.com"&gt;Florent Thiery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hauntsaninja&lt;/li&gt;
&lt;li&gt;Hugo van Kemenade&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ivan.katanic@gmail.com"&gt;Ivan Katani&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@jasonfried.info"&gt;Jason Fried&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ijkl@netc.fr"&gt;jgirardet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jma353@cornell.edu"&gt;Joe Antonakakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jon.dufresne@gmail.com"&gt;Jon Dufresne&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ojiidotch@gmail.com"&gt;Jonas Obrist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:joshbode@fastmail.com"&gt;Josh Bode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hello@juanlu.space"&gt;Juan Luis Cano Rodrguez&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:katie@glasnt.com"&gt;Katie McLaughlin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lawrence Chan&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@linusgroh.de"&gt;Linus Groh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:luka.sterbic@gmail.com"&gt;Luka Sterbic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mariatta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:vaneseltine@gmail.com"&gt;Matt VanEseltine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael.flaxman@gmail.com"&gt;Michael Flaxman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sully@msully.net"&gt;Michael J. Sullivan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael@mcclimon.org"&gt;Michael McClimon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miggaiowski@gmail.com"&gt;Miguel Gaiowski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:roshi@fedoraproject.org"&gt;Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:minho42@gmail.com"&gt;Min ho Kim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miroslav@miki725.com"&gt;Miroslav Shubernetskiy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:neraste.herr10@gmail.com"&gt;Neraste&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ofekmeister@gmail.com"&gt;Ofek Lev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:osaetindaniel@gmail.com"&gt;Osaetin Daniel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:Pablogsal@gmail.com"&gt;Pablo Galindo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@peterbe.com"&gt;Peter Bengtsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pmacosta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:rishijha424@gmail.com"&gt;Rishikesh Jha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hi@stavros.io"&gt;Stavros Korokithakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sirosen@globus.org"&gt;Stephen Rosen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:snlkapil@gmail.com"&gt;Sunil Kapil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:thomas.c.lu@gmail.com"&gt;Thom Lu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:tom@tomchristie.com"&gt;Tom Christie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:uranusjr@gmail.com"&gt;Tzu-ping Chung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ukshah2@illinois.edu"&gt;Utsav Shah&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vezeli&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sharma.vishwas88@gmail.com"&gt;Vishwas B Sharma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:yngve@hoiseth.net"&gt;Yngve Hiseth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:1998uriyyo@gmail.com"&gt;Yurii Karabas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>psf</author><guid isPermaLink="false">https://github.com/psf/black</guid><pubDate>Tue, 05 Nov 2019 00:20:00 GMT</pubDate></item><item><title>huggingface/transformers #21 in Python, This week</title><link>https://github.com/huggingface/transformers</link><description>&lt;p&gt;&lt;i&gt; Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;br&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://circleci.com/gh/huggingface/transformers" rel="nofollow"&gt;
        &lt;img alt="Build" src="https://camo.githubusercontent.com/045b8639882280ff5cd38c403499977386c25134/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f68756767696e67666163652f7472616e73666f726d6572732f6d6173746572" data-canonical-src="https://img.shields.io/circleci/build/github/huggingface/transformers/master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/blob/master/LICENSE"&gt;
        &lt;img alt="GitHub" src="https://camo.githubusercontent.com/440e73b137335cc0088bb06e6c90cc7b503b14a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68756767696e67666163652f7472616e73666f726d6572732e7376673f636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://huggingface.co/transformers/index.html" rel="nofollow"&gt;
        &lt;img alt="Documentation" src="https://camo.githubusercontent.com/b104c21f478c4d4a37f63292ab2898047f19ee24/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f68756767696e67666163652e636f2f7472616e73666f726d6572732f696e6465782e68746d6c2e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65" data-canonical-src="https://img.shields.io/website/http/huggingface.co/transformers/index.html.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/releases"&gt;
        &lt;img alt="GitHub release" src="https://camo.githubusercontent.com/8409fd8716dd1a11afa7ab38e1218b34918164eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f68756767696e67666163652f7472616e73666f726d6572732e737667" data-canonical-src="https://img.shields.io/github/release/huggingface/transformers.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch" class="anchor" aria-hidden="true" href="#state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;p&gt;State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch
&lt;/p&gt;&lt;/h3&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;&lt;/g-emoji&gt; Transformers (formerly known as &lt;code&gt;pytorch-transformers&lt;/code&gt; and &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;As easy to use as pytorch-transformers&lt;/li&gt;
&lt;li&gt;As powerful and concise as Keras&lt;/li&gt;
&lt;li&gt;High performance on NLU and NLG tasks&lt;/li&gt;
&lt;li&gt;Low barrier to entry for educators and practitioners&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;State-of-the-art NLP for everyone&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning researchers&lt;/li&gt;
&lt;li&gt;Hands-on practitioners&lt;/li&gt;
&lt;li&gt;AI/ML/NLP teachers and educators&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lower compute costs, smaller carbon footprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Researchers can share trained models instead of always retraining&lt;/li&gt;
&lt;li&gt;Practitioners can reduce compute time and production costs&lt;/li&gt;
&lt;li&gt;10 architectures with over 30 pretrained models, some in more than 100 languages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Choose the right framework for every part of a model's lifetime&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train state-of-the-art models in 3 lines of code&lt;/li&gt;
&lt;li&gt;Deep interoperability between TensorFlow 2.0 and PyTorch models&lt;/li&gt;
&lt;li&gt;Move a single model between TF2.0/PyTorch frameworks at will&lt;/li&gt;
&lt;li&gt;Seamlessly pick the right framework for training, evaluation, production&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Section&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;How to install the package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#model-architectures"&gt;Model architectures&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Architectures (with pretrained weights)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#online-demo"&gt;Online demo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimenting with this repos text generation capabilities&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour"&gt;Quick tour: Usage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tokenizers &amp;amp; models usage: Bert and GPT-2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Quick-tour-TF-20-training-and-PyTorch-interoperability"&gt;Quick tour: TF 2.0 and PyTorch &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Train a TF 2.0 model in 10 lines of code, load it in PyTorch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;Quick tour: Fine-tuning/usage scripts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Using provided scripts: GLUE, SQuAD and Text generation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-transformers-to-transformers"&gt;Migrating from pytorch-transformers to transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-transformers to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-pretrained-bert-to-transformers"&gt;Migrating from pytorch-pretrained-bert to pytorch-transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-pretrained-bert to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;Documentation&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.1.1" rel="nofollow"&gt;(v2.1.1)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.0.0" rel="nofollow"&gt;(v2.0.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.2.0" rel="nofollow"&gt;(v1.2.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.1.0" rel="nofollow"&gt;(v1.1.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.0.0" rel="nofollow"&gt;(v1.0.0)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Full API documentation and more&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+), PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-with-pip" class="anchor" aria-hidden="true" href="#with-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;With pip&lt;/h3&gt;
&lt;p&gt;First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;&lt;/g-emoji&gt; Transformers can be installed using pip as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install transformers&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-from-source" class="anchor" aria-hidden="true" href="#from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From source&lt;/h3&gt;
&lt;p&gt;Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install [--editable] &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h3&gt;
&lt;p&gt;A series of tests are included for the library and the example scripts. Library tests can be found in the &lt;a href="https://github.com/huggingface/transformers/tree/master/transformers/tests"&gt;tests folder&lt;/a&gt; and examples tests in the &lt;a href="https://github.com/huggingface/transformers/tree/master/examples"&gt;examples folder&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These tests can be run using &lt;code&gt;pytest&lt;/code&gt; (install pytest if needed with &lt;code&gt;pip install pytest&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.&lt;/p&gt;
&lt;p&gt;You can run the tests from the root of the cloned repository with the commands:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m pytest -sv ./transformers/tests/
python -m pytest -sv ./examples/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-do-you-want-to-run-a-transformer-model-on-a-mobile-device" class="anchor" aria-hidden="true" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do you want to run a Transformer model on a mobile device?&lt;/h3&gt;
&lt;p&gt;You should check out our &lt;a href="https://github.com/huggingface/swift-coreml-transformers"&gt;&lt;code&gt;swift-coreml-transformers&lt;/code&gt;&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains &lt;code&gt;GPT-2&lt;/code&gt;, &lt;code&gt;DistilGPT-2&lt;/code&gt;, &lt;code&gt;BERT&lt;/code&gt;, and &lt;code&gt;DistilBERT&lt;/code&gt;) to CoreML models that run on iOS devices.&lt;/p&gt;
&lt;p&gt;At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-architectures" class="anchor" aria-hidden="true" href="#model-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model architectures&lt;/h2&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;&lt;/g-emoji&gt; Transformers currently provides 10 NLU/NLG architectures:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/bert"&gt;BERT&lt;/a&gt;&lt;/strong&gt; (from Google) released with the paper &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/openai/finetune-transformer-lm"&gt;GPT&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt; by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;GPT-2&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt; by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/kimiyoung/transformer-xl"&gt;Transformer-XL&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1901.02860" rel="nofollow"&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt; by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/zihangdai/xlnet/"&gt;XLNet&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1906.08237" rel="nofollow"&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt; by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/XLM/"&gt;XLM&lt;/a&gt;&lt;/strong&gt; (from Facebook) released together with the paper &lt;a href="https://arxiv.org/abs/1901.07291" rel="nofollow"&gt;Cross-lingual Language Model Pretraining&lt;/a&gt; by Guillaume Lample and Alexis Conneau.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta"&gt;RoBERTa&lt;/a&gt;&lt;/strong&gt; (from Facebook), released together with the paper a &lt;a href="https://arxiv.org/abs/1907.11692" rel="nofollow"&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt; by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilBERT&lt;/a&gt;&lt;/strong&gt; (from HuggingFace), released together with the paper &lt;a href="https://arxiv.org/abs/1910.01108" rel="nofollow"&gt;DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter&lt;/a&gt; by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilGPT2&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/salesforce/ctrl/"&gt;CTRL&lt;/a&gt;&lt;/strong&gt; (from Salesforce) released with the paper &lt;a href="https://arxiv.org/abs/1909.05858" rel="nofollow"&gt;CTRL: A Conditional Transformer Language Model for Controllable Generation&lt;/a&gt; by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.&lt;/li&gt;
&lt;li&gt;Want to contribute a new model? We have added a &lt;strong&gt;detailed guide and templates&lt;/strong&gt; to guide you in the process of adding a new model. You can find them in the &lt;a href="./templates"&gt;&lt;code&gt;templates&lt;/code&gt;&lt;/a&gt; folder of the repository. Be sure to check the &lt;a href="./CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; and contact the maintainers or open an issue to collect feedbacks before starting your PR.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the &lt;a href="https://huggingface.co/transformers/examples.html" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-online-demo" class="anchor" aria-hidden="true" href="#online-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online demo&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://transformer.huggingface.co" rel="nofollow"&gt;Write With Transformer&lt;/a&gt;&lt;/strong&gt;, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repos text generation capabilities.
You can use it to experiment with completions generated by &lt;code&gt;GPT2Model&lt;/code&gt;, &lt;code&gt;TransfoXLModel&lt;/code&gt;, and &lt;code&gt;XLNetModel&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="unicorn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f984.png"&gt;&lt;/g-emoji&gt; Write with transformer is to writing what calculators are to calculus.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67" alt="write_with_transformer" data-canonical-src="https://transformer.huggingface.co/front/assets/thumbnail-large.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour" class="anchor" aria-hidden="true" href="#quick-tour"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour&lt;/h2&gt;
&lt;p&gt;Let's do a very quick overview of the model architectures in &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;&lt;/g-emoji&gt; Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;full documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Transformers has a unified API&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; for 8 transformer architectures and 30 pretrained weights.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;          Model          | Tokenizer          | Pretrained weights shortcut&lt;/span&gt;
&lt;span class="pl-c1"&gt;MODELS&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [(BertModel,       BertTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;openai-gpt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (GPT2Model,       GPT2Tokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;gpt2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (CTRLModel,       CTRLTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ctrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (TransfoXLModel,  TransfoXLTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;transfo-xl-wt103&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLNetModel,      XLNetTokenizer,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlnet-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLMModel,        XLMTokenizer,        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlm-mlm-enfr-1024&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (DistilBertModel, DistilBertTokenizer, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;distilbert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (RobertaModel,    RobertaTokenizer,    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;roberta-base&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's encode some text in a sequence of hidden-states using each model:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class, tokenizer_class, pretrained_weights &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;MODELS&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer_class.from_pretrained(pretrained_weights)
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Encode text&lt;/span&gt;
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Here is some text to encode&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)])  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Add special tokens takes care of adding [CLS], [SEP], &amp;lt;s&amp;gt;... tokens in the right way for each model.&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; torch.no_grad():
        last_hidden_states &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models outputs are now tuples&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.&lt;/span&gt;
&lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,
                      BertForQuestionAnswering]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; All the classes for an architecture can be initiated from pretrained weights for this architecture&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Note that additional weights added for fine-tuning are only initialized&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; and need to be trained on the down-stream task&lt;/span&gt;
pretrained_weights &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(pretrained_weights)
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models can return full list of hidden-states &amp;amp; attentions weights at each layer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights,
                                        &lt;span class="pl-v"&gt;output_hidden_states&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;,
                                        &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Let's see all hidden-states and attentions on this text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)])
    all_hidden_states, all_attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;:]

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models are compatible with Torchscript&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights, &lt;span class="pl-v"&gt;torchscript&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    traced_model &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.jit.trace(model, (input_ids,))

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Simple serialization for models and tokenizers&lt;/span&gt;
    model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;
    tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; SOTA examples for GLUE, SQUAD, text generation...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-tf-20-training-and-pytorch-interoperability" class="anchor" aria-hidden="true" href="#quick-tour-tf-20-training-and-pytorch-interoperability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour TF 2.0 training and PyTorch interoperability&lt;/h2&gt;
&lt;p&gt;Let's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;&lt;/g-emoji&gt; Transformers and then loaded in PyTorch for fast inspection/tests.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow_datasets
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load dataset, tokenizer, model from pretrained model/vocabulary&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model &lt;span class="pl-k"&gt;=&lt;/span&gt; TFBertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; tensorflow_datasets.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;glue/mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare dataset for GLUE as a tf.data.Dataset instance&lt;/span&gt;
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;validation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; train_dataset.shuffle(&lt;span class="pl-c1"&gt;100&lt;/span&gt;).batch(&lt;span class="pl-c1"&gt;32&lt;/span&gt;).repeat(&lt;span class="pl-c1"&gt;2&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; valid_dataset.batch(&lt;span class="pl-c1"&gt;64&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule &lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.optimizers.Adam(&lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-5&lt;/span&gt;, &lt;span class="pl-v"&gt;epsilon&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1e-08&lt;/span&gt;, &lt;span class="pl-v"&gt;clipnorm&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1.0&lt;/span&gt;)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.losses.SparseCategoricalCrossentropy(&lt;span class="pl-v"&gt;from_logits&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
metric &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.metrics.SparseCategoricalAccuracy(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;accuracy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model.compile(&lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;optimizer, &lt;span class="pl-v"&gt;loss&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;loss, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[metric])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train and evaluate using tf.keras.Model.fit()&lt;/span&gt;
history &lt;span class="pl-k"&gt;=&lt;/span&gt; model.fit(train_dataset, &lt;span class="pl-v"&gt;epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-v"&gt;steps_per_epoch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;115&lt;/span&gt;,
                    &lt;span class="pl-v"&gt;validation_data&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;valid_dataset, &lt;span class="pl-v"&gt;validation_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load the TensorFlow model in PyTorch for inspection&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
pytorch_model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;from_tf&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task&lt;/span&gt;
sentence_0 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;This research was consistent with his findings.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were not compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
inputs_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_1, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
inputs_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_2, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

pred_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()
pred_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_1 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_1 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_2 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_2 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-of-the-fine-tuningusage-scripts" class="anchor" aria-hidden="true" href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour of the fine-tuning/usage scripts&lt;/h2&gt;
&lt;p&gt;The library comprises several example scripts with SOTA performances for NLU and NLG tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (&lt;em&gt;sequence-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (&lt;em&gt;token-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation&lt;/li&gt;
&lt;li&gt;other model-specific examples (see the documentation).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are three quick usage examples for these scripts:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification" class="anchor" aria-hidden="true" href="#run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: Fine-tuning on GLUE tasks for sequence classification&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://gluebenchmark.com/" rel="nofollow"&gt;General Language Understanding Evaluation (GLUE) benchmark&lt;/a&gt; is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.&lt;/p&gt;
&lt;p&gt;Before running anyone of these GLUE tasks you should download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You should also install the additional packages required by the examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r ./examples/requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name &lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt;/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.&lt;/p&gt;
&lt;p&gt;The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-xlnet-model-on-the-sts-b-regression-task" class="anchor" aria-hidden="true" href="#fine-tuning-xlnet-model-on-the-sts-b-regression-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning XLNet model on the STS-B regression task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python ./examples/run_glue.py \
    --model_type xlnet \
    --model_name_or_path xlnet-large-cased \
    --do_train  \
    --do_eval   \
    --task_name=sts-b     \
    --data_dir=&lt;span class="pl-smi"&gt;${GLUE_DIR}&lt;/span&gt;/STS-B  \
    --output_dir=./proc_data/sts-b-110   \
    --max_seq_length=128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --gradient_accumulation_steps=1 \
    --max_steps=1200  \
    --model_name=xlnet-large-cased   \
    --overwrite_output_dir   \
    --overwrite_cache \
    --warmup_steps=120&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On this machine we thus have a batch size of 32, please increase &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of &lt;code&gt;+0.917&lt;/code&gt; on the development set.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-bert-model-on-the-mrpc-classification-task" class="anchor" aria-hidden="true" href="#fine-tuning-bert-model-on-the-mrpc-classification-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning Bert model on the MRPC classification task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &amp;gt; 92.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name MRPC \
    --do_train   \
    --do_eval   \
    --do_lower_case   \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC/   \
    --max_seq_length 128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5   \
    --num_train_epochs 3.0  \
    --output_dir /tmp/mrpc_output/ \
    --overwrite_output_dir   \
    --overwrite_cache \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  acc = 0.8823529411764706
  acc_and_f1 = 0.901702786377709
  eval_loss = 0.3418912578906332
  f1 = 0.9210526315789473
  global_step = 174
  loss = 0.07231863956341798&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run_squadpy-fine-tuning-on-squad-for-question-answering" class="anchor" aria-hidden="true" href="#run_squadpy-fine-tuning-on-squad-for-question-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: Fine-tuning on SQuAD for question-answering&lt;/h3&gt;
&lt;p&gt;This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &amp;gt; 93 on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
    --predict_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../models/wwm_uncased_finetuned_squad/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 86.91579943235573, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 93.1532499015869}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the model provided as &lt;code&gt;bert-large-uncased-whole-word-masking-finetuned-squad&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet" class="anchor" aria-hidden="true" href="#run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet&lt;/h3&gt;
&lt;p&gt;A conditional generation script is also included to generate text from a prompt.
The generation script includes the &lt;a href="https://github.com/rusiaaman/XLNet-gen#methodology"&gt;tricks&lt;/a&gt; proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).&lt;/p&gt;
&lt;p&gt;Here is how to run the script with the small version of OpenAI GPT-2 model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=gpt2 \
    --length=20 \
    --model_name_or_path=gpt2 \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and from the Salesforce CTRL model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=ctrl \
    --length=20 \
    --model_name_or_path=ctrl \
    --temperature=0 \
    --repetition_penalty=1.2 \&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-transformers-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-transformers-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-transformers to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-transformers&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed" class="anchor" aria-hidden="true" href="#positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Positional order of some models' keywords inputs (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) changed&lt;/h3&gt;
&lt;p&gt;To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models &lt;strong&gt;keywords inputs&lt;/strong&gt; (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) has been changed.&lt;/p&gt;
&lt;p&gt;If you used to call the models with keyword names for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)&lt;/code&gt;, this should not cause any change.&lt;/p&gt;
&lt;p&gt;If you used to call the models with positional inputs for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask, token_type_ids)&lt;/code&gt;, you may have to double check the exact order of input arguments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-pretrained-bert-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-pretrained-bert-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-pretrained-bert to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-models-always-output-tuples" class="anchor" aria-hidden="true" href="#models-always-output-tuples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models always output &lt;code&gt;tuples&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The main breaking change when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; is that every model's forward method always outputs a &lt;code&gt;tuple&lt;/code&gt; with various elements depending on the model and the configuration parameters.&lt;/p&gt;
&lt;p&gt;The exact content of the tuples for each model is detailed in the models' docstrings and the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; conversion example for a &lt;code&gt;BertForSequenceClassification&lt;/code&gt; classification model:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's load our model&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you used to have this line in pytorch-pretrained-bert:&lt;/span&gt;
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Now just use this line in transformers to extract the loss from the output tuple:&lt;/span&gt;
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; In transformers you can also have access to the logits:&lt;/span&gt;
loss, logits &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[:&lt;span class="pl-c1"&gt;2&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss, logits, attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-using-hidden-states" class="anchor" aria-hidden="true" href="#using-hidden-states"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using hidden states&lt;/h3&gt;
&lt;p&gt;By enabling the configuration option &lt;code&gt;output_hidden_states&lt;/code&gt;, it was possible to retrieve the last hidden states of the encoder. In &lt;code&gt;pytorch-transformers&lt;/code&gt; as well as &lt;code&gt;transformers&lt;/code&gt; the return value has changed slightly: &lt;code&gt;all_hidden_states&lt;/code&gt; now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serialization" class="anchor" aria-hidden="true" href="#serialization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serialization&lt;/h3&gt;
&lt;p&gt;Breaking change in the &lt;code&gt;from_pretrained()&lt;/code&gt; method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Models are now set in evaluation mode by default when instantiated with the &lt;code&gt;from_pretrained()&lt;/code&gt; method. To train them, don't forget to set them back in training mode (&lt;code&gt;model.train()&lt;/code&gt;) to activate the dropout modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The additional &lt;code&gt;*input&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; arguments supplied to the &lt;code&gt;from_pretrained()&lt;/code&gt; method used to be directly passed to the underlying model's class &lt;code&gt;__init__()&lt;/code&gt; method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous &lt;code&gt;BertForSequenceClassification&lt;/code&gt; examples. We are working on a way to mitigate this breaking change in &lt;a href="https://github.com/huggingface/transformers/pull/866"&gt;#866&lt;/a&gt; by forwarding the the model's &lt;code&gt;__init__()&lt;/code&gt; method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method &lt;code&gt;save_pretrained(save_directory)&lt;/code&gt; if you were using any other serialization method before.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Let's load a model and tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Do some stuff to our model and tokenizer&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Ex: add new tokens to the vocabulary and embeddings of our model&lt;/span&gt;
tokenizer.add_tokens([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_1]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_2]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
model.resize_token_embeddings(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokenizer))
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train our model&lt;/span&gt;
train(model)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Now let's save our model and tokenizer to a directory&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Reload the model and the tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules" class="anchor" aria-hidden="true" href="#optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimizers: BertAdam &amp;amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules&lt;/h3&gt;
&lt;p&gt;The two optimizers previously included, &lt;code&gt;BertAdam&lt;/code&gt; and &lt;code&gt;OpenAIAdam&lt;/code&gt;, have been replaced by a single &lt;code&gt;AdamW&lt;/code&gt; optimizer which has a few differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it only implements weights decay correction,&lt;/li&gt;
&lt;li&gt;schedules are now externals (see below),&lt;/li&gt;
&lt;li&gt;gradient clipping is now also external (see below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The new optimizer &lt;code&gt;AdamW&lt;/code&gt; matches PyTorch &lt;code&gt;Adam&lt;/code&gt; optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.&lt;/p&gt;
&lt;p&gt;The schedules are now standard &lt;a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" rel="nofollow"&gt;PyTorch learning rate schedulers&lt;/a&gt; and not part of the optimizer anymore.&lt;/p&gt;
&lt;p&gt;Here is a conversion examples from &lt;code&gt;BertAdam&lt;/code&gt; with a linear warmup and decay schedule to &lt;code&gt;AdamW&lt;/code&gt; and the same schedule:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Parameters:&lt;/span&gt;
lr &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1e-3&lt;/span&gt;
max_grad_norm &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1.0&lt;/span&gt;
num_total_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1000&lt;/span&gt;
num_warmup_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;
warmup_proportion &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_warmup_steps) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_total_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 0.1&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Previously BertAdam optimizer was instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertAdam(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;schedule&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;warmup_linear&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;warmup&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;warmup_proportion, &lt;span class="pl-v"&gt;t_total&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_total_steps)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    optimizer.step()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## In Transformers, optimizer and schedules are splitted and instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; AdamW(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;correct_bias&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To reproduce BertAdam specific behavior set correct_bias=False&lt;/span&gt;
scheduler &lt;span class="pl-k"&gt;=&lt;/span&gt; WarmupLinearSchedule(optimizer, &lt;span class="pl-v"&gt;warmup_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_warmup_steps, &lt;span class="pl-v"&gt;t_total&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_total_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; PyTorch scheduler&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    model.train()
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Gradient clipping is not in AdamW anymore (so you can use amp without issue)&lt;/span&gt;
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;We now have a paper you can cite for the &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;&lt;/g-emoji&gt; Transformers library:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huggingface</author><guid isPermaLink="false">https://github.com/huggingface/transformers</guid><pubDate>Tue, 05 Nov 2019 00:21:00 GMT</pubDate></item><item><title>WenDesi/lihang_book_algorithm #22 in Python, This week</title><link>https://github.com/WenDesi/lihang_book_algorithm</link><description>&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;h1&gt;&lt;a id="user-content-lihang_book_algorithm" class="anchor" aria-hidden="true" href="#lihang_book_algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;lihang_book_algorithm&lt;/h1&gt;
&lt;p&gt;
&lt;br&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/WenDesi/lihang_book_algorithm/master/weibo.png"&gt;&lt;img src="https://raw.githubusercontent.com/WenDesi/lihang_book_algorithm/master/weibo.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;
&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/51923546" rel="nofollow"&gt;PythonMNIST&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/perceptron/binary_perceptron.py"&gt;perceptron/binary_perceptron.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--k" class="anchor" aria-hidden="true" href="#-k"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; K&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/51933044" rel="nofollow"&gt;PythonKNNMNIST&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/knn/knn.py"&gt;knn/knn.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/51967839" rel="nofollow"&gt;PythonMNIST&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/naive_bayes/naive_bayes.py"&gt;naive_bayes/naive_bayes.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/52849400" rel="nofollow"&gt;PythonMNIST&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/decision_tree/decision_tree.py"&gt;decision_tree/decision_tree.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53084871" rel="nofollow"&gt;PythonMNIST&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/logistic_regression/logistic_regression.py"&gt;logistic_regression/logistic_regression.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53106579" rel="nofollow"&gt;PythonMNIST&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/maxENT/maxENT.py"&gt;maxENT/maxENT.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53156589" rel="nofollow"&gt;Python&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/svm/svm.py"&gt;svm/svm.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53195725" rel="nofollow"&gt;Python+CppAdaBoostMNIST&lt;/a&gt;
&lt;br&gt;Python&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/adaboost.py"&gt;AdaBoost/adaboost.py&lt;/a&gt;
&lt;br&gt;Python C++&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/adaboost_cpp.py"&gt;AdaBoost/adaboost_cpp.py&lt;/a&gt;,&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/Sign/Sign/sign.h"&gt;AdaBoost/Sign/Sign/sign.h&lt;/a&gt;,&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/Sign/Sign/sign.cpp"&gt;AdaBoost/Sign/Sign/sign.cpp&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content--" class="anchor" aria-hidden="true" href="#-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt; &lt;/h3&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/75212599" rel="nofollow"&gt;Python&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/hmm/hmm.py"&gt;hmm/hmm.py&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;###softmax
&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53699778" rel="nofollow"&gt;python  softmaxMNIST&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/softmax/softmax.py"&gt;softmax/softmax.py&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>WenDesi</author><guid isPermaLink="false">https://github.com/WenDesi/lihang_book_algorithm</guid><pubDate>Tue, 05 Nov 2019 00:22:00 GMT</pubDate></item><item><title>dbolya/yolact #23 in Python, This week</title><link>https://github.com/dbolya/yolact</link><description>&lt;p&gt;&lt;i&gt;A simple, fully convolutional model for real-time instance segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-you-only-look-at-coefficients" class="anchor" aria-hidden="true" href="#you-only-look-at-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Y&lt;/strong&gt;ou &lt;strong&gt;O&lt;/strong&gt;nly &lt;strong&gt;L&lt;/strong&gt;ook &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;C&lt;/strong&gt;oefficien&lt;strong&gt;T&lt;/strong&gt;s&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;                 
          
                         
                           
                  
                   
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple, fully convolutional model for real-time instance segmentation. This is the code for &lt;a href="https://arxiv.org/abs/1904.02689" rel="nofollow"&gt;our paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-iccv-update-v11-released-check-out-the-iccv-trailer-here" class="anchor" aria-hidden="true" href="#iccv-update-v11-released-check-out-the-iccv-trailer-here"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ICCV update (v1.1) released! Check out the ICCV trailer here:&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=0pMfmo8qfpQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c9f0f1403e25276c0beea78732b5cec6c9b610ab/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f30704d666d6f38716670512f302e6a7067" alt="IMAGE ALT TEXT HERE" data-canonical-src="https://img.youtube.com/vi/0pMfmo8qfpQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Read &lt;a href="CHANGELOG.md"&gt;the changelog&lt;/a&gt; for details on, well, what changed. Oh, and the paper got updated too with pascal results and an appendix with box mAP.&lt;/p&gt;
&lt;p&gt;Some examples from our base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's &lt;code&gt;test-dev&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_0.png"&gt;&lt;img src="data/yolact_example_0.png" alt="Example 0" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_1.png"&gt;&lt;img src="data/yolact_example_1.png" alt="Example 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_2.png"&gt;&lt;img src="data/yolact_example_2.png" alt="Example 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Set up a Python3 environment.&lt;/li&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org/" rel="nofollow"&gt;Pytorch&lt;/a&gt; 1.0.1 (or higher) and TorchVision.&lt;/li&gt;
&lt;li&gt;Install some other packages:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Cython needs to be installed before pycocotools&lt;/span&gt;
pip install cython
pip install opencv-python pillow pycocotools matplotlib &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Clone this repository and enter it:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/dbolya/yolact.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolact&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into &lt;code&gt;./data/coco&lt;/code&gt;.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to evaluate YOLACT on &lt;code&gt;test-dev&lt;/code&gt;, download &lt;code&gt;test-dev&lt;/code&gt; with this script.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO_test.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluation&lt;/h1&gt;
&lt;p&gt;As of April 5th, 2019 here are our latest models along with their FPS on a Titan Xp and mAP on &lt;code&gt;test-dev&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Image Size&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;FPS&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th&gt;Weights&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet50-FPN&lt;/td&gt;
&lt;td align="center"&gt;42.5&lt;/td&gt;
&lt;td align="center"&gt;28.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing" rel="nofollow"&gt;yolact_resnet50_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Darknet53-FPN&lt;/td&gt;
&lt;td align="center"&gt;40.0&lt;/td&gt;
&lt;td align="center"&gt;28.7&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing" rel="nofollow"&gt;yolact_darknet53_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;33.0&lt;/td&gt;
&lt;td align="center"&gt;29.8&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing" rel="nofollow"&gt;yolact_base_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;700&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;23.6&lt;/td&gt;
&lt;td align="center"&gt;31.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing" rel="nofollow"&gt;yolact_im700_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To evalute the model, put the corresponding weights file in the &lt;code&gt;./weights&lt;/code&gt; directory and run one of the following commands.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quantitative-results-on-coco" class="anchor" aria-hidden="true" href="#quantitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This should get 29.92 validation mask mAP last time I checked.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.&lt;/span&gt;
python run_coco_eval.py

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-qualitative-results-on-coco" class="anchor" aria-hidden="true" href="#qualitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qualitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-on-coco" class="anchor" aria-hidden="true" href="#benchmarking-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run just the raw model on the first 1k images of the validation set&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-images" class="anchor" aria-hidden="true" href="#images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Images&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on the specified image.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process an image and save it to another file.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a whole folder of images.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a video in real-time. "--video_multiframe" will process that many frames at once for improved performance.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you want, use "--display_fps" to draw the FPS directly on the frame.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can tell, &lt;code&gt;eval.py&lt;/code&gt; can do a ton of stuff. Run the &lt;code&gt;--help&lt;/code&gt; command to see everything it can do.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python eval.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;By default, we train on COCO. Make sure to download the entire dataset using the commands above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To train, grab an imagenet-pretrained model and put it in &lt;code&gt;./weights&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;For Resnet101, download &lt;code&gt;resnet101_reducedfc.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Resnet50, download &lt;code&gt;resnet50-19c8e357.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Darknet53, download &lt;code&gt;darknet53.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run one of the training commands below.
&lt;ul&gt;
&lt;li&gt;Note that you can press ctrl+c while training and it will save an &lt;code&gt;*_interrupt.pth&lt;/code&gt; file at the current iteration.&lt;/li&gt;
&lt;li&gt;All weights are saved in the &lt;code&gt;./weights&lt;/code&gt; directory by default with the file name &lt;code&gt;&amp;lt;config&amp;gt;_&amp;lt;epoch&amp;gt;_&amp;lt;iter&amp;gt;.pth&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains using the base config with a batch size of 8 (the default).&lt;/span&gt;
python train.py --config=yolact_base_config

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.&lt;/span&gt;
python train.py --config=yolact_base_config --batch_size=5

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.&lt;/span&gt;
python train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Use the help option to see a description of all available command line arguments&lt;/span&gt;
python train.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-support" class="anchor" aria-hidden="true" href="#multi-gpu-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU Support&lt;/h2&gt;
&lt;p&gt;YOLACT now supports multiple GPUs seamlessly during training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before running any of the scripts, run: &lt;code&gt;export CUDA_VISIBLE_DEVICES=[gpus]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).&lt;/li&gt;
&lt;li&gt;You should still do this if only using 1 GPU.&lt;/li&gt;
&lt;li&gt;You can check the indices of your GPUs with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then, simply set the batch size to &lt;code&gt;8*num_gpus&lt;/code&gt; with the training commands above. The training script will automatically scale the hyperparameters to the right values.
&lt;ul&gt;
&lt;li&gt;If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.&lt;/li&gt;
&lt;li&gt;If you want to allocate the images per GPU specific for different GPUs, you can use &lt;code&gt;--batch_alloc=[alloc]&lt;/code&gt; where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to &lt;code&gt;batch_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Logging&lt;/h2&gt;
&lt;p&gt;YOLACT now logs training and validation information by default. You can disable this with &lt;code&gt;--no_log&lt;/code&gt;. A guide on how to visualize these logs is coming soon, but now you can look at &lt;code&gt;LogVizualizer&lt;/code&gt; in &lt;code&gt;utils/logger.py&lt;/code&gt; for help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pascal-sbd" class="anchor" aria-hidden="true" href="#pascal-sbd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pascal SBD&lt;/h2&gt;
&lt;p&gt;We also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the dataset from &lt;a href="http://home.bharathh.info/pubs/codes/SBD/download.html" rel="nofollow"&gt;here&lt;/a&gt;. It's the first link in the top "Overview" section (and the file is called &lt;code&gt;benchmark.tgz&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Extract the dataset somewhere. In the dataset there should be a folder called &lt;code&gt;dataset/img&lt;/code&gt;. Create the directory &lt;code&gt;./data/sbd&lt;/code&gt; (where &lt;code&gt;.&lt;/code&gt; is YOLACT's root) and copy &lt;code&gt;dataset/img&lt;/code&gt; to &lt;code&gt;./data/sbd/img&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Download the COCO-style annotations from &lt;a href="https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Extract the annotations into &lt;code&gt;./data/sbd/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Now you can train using &lt;code&gt;--config=yolact_resnet50_pascal_config&lt;/code&gt;. Check that config to see how to extend it to other models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in &lt;code&gt;./scripts/convert_sbd.py&lt;/code&gt;, but you'll have to check how it works to be able to use it because I don't actually remember at this point.&lt;/p&gt;
&lt;p&gt;If you want to verify our results, you can download our &lt;code&gt;yolact_resnet50_pascal_config&lt;/code&gt; weights from &lt;a href="https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S" rel="nofollow"&gt;here&lt;/a&gt;. This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the "all" AP isn't the same as the "vol" AP reported in others papers for pascal (they use an averages of the thresholds from &lt;code&gt;0.1 - 0.9&lt;/code&gt; in increments of &lt;code&gt;0.1&lt;/code&gt; instead of what COCO uses).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h2&gt;
&lt;p&gt;You can also train on your own dataset by following these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found &lt;a href="http://cocodataset.org/#format-data" rel="nofollow"&gt;here&lt;/a&gt;. Note that we don't use some fields, so the following may be omitted:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;liscense&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;code&gt;image&lt;/code&gt;: &lt;code&gt;license, flickr_url, coco_url, date_captured&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (we use our own format for categories, see below)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create a definition for your dataset under &lt;code&gt;dataset_base&lt;/code&gt; in &lt;code&gt;data/config.py&lt;/code&gt; (see the comments in &lt;code&gt;dataset_base&lt;/code&gt; for an explanation of each field):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;my_custom_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; dataset_base.copy({
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;My Dataset&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;has_gt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;class_names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;...&lt;/span&gt;)
})&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A couple things to note:
&lt;ul&gt;
&lt;li&gt;Class IDs in the annotation file should start at 1 and increase sequentially on the order of &lt;code&gt;class_names&lt;/code&gt;. If this isn't the case for your annotation file (like in COCO), see the field &lt;code&gt;label_map&lt;/code&gt; in &lt;code&gt;dataset_base&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you do not want to create a validation split, use the same image path and annotations file for validation. By default (see &lt;code&gt;python train.py --help&lt;/code&gt;), &lt;code&gt;train.py&lt;/code&gt; will output validation mAP for the first 5000 images in the dataset every 2 epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally, in &lt;code&gt;yolact_base_config&lt;/code&gt; in the same file, change the value for &lt;code&gt;'dataset'&lt;/code&gt; to &lt;code&gt;'my_custom_dataset'&lt;/code&gt; or whatever you named the config object above. Then you can use any of the training commands in the previous section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-creating-a-custom-dataset-from-scratch" class="anchor" aria-hidden="true" href="#creating-a-custom-dataset-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a Custom Dataset from Scratch&lt;/h4&gt;
&lt;p&gt;See &lt;a href="https://github.com/dbolya/yolact/issues/70#issuecomment-504283008"&gt;this nice post by @Amit12690&lt;/a&gt; for tips on how to annotate a custom dataset and prepare it for use with YOLACT.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;If you use YOLACT or this code base in your work, please cite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{bolya-iccv2019,
  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  title     = {YOLACT: {Real-time} Instance Segmentation},
  booktitle = {ICCV},
  year      = {2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;For questions about our paper or code, please contact &lt;a href="mailto:dbolya@ucdavis.edu"&gt;Daniel Bolya&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dbolya</author><guid isPermaLink="false">https://github.com/dbolya/yolact</guid><pubDate>Tue, 05 Nov 2019 00:23:00 GMT</pubDate></item><item><title>pytorch/examples #24 in Python, This week</title><link>https://github.com/pytorch/examples</link><description>&lt;p&gt;&lt;i&gt;A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-examples" class="anchor" aria-hidden="true" href="#pytorch-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Examples&lt;/h1&gt;
&lt;p&gt;A repository showcasing examples of using &lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mnist"&gt;Image classification (MNIST) using Convnets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="word_language_model"&gt;Word level Language Modeling using LSTM RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="imagenet"&gt;Training Imagenet Classifiers with Residual Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="dcgan"&gt;Generative Adversarial Networks (DCGAN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vae"&gt;Variational Auto-Encoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="super_resolution"&gt;Superresolution using an efficient sub-pixel convolutional neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mnist_hogwild"&gt;Hogwild training of shared ConvNets across multiple processes on MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning"&gt;Training a CartPole to balance in OpenAI Gym with actor-critic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="snli"&gt;Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="time_sequence_prediction"&gt;Time sequence prediction - use an LSTM to learn Sine waves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="fast_neural_style"&gt;Implement the Neural Style Transfer algorithm on images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="cpp"&gt;Several examples illustrating the C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, a list of good examples hosted in their own repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenNMT/OpenNMT-py"&gt;Neural Machine Translation using sequence-to-sequence RNN with attention (OpenNMT)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/examples</guid><pubDate>Tue, 05 Nov 2019 00:24:00 GMT</pubDate></item><item><title>fighting41love/funNLP #25 in Python, This week</title><link>https://github.com/fighting41love/funNLP</link><description>&lt;p&gt;&lt;i&gt;//ITbert&amp;cocoNLPXLORE:NLUcs224n /+ASR  + Microsoft//api()SpaCy Common Voicebert(Keyphrase)pke4cnocrOCRPython3nlpspeech-aligner: AmpliGraph: (Python)Scattertext (python)/BERT &amp; ERNIENLPSynonymsHarvestText--word2word(Python)-62/3,564/(ASR)Kashgarigpt-2TextTeaser: 14W--andSiamese bilstmTransformerHacker NewsBERTLitBankNLP100Facebook: LAMATransformer-XL/BERT/ELMo/GPTCommonsenseQAQA PDF  PPTSQLNLPEDANLP mongodbULMFiT--- qingyun seqGAN-----masr: --PythonBERTConvLabrasaTensorFlowBERT/NLPTOPOpenCLaP-()g2pCZincbase /&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;github&lt;/p&gt;
&lt;p&gt;
star&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;//ITbert&amp;amp;cocoNLPXLORE:NLUcs224n /+ASR  + Microsoft//api()SpaCy Common Voicebert(Keyphrase)pke4cnocrOCRPython3nlpspeech-aligner: AmpliGraph: (Python)Scattertext (python)/BERT &amp;amp; ERNIENLPSynonymsHarvestText--word2word(Python)-62/3,564/(ASR)Kashgarigpt-2TextTeaser: 14W--andSiamese bilstmTransformerHacker NewsBERTLitBankNLP100Facebook: LAMATransformer-XL/BERT/ELMo/GPTCommonsenseQAQA PDF  PPTSQLNLPEDANLP mongodbULMFiT--- qingyun seqGAN-----masr: --PythonBERTConvLabrasaTensorFlowBERT/NLPTOPOpenCLaPUER++-()g2pCZincbase /jieba_fast jiebaBERTPythonStanfordNLP 0.2.0PythonNeuralNLP-NeuralClassifierNeuroNER vs. BertNER2019GPT2ML-NLP - (Machine Learning)NLPnlp4han:(//////NER/N/HMM///XLMFacebookBERT--CoupletAI - CNN+Bi-LSTM+Attention MiningZhiDaoQACorpus - 580&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. textfilter: &lt;/strong&gt;  &lt;a href="https://github.com/observerss/textfilter"&gt;observerss/textfilter&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;gt;&amp;gt;&amp;gt; f = DFAFilter()
 &amp;gt;&amp;gt;&amp;gt; f.add("sexy")
 &amp;gt;&amp;gt;&amp;gt; f.filter("hello sexy baby")
 hello **** baby
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;keyword&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. langid97&lt;/strong&gt; &lt;a href="https://github.com/saffsd/langid.py"&gt;https://github.com/saffsd/langid.py&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install langid&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import langid
&amp;gt;&amp;gt;&amp;gt; langid.classify("This is a test")
('en', -54.41310358047485)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. langdetect&lt;/strong&gt;&lt;a href="https://code.google.com/archive/p/language-detection/" rel="nofollow"&gt;https://code.google.com/archive/p/language-detection/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install langdetect&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;from langdetect import detect
from langdetect import detect_langs

s1 = ""
s2 = 'We are pleased to introduce today a new technology'
print(detect(s1))
print(detect(s2))
print(detect_langs(s3))    # detect_langs()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; ISO 639-1&lt;a href="https://baike.baidu.com/item/ISO%20639-1" rel="nofollow"&gt;ISO 639-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. phone &lt;/strong&gt; &lt;a href="https://github.com/ls0f/phone"&gt;ls0f/phone&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;from phone import Phone
p  = Phone()
p.find(18100065143)
#return {'phone': '18100065143', 'province': '', 'city': '', 'zip_code': '200000', 'area_code': '021', 'phone_type': ''}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;: 13*,15*,18*,14[5,7],17[0,6,7,8]&lt;/p&gt;
&lt;p&gt;: 360569 (updated:20174)&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/lovedboy/phone/raw/master/phone/phone.dat"&gt;phone.dat&lt;/a&gt; pythonLoad&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. phone&lt;/strong&gt;&lt;a href="https://github.com/AfterShip/phone"&gt;AfterShip/phone&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;npm install phone&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;import phone from 'phone';
phone('+852 6569-8900'); // return ['+85265698900', 'HKG']
phone('(817) 569-8900'); // return ['+18175698900, 'USA']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;6. ngender &lt;/strong&gt;&lt;a href="https://github.com/observerss/ngender"&gt;observerss/ngender&lt;/a&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install ngender&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import ngender
&amp;gt;&amp;gt;&amp;gt; ngender.guess('')
('male', 0.9836229687547046)
&amp;gt;&amp;gt;&amp;gt; ngender.guess('')
('female', 0.9759486128949907)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;7. email&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;email_pattern = '^[*#\u4e00-\u9fa5 a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\.[a-zA-Z0-9-]+)*\.[a-zA-Z0-9]{2,6}$'
emails = re.findall(email_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;8. phone_number&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;cellphone_pattern = '^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\d{8}$'
phoneNumbers = re.findall(cellphone_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;9. &lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IDCards_pattern = r'^([1-9]\d{5}[12]\d{3}(0[1-9]|1[012])(0[1-9]|[12][0-9]|3[01])\d{3}[0-9xX])$'
IDs = re.findall(IDCards_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;10.  &lt;/strong&gt; &lt;a href="https://github.com/wainshine/Chinese-Names-Corpus"&gt;wainshine/Chinese-Names-Corpus&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;-&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;11. &lt;/strong&gt;&lt;a href="https://github.com/zhangyics/Chinese-abbreviation-dataset/blob/master/dev_set.txt"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;: /n /n /n
: /ns
: /n /n /vn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;12. &lt;/strong&gt;&lt;a href="https://github.com/kfcd/chaizi"&gt;kfcd/chaizi&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	 ()	 ()	 ()
	 	 	 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;13. &lt;/strong&gt;&lt;a href="https://github.com/rainarch/SentiBridge/blob/master/Entity_Emotion_Express/CCF_data/pair_mine_result"&gt;rainarch/SentiBridge&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;		0.400704566541	0.370067395878
	        	0.305762728932	0.325320747491
		0.312137906517	0.378594957281
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;14. &lt;/strong&gt; &lt;a href="https://github.com/fighting41love/Chinese_from_dongxiexidian"&gt;dongxiexidian/Chinese&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;package&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;&lt;/a&gt; &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;&lt;/a&gt; &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;&lt;/a&gt; &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;&lt;/a&gt; &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;15. &lt;/strong&gt;&lt;a href="https://github.com/mozillazg/python-pinyin"&gt;mozillazg/python-pinyin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;16. &lt;/strong&gt;&lt;a href="https://github.com/skydark/nstools/tree/master/zhtools"&gt;skydark/nstools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;17. &lt;/strong&gt; funny chinese text to speech enginee&lt;a href="https://github.com/tinyfool/ChineseWithEnglish"&gt;tinyfool/ChineseWithEnglish&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;say wo i ni
#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;18. &lt;/strong&gt;&lt;a href="https://github.com/phunterlau/wangfeng-rnn"&gt;phunterlau/wangfeng-rnn&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;




&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;19. &lt;/strong&gt;&lt;a href="https://github.com/guotong1988/chinese_dictionary"&gt;guotong1988/chinese_dictionary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;20. &lt;/strong&gt;&lt;a href="https://github.com/keredson/wordninja"&gt;wordinja&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import wordninja
&amp;gt;&amp;gt;&amp;gt; wordninja.split('derekanderson')
['derek', 'anderson']
&amp;gt;&amp;gt;&amp;gt; wordninja.split('imateapot')
['im', 'a', 'teapot']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;21. IP&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;22. QQ&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1-9]([0-9]{5,11})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;23. &lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[0-9-()]{7,18}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;24. &lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[A-Za-z0-9_\-\u4e00-\u9fa5]+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;25. &lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;repodata [data](https://github.com/fighting41love/funNLP/tree/master/data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;26. &lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;2016679:44

Hiall

&amp;gt;&amp;gt; 2016-06-13 15:00:00-false



&amp;gt;&amp;gt; 2016-06-13 00:00:00-true



&amp;gt;&amp;gt; 2016-06-20 00:00:00-true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://github.com/shinyke/Time-NLP"&gt;java version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanzecheng/Time_NLP"&gt;python version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;27. &lt;/strong&gt; &lt;a href="https://github.com/Embedding/Chinese-Word-Vectors"&gt;github repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;28. &lt;/strong&gt; &lt;a href="https://github.com/wainshine/Company-Names-Corpus"&gt;github repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;29. &lt;/strong&gt; &lt;a href="https://github.com/panhaiqi/AncientPoetry"&gt;github repo&lt;/a&gt; &lt;a href="https://github.com/chinese-poetry/chinese-poetry"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;30. THU&lt;/strong&gt; &lt;a href="http://thuocl.thunlp.org/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;repodata.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;31. &lt;/strong&gt; &lt;a href="https://github.com/codemayq/chaotbot_corpus_Chinese"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;:, PTT, , , ,,
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;32. :&lt;/strong&gt; &lt;a href="https://github.com/thunlp/Chinese_Rumor_Dataset"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;json

rumorCode: 
title: 
informerName: 
informerUrl: 
rumormongerName: 
rumormongerUr: 
rumorText: 
visitTimes: 
result: 
publishTime: 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;33. &lt;/strong&gt;&lt;a href="https://github.com/CasterWx/python-girlfriend-mood/"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;repodata.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;repo, data.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;34. &lt;/strong&gt;&lt;a href="https://pan.baidu.com/s/1QUsKcFWZ7Tg1dk_AbldZ1A" rel="nofollow"&gt;&lt;/a&gt; : 2dva&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;35. QA:MatchZoo&lt;/strong&gt;  &lt;a href="https://github.com/NTMC-Community/MatchZoo"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;36. bert&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bert: &lt;a href="https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;bertslides: &lt;a href="https://pan.baidu.com/s/1OSPsIu2oh1iJ-bcXoDZpJQ" rel="nofollow"&gt;link&lt;/a&gt;
: iarj&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;: &lt;a href="https://github.com/NLPScott/bert-Chinese-classification-task"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert tutorial: &lt;a href="https://github.com/Socialbird-AILab/BERT-Classification-Tutorial"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert pytorch:  &lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert tensorflow: &lt;a href="https://github.com/macanv/BERT-BiLSTM-CRF-NER"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERTBERT&lt;a href="https://github.com/terrifyzhao/bert-utils"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert  keras  Kashgari: &lt;a href="https://github.com/BrikerMan/Kashgari"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bertELMO &lt;a href="https://jalammar.github.io/illustrated-bert/" rel="nofollow"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT: Pre-trained models and downstream applications: &lt;a href="https://github.com/asyml/texar/tree/master/examples/bert"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;37. Texar - Toolkit for Text Generation and Beyond&lt;/strong&gt;: &lt;a href="https://github.com/asyml/texar"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tensorflow&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;38. &lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ComplexEventExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;39. cocoNLP:&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; rake&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install cocoNLP&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from cocoNLP.extractor import extractor

&amp;gt;&amp;gt;&amp;gt; ex = extractor()

&amp;gt;&amp;gt;&amp;gt; text = '2018112711...18100065143132-6156-2938baizhantang@sina.com.cn yangyangfuture at gmail dot com'

# 
&amp;gt;&amp;gt;&amp;gt; emails = ex.extract_email(text)
&amp;gt;&amp;gt;&amp;gt; print(emails)

['baizhantang@sina.com.cn', 'yangyangfuture@gmail.com.cn']
# 
&amp;gt;&amp;gt;&amp;gt; cellphones = ex.extract_cellphone(text,nation='CHN')
&amp;gt;&amp;gt;&amp;gt; print(cellphones)

['18100065143', '13261562938']
# 
&amp;gt;&amp;gt;&amp;gt; cell_locs = [ex.extract_cellphone_location(cell,'CHN') for cell in cellphones]
&amp;gt;&amp;gt;&amp;gt; print(cell_locs)

cellphone_location [{'phone': '18100065143', 'province': '', 'city': '', 'zip_code': '200000', 'area_code': '021', 'phone_type': ''}]
# 
&amp;gt;&amp;gt;&amp;gt; locations = ex.extract_locations(text)
&amp;gt;&amp;gt;&amp;gt; print(locations)
['', '', '']
# 
&amp;gt;&amp;gt;&amp;gt; times = ex.extract_time(text)
&amp;gt;&amp;gt;&amp;gt; print(times)
time {"type": "timestamp", "timestamp": "2018-11-27 11:00:00"}
# 
&amp;gt;&amp;gt;&amp;gt; name = ex.extract_name(text)
&amp;gt;&amp;gt;&amp;gt; print(name)


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;40. +:&lt;/strong&gt; &lt;a href="https://github.com/VincentSit/ChinaMobilePhoneNumberRegex"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;41. XLORE::&lt;/strong&gt; &lt;a href="https://xlore.org/download.html" rel="nofollow"&gt;link&lt;/a&gt;&lt;br&gt;
TTL
&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;32,009&lt;/td&gt;
&lt;td&gt;150,241&lt;/td&gt;
&lt;td&gt;326,518&lt;/td&gt;
&lt;td&gt;508,768&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;1,629,591&lt;/td&gt;
&lt;td&gt;640,622&lt;/td&gt;
&lt;td&gt;1,235,178&lt;/td&gt;
&lt;td&gt;3,505,391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;157,370&lt;/td&gt;
&lt;td&gt;45,190&lt;/td&gt;
&lt;td&gt;26,723&lt;/td&gt;
&lt;td&gt;229.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;InstanceOf&lt;/td&gt;
&lt;td&gt;7,584,931&lt;/td&gt;
&lt;td&gt;1,449,925&lt;/td&gt;
&lt;td&gt;3,032,515&lt;/td&gt;
&lt;td&gt;12,067,371&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SubClassOf&lt;/td&gt;
&lt;td&gt;2,784&lt;/td&gt;
&lt;td&gt;191,577&lt;/td&gt;
&lt;td&gt;555,538&lt;/td&gt;
&lt;td&gt;749,899&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;/&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;10,216/336,890&lt;/td&gt;
&lt;td&gt;4,846/303,108&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;10,216/336,890&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;28,921/454,579&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;4,846/303,108&lt;/td&gt;
&lt;td&gt;28,921/454,579&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;42. &lt;/strong&gt; &lt;a href="https://reports.aminer.cn" rel="nofollow"&gt;link&lt;/a&gt;&lt;br&gt;
AI&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/nlp.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://www.aminer.cn/research_report/5c3d5a8709%20e961951592a49d?download=true&amp;amp;pathname=knowledgegraph.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://www.aminer.cn/research_report/5c3d5a5cecb160952fa10b76?download=true&amp;amp;pathname=datamining.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/selfdriving.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/translation.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/blockchain_public.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/robotics_beta.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/cg.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D &lt;a href="https://static.aminer.cn/misc/article/3d.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/facerecognition.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://static.aminer.cn/misc/article/aichip.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;43.:&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://ehudreiter.com" rel="nofollow"&gt;Ehud Reiter&lt;/a&gt;  NLG&lt;br&gt;
&lt;a href="https://github.com/ChenChengKuan/awesome-text-generation"&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href="https://drive.google.com/file/d/1Mdna3q986k6OoJNsfAHznTtnMAEVzv5z/view" rel="nofollow"&gt; - &lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/harvardnlp/Talk-Latent/blob/master/main.pdf"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;44.:&lt;/strong&gt;
&lt;a href="https://github.com/fxsjy/jieba"&gt;jieba&lt;/a&gt;&lt;a href="https://github.com/hankcs/pyhanlp"&gt;hanlp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;45.NLP:&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/hardNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; &lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;46.:&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://github.com/wb14123/couplet-dataset"&gt;70 link&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/wb14123/seq2seq-couplet"&gt; link&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;47.&lt;/strong&gt; &lt;a href="https://github.com/marteinn/The-Big-Username-Blacklist"&gt;github&lt;/a&gt;
: &lt;a href="https://github.com/marteinn/The-Big-Username-Blacklist/blob/master/list_raw.txt"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;administrator
administration
autoconfig
autodiscover
broadcasthost
domain
editor
guest
host
hostmaster
info
keybase.txt
localdomain
localhost
master
mail
mail0
mail1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;48.:&lt;/strong&gt;   &lt;a href="https://github.com/liuhuanyong/CrimeKgAssitant"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;856, 280,20W13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;49.:&lt;/strong&gt; &lt;a href="https://github.com/nonamestreet/weixin_public_corpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3GHTMLJSONnameaccountIDtitlecontent&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;50.cs224n&lt;/strong&gt;&lt;a href="http://web.stanford.edu/class/cs224n/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pytorch &lt;a href="https://github.com/DSKSD/DeepNLP-models-Pytorch"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href="https://github.com/graykode/nlp-tutorial"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;51.&lt;/strong&gt;&lt;a href="https://github.com/chizhanyuefeng/Chinese_OCR_CNN-RNN-CTC"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;52. /&lt;/strong&gt;&lt;a href="https://github.com/SophonPlus/ChineseNlpCorpus"&gt;github&lt;/a&gt;
&lt;a href="https://github.com/thunlp/THUOCL"&gt;THUOCLTHU Open Chinese Lexicon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;53.&lt;/strong&gt;&lt;a href="https://github.com/unbug/codelf"&gt;github&lt;/a&gt; &lt;a href="https://unbug.github.io/codelf/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;54.+&lt;/strong&gt;&lt;a href="https://pan.baidu.com/s/1MXZONaLgeaw0_TxZZDAIYQ" rel="nofollow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;: pea6&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GlassyWing/bi-lstm-crf"&gt;kerasBi-LSTM + CRF+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GlassyWing/transformer-word-segmenter"&gt;Universal Transformer + CRF &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yaoguangluo/NeroParser"&gt; java version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;55. NLPNatural Language Processingby Jacob Eisenstein&lt;/strong&gt; &lt;a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;56. &lt;/strong&gt;   &lt;a href="https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey"&gt;github&lt;/a&gt;&lt;br&gt;
LeaderboardState-of-the-art&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;57. ASR  + &lt;/strong&gt;  &lt;a href="https://github.com/nl8590687/ASRT_SpeechRecognition"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data Sets &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;THCHS30&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;data_thchs30.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/data_thchs30.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/data_thchs30.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;test-noise.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/test-noise.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/test-noise.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;resource.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/resource.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/resource.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Free ST Chinese Mandarin Corpus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ST-CMDS-20170001_1-OS.tar.gz
&lt;a href="http://cn-mirror.openslr.org/resources/38/ST-CMDS-20170001_1-OS.tar.gz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/38/ST-CMDS-20170001_1-OS.tar.gz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AIShell-1 &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;data_aishell.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/33/data_aishell.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/33/data_aishell.tgz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tar xzf data_aishell.tgz
$ cd data_aishell/wav
$ for tar in *.tar.gz;  do tar xvf $tar; done
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Primewords Chinese Corpus Set 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;primewords_md_2018_set1.tar.gz
&lt;a href="http://cn-mirror.openslr.org/resources/47/primewords_md_2018_set1.tar.gz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/47/primewords_md_2018_set1.tar.gz" rel="nofollow"&gt;OpenSLR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;58. &lt;/strong&gt;  &lt;a href="https://github.com/ideo/LaughDetection"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;59. Microsoft//&lt;/strong&gt; [github](&lt;a href="https://github.com/Microsoft/Recognizers-Text"&gt;https://github.com/Microsoft/Recognizers-Text&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;60. chinese-xinhua api&lt;/strong&gt; &lt;a href="https://github.com/pwxcoo/chinese-xinhua"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;61. &lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/TextGrapher"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TextGrapher - Text Content Grapher based on keyinfo extraction by NLP method&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;62. SpaCy &lt;/strong&gt; &lt;a href="https://github.com/howl-anderson/Chinese_models_for_SpaCy"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parser, NER, packagespacyspacy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;63. Common Voice&lt;/strong&gt;  &lt;a href="https://voice.mozilla.org/en/datasets" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;42,0001,400github&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;64.  pytorch&lt;/strong&gt;  &lt;a href="https://github.com/ShulinCao/OpenNRE-PyTorch"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;65. bert pytorch&lt;/strong&gt;  &lt;a href="https://github.com/Kyubyong/bert_ner"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;66. (Keyphrase) pke&lt;/strong&gt;  &lt;a href="https://github.com/boudinfl/pke"&gt;github&lt;/a&gt;&lt;br&gt;
&lt;a href="http://aclweb.org/anthology/C16-2015" rel="nofollow"&gt;pke: an open source python-based keyphrase extraction toolkit&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
github&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;67. &lt;/strong&gt;  &lt;a href="https://github.com/zhihao-chen/QASystemOnMedicalGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;repo&lt;a href="https://github.com/liuhuanyong/QASystemOnMedicalKG"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;68. &lt;/strong&gt;  &lt;a href="https://github.com/liuhuanyong/EventTriplesExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;69. 4&lt;/strong&gt; by SUCDT
&lt;a href="http://hlt.suda.edu.cn/index.php/Nlpcc-2019-shared-task" rel="nofollow"&gt;Homepage&lt;/a&gt;
homepage&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;70. cnocrOCRPython3&lt;/strong&gt; &lt;a href="https://github.com/breezedeus/cnocr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;71. &lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/PersonRelationKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;bootstrapping&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;72. nlp&lt;/strong&gt; &lt;a href="https://github.com/geekinglcq/CDCS"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Byte Cup 2018 &lt;/li&gt;
&lt;li&gt;MMC&lt;/li&gt;
&lt;li&gt; 2018&amp;amp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;73. &lt;/strong&gt; &lt;a href="https://github.com/skishore/makemeahanzi"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;/&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;74. speech-aligner: &lt;/strong&gt; &lt;a href="https://github.com/open-speech/speech-aligner"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;75. AmpliGraph: (Python)&lt;/strong&gt; &lt;a href="https://github.com/Accenture/AmpliGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;76. Scattertext (python)&lt;/strong&gt; &lt;a href="https://github.com/JasonKessler/scattertext"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;77. /BERT &amp;amp; ERNIE&lt;/strong&gt; &lt;a href="https://github.com/PaddlePaddle/LARK"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ERNIEnlpbert&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;78. NLP&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/LQU_HJ4q74lL5oCIk7w5RA" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;79. Synonyms&lt;/strong&gt; &lt;a href="https://github.com/huyingxi/Synonyms"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Synonyms &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;80. HarvestText--&lt;/strong&gt; &lt;a href="https://github.com/blmoistawinde/HarvestText"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;81. word2word(Python)-62/3,564&lt;/strong&gt; &lt;a href="https://github.com/Kyubyong/word2word"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;82. /(ASR)&lt;/strong&gt; &lt;a href="https://github.com/yc9701/pansori"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;83. ASR/&lt;/strong&gt; github&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;84. python:&lt;/strong&gt; &lt;a href="https://github.com/yixiu00001/LSTM-CRF-medical"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;85. &lt;/strong&gt; &lt;a href="https://github.com/LIAAD/yake"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;86. Kashgarigpt-2&lt;/strong&gt; &lt;a href="https://github.com/BrikerMan/Kashgari"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;87.  &lt;/strong&gt; &lt;a href="https://github.com/PKUJohnson/OpenData"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;88. TextTeaser: &lt;/strong&gt; &lt;a href="https://github.com/IndigoResearch/textteaser"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;89. &lt;/strong&gt; &lt;a href="https://github.com/howl-anderson/tools_for_corpus_of_people_daily"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;90. &lt;/strong&gt; &lt;a href="https://github.com/lpty/nlp_base"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;91. 14W&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/MusicLyricChatbot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;92. Siamese bilstm,&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/SiameseSentenceSimilarity"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;93. TransformerHacker News&lt;/strong&gt; &lt;a href="https://github.com/leod/hncynic"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;94. BERT&lt;/strong&gt; &lt;a href="https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;95. LitBankNLP100&lt;/strong&gt; &lt;a href="https://github.com/dbamman/litbank"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;96. &lt;/strong&gt; &lt;a href="https://github.com/baidu/information-extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;97.  fake news corpus&lt;/strong&gt; &lt;a href="https://github.com/several27/FakeNewsCorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;98. Facebook: LAMATransformer-XL/BERT/ELMo/GPT&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/LAMA"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;99. CommonsenseQAQA&lt;/strong&gt; &lt;a href="https://www.tau-nlp.org/commonsenseqa" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;100. &lt;/strong&gt; &lt;a href="https://github.com/husthuke/awesome-knowledge-graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;101.  PDF  PPT&lt;/strong&gt; &lt;a href="https://github.com/0voice/from_coder_to_expert"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;102. SQL&lt;/strong&gt; &lt;a href="https://github.com/paulfitz/mlsql"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;103. NLPEDA&lt;/strong&gt; &lt;a href="https://github.com/zhanlaoban/eda_nlp_for_Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; NLP &lt;a href="https://github.com/makcedward/nlpaug"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;104. &lt;/strong&gt; &lt;a href="https://github.com/YeYzheng/KGQA-Based-On-medicine"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;105. &lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ProductKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;13001065&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;106. mongodb&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/QAonMilitaryKG"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mongodb81005800jiebademo&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;107. &lt;/strong&gt; &lt;a href="https://github.com/xiaolalala/Distant-Supervised-Chinese-Relation-Extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;108. &lt;/strong&gt; &lt;a href="https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;109. ULMFiT   &lt;/strong&gt; &lt;a href="https://github.com/bigboNed3/chinese_ulmfit"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;110. &lt;/strong&gt; &lt;a href="https://github.com/Roujack/mathAI"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;111. &lt;/strong&gt; &lt;a href="https://github.com/philipperemy/name-dataset"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;112.  qingyun &lt;/strong&gt; &lt;a href="https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;10repo&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;113.  &lt;/strong&gt; &lt;a href="https://github.com/zhaoyingjun/chatbot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;seqGAN&lt;/li&gt;
&lt;li&gt;repo&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;114. &lt;/strong&gt; &lt;a href="https://github.com/xiangyuecn/AreaCity-JsSpider-StatsGov"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;js2019csvcsvjs&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;115.   &lt;/strong&gt; &lt;a href="https://github.com/wonderfulsuccess/chinese_abstractive_corpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;116. &lt;/strong&gt; &lt;a href="https://www.ownthink.com/#header-n30" rel="nofollow"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;qa &lt;a href="https://github.com/WenRichard/QAmodel-for-Retrievalchatbot"&gt;Amodel-for-Retrivalchatbot - Chinese Retreival chatbot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;117. &lt;/strong&gt; &lt;a href="https://github.com/lixiang0/WEB_KG"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;118. masr: &lt;/strong&gt; &lt;a href="https://github.com/lukhy/masr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;119. Python&lt;/strong&gt; &lt;a href="https://github.com/iver56/audiomentations"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;120. BERT&lt;/strong&gt; &lt;a href="https://github.com/ymcui/Chinese-BERT-wwm"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DRCD&lt;/strong&gt;SQuAD&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CMRC 2018&lt;/strong&gt;SQuAD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;121. ConvLab&lt;/strong&gt; &lt;a href="https://github.com/ConvLab/ConvLab"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;122. &lt;/strong&gt; &lt;a href="https://github.com/InsaneLife/ChineseNLPCorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;123. rasa&lt;/strong&gt; &lt;a href="https://github.com/GaoQ1/rasa_chatbot_cn"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;124. TensorFlowBERT&lt;/strong&gt; &lt;a href="https://github.com/yuanxiaosc/Entity-Relation-Extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entity and Relation Extraction Based on TensorFlow and BERT. TensorFlowBERT2019Schema based Knowledge Extraction, SKE 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;125. /&lt;/strong&gt; &lt;a href="https://github.com/lemonhu/stock-knowledge-graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;126. NLPTOP&lt;/strong&gt; &lt;a href="https://github.com/zhpmatrix/nlp-competitions-list-review"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;127. OpenCLaP&lt;/strong&gt; &lt;a href="https://github.com/thunlp/OpenCLaP"&gt;github&lt;/a&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT	bert-base		2654	22554	370MB&lt;/li&gt;
&lt;li&gt;BERT	bert-base		663	22554	370MB&lt;/li&gt;
&lt;li&gt;BERT	bert-base		903	22166	367MB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;128. UERBERTGPTELMO&lt;/strong&gt; &lt;a href="https://github.com/dbiir/UER-py"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyTorchUER&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;129. &lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ChineseEmbedding"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;,,,,.5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;130. -()&lt;/strong&gt; &lt;a href="https://github.com/charlesXu86/Chatbot_CN"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLUNLGDjango,nlpkgrestful&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;131. g2pC&lt;/strong&gt; &lt;a href="https://github.com/Kyubyong/g2pC"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;132. Zincbase &lt;/strong&gt; &lt;a href="https://github.com/tomgrek/zincbase"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;133. /&lt;/strong&gt; &lt;a href="https://github.com/THUNLP-AIPoet/Datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;134. &lt;/strong&gt; &lt;a href="https://github.com/HaveTwoBrush/cn2an"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;135. &lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/MiningZhiDaoQACorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5809385800&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;136. &lt;/strong&gt; &lt;a href="https://github.com/WenRichard/KBQA-BERT"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERTonlineoutline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;137. jieba_fast jieba&lt;/strong&gt; &lt;a href="https://github.com/deepcs233/jieba_fast"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cpythonjiebaDAGHMMvitrebi&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;138. &lt;/strong&gt; &lt;a href="https://github.com/ziishaned/learn-regex/blob/master/translations/README-cn.md"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;139. &lt;/strong&gt; &lt;a href="https://github.com/ymcui/Chinese-RC-Datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;140. BERT&lt;/strong&gt; &lt;a href="https://github.com/Hellisotherpeople/CX_DB8"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;141. Python&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/gDZyTbM1nw3fbEnU--y3nQ" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;142. &lt;/strong&gt; &lt;a href="https://github.com/lihanghang/Knowledge-Graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;143. &lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;851620135M&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;144. StanfordNLP 0.2.0Python&lt;/strong&gt; &lt;a href="https://stanfordnlp.github.io/stanfordnlp/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;145. NeuralNLP-NeuralClassifier&lt;/strong&gt; &lt;a href="https://github.com/Tencent/NeuralNLP-NeuralClassifier"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;146. &lt;/strong&gt; &lt;a href="https://github.com/cdqa-suite/cdQA"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;147. NeuroNER vs. BertNER&lt;/strong&gt; &lt;a href="https://github.com/EOA-AILab/NER-Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;148. &lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ImportantEventExtractor"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An exploration for Eventline (important news Rank organized by pulic time)docrank&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;149. 2019(7)&lt;/strong&gt; &lt;a href="https://github.com/bojone/kg-2019"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;150. &lt;/strong&gt; &lt;a href="https://github.com/lemonhu/open-entity-relation-extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;151. GPT2&lt;/strong&gt; &lt;a href="https://github.com/Morizeyao/GPT2-Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;152. ML-NLP - (Machine Learning)NLP&lt;/strong&gt; &lt;a href="https://github.com/NLP-LOVE/ML-NLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;153. nlp4han:(//////NER/N/HMM///&lt;/strong&gt; &lt;a href="https://github.com/kidden/nlp4han"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;154. XLMFacebook&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/XLM"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;155. BERT&lt;/strong&gt; &lt;a href="https://github.com/sakuranew/BERT-AttributeExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;156. , &lt;/strong&gt; &lt;a href="https://github.com/didi/ChineseNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;157. CoupletAI - CNN+Bi-LSTM+Attention &lt;/strong&gt; &lt;a href="https://github.com/WiseDoge/CoupletAI"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;158. 50&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/AbstractKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;159. MiningZhiDaoQACorpus - 580&lt;/strong&gt; &lt;a href="%E7%99%BE%E5%BA%A6%E7%9F%A5%E9%81%93%E9%97%AE%E7%AD%94%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%8C%E5%8C%85%E6%8B%AC%E8%B6%85%E8%BF%87580%E4%B8%87%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E6%AF%8F%E4%B8%AA%E9%97%AE%E9%A2%98%E5%B8%A6%E6%9C%89%E9%97%AE%E9%A2%98%E6%A0%87%E7%AD%BE%E3%80%82%E5%9F%BA%E4%BA%8E%E8%AF%A5%E9%97%AE%E7%AD%94%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%8C%E5%8F%AF%E6%94%AF%E6%8C%81%E5%A4%9A%E7%A7%8D%E5%BA%94%E7%94%A8%EF%BC%8C%E5%A6%82%E9%80%BB%E8%BE%91%E6%8C%96%E6%8E%98"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;160. brat rapid annotation tool: &lt;/strong&gt; &lt;a href="http://brat.nlplab.org/index.html" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;161. 1.4&lt;/strong&gt; &lt;a href="https://github.com/ownthink/KnowledgeGraphData"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;162. nlp&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/_aVwSWuYho_7MUT0LuFgVA" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;163. allennlp:&lt;/strong&gt; &lt;a href="https://github.com/allenai/allennlp-reading-comprehension"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;164. PDF&lt;/strong&gt; &lt;a href="https://github.com/camelot-dev/camelot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;165. GraphbrainAI&lt;/strong&gt; &lt;a href="https://github.com/graphbrain/graphbrain"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;166. &lt;/strong&gt; &lt;a href="https://github.com/JAIJANYANI/Automated-Resume-Screening-System"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;167. &lt;/strong&gt; &lt;a href="https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;168. &amp;amp;&amp;amp;&amp;amp;&lt;/strong&gt; &lt;a href="https://github.com/brightmart/ChineseGLUE"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;169.  OCR &lt;/strong&gt; &lt;a href="https://github.com/AnyListen/tools-ocr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;c++ OCR &lt;a href="https://github.com/myhub/tr"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;170. &lt;/strong&gt; &lt;a href="https://github.com/bitdata/ocrtable"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;171. &lt;/strong&gt; &lt;a href="https://github.com/fighting41love/become-yukarin"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;172. Python()&lt;/strong&gt; &lt;a href="https://github.com/gooofy/py-nltools"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;173. similarityjava&lt;/strong&gt; &lt;a href="https://github.com/shibing624/similarity"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;174. ALBERT&lt;/strong&gt; &lt;a href="https://github.com/brightmart/albert_zh"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;175. Transformers 2.0&lt;/strong&gt; &lt;a href="https://github.com/huggingface/transformers"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow 2.0  PyTorch (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet) 8/33/102&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;176. Audioset&lt;/strong&gt; &lt;a href="https://github.com/AppleHolic/audioset_augmentor"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;177. Poplar&lt;/strong&gt; &lt;a href="https://github.com/synyi/poplar"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;178. &lt;/strong&gt; &lt;a href="https://github.com/yu45020/Text_Segmentation_Image_Inpainting"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;179. 186&lt;/strong&gt; &lt;a href="https://github.com/google/UniNum"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;180. Amazon-&lt;/strong&gt; &lt;a href="https://github.com/alexa/alexa-prize-topical-chat-dataset/"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;181. &lt;/strong&gt; &lt;a href="https://github.com/zedom1/error-detection"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;182. &lt;/strong&gt; &lt;a href="https://github.com/berniey/hanziconv"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;183. Python&lt;/strong&gt; &lt;a href="https://github.com/cdimascio/py-readability-metrics"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;184. //&lt;/strong&gt; &lt;a href="https://github.com/LG-1/video_music_book_datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;185. ()&lt;/strong&gt; &lt;a href="https://github.com/npubird/KnowledgeGraphCourse"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fighting41love</author><guid isPermaLink="false">https://github.com/fighting41love/funNLP</guid><pubDate>Tue, 05 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>