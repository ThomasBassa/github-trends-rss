<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, This week</title><link>https://github.com/trending/python?since=weekly</link><description>The top repositories on GitHub for python, measured weekly</description><pubDate>Fri, 01 Nov 2019 00:05:40 GMT</pubDate><lastBuildDate>Fri, 01 Nov 2019 00:05:40 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>1400</ttl><item><title>google-research/google-research #1 in Python, This week</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>apachecn/AiLearning #2 in Python, This week</title><link>https://github.com/apachecn/AiLearning</link><description>&lt;p&gt;&lt;i&gt;AiLearning: 机器学习 - MachineLearning - ML、深度学习 - DeepLearning - DL、自然语言处理 NLP&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a href="https://www.apachecn.org" rel="nofollow"&gt;
        &lt;img width="200" src="https://camo.githubusercontent.com/9d35c24a9d070c56093b5598ef22afae12a2f45b/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f6c6f676f2e6a7067" data-canonical-src="http://data.apachecn.org/img/logo.jpg" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;br&gt;
    &lt;a href="https://www.apachecn.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/63fa00e49cf2df161cf6022c501b145d225bc335/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d484f4d452d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-HOME-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="http://home.apachecn.org/about/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/971440c5a29c84e984688b2ebe165dc27aa8cf61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d41424f55542d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-ABOUT-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="mailto:apache@163.com"&gt;&lt;img src="https://camo.githubusercontent.com/f1fab6e562c98b86b95a7c44eae041e04022ec3e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d456d61696c2d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-Email-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a id="user-content-ai-learning" class="anchor" aria-hidden="true" href="#ai-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/apachecn/AiLearning"&gt;AI learning&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-组织介绍" class="anchor" aria-hidden="true" href="#组织介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;组织介绍&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;合作or侵权，请联系: &lt;code&gt;apachecn@163.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;我们不是 Apache 的官方组织/机构/团体，只是 Apache 技术栈（以及 AI）的爱好者！&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApacheCN - 学习机器学习群【629470233】&lt;a href="//shang.qq.com/wpa/qunwpa?idkey=30e5f1123a79867570f665aa3a483ca404b1c3f77737bc01ec520ed5f078ddef" rel="nofollow"&gt;&lt;img border="0" src="https://camo.githubusercontent.com/6ef3c468024fa0a3ac83d0084d8d0847c6f57769/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f6c6f676f2f417061636865434e2d67726f75702e706e67" alt="ApacheCN - 学习机器学习群[629470233]" title="ApacheCN - 学习机器学习群[629470233]" data-canonical-src="http://data.apachecn.org/img/logo/ApacheCN-group.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;欢迎任何人参与和完善：一个人可以走的很快，但是一群人却可以走的更远&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-路线图" class="anchor" aria-hidden="true" href="#路线图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;路线图&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;入门只看: 步骤 1 =&amp;gt; 2 =&amp;gt; 3，你可以当大牛！&lt;/li&gt;
&lt;li&gt;中级补充 - 资料库: &lt;a href="https://github.com/apachecn/ai-roadmap"&gt;https://github.com/apachecn/ai-roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-1机器学习---基础" class="anchor" aria-hidden="true" href="#1机器学习---基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.机器学习 - 基础&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-基本介绍" class="anchor" aria-hidden="true" href="#基本介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;基本介绍&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;资料来源: Machine Learning in Action(机器学习实战-个人笔记)&lt;/li&gt;
&lt;li&gt;统一数据地址: &lt;a href="https://github.com/apachecn/data"&gt;https://github.com/apachecn/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;书籍下载地址: &lt;a href="https://github.com/apachecn/data/tree/master/book"&gt;https://github.com/apachecn/data/tree/master/book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;机器学习下载地址: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"&gt;https://github.com/apachecn/data/tree/master/机器学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;深度学习数据地址: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"&gt;https://github.com/apachecn/data/tree/master/深度学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;推荐系统数据地址: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"&gt;https://github.com/apachecn/data/tree/master/推荐系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;视频网站：优酷 ／bilibili / Acfun / 网易云课堂，可直接在线播放。（最下方有相应链接）&lt;/li&gt;
&lt;li&gt;-- 推荐 &lt;a href="https://github.com/RedstoneWill"&gt;红色石头&lt;/a&gt;: &lt;a href="https://github.com/apachecn/ntu-hsuantienlin-ml"&gt;台湾大学林轩田机器学习笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;-- 推荐 &lt;a href="https://feisky.xyz/machine-learning" rel="nofollow"&gt;机器学习笔记&lt;/a&gt;: &lt;a href="https://feisky.xyz/machine-learning" rel="nofollow"&gt;https://feisky.xyz/machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-学习文档" class="anchor" aria-hidden="true" href="#学习文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;学习文档&lt;/h3&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;th&gt;模块&lt;/th&gt;
    &lt;th&gt;章节&lt;/th&gt;
    &lt;th&gt;类型&lt;/th&gt;
    &lt;th&gt;负责人(GitHub)&lt;/th&gt;
    &lt;th&gt;QQ&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/1.机器学习基础.md"&gt; 第 1 章: 机器学习基础&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;介绍&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/ElmaDavies"&gt;@毛红动&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1306014226&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/2.k-近邻算法.md"&gt;第 2 章: KNN 近邻算法&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/youyj521"&gt;@尤永江&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;279393323&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/3.决策树.md"&gt;第 3 章: 决策树&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/jingwangfei"&gt;@景涛&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;844300439&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/4.朴素贝叶斯.md"&gt;第 4 章: 朴素贝叶斯&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/wnma3mz"&gt;@wnma3mz&lt;/a&gt;&lt;br&gt;&lt;a href="https://github.com/kailian"&gt;@分析&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1003324213&lt;br&gt;244970749&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/5.Logistic回归.md"&gt;第 5 章: Logistic回归&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@微光同尘&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/6.支持向量机.md"&gt;第 6 章: SVM 支持向量机&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/VPrincekin"&gt;@王德红&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;934969547&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;网上组合内容&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/7.集成方法-随机森林和AdaBoost.md"&gt;第 7 章: 集成方法（随机森林和 AdaBoost）&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529815144&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/8.回归.md"&gt;第 8 章: 回归&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;回归&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@微光同尘&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/9.树回归.md"&gt;第 9 章: 树回归&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;回归&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@微光同尘&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/10.k-means聚类.md"&gt;第 10 章: K-Means 聚类&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;聚类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/xuzhaoqing"&gt;@徐昭清&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;827106588&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/11.使用Apriori算法进行关联分析.md"&gt;第 11 章: 利用 Apriori 算法进行关联分析&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;频繁项集&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/WindZQ"&gt;@刘海飞&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1049498972&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/12.使用FP-growth算法来高效发现频繁项集.md"&gt;第 12 章: FP-growth 高效发现频繁项集&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;频繁项集&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/mikechengwei"&gt;@程威&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;842725815&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/13.利用PCA来简化数据.md"&gt;第 13 章: 利用 PCA 来简化数据&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;工具&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/lljuan330"&gt;@廖立娟&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;835670618&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/14.利用SVD简化数据.md"&gt;第 14 章: 利用 SVD 来简化数据&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;工具&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/marsjhao"&gt;@张俊皓&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;714974242&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/15.大数据与MapReduce.md"&gt;第 15 章: 大数据与 MapReduce&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;工具&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/wnma3mz"&gt;@wnma3mz&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1003324213&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Ml项目实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/16.推荐系统.md"&gt;第 16 章: 推荐系统（已迁移）&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;项目&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/apachecn/RecommenderSystems"&gt;推荐系统（迁移后地址）&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;第一期的总结&lt;/td&gt;
    &lt;td&gt;&lt;a href="report/2017-04-08_第一期的总结.md"&gt;2017-04-08: 第一期的总结&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;总结&lt;/td&gt;
    &lt;td&gt;总结&lt;/td&gt;
    &lt;td&gt;529815144&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-网站视频" class="anchor" aria-hidden="true" href="#网站视频"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;网站视频&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/question/20691338/answer/248678328" rel="nofollow"&gt;知乎问答-爆炸啦-机器学习该怎么入门？&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当然我知道，第一句就会被吐槽，因为科班出身的人，不屑的吐了一口唾沫，说傻X，还评论 Andrew Ng 的视频。。&lt;/p&gt;
&lt;p&gt;我还知道还有一部分人，看 Andrew Ng 的视频就是看不懂，那神秘的数学推导，那迷之微笑的英文版的教学，我何尝又不是这样走过来的？？ 我的心可能比你们都痛，因为我在网上收藏过上10部《机器学习》相关视频，外加国内本土风格的教程：7月+小象 等等，我都很难去听懂，直到有一天，被一个百度的高级算法分析师推荐说：《机器学习实战》还不错，通俗易懂，你去试试？？&lt;/p&gt;
&lt;p&gt;我试了试，还好我的Python基础和调试能力还不错，基本上代码都调试过一遍，很多高大上的 "理论+推导"，在我眼中变成了几个 "加减乘除+循环"，我想这不就是像我这样的程序员想要的入门教程么？&lt;/p&gt;
&lt;p&gt;很多程序员说机器学习 TM 太难学了，是的，真 TM 难学，我想最难的是：没有一本像《机器学习实战》那样的作者愿意以程序员 Coding 角度去给大家讲解！！&lt;/p&gt;
&lt;p&gt;最近几天，GitHub 涨了 300颗 star，加群的200人， 现在还在不断的增加++，我想大家可能都是感同身受吧！&lt;/p&gt;
&lt;p&gt;很多想入门新手就是被忽悠着收藏收藏再收藏，但是最后还是什么都没有学到，也就是"资源收藏家"，也许新手要的就是 &lt;a href="https://docs.apachecn.org/map" rel="nofollow"&gt;MachineLearning(机器学习) 学习路线图&lt;/a&gt;。没错，我可以给你们的一份，因为我们还通过视频记录下来我们的学习过程。水平当然也有限，不过对于新手入门，绝对没问题，如果你还不会，那算我输！！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;视频怎么看？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/4aeb51811bbc73e4715ced3ebc64ab26670bdc67/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692d636f6d706172652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/4aeb51811bbc73e4715ced3ebc64ab26670bdc67/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692d636f6d706172652e6a7067" alt="" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-bilibili-compare.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;理论科班出身-建议去学习 Andrew Ng 的视频（Ng 的视频绝对是权威，这个毋庸置疑）&lt;/li&gt;
&lt;li&gt;编码能力强 - 建议看我们的&lt;a href="https://space.bilibili.com/97678687/#!/channel/detail?cid=22486" rel="nofollow"&gt;《机器学习实战-教学版》&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;编码能力弱 - 建议看我们的&lt;a href="https://space.bilibili.com/97678687/#!/channel/detail?cid=13045" rel="nofollow"&gt;《机器学习实战-讨论版》&lt;/a&gt;，不过在看理论的时候，看 教学版-理论部分；讨论版的废话太多，不过在讲解代码的时候是一行一行讲解的；所以，根据自己的需求，自由的组合。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;【免费】数学教学视频 - 可汗学院 入门篇&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@于振梓&lt;/a&gt; 推荐: 可汗学院-网易公开课&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;概率&lt;/th&gt;
&lt;th&gt;统计&lt;/th&gt;
&lt;th&gt;线性代数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/probability.html" rel="nofollow"&gt;可汗学院(概率)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/khstatistics.html" rel="nofollow"&gt;可汗学院(统计学)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/linearalgebra.html" rel="nofollow"&gt;可汗学院(线性代数)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;机器学习视频 - ApacheCN 教学版&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AcFun&lt;/td&gt;
&lt;td&gt;B站&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a title="AcFun（机器学习视频）" href="http://www.acfun.cn/u/12540256.aspx#page=1" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/122aa82278121b78486f6cc20b3813851832d1b0/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d416346756e2e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-AcFun.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a title="bilibili（机器学习视频）" href="https://space.bilibili.com/97678687/#!/channel/index" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/c04107f0478a7e3e15b5103004d534af8ec36afd/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-bilibili.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;优酷&lt;/td&gt;
&lt;td&gt;网易云课堂&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a title="YouKu（机器学习视频）" href="http://i.youku.com/apachecn" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/ff306cb8a0eac3f6a9a59f01e73b538b0be8588e/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434d2d4d4c2d796f756b752e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCM-ML-youku.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a title="WangYiYunKeTang（机器学习视频）" href="http://study.163.com/course/courseMain.htm?courseId=1004582003" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/615d94638b0fefc25253ca5a4147a5153e35379f/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434d2d4d4c2d57616e67596959756e4b6554616e672e706e67" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCM-ML-WangYiYunKeTang.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;【免费】机器/深度学习视频 - 吴恩达&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;机器学习&lt;/th&gt;
&lt;th&gt;深度学习&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://study.163.com/course/courseMain.htm?courseId=1004570029" rel="nofollow"&gt;吴恩达机器学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://mooc.study.163.com/course/2001281002?tid=2001392029" rel="nofollow"&gt;神经网络和深度学习&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-2深度学习" class="anchor" aria-hidden="true" href="#2深度学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.深度学习&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-入门基础" class="anchor" aria-hidden="true" href="#入门基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;入门基础&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="/docs/dl/%E5%8F%8D%E5%90%91%E4%BC%A0%E9%80%92.md"&gt;反向传递&lt;/a&gt;: &lt;a href="https://www.cnblogs.com/charlotte77/p/5629865.html" rel="nofollow"&gt;https://www.cnblogs.com/charlotte77/p/5629865.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/CNN%E5%8E%9F%E7%90%86.md"&gt;CNN原理&lt;/a&gt;: &lt;a href="http://www.cnblogs.com/charlotte77/p/7759802.html" rel="nofollow"&gt;http://www.cnblogs.com/charlotte77/p/7759802.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/RNN%E5%8E%9F%E7%90%86.md"&gt;RNN原理&lt;/a&gt;: &lt;a href="https://blog.csdn.net/qq_39422642/article/details/78676567" rel="nofollow"&gt;https://blog.csdn.net/qq_39422642/article/details/78676567&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/LSTM%E5%8E%9F%E7%90%86.md"&gt;LSTM原理&lt;/a&gt;: &lt;a href="https://blog.csdn.net/weixin_42111770/article/details/80900575" rel="nofollow"&gt;https://blog.csdn.net/weixin_42111770/article/details/80900575&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-pytorch---教程" class="anchor" aria-hidden="true" href="#pytorch---教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pytorch - 教程&lt;/h3&gt;
&lt;p&gt;-- 待更新&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-20---教程" class="anchor" aria-hidden="true" href="#tensorflow-20---教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 - 教程&lt;/h3&gt;
&lt;p&gt;-- 待更新&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目录结构:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97.md"&gt;安装指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/Keras%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.md"&gt;Kears 快速入门&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE_1_%E7%94%B5%E5%BD%B1%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB.md"&gt;实战项目 1 电影情感分类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE_2_%E6%B1%BD%E8%BD%A6%E7%87%83%E6%B2%B9%E6%95%88%E7%8E%87.md"&gt;实战项目 2 汽车燃油效率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96_%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88.md"&gt;实战项目 优化: 过拟合，欠拟合&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-3自然语言处理" class="anchor" aria-hidden="true" href="#3自然语言处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.自然语言处理&lt;/h2&gt;
&lt;p&gt;学习过程中-内心复杂的变化！！！&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;自从学习&lt;span class="pl-c1"&gt;NLP&lt;/span&gt;以后，才发现国内与国外的典型区别:
&lt;span class="pl-c1"&gt;1&lt;/span&gt;. 对资源的态度是完全相反的:
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;) 国内：就好像为了名气，举办工作装逼的会议，就是没有干货，全部都是象征性的&lt;span class="pl-c1"&gt;PPT&lt;/span&gt;介绍，不是针对在做的各位
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;）国外：就好像是为了推动nlp进步一样，分享者各种干货资料和具体的实现。（特别是: python自然语言处理）
&lt;span class="pl-c1"&gt;2&lt;/span&gt;. 论文的实现：
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;) 各种高大上的论文实现，却还是没看到一个像样的GitHub项目！（可能我的搜索能力差了点，一直没找到）
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;）国外就不举例了，我看不懂！
&lt;span class="pl-c1"&gt;3&lt;/span&gt;. 开源的框架
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;）国外的开源框架： tensorflow&lt;span class="pl-k"&gt;/&lt;/span&gt;pytorch 文档&lt;span class="pl-k"&gt;+&lt;/span&gt;教程&lt;span class="pl-k"&gt;+&lt;/span&gt;视频（官方提供）
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;) 国内的开源框架: 额额，还真举例不出来！但是牛逼吹得不比国外差！（MXNet虽然有众多国人参与开发，但不能算是国内开源框架。基于MXNet的动手学深度学习(http:&lt;span class="pl-k"&gt;//&lt;/span&gt;zh.d2l.ai &lt;span class="pl-k"&gt;&amp;amp;&lt;/span&gt; https:&lt;span class="pl-k"&gt;//&lt;/span&gt;discuss.gluon.ai&lt;span class="pl-k"&gt;/&lt;/span&gt;t&lt;span class="pl-k"&gt;/&lt;/span&gt;topic&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;753&lt;/span&gt;)中文教程,已经由沐神(李沐)以及阿斯顿·张讲授录制，公开发布(文档&lt;span class="pl-k"&gt;+&lt;/span&gt;第一季教程&lt;span class="pl-k"&gt;+&lt;/span&gt;视频）。)
每一次深入都要去翻墙，每一次深入都要Google，每一次看着国内的说：哈工大、讯飞、中科大、百度、阿里多牛逼，但是资料还是得国外去找！
有时候真的挺恨的！真的有点瞧不起自己国内的技术环境！

当然谢谢国内很多博客大佬，特别是一些入门的Demo和基本概念。【深入的水平有限，没看懂】&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/6535fd936f9a2b1db5392b626320ce725854f675/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6e6c702f46393435383146363443323141313039344134373333393744464134324639432e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/6535fd936f9a2b1db5392b626320ce725854f675/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6e6c702f46393435383146363443323141313039344134373333393744464134324639432e6a7067" alt="" data-canonical-src="http://data.apachecn.org/img/AiLearning/nlp/F94581F64C21A1094A473397DFA42F9C.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;【入门须知】必须了解&lt;/strong&gt;: &lt;a href="https://github.com/apachecn/AiLearning/tree/master/docs/nlp"&gt;https://github.com/apachecn/AiLearning/tree/master/docs/nlp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;【入门教程】强烈推荐: PyTorch 自然语言处理&lt;/strong&gt;: &lt;a href="https://github.com/apachecn/NLP-with-PyTorch"&gt;https://github.com/apachecn/NLP-with-PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python 自然语言处理 第二版: &lt;a href="https://usyiyi.github.io/nlp-py-2e-zh" rel="nofollow"&gt;https://usyiyi.github.io/nlp-py-2e-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;推荐一个&lt;a href="https://github.com/liuhuanyong"&gt;liuhuanyong大佬&lt;/a&gt;整理的nlp全面知识体系: &lt;a href="https://liuhuanyong.github.io" rel="nofollow"&gt;https://liuhuanyong.github.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开源 - 词向量库集合:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Embedding/Chinese-Word-Vectors"&gt;https://github.com/Embedding/Chinese-Word-Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/brightmart/nlp_chinese_corpus"&gt;https://github.com/brightmart/nlp_chinese_corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/codemayq/chinese_chatbot_corpus"&gt;https://github.com/codemayq/chinese_chatbot_corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/candlewill/Dialog_Corpus"&gt;https://github.com/candlewill/Dialog_Corpus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-1使用场景-百度公开课" class="anchor" aria-hidden="true" href="#1使用场景-百度公开课"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.使用场景 （百度公开课）&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;第一部分 入门介绍&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;1.) &lt;a href="/docs/nlp/1.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md"&gt;自然语言处理入门介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;第二部分 机器翻译&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;2.) &lt;a href="/docs/nlp/2.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.md"&gt;机器翻译&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;第三部分 篇章分析&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;3.1.) &lt;a href="/docs/nlp/3.1.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E5%86%85%E5%AE%B9%E6%A6%82%E8%BF%B0.md"&gt;篇章分析-内容概述&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2.) &lt;a href="/docs/nlp/3.2.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E5%86%85%E5%AE%B9%E6%A0%87%E7%AD%BE.md"&gt;篇章分析-内容标签&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3.) &lt;a href="/docs/nlp/3.3.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md"&gt;篇章分析-情感分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.4.) &lt;a href="/docs/nlp/3.4.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81.md"&gt;篇章分析-自动摘要&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;第四部分 UNIT-语言理解与交互技术&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;4.) &lt;a href="/docs/nlp/4.UNIT-%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E4%BA%A4%E4%BA%92%E6%8A%80%E6%9C%AF.md"&gt;UNIT-语言理解与交互技术&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-应用领域" class="anchor" aria-hidden="true" href="#应用领域"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;应用领域&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-中文分词" class="anchor" aria-hidden="true" href="#中文分词"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;中文分词：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;构建DAG图&lt;/li&gt;
&lt;li&gt;动态规划查找，综合正反向（正向加权反向输出）求得DAG最大概率路径&lt;/li&gt;
&lt;li&gt;使用了SBME语料训练了一套 HMM + Viterbi 模型，解决未登录词问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-1文本分类text-classification" class="anchor" aria-hidden="true" href="#1文本分类text-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.文本分类（Text Classification）&lt;/h4&gt;
&lt;p&gt;文本分类是指标记句子或文档，例如电子邮件垃圾邮件分类和情感分析。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者文本分类数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html" rel="nofollow"&gt;路透社Newswire主题分类&lt;/a&gt;（路透社-21578）。1987年路透社出现的一系列新闻文件，按类别编制索引。&lt;a href="http://trec.nist.gov/data/reuters/reuters.html" rel="nofollow"&gt;另见RCV1，RCV2和TRC2&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ai.stanford.edu/~amaas/data/sentiment" rel="nofollow"&gt;IMDB电影评论情感分类（斯坦福）&lt;/a&gt;。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="nofollow"&gt;新闻组电影评论情感分类（康奈尔）&lt;/a&gt;。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有关更多信息，请参阅帖子：
&lt;a href="http://ana.cachopo.org/datasets-for-single-label-text-categorization" rel="nofollow"&gt;单标签文本分类的数据集&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;情感分析&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;比赛地址: &lt;a href="https://www.kaggle.com/c/word2vec-nlp-tutorial" rel="nofollow"&gt;https://www.kaggle.com/c/word2vec-nlp-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方案一(0.86)：WordCount + 朴素 Bayes&lt;/li&gt;
&lt;li&gt;方案二(0.94)：LDA + 分类模型（knn/决策树/逻辑回归/svm/xgboost/随机森林）
&lt;ul&gt;
&lt;li&gt;a) 决策树效果不是很好，这种连续特征不太适合的&lt;/li&gt;
&lt;li&gt;b) 通过参数调整 200 个topic，信息量保存效果较优（计算主题）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;方案三(0.72)：word2vec + CNN
&lt;ul&gt;
&lt;li&gt;说实话：没有一个好的机器，是调不出来一个好的结果 (: 逃&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;通过AUC 来评估模型的效果&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-2语言模型language-modeling" class="anchor" aria-hidden="true" href="#2语言模型language-modeling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.语言模型（Language Modeling）&lt;/h4&gt;
&lt;p&gt;语言建模涉及开发一种统计模型，用于预测句子中的下一个单词或一个单词中的下一个单词。它是语音识别和机器翻译等任务中的前置任务。&lt;/p&gt;
&lt;p&gt;它是语音识别和机器翻译等任务中的前置任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者语言建模数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.gutenberg.org/" rel="nofollow"&gt;古腾堡项目&lt;/a&gt;，一系列免费书籍，可以用纯文本检索各种语言。&lt;/li&gt;
&lt;li&gt;还有更多正式的语料库得到了很好的研究; 例如：
&lt;a href="https://en.wikipedia.org/wiki/Brown_Corpus" rel="nofollow"&gt;布朗大学现代美国英语标准语料库&lt;/a&gt;。大量英语单词样本。
&lt;a href="https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark"&gt;谷歌10亿字语料库&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;新词发现&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;中文分词新词发现&lt;/li&gt;
&lt;li&gt;python3利用互信息和左右信息熵的中文分词新词发现&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanzecheng/Chinese_segment_augment"&gt;https://github.com/zhanzecheng/Chinese_segment_augment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;句子相似度识别&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;项目地址: &lt;a href="https://www.kaggle.com/c/quora-question-pairs" rel="nofollow"&gt;https://www.kaggle.com/c/quora-question-pairs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;解决方案: word2vec + Bi-GRU&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;文本纠错&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;bi-gram + levenshtein&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-3图像字幕image-captioning" class="anchor" aria-hidden="true" href="#3图像字幕image-captioning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.图像字幕（Image Captioning）&lt;/h4&gt;
&lt;p&gt;mage字幕是为给定图像生成文本描述的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者图像字幕数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://mscoco.org/dataset/#overview" rel="nofollow"&gt;上下文中的公共对象（COCO）&lt;/a&gt;。包含超过12万张带描述的图像的集合&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html" rel="nofollow"&gt;Flickr 8K&lt;/a&gt;。从flickr.com获取的8千个描述图像的集合。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://shannon.cs.illinois.edu/DenotationGraph/" rel="nofollow"&gt;Flickr 30K&lt;/a&gt;。从flickr.com获取的3万个描述图像的集合。
欲了解更多，请看帖子：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://sidgan.me/technical/2016/01/09/Exploring-Datasets" rel="nofollow"&gt;探索图像字幕数据集，2016年&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-4机器翻译machine-translation" class="anchor" aria-hidden="true" href="#4机器翻译machine-translation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.机器翻译（Machine Translation）&lt;/h4&gt;
&lt;p&gt;机器翻译是将文本从一种语言翻译成另一种语言的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者机器翻译数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.isi.edu/natural-language/download/hansard/" rel="nofollow"&gt;加拿大第36届议会的协调国会议员&lt;/a&gt;。成对的英语和法语句子。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.statmt.org/europarl/" rel="nofollow"&gt;欧洲议会诉讼平行语料库1996-2011&lt;/a&gt;。句子对一套欧洲语言。
有大量标准数据集用于年度机器翻译挑战; 看到：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://www.statmt.org/" rel="nofollow"&gt;统计机器翻译&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;机器翻译&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Encoder + Decoder(Attention)&lt;/li&gt;
&lt;li&gt;参考案例: &lt;a href="http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="nofollow"&gt;http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-5问答系统question-answering" class="anchor" aria-hidden="true" href="#5问答系统question-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.问答系统（Question Answering）&lt;/h4&gt;
&lt;p&gt;问答是一项任务，其中提供了一个句子或文本样本，从中提出问题并且必须回答问题。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者问题回答数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;斯坦福问题回答数据集（SQuAD）&lt;/a&gt;。回答有关维基百科文章的问题。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/deepmind/rc-data"&gt;Deepmind问题回答语料库&lt;/a&gt;。从每日邮报回答有关新闻文章的问题。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmcauley.ucsd.edu/data/amazon/qa/" rel="nofollow"&gt;亚马逊问答数据&lt;/a&gt;。回答有关亚马逊产品的问题。
有关更多信息，请参阅帖子：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/Datasets-How-can-I-get-corpus-of-a-question-answering-website-like-Quora-or-Yahoo-Answers-or-Stack-Overflow-for-analyzing-answer-quality" rel="nofollow"&gt;数据集：我如何获得问答网站的语料库，如Quora或Yahoo Answers或Stack Overflow来分析答案质量？&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-6语音识别speech-recognition" class="anchor" aria-hidden="true" href="#6语音识别speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6.语音识别（Speech Recognition）&lt;/h4&gt;
&lt;p&gt;语音识别是将口语的音频转换为人类可读文本的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者语音识别数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://catalog.ldc.upenn.edu/LDC93S1" rel="nofollow"&gt;TIMIT声学 - 语音连续语音语料库&lt;/a&gt;。不是免费的，但因其广泛使用而上市。口语美国英语和相关的转录。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://voxforge.org/" rel="nofollow"&gt;VoxForge&lt;/a&gt;。用于构建用于语音识别的开源数据库的项目。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.openslr.org/12/" rel="nofollow"&gt;LibriSpeech ASR语料库&lt;/a&gt;。从LibriVox收集的大量英语有声读物。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-7自动文摘document-summarization" class="anchor" aria-hidden="true" href="#7自动文摘document-summarization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;7.自动文摘（Document Summarization）&lt;/h4&gt;
&lt;p&gt;文档摘要是创建较大文档的简短有意义描述的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者文档摘要数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports" rel="nofollow"&gt;法律案例报告数据集&lt;/a&gt;。收集了4000份法律案件及其摘要。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www-nlpir.nist.gov/related_projects/tipster_summac/cmp_lg.html" rel="nofollow"&gt;TIPSTER文本摘要评估会议语料库&lt;/a&gt;。收集了近200份文件及其摘要。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://catalog.ldc.upenn.edu/LDC2002T31" rel="nofollow"&gt;英语新闻文本的AQUAINT语料库&lt;/a&gt;。不是免费的，而是广泛使用的。新闻文章的语料库。
欲了解更多信息：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://www-nlpir.nist.gov/projects/duc/data.html" rel="nofollow"&gt;文档理解会议（DUC）任务&lt;/a&gt;。
&lt;a href="https://www.quora.com/Where-can-I-find-good-data-sets-for-text-summarization" rel="nofollow"&gt;在哪里可以找到用于文本摘要的良好数据集？&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;命名实体识别&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Bi-LSTM CRF&lt;/li&gt;
&lt;li&gt;参考案例: &lt;a href="http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html" rel="nofollow"&gt;http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CRF推荐文档: &lt;a href="https://www.jianshu.com/p/55755fc649b1" rel="nofollow"&gt;https://www.jianshu.com/p/55755fc649b1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;文本摘要&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;抽取式&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;word2vec + textrank&lt;/li&gt;
&lt;li&gt;word2vec推荐文档: &lt;a href="https://www.zhihu.com/question/44832436/answer/266068967" rel="nofollow"&gt;https://www.zhihu.com/question/44832436/answer/266068967&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;textrank推荐文档: &lt;a href="https://blog.csdn.net/BaiHuaXiu123/article/details/77847232" rel="nofollow"&gt;https://blog.csdn.net/BaiHuaXiu123/article/details/77847232&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph图计算慢慢更新" class="anchor" aria-hidden="true" href="#graph图计算慢慢更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph图计算【慢慢更新】&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据集: &lt;a href="data/nlp/graph"&gt;data/nlp/graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学习资料: spark graphX实战.pdf 【文件太大不方便提供，自己百度】&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-知识图谱" class="anchor" aria-hidden="true" href="#知识图谱"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;知识图谱&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;知识图谱，我只认 &lt;a href="https://www.zhihu.com/people/simmerchan" rel="nofollow"&gt;SimmerChan&lt;/a&gt;: &lt;a href="https://zhuanlan.zhihu.com/knowledgegraph" rel="nofollow"&gt;【知识图谱-给AI装个大脑】&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;说实话，我是看这博主老哥写的博客长大的，写的真的是深入浅出。我很喜欢，所以就分享给大家，希望你们也喜欢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-进一步阅读" class="anchor" aria-hidden="true" href="#进一步阅读"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;进一步阅读&lt;/h3&gt;
&lt;p&gt;如果您希望更深入，本节提供了其他数据集列表。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Text_data" rel="nofollow"&gt;维基百科研究中使用的文本数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/Datasets-What-are-the-major-text-corpora-used-by-computational-linguists-and-natural-language-processing-researchers-and-what-are-the-characteristics-biases-of-each-corpus" rel="nofollow"&gt;数据集：计算语言学家和自然语言处理研究人员使用的主要文本语料库是什么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nlp.stanford.edu/links/statnlp.html#Corpora" rel="nofollow"&gt;斯坦福统计自然语言处理语料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/niderhoff/nlp-datasets"&gt;按字母顺序排列的NLP数据集列表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/nltk_data/" rel="nofollow"&gt;该机构NLTK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deeplearning4j.org/opendata" rel="nofollow"&gt;在DL4J上打开深度学习数据&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/caesar0301/awesome-public-datasets#natural-language"&gt;NLP数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;国内开放数据集: &lt;a href="https://bosonnlp.com/dev/resource" rel="nofollow"&gt;https://bosonnlp.com/dev/resource&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-项目负责人" class="anchor" aria-hidden="true" href="#项目负责人"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目负责人&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第一期 (2017-02-27)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@那伊抹微笑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@瑶妹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/report/2017-04-08_%E7%AC%AC%E4%B8%80%E6%9C%9F%E7%9A%84%E6%80%BB%E7%BB%93.md"&gt;2017-04-08_第一期的总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第二期 (2017-08-14)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@那伊抹微笑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@瑶妹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikechengwei"&gt;@Mike&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第三期 (2018-04-16)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-项目贡献者" class="anchor" aria-hidden="true" href="#项目贡献者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目贡献者&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第一期 (2017-02-27)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/geekidentity"&gt;@侯法超&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hello19883"&gt;@hello19883&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sheepmen"&gt;@徐鑫&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/highfei2011"&gt;@ibe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第二期 (2017-08-14)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/LeeMoonCh"&gt;@Arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/caopeirui"&gt;@Veyron C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cugtyt"&gt;@Cugtyt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hey-bruce"&gt;@BBruceyuan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第三期 (2018-04-16)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-群管理员换届" class="anchor" aria-hidden="true" href="#群管理员换届"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;群管理员换届&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@瑶妹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wizardforcel"&gt;@飞龙&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Watermelon233"&gt;@伪文艺.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@那伊抹微笑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@LAMDA-健忘症&lt;/a&gt; 永久留任-非常感谢对群的贡献&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第一届 (2017-09-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@易漠&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikechengwei"&gt;@Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Books&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@李孟禹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@张假飞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Glassy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@红色石头&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@微光同尘&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第二届 (2018-07-04)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@张假飞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@李孟禹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@小明教主&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@平淡的天&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@凌少skierゞ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@じ☆νЁ坐看云起&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;古柳-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;woodchuck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;自由精灵&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;楚盟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;99杆清台&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;时空守望者@&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;只想发论文的渣渣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;目标: ml劝退专家&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第三届 (2019-01-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;只会喊666的存在&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;codefun007.xyz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;荼靡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;大鱼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;青鸟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;古柳-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Alluka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;不发篇paper不改名片&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;FontTian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Bigjing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;仁 礼 智 爱&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;可啪的小乖受&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;老古董&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;时空守望者&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;我好菜啊&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Messi 19&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;萌Jay小公举&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第四届 (2019-06-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;佛学爱好者&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;楚盟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;codefun007.xyz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;大鱼-群花-声优&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;大海&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;if only&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;李孟禹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;平静&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;任务做不完&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;仁礼智爱&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;园时空守望者@&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;坐看云起&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;阿花君霸占路人&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;烦焖鸡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;古柳-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;青鸟(服务员)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;小明教主&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;zhiqing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;SrL.z&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献者不断的追加&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-免责声明---只供学习参考" class="anchor" aria-hidden="true" href="#免责声明---只供学习参考"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;免责声明 - 【只供学习参考】&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ApacheCN 纯粹出于学习目的与个人兴趣翻译本书&lt;/li&gt;
&lt;li&gt;ApacheCN 保留对此版本译文的署名权及其它相关权利&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-协议" class="anchor" aria-hidden="true" href="#协议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;协议&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;以各项目协议为准。&lt;/li&gt;
&lt;li&gt;ApacheCN 账号下没有协议的项目，一律视为 &lt;a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="nofollow"&gt;CC BY-NC-SA 4.0&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-资料来源" class="anchor" aria-hidden="true" href="#资料来源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;资料来源:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;【比赛收集平台】: &lt;a href="https://github.com/iphysresearch/DataSciComp"&gt;https://github.com/iphysresearch/DataSciComp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pbharrin/machinelearninginaction"&gt;https://github.com/pbharrin/machinelearninginaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/datasets-natural-language-processing" rel="nofollow"&gt;https://machinelearningmastery.com/datasets-natural-language-processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-感谢信" class="anchor" aria-hidden="true" href="#感谢信"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢信&lt;/h2&gt;
&lt;p&gt;最近无意收到群友推送的链接，发现得到大佬高度的认可，并在热心的推广&lt;/p&gt;
&lt;p&gt;在此感谢:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/org/liang-zi-wei-48" rel="nofollow"&gt;量子位&lt;/a&gt;: &lt;a href="https://www.zhihu.com/question/20472776/answer/691646493" rel="nofollow"&gt;https://www.zhihu.com/question/20472776/answer/691646493&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人工智能前沿讲习: &lt;a href="https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ" rel="nofollow"&gt;https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-赞助我们" class="anchor" aria-hidden="true" href="#赞助我们"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;赞助我们&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2814efae28977e977f631af3a30acfe4e9089dd9/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f61626f75742f646f6e6174652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/2814efae28977e977f631af3a30acfe4e9089dd9/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f61626f75742f646f6e6174652e6a7067" alt="微信&amp;amp;支付宝" data-canonical-src="http://data.apachecn.org/img/about/donate.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;特别赞助商(欢迎“私聊”赞助)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td align="center" valign="middle"&gt;
            &lt;a href="https://coding.net/?utm_source=ApacheCN&amp;amp;utm_medium=banner&amp;amp;utm_campaign=march2019" rel="nofollow"&gt;
              &lt;img width="1080" src="https://camo.githubusercontent.com/8d33a9d36a6822434ce78147cdb7cb41aba56a02/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f5370656369616c53706f6e736f72732f436f64696e674e65742e706e67" data-canonical-src="http://data.apachecn.org/img/SpecialSponsors/CodingNet.png" style="max-width:100%;"&gt;
            &lt;/a&gt;
          &lt;/td&gt;
      &lt;/tr&gt;&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>apachecn</author><guid isPermaLink="false">https://github.com/apachecn/AiLearning</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>iGhibli/iOS-DeviceSupport #3 in Python, This week</title><link>https://github.com/iGhibli/iOS-DeviceSupport</link><description>&lt;p&gt;&lt;i&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ios-devicesupport" class="anchor" aria-hidden="true" href="#ios-devicesupport"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;iOS-DeviceSupport&lt;/h1&gt;
&lt;p&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;See docs: &lt;a href="https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/" rel="nofollow"&gt;https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Below command will try to unzip all new device support files to &lt;code&gt;/Applications/Xcode.app&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can use &lt;code&gt;-t&lt;/code&gt; if your Xcode is not in &lt;code&gt;/Applications/&lt;/code&gt; or has different name.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py -t /Applications/Xcode&lt;span class="pl-cce"&gt;\ &lt;/span&gt;9.app&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  -t TARGET   The path &lt;span class="pl-k"&gt;for&lt;/span&gt; Xcode&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-supported-versions" class="anchor" aria-hidden="true" href="#supported-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported versions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;iOS8
&lt;ul&gt;
&lt;li&gt;8.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.4 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS9
&lt;ul&gt;
&lt;li&gt;9.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS10
&lt;ul&gt;
&lt;li&gt;10.0 (14A345) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 (14B72) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 (14C92) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 (14E269) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS11
&lt;ul&gt;
&lt;li&gt;11.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 (15B87) &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 (15C107) &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5167d) &lt;code&gt;2018/01/30&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5201e) &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5037c) &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5061c) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F79) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS12
&lt;ul&gt;
&lt;li&gt;12.0 (16A5288q) &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5308d) &lt;code&gt;2018/06/19&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5318d) &lt;code&gt;2018/06/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5327d) &lt;code&gt;2018/07/20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5339e) &lt;code&gt;2018/07/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5354b) &lt;code&gt;2018/08/15&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A366) &lt;code&gt;2018/09/18&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5059d) &lt;code&gt;2018/09/21&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5068g) &lt;code&gt;2018/10/08&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B91) &lt;code&gt;2018/10/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5181e) &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5212e) &lt;code&gt;2019/03/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E226) &lt;code&gt;2019/03/27&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.3 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (16G73) &lt;code&gt;2019/07/22&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS13
&lt;ul&gt;
&lt;li&gt;13.0 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.0 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.1 &lt;code&gt;2019/08/28&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.2 &lt;code&gt;2019/10/02&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iGhibli</author><guid isPermaLink="false">https://github.com/iGhibli/iOS-DeviceSupport</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>eriklindernoren/ML-From-Scratch #4 in Python, This week</title><link>https://github.com/eriklindernoren/ML-From-Scratch</link><description>&lt;p&gt;&lt;i&gt;Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-from-scratch" class="anchor" aria-hidden="true" href="#machine-learning-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning From Scratch&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.&lt;/p&gt;
&lt;p&gt;The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine-learning-from-scratch"&gt;Machine Learning From Scratch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about"&gt;About&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#polynomial-regression"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification-with-cnn"&gt;Classification With CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#density-based-clustering"&gt;Density-Based Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generating-handwritten-digits"&gt;Generating Handwritten Digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-reinforcement-learning"&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-reconstruction-with-rbm"&gt;Image Reconstruction With RBM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evolutionary-evolved-neural-network"&gt;Evolutionary Evolved Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genetic-algorithm"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#association-analysis"&gt;Association Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-polynomial-regression" class="anchor" aria-hidden="true" href="#polynomial-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Polynomial Regression&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/polynomial_regression.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/p_reg.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in Linköping, Sweden 2016.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-classification-with-cnn" class="anchor" aria-hidden="true" href="#classification-with-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification With CNN&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_cnn1.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-density-based-clustering" class="anchor" aria-hidden="true" href="#density-based-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Density-Based Clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/dbscan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dbscan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generating-handwritten-digits" class="anchor" aria-hidden="true" href="#generating-handwritten-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Handwritten Digits&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966"&gt;&lt;img src="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/gan_mnist5.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-reinforcement-learning" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Reinforcement Learning&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dql1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-reconstruction-with-rbm" class="anchor" aria-hidden="true" href="#image-reconstruction-with-rbm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Reconstruction With RBM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/restricted_boltzmann_machine.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/rbm_digits1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-evolutionary-evolved-neural-network" class="anchor" aria-hidden="true" href="#evolutionary-evolved-neural-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evolutionary Evolved Neural Network&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/evo_nn4.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-genetic-algorithm" class="anchor" aria-hidden="true" href="#genetic-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Genetic Algorithm&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-association-analysis" class="anchor" aria-hidden="true" href="#association-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Association Analysis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -&amp;gt; 2 (support: 0.43, confidence: 1.0)
    4 -&amp;gt; 2 (support: 0.57, confidence: 0.8)
    [1, 4] -&amp;gt; 2 (support: 0.29, confidence: 1.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/adaboost.py"&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/bayesian_regression.py"&gt;Bayesian Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/decision_tree.py"&gt;Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Elastic Net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/gradient_boosting.py"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/k_nearest_neighbors.py"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/linear_discriminant_analysis.py"&gt;Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/logistic_regression.py"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multi_class_lda.py"&gt;Multi-class Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/naive_bayes.py"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/neuroevolution.py"&gt;Neuroevolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/particle_swarm_optimization.py"&gt;Particle Swarm Optimization of Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/perceptron.py"&gt;Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/random_forest.py"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/support_vector_machine.py"&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/xgboost.py"&gt;XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/apriori.py"&gt;Apriori&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/dbscan.py"&gt;DBSCAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/fp_growth.py"&gt;FP-Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/gaussian_mixture_model.py"&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/generative_adversarial_network.py"&gt;Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/genetic_algorithm.py"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/k_means.py"&gt;K-Means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/partitioning_around_medoids.py"&gt;Partitioning Around Medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/principal_component_analysis.py"&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py"&gt;Restricted Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/reinforcement_learning/deep_q_network.py"&gt;Deep Q-Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/neural_network.py"&gt;Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/layers.py"&gt;Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Activation Layer&lt;/li&gt;
&lt;li&gt;Average Pooling Layer&lt;/li&gt;
&lt;li&gt;Batch Normalization Layer&lt;/li&gt;
&lt;li&gt;Constant Padding Layer&lt;/li&gt;
&lt;li&gt;Convolutional Layer&lt;/li&gt;
&lt;li&gt;Dropout Layer&lt;/li&gt;
&lt;li&gt;Flatten Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected (Dense) Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected RNN Layer&lt;/li&gt;
&lt;li&gt;Max Pooling Layer&lt;/li&gt;
&lt;li&gt;Reshape Layer&lt;/li&gt;
&lt;li&gt;Up Sampling Layer&lt;/li&gt;
&lt;li&gt;Zero Padding Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Types
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/convolutional_neural_network.py"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/recurrent_neural_network.py"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If there's some implementation you would like to see here or if you're just feeling social,
feel free to &lt;a href="mailto:eriklindernoren@gmail.com"&gt;email&lt;/a&gt; me or connect with me on &lt;a href="https://www.linkedin.com/in/eriklindernoren/" rel="nofollow"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/ML-From-Scratch</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>hanxiao/bert-as-service #5 in Python, This week</title><link>https://github.com/hanxiao/bert-as-service</link><description>&lt;p&gt;&lt;i&gt;Mapping a variable-length sentence to a fixed-length vector using BERT model&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-bert-as-service" class="anchor" aria-hidden="true" href="#bert-as-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bert-as-service&lt;/h1&gt;
&lt;p align="center"&gt;Using BERT model as a sentence encoding service, i.e. mapping a variable-length sentence to a fixed-length vector.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/stargazers"&gt;
    &lt;img src="https://camo.githubusercontent.com/827fc64cf3b84a82a3057b15bd67bd110c3f094f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616e7869616f2f626572742d61732d736572766963652e7376673f636f6c6f72413d6f72616e676526636f6c6f72423d6f72616e6765266c6f676f3d676974687562" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/hanxiao/bert-as-service.svg?colorA=orange&amp;amp;colorB=orange&amp;amp;logo=github" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://pypi.org/search/?q=bert-serving" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/17c79028fcd99fcb6ffd09d9078a9e90ca72dabe/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626572742d73657276696e672d7365727665722e7376673f636f6c6f72423d627269676874677265656e" alt="Pypi package" data-canonical-src="https://img.shields.io/pypi/v/bert-serving-server.svg?colorB=brightgreen" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;a href="https://bert-as-service.readthedocs.io/" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/97f0af3aadd65722bd4510764170eb12e26c20a2/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;a href="https://pypi.org/search/?q=bert-serving" rel="nofollow"&gt;
      &lt;img alt="PyPI - Downloads" src="https://camo.githubusercontent.com/f0c139454d4564bf4abdcf294e009d886e53e1f5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626572742d73657276696e672d736572766572" data-canonical-src="https://img.shields.io/pypi/dm/bert-serving-server" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/issues"&gt;
        &lt;img src="https://camo.githubusercontent.com/37c917311b11a54f44462c0271a9e62fbd82dc03/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f68616e7869616f2f626572742d61732d736572766963652e737667" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/hanxiao/bert-as-service.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/blob/master/LICENSE"&gt;
        &lt;img src="https://camo.githubusercontent.com/82e75359cfc65c373073242222565e1d21cd5979/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68616e7869616f2f626572742d61732d736572766963652e737667" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/hanxiao/bert-as-service.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://twitter.com/intent/tweet?text=Wow:&amp;amp;url=https%3A%2F%2Fgithub.com%2Fhanxiao%2Fbert-as-service" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/c03e98ee22b873659d1c89f929e35fc8eafbeada/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f6769746875622e636f6d2f68616e7869616f2f626572742d61732d736572766963652e7376673f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/https/github.com/hanxiao/bert-as-service.svg?style=social" style="max-width:100%;"&gt;
  &lt;/a&gt;      
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#highlights"&gt;Highlights&lt;/a&gt; •
  &lt;a href="#what-is-it"&gt;What is it&lt;/a&gt; •
  &lt;a href="#install"&gt;Install&lt;/a&gt; •
  &lt;a href="#getting-started"&gt;Getting Started&lt;/a&gt; •
  &lt;a href="#server-and-client-api"&gt;API&lt;/a&gt; •
  &lt;a href="#book-tutorial"&gt;Tutorials&lt;/a&gt; •
  &lt;a href="#speech_balloon-faq"&gt;FAQ&lt;/a&gt; •
  &lt;a href="#zap-benchmark"&gt;Benchmark&lt;/a&gt; •
  &lt;a href="https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" rel="nofollow"&gt;Blog&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href=".github/demo.gif?raw=true"&gt;&lt;img src=".github/demo.gif?raw=true" width="700" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-made-by-han-xiao--globe_with_meridians-httpshanxiaogithubio" class="anchor" aria-hidden="true" href="#made-by-han-xiao--globe_with_meridians-httpshanxiaogithubio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Made by Han Xiao • &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;🌐&lt;/g-emoji&gt; &lt;a href="https://hanxiao.github.io" rel="nofollow"&gt;https://hanxiao.github.io&lt;/a&gt;&lt;/h6&gt;

&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
  &lt;td width="25%"&gt;&lt;a href="https://github.com/gnes-ai/gnes"&gt;
      &lt;img src=".github/gnes-logo-tight.svg" alt="GNES is Generic Neural Elastic Search (logo made by Han Xiao)" style="max-width:100%;"&gt;
      &lt;/a&gt;&lt;/td&gt;
  &lt;td&gt;
  &lt;b&gt;&lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt;Looking for X-as-service? Or more generic and cloud-native solution?&lt;/b&gt;
  &lt;p&gt;&lt;br&gt;Checkout my new project &lt;a href="https://github.com/gnes-ai/gnes"&gt;GNES&lt;/a&gt;! GNES is Generic Neural Elastic Search, a cloud-native semantic search system based on deep neural network. GNES enables large-scale index and semantic search for text-to-text, image-to-image, video-to-video and any-to-any content form.&lt;/p&gt;
&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2 align="center"&gt;&lt;a id="user-content-what-is-it" class="anchor" aria-hidden="true" href="#what-is-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is it&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt; is a NLP model &lt;a href="https://github.com/google-research/bert"&gt;developed by Google&lt;/a&gt; for pre-training language representations. It leverages an enormous amount of plain text data publicly available on the web and is trained in an unsupervised manner. Pre-training a BERT model is a fairly expensive yet one-time procedure for each language. Fortunately, Google released several pre-trained models where &lt;a href="https://github.com/google-research/bert#pre-trained-models"&gt;you can download from here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sentence Encoding/Embedding&lt;/strong&gt; is a upstream task required in many NLP applications, e.g. sentiment analysis, text classification. The goal is to represent a variable length sentence into a fixed length vector, e.g. &lt;code&gt;hello world&lt;/code&gt; to &lt;code&gt;[0.1, 0.3, 0.9]&lt;/code&gt;. Each element of the vector should "encode" some semantics of the original sentence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finally, &lt;code&gt;bert-as-service&lt;/code&gt;&lt;/strong&gt; uses BERT as a sentence encoder and hosts it as a service via ZeroMQ, allowing you to map sentences into fixed-length representations in just two lines of code.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-highlights" class="anchor" aria-hidden="true" href="#highlights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="telescope" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52d.png"&gt;🔭&lt;/g-emoji&gt; &lt;strong&gt;State-of-the-art&lt;/strong&gt;: build on pretrained 12/24-layer BERT models released by Google AI, which is considered as a milestone in the NLP community.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="hatching_chick" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f423.png"&gt;🐣&lt;/g-emoji&gt; &lt;strong&gt;Easy-to-use&lt;/strong&gt;: require only two lines of code to get sentence/token-level encodes.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="zap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png"&gt;⚡️&lt;/g-emoji&gt; &lt;strong&gt;Fast&lt;/strong&gt;: 900 sentences/s on a single Tesla M40 24GB. Low latency, optimized for speed. See &lt;a href="#zap-benchmark"&gt;benchmark&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="octopus" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f419.png"&gt;🐙&lt;/g-emoji&gt; &lt;strong&gt;Scalable&lt;/strong&gt;: scale nicely and smoothly on multiple GPUs and multiple clients without worrying about concurrency. See &lt;a href="#speed-wrt-num_client"&gt;benchmark&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gem" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f48e.png"&gt;💎&lt;/g-emoji&gt; &lt;strong&gt;Reliable&lt;/strong&gt;: tested on multi-billion sentences; days of running without a break or OOM or any nasty exceptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More features: &lt;a href="#speed-wrt--fp16-and--xla"&gt;XLA &amp;amp; FP16 support&lt;/a&gt;; mix GPU-CPU workloads; optimized graph; &lt;code&gt;tf.data&lt;/code&gt; friendly; customized tokenizer; flexible pooling strategy; &lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;build-in HTTP server&lt;/a&gt; and dashboard; &lt;a href="#asynchronous-encoding"&gt;async encoding&lt;/a&gt;; &lt;a href="#broadcasting-to-multiple-clients"&gt;multicasting&lt;/a&gt;; etc.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;Install the server and client via &lt;code&gt;pip&lt;/code&gt;. They can be installed separately or even on &lt;em&gt;different&lt;/em&gt; machines:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install bert-serving-server  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; server&lt;/span&gt;
pip install bert-serving-client  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; client, independent of `bert-serving-server`&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the server MUST be running on &lt;strong&gt;Python &amp;gt;= 3.5&lt;/strong&gt; with &lt;strong&gt;Tensorflow &amp;gt;= 1.10&lt;/strong&gt; (&lt;em&gt;one-point-ten&lt;/em&gt;). Again, the server does not support Python 2!&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png"&gt;☝️&lt;/g-emoji&gt; The client can be running on both Python 2 and 3 &lt;a href="#q-can-i-run-it-in-python-2"&gt;for the following consideration&lt;/a&gt;.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-1-download-a-pre-trained-bert-model" class="anchor" aria-hidden="true" href="#1-download-a-pre-trained-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Download a Pre-trained BERT Model&lt;/h4&gt;
&lt;p&gt;Download a model listed below, then uncompress the zip file into some folder, say &lt;code&gt;/tmp/english_L-12_H-768_A-12/&lt;/code&gt;&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;List of released pretrained BERT models (click to expand...)&lt;/summary&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Uncased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;BERT-Large, Uncased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Cased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;12-layer, 768-hidden, 12-heads , 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;BERT-Large, Cased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Multilingual Cased (New)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Multilingual Cased (Old)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Chinese&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/details&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt; fine-tuning the model on your downstream task. &lt;a href="#q-are-you-suggesting-using-bert-without-fine-tuning"&gt;Why is it optional?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-2-start-the-bert-service" class="anchor" aria-hidden="true" href="#2-start-the-bert-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Start the BERT service&lt;/h4&gt;
&lt;p&gt;After installing the server, you should be able to use &lt;code&gt;bert-serving-start&lt;/code&gt; CLI as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will start a service with four workers, meaning that it can handle up to four &lt;strong&gt;concurrent&lt;/strong&gt; requests. More concurrent requests will be queued in a load balancer. Details can be found in our &lt;a href="#q-what-is-the-parallel-processing-model-behind-the-scene"&gt;FAQ&lt;/a&gt; and &lt;a href="#speed-wrt-num_client"&gt;the benchmark on number of clients&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below shows what the server looks like when starting correctly:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/server-demo.gif?raw=true"&gt;&lt;img src=".github/server-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Alternatively, one can start the BERT Service in a Docker Container (click to expand...)&lt;/summary&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker build -t bert-as-service -f ./docker/Dockerfile &lt;span class="pl-c1"&gt;.&lt;/span&gt;
NUM_WORKER=1
PATH_MODEL=/PATH_TO/_YOUR_MODEL/
docker run --runtime nvidia -dit -p 5555:5555 -p 5556:5556 -v &lt;span class="pl-smi"&gt;$PATH_MODEL&lt;/span&gt;:/model -t bert-as-service &lt;span class="pl-smi"&gt;$NUM_WORKER&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h4&gt;&lt;a id="user-content-3-use-client-to-get-sentence-encodes" class="anchor" aria-hidden="true" href="#3-use-client-to-get-sentence-encodes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Use Client to Get Sentence Encodes&lt;/h4&gt;
&lt;p&gt;Now you can encode sentences simply as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.client &lt;span class="pl-k"&gt;import&lt;/span&gt; BertClient
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it better&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It will return a &lt;code&gt;ndarray&lt;/code&gt; (or &lt;code&gt;List[List[float]]&lt;/code&gt; if you wish), in which each row is a fixed-length vector representing a sentence. Having thousands of sentences? Just &lt;code&gt;encode&lt;/code&gt;! &lt;em&gt;Don't even bother to batch&lt;/em&gt;, the server will take care of it.&lt;/p&gt;
&lt;p&gt;As a feature of BERT, you may get encodes of a pair of sentences by concatenating them with &lt;code&gt;|||&lt;/code&gt; (with whitespace before and after), e.g.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it ||| then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Below shows what the server looks like while encoding:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/server-run-demo.gif?raw=true"&gt;&lt;img src=".github/server-run-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-use-bert-service-remotely" class="anchor" aria-hidden="true" href="#use-bert-service-remotely"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use BERT Service Remotely&lt;/h4&gt;
&lt;p&gt;One may also start the service on one (GPU) machine and call it from another (CPU) machine as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; on another CPU machine&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.client &lt;span class="pl-k"&gt;import&lt;/span&gt; BertClient
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;ip&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xx.xx.xx.xx&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ip address of the GPU machine&lt;/span&gt;
bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it better&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that you only need &lt;code&gt;pip install -U bert-serving-client&lt;/code&gt; in this case, the server side is not required. You may also &lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;call the service via HTTP requests.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png"&gt;💡&lt;/g-emoji&gt; &lt;strong&gt;Want to learn more? Checkout our tutorials:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;Building a QA semantic search engine in 3 min.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serving-a-fine-tuned-bert-model"&gt;Serving a fine-tuned BERT model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-elmo-like-contextual-word-embedding"&gt;Getting ELMo-like contextual word embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-your-own-tokenizer"&gt;Using your own tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bertclient-with-tfdata-api"&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;Training a text classifier using BERT features and tf.estimator API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#saving-and-loading-with-tfrecord-data"&gt;Saving and loading with TFRecord data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asynchronous-encoding"&gt;Asynchronous encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#broadcasting-to-multiple-clients"&gt;Broadcasting to multiple clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#monitoring-the-service-status-in-a-dashboard"&gt;Monitoring the service status in a dashboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#starting-bertserver-from-python"&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-server-and-client-api" class="anchor" aria-hidden="true" href="#server-and-client-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server and Client API&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bert-as-service.readthedocs.io" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The best way to learn &lt;code&gt;bert-as-service&lt;/code&gt; &lt;strong&gt;latest API&lt;/strong&gt; is &lt;a href="http://bert-as-service.readthedocs.io" rel="nofollow"&gt;reading the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-server-api" class="anchor" aria-hidden="true" href="#server-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server API&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/server.html#server-side-api" rel="nofollow"&gt;Please always refer to the latest server-side API documented here.&lt;/a&gt;, you may get the latest usage via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start --help
bert-serving-terminate --help
bert-serving-benchmark --help&lt;/pre&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Required&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;folder path of the pre-trained BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tuned_model_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;(Optional)&lt;/td&gt;
&lt;td&gt;folder path of a fine-tuned BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ckpt_name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_model.ckpt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;filename of the checkpoint file.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;config_name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;filename of the JSON config file for BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;graph_tmp_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;path to graph temp file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_seq_len&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;maximum length of sequence, longer sequence will be trimmed on the right side. Set it to NONE for dynamically using the longest sequence in a (mini)batch.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cased_tokenization&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;Whether tokenizer should skip the default lowercasing and accent removal. Should be used for e.g. the multilingual cased pretrained BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mask_cls_sep&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;masking the embedding on [CLS] and [SEP] with zero.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;num_worker&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;number of (GPU/CPU) worker runs BERT model, each works in a separate process.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_batch_size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;maximum number of sequences handled by each worker, larger batch will be partitioned into small batches.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;priority_batch_size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;batch smaller than this size will be labeled as high priority, and jumps forward in the job queue to get result faster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5555&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for pushing data from client to server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port_out&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5556&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for publishing results from server to client&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;http_port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;server port for receiving HTTP requests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cors&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;*&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;setting "Access-Control-Allow-Origin" for HTTP requests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pooling_strategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the pooling strategy for generating encoding vectors, valid values are &lt;code&gt;NONE&lt;/code&gt;, &lt;code&gt;REDUCE_MEAN&lt;/code&gt;, &lt;code&gt;REDUCE_MAX&lt;/code&gt;, &lt;code&gt;REDUCE_MEAN_MAX&lt;/code&gt;, &lt;code&gt;CLS_TOKEN&lt;/code&gt;, &lt;code&gt;FIRST_TOKEN&lt;/code&gt;, &lt;code&gt;SEP_TOKEN&lt;/code&gt;, &lt;code&gt;LAST_TOKEN&lt;/code&gt;. Explanation of these strategies &lt;a href="#q-what-are-the-available-pooling-strategies"&gt;can be found here&lt;/a&gt;. To get encoding for each token in the sequence, please set this to &lt;code&gt;NONE&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pooling_layer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[-2]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the encoding layer that pooling operates on, where &lt;code&gt;-1&lt;/code&gt; means the last layer, &lt;code&gt;-2&lt;/code&gt; means the second-to-last, &lt;code&gt;[-1, -2]&lt;/code&gt; means concatenating the result of last two layers, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gpu_memory_fraction&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the fraction of the overall amount of memory that each GPU should be allocated per worker&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cpu&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;run on CPU instead of GPU&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xla&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;enable &lt;a href="https://www.tensorflow.org/xla/jit" rel="nofollow"&gt;XLA compiler&lt;/a&gt; for graph optimization (&lt;em&gt;experimental!&lt;/em&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fp16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;use float16 precision (experimental)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;device_map&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;specify the list of GPU device ids that will be used (id starts from 0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;show_tokens_to_client&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;sending tokenization results to client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-client-api" class="anchor" aria-hidden="true" href="#client-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client API&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/client.html#module-client" rel="nofollow"&gt;Please always refer to the latest client-side API documented here.&lt;/a&gt; Client-side provides a Python class called &lt;code&gt;BertClient&lt;/code&gt;, which accepts arguments as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ip&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address of the server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5555&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for pushing data from client to server, &lt;em&gt;must be consistent with the server side config&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port_out&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5556&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for publishing results from server to client, &lt;em&gt;must be consistent with the server side config&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;output_fmt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ndarray&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the output format of the sentence encodes, either in numpy array or python List[List[float]] (&lt;code&gt;ndarray&lt;/code&gt;/&lt;code&gt;list&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;show_server_config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;whether to show server configs when first connected&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;check_version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;whether to force client and server to have the same version&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;identity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;a UUID that identifies the client, useful in multi-casting&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;timeout&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;set the timeout (milliseconds) for receive operation on the client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A &lt;code&gt;BertClient&lt;/code&gt; implements the following methods and properties:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.encode()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Encode a list of strings to a list of vectors&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.encode_async()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Asynchronous encode batches from a generator&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.fetch()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch all encoded vectors from server and return them in a generator, use it with &lt;code&gt;.encode_async()&lt;/code&gt; or &lt;code&gt;.encode(blocking=False)&lt;/code&gt;. Sending order is NOT preserved.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.fetch_all()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch all encoded vectors from server and return them in a list, use it with &lt;code&gt;.encode_async()&lt;/code&gt; or &lt;code&gt;.encode(blocking=False)&lt;/code&gt;. Sending order is preserved.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.close()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Gracefully close the connection between the client and the server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Get the client status in JSON format&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.server_status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Get the server status in JSON format&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-book-tutorial" class="anchor" aria-hidden="true" href="#book-tutorial"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; Tutorial&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/faq.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The full list of examples can be found in &lt;a href="example"&gt;&lt;code&gt;example/&lt;/code&gt;&lt;/a&gt;. You can run each via &lt;code&gt;python example/example-k.py&lt;/code&gt;. Most of examples require you to start a BertServer first, please follow &lt;a href="#2-start-the-bert-service"&gt;the instruction here&lt;/a&gt;. Note that although &lt;code&gt;BertClient&lt;/code&gt; works universally on both Python 2.x and 3.x, examples are only tested on Python 3.6.&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Table of contents (click to expand...)&lt;/summary&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;Building a QA semantic search engine in 3 min.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serving-a-fine-tuned-bert-model"&gt;Serving a fine-tuned BERT model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-elmo-like-contextual-word-embedding"&gt;Getting ELMo-like contextual word embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-your-own-tokenizer"&gt;Using your own tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bertclient-with-tfdata-api"&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;Training a text classifier using BERT features and tf.estimator API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#saving-and-loading-with-tfrecord-data"&gt;Saving and loading with TFRecord data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asynchronous-encoding"&gt;Asynchronous encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#broadcasting-to-multiple-clients"&gt;Broadcasting to multiple clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#monitoring-the-service-status-in-a-dashboard"&gt;Monitoring the service status in a dashboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#starting-bertserver-from-python"&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-building-a-qa-semantic-search-engine-in-3-minutes" class="anchor" aria-hidden="true" href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building a QA semantic search engine in 3 minutes&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example8.py"&gt;be found example8.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As the first example, we will implement a simple QA search engine using &lt;code&gt;bert-as-service&lt;/code&gt; in just three minutes. No kidding! The goal is to find similar questions to user's input and return the corresponding answer. To start, we need a list of question-answer pairs. Fortunately, this README file already contains &lt;a href="#speech_balloon-faq"&gt;a list of FAQ&lt;/a&gt;, so I will just use that to make this example perfectly self-contained. Let's first load all questions and show some statistics.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;prefix_q &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;##### **Q:** &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;README.md&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; fp:
    questions &lt;span class="pl-k"&gt;=&lt;/span&gt; [v.replace(prefix_q, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).strip() &lt;span class="pl-k"&gt;for&lt;/span&gt; v &lt;span class="pl-k"&gt;in&lt;/span&gt; fp &lt;span class="pl-k"&gt;if&lt;/span&gt; v.strip() &lt;span class="pl-k"&gt;and&lt;/span&gt; v.startswith(prefix_q)]
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-c1"&gt;%d&lt;/span&gt; questions loaded, avg. len of &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (&lt;span class="pl-c1"&gt;len&lt;/span&gt;(questions), np.mean([&lt;span class="pl-c1"&gt;len&lt;/span&gt;(d.split()) &lt;span class="pl-k"&gt;for&lt;/span&gt; d &lt;span class="pl-k"&gt;in&lt;/span&gt; questions])))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives &lt;code&gt;33 questions loaded, avg. len of 9&lt;/code&gt;. So looks like we have enough questions. Now start a BertServer with &lt;code&gt;uncased_L-12_H-768_A-12&lt;/code&gt; pretrained BERT model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -num_worker=1 -model_dir=/data/cips/data/lab/data/model/uncased_L-12_H-768_A-12&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we need to encode our questions into vectors:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;port&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4000&lt;/span&gt;, &lt;span class="pl-v"&gt;port_out&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4001&lt;/span&gt;)
doc_vecs &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(questions)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we are ready to receive new query and perform a simple "fuzzy" search against the existing questions. To do that, every time a new query is coming, we encode it as a vector and compute its dot product with &lt;code&gt;doc_vecs&lt;/code&gt;; sort the result descendingly; and return the top-k similar questions as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
    query &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;input&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;your question: &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    query_vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode([query])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; compute normalized dot product as score&lt;/span&gt;
    score &lt;span class="pl-k"&gt;=&lt;/span&gt; np.sum(query_vec &lt;span class="pl-k"&gt;*&lt;/span&gt; doc_vecs, &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;) &lt;span class="pl-k"&gt;/&lt;/span&gt; np.linalg.norm(doc_vecs, &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)
    topk_idx &lt;span class="pl-k"&gt;=&lt;/span&gt; np.argsort(score)[::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;][:topk]
    &lt;span class="pl-k"&gt;for&lt;/span&gt; idx &lt;span class="pl-k"&gt;in&lt;/span&gt; topk_idx:
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;gt; &lt;span class="pl-c1"&gt;%s&lt;/span&gt;&lt;span class="pl-cce"&gt;\t&lt;/span&gt;&lt;span class="pl-c1"&gt;%s&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (score[idx], questions[idx]))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! Now run the code and type your query, see how this search engine handles fuzzy match:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/qasearch-demo.gif?raw=true"&gt;&lt;img src=".github/qasearch-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serving-a-fine-tuned-bert-model" class="anchor" aria-hidden="true" href="#serving-a-fine-tuned-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serving a fine-tuned BERT model&lt;/h3&gt;
&lt;p&gt;Pretrained BERT models often show quite "okayish" performance on many tasks. However, to release the true power of BERT a fine-tuning on the downstream task (or on domain-specific data) is necessary. In this example, I will show you how to serve a fine-tuned BERT model.&lt;/p&gt;
&lt;p&gt;We follow the instruction in &lt;a href="https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks"&gt;"Sentence (and sentence-pair) classification tasks"&lt;/a&gt; and use &lt;code&gt;run_classifier.py&lt;/code&gt; to fine tune &lt;code&gt;uncased_L-12_H-768_A-12&lt;/code&gt; model on MRPC task. The fine-tuned model is stored at &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;, which can be changed by specifying &lt;code&gt;--output_dir&lt;/code&gt; of &lt;code&gt;run_classifier.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you look into &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;, it contains something like:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;checkpoint                                        128
&lt;span class="pl-c1"&gt;eval&lt;/span&gt;                                              4.0K
eval_results.txt                                  86
eval.tf_record                                    219K
events.out.tfevents.1545202214.TENCENT64.site     6.1M
events.out.tfevents.1545203242.TENCENT64.site     14M
graph.pbtxt                                       9.0M
model.ckpt-0.data-00000-of-00001                  1.3G
model.ckpt-0.index                                23K
model.ckpt-0.meta                                 3.9M
model.ckpt-343.data-00000-of-00001                1.3G
model.ckpt-343.index                              23K
model.ckpt-343.meta                               3.9M
train.tf_record                                   2.0M&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Don't be afraid of those mysterious files, as the only important one to us is &lt;code&gt;model.ckpt-343.data-00000-of-00001&lt;/code&gt; (looks like my training stops at the 343 step. One may get &lt;code&gt;model.ckpt-123.data-00000-of-00001&lt;/code&gt; or &lt;code&gt;model.ckpt-9876.data-00000-of-00001&lt;/code&gt; depending on the total training steps). Now we have collected all three pieces of information that are needed for serving this fine-tuned model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pretrained model is downloaded to &lt;code&gt;/path/to/bert/uncased_L-12_H-768_A-12&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Our fine-tuned model is stored at &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Our fine-tuned model checkpoint is named as &lt;code&gt;model.ckpt-343&lt;/code&gt; something something.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now start a BertServer by putting three pieces together:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir=/pretrained/uncased_L-12_H-768_A-12 -tuned_model_dir=/tmp/mrpc_output/ -ckpt_name=model.ckpt-343&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the server started, you should find this line in the log:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;I:GRAPHOPT:[gra:opt: 50]:checkpoint (override by fine-tuned model): /tmp/mrpc_output/model.ckpt-343
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which means the BERT parameters is overrode and successfully loaded from our fine-tuned &lt;code&gt;/tmp/mrpc_output/model.ckpt-343&lt;/code&gt;. Done!&lt;/p&gt;
&lt;p&gt;In short, find your fine-tuned model path and checkpoint name, then feed them to &lt;code&gt;-tuned_model_dir&lt;/code&gt; and &lt;code&gt;-ckpt_name&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-elmo-like-contextual-word-embedding" class="anchor" aria-hidden="true" href="#getting-elmo-like-contextual-word-embedding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting ELMo-like contextual word embedding&lt;/h3&gt;
&lt;p&gt;Start the server with &lt;code&gt;pooling_strategy&lt;/code&gt; set to NONE.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -pooling_strategy NONE -model_dir /tmp/english_L-12_H-768_A-12/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To get the word embedding corresponds to every token, you can simply use slice index as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; max_seq_len = 25&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pooling_strategy = NONE&lt;/span&gt;

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hey you&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;whats up?&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])

vec  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [2, 25, 768]&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 25, 768], sentence embeddings for `hey you`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `[CLS]`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;1&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `hey`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;2&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `you`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;3&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `[SEP]`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;4&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for padding symbol&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;25&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; error, out of index!&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that no matter how long your original sequence is, the service will always return a &lt;code&gt;[max_seq_len, 768]&lt;/code&gt; matrix for every sequence. When using slice index to get the word embedding, beware of the special tokens padded to the sequence, i.e. &lt;code&gt;[CLS]&lt;/code&gt;, &lt;code&gt;[SEP]&lt;/code&gt;, &lt;code&gt;0_PAD&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-your-own-tokenizer" class="anchor" aria-hidden="true" href="#using-your-own-tokenizer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using your own tokenizer&lt;/h3&gt;
&lt;p&gt;Often you want to use your own tokenizer to segment sentences instead of the default one from BERT. Simply call &lt;code&gt;encode(is_tokenized=True)&lt;/code&gt; on the client slide as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;texts &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;good day&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; a naive whitespace tokenizer&lt;/span&gt;
texts2 &lt;span class="pl-k"&gt;=&lt;/span&gt; [s.split() &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; texts]

vecs &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(texts2, &lt;span class="pl-v"&gt;is_tokenized&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives &lt;code&gt;[2, 25, 768]&lt;/code&gt; tensor where the first &lt;code&gt;[1, 25, 768]&lt;/code&gt; corresponds to the token-level encoding of "hello world!". If you look into its values, you will find that only the first four elements, i.e. &lt;code&gt;[1, 0:3, 768]&lt;/code&gt; have values, all the others are zeros. This is due to the fact that BERT considers "hello world!" as four tokens: &lt;code&gt;[CLS]&lt;/code&gt; &lt;code&gt;hello&lt;/code&gt; &lt;code&gt;world!&lt;/code&gt; &lt;code&gt;[SEP]&lt;/code&gt;, the rest are padding symbols and are masked out before output.&lt;/p&gt;
&lt;p&gt;Note that there is no need to start a separate server for handling tokenized/untokenized sentences. The server can tell and handle both cases automatically.&lt;/p&gt;
&lt;p&gt;Sometimes you want to know explicitly the tokenization performed on the server side to have better understanding of the embedding result. One such case is asking word embedding from the server (with &lt;code&gt;-pooling_strategy NONE&lt;/code&gt;), one wants to tell which word is tokenized and which is unrecognized. You can get such information with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;enabling &lt;code&gt;-show_tokens_to_client&lt;/code&gt; on the server side;&lt;/li&gt;
&lt;li&gt;calling the server via &lt;code&gt;encode(..., show_tokens=True)&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, a basic usage like&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thisis it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;show_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;returns a tuple, where the first element is the embedding and the second is the tokenization result from the server:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;(array([[[ 0.        , -0.        ,  0.        , ...,  0.        , -0.        , -0.        ],
        [ 1.1100919 , -0.20474958,  0.9895898 , ...,  0.3873255  , -1.4093989 , -0.47620595],
        ..., -0.        , -0.        ]],

       [[ 0.        , -0.        ,  0.        , ...,  0.        , 0.        ,  0.        ],
        [ 0.6293478 , -0.4088499 ,  0.6022662 , ...,  0.41740108, 1.214456  ,  1.2532915 ],
        ..., 0.        ,  0.        ]]], dtype=float32),
         
          [['[CLS]', 'hello', 'world', '!', '[SEP]'], ['[CLS]', 'this', '##is', 'it', '[SEP]']])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When using your own tokenization, you may still want to check if the server respects your tokens. For example,&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thisis&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]], &lt;span class="pl-v"&gt;show_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;is_tokenized&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;returns:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;(array([[[ 0.        , -0.        ,  0.        , ...,  0.       ,  -0.        ,  0.        ],
        [ 1.1111546 , -0.56572634,  0.37183186, ...,  0.02397121,  -0.5445367 ,  1.1009651 ],
        ..., -0.        ,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,  -0.        ,  0.        ],
        [ 0.39262453,  0.3782491 ,  0.27096173, ...,  0.7122045 ,  -0.9874849 ,  0.9318679 ],
        ..., -0.        ,  0.        ]]], dtype=float32),
         
         [['[CLS]', 'hello', '[UNK]', '[SEP]'], ['[CLS]', '[UNK]', 'it', '[SEP]']])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One can observe that &lt;code&gt;world!&lt;/code&gt; and &lt;code&gt;thisis&lt;/code&gt; are not recognized on the server, hence they are set to &lt;code&gt;[UNK]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, beware that the pretrained BERT Chinese from Google is character-based, i.e. its vocabulary is made of single Chinese characters. Therefore it makes no sense if you use word-level segmentation algorithm to pre-process the data and feed to such model.&lt;/p&gt;
&lt;p&gt;Extremely curious readers may notice that the first row in the above example is all-zero even though the tokenization result includes &lt;code&gt;[CLS]&lt;/code&gt; (well done, detective!). The reason is that the tokenization result will &lt;strong&gt;always&lt;/strong&gt; includes &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[UNK]&lt;/code&gt; regardless the setting of &lt;code&gt;-mask_cls_sep&lt;/code&gt;. This could be useful when you want to align the tokens afterwards. Remember, &lt;code&gt;-mask_cls_sep&lt;/code&gt; only masks &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; out of the computation. It doesn't affect the tokenization algorithm.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-bertclient-with-tfdata-api" class="anchor" aria-hidden="true" href="#using-bertclient-with-tfdata-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example4.py"&gt;be found example4.py&lt;/a&gt;. There is also &lt;a href="https://github.com/hanxiao/bert-as-service/issues/29#issuecomment-442362241"&gt;an example in Keras&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;a href="https://www.tensorflow.org/guide/datasets" rel="nofollow"&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt; API enables you to build complex input pipelines from simple, reusable pieces. One can also use &lt;code&gt;BertClient&lt;/code&gt; to encode sentences on-the-fly and use the vectors in a downstream model. Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;batch_size &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;256&lt;/span&gt;
num_parallel_calls &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;4&lt;/span&gt;
num_clients &lt;span class="pl-k"&gt;=&lt;/span&gt; num_parallel_calls &lt;span class="pl-k"&gt;*&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; should be at least greater than `num_parallel_calls`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; start a pool of clients&lt;/span&gt;
bc_clients &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertClient(&lt;span class="pl-v"&gt;show_server_config&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;) &lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(num_clients)]


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;get_encodes&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;):
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; x is `batch_size` of lines, each of which is a json object&lt;/span&gt;
    samples &lt;span class="pl-k"&gt;=&lt;/span&gt; [json.loads(l) &lt;span class="pl-k"&gt;for&lt;/span&gt; l &lt;span class="pl-k"&gt;in&lt;/span&gt; x]
    text &lt;span class="pl-k"&gt;=&lt;/span&gt; [s[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;raw_text&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; samples]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; List[List[str]]&lt;/span&gt;
    labels &lt;span class="pl-k"&gt;=&lt;/span&gt; [s[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;label&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; samples]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; List[str]&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get a client from available clients&lt;/span&gt;
    bc_client &lt;span class="pl-k"&gt;=&lt;/span&gt; bc_clients.pop()
    features &lt;span class="pl-k"&gt;=&lt;/span&gt; bc_client.encode(text)
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; after use, put it back&lt;/span&gt;
    bc_clients.append(bc_client)
    &lt;span class="pl-k"&gt;return&lt;/span&gt; features, labels


ds &lt;span class="pl-k"&gt;=&lt;/span&gt; (tf.data.TextLineDataset(train_fp).batch(batch_size)
        .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: tf.py_func(get_encodes, [x], [tf.float32, tf.string]),  &lt;span class="pl-v"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_parallel_calls)
        .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: x, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;label&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: y})
        .make_one_shot_iterator().get_next())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The trick here is to start a pool of &lt;code&gt;BertClient&lt;/code&gt; and reuse them one by one. In this way, we can fully harness the power of &lt;code&gt;num_parallel_calls&lt;/code&gt; of &lt;code&gt;Dataset.map()&lt;/code&gt; API.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training-a-text-classifier-using-bert-features-and-tfestimator-api" class="anchor" aria-hidden="true" href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training a text classifier using BERT features and &lt;code&gt;tf.estimator&lt;/code&gt; API&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example5.py"&gt;be found example5.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Following the last example, we can easily extend it to a full classifier using &lt;code&gt;tf.estimator&lt;/code&gt; API. One only need minor change on the input function as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;estimator &lt;span class="pl-k"&gt;=&lt;/span&gt; DNNClassifier(
    &lt;span class="pl-v"&gt;hidden_units&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[&lt;span class="pl-c1"&gt;512&lt;/span&gt;],
    &lt;span class="pl-v"&gt;feature_columns&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[tf.feature_column.numeric_column(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;shape&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(&lt;span class="pl-c1"&gt;768&lt;/span&gt;,))],
    &lt;span class="pl-v"&gt;n_classes&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;len&lt;/span&gt;(laws),
    &lt;span class="pl-v"&gt;config&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;run_config,
    &lt;span class="pl-v"&gt;label_vocabulary&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;laws_str,
    &lt;span class="pl-v"&gt;dropout&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.1&lt;/span&gt;)

input_fn &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;fp&lt;/span&gt;: (tf.data.TextLineDataset(fp)
                       .apply(tf.contrib.data.shuffle_and_repeat(&lt;span class="pl-v"&gt;buffer_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10000&lt;/span&gt;))
                       .batch(batch_size)
                       .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: tf.py_func(get_encodes, [x], [tf.float32, tf.string]), &lt;span class="pl-v"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_parallel_calls)
                       .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: ({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: x}, y))
                       .prefetch(&lt;span class="pl-c1"&gt;20&lt;/span&gt;))

train_spec &lt;span class="pl-k"&gt;=&lt;/span&gt; TrainSpec(&lt;span class="pl-v"&gt;input_fn&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;lambda&lt;/span&gt;: input_fn(train_fp))
eval_spec &lt;span class="pl-k"&gt;=&lt;/span&gt; EvalSpec(&lt;span class="pl-v"&gt;input_fn&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;lambda&lt;/span&gt;: input_fn(eval_fp), &lt;span class="pl-v"&gt;throttle_secs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
train_and_evaluate(estimator, train_spec, eval_spec)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complete example can &lt;a href="example/example5.py"&gt;be found example5.py&lt;/a&gt;, in which a simple MLP is built on BERT features for predicting the relevant articles according to the fact description in the law documents. The problem is a part of the &lt;a href="https://github.com/thunlp/CAIL/blob/master/README_en.md"&gt;Chinese AI and Law Challenge Competition&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-saving-and-loading-with-tfrecord-data" class="anchor" aria-hidden="true" href="#saving-and-loading-with-tfrecord-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Saving and loading with TFRecord data&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example6.py"&gt;be found example6.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. You can also pre-encode all your sequences and store their encodings to a TFRecord file, then later load it to build a &lt;code&gt;tf.Dataset&lt;/code&gt;. For example, to write encoding into a TFRecord file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
list_vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(lst_str)
list_label &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; lst_str]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; a dummy list of all-zero labels&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; write to tfrecord&lt;/span&gt;
&lt;span class="pl-k"&gt;with&lt;/span&gt; tf.python_io.TFRecordWriter(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tmp.tfrecord&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; writer:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_float_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;float_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.FloatList(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;values))

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_int_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;int64_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.Int64List(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;list&lt;/span&gt;(values)))

    &lt;span class="pl-k"&gt;for&lt;/span&gt; (vec, label) &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;zip&lt;/span&gt;(list_vec, list_label):
        features &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;features&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: create_float_feature(vec), &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;labels&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: create_int_feature([label])}
        tf_example &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.train.Example(&lt;span class="pl-v"&gt;features&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.Features(&lt;span class="pl-v"&gt;feature&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;features))
        writer.write(tf_example.SerializeToString())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can load from it and build a &lt;code&gt;tf.Dataset&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_decode_record&lt;/span&gt;(&lt;span class="pl-smi"&gt;record&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Decodes a record to a TensorFlow example.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.parse_single_example(record, {
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;features&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([&lt;span class="pl-c1"&gt;768&lt;/span&gt;], tf.float32),
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;labels&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([], tf.int64),
    })

ds &lt;span class="pl-k"&gt;=&lt;/span&gt; (tf.data.TFRecordDataset(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tmp.tfrecord&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).repeat().shuffle(&lt;span class="pl-v"&gt;buffer_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;).apply(
    tf.contrib.data.map_and_batch(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;record&lt;/span&gt;: _decode_record(record), &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;))
      .make_one_shot_iterator().get_next())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To save word/token-level embedding to TFRecord, one needs to first flatten &lt;code&gt;[max_seq_len, num_hidden]&lt;/code&gt; tensor into an 1D array as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_float_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;float_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.FloatList(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;values.reshape(&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And later reconstruct the shape when loading it:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;name_to_features &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;feature&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([max_seq_length &lt;span class="pl-k"&gt;*&lt;/span&gt; num_hidden], tf.float32),
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;label_ids&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([], tf.int64),
}
    
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_decode_record&lt;/span&gt;(&lt;span class="pl-smi"&gt;record&lt;/span&gt;, &lt;span class="pl-smi"&gt;name_to_features&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Decodes a record to a TensorFlow example.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    example &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.parse_single_example(record, name_to_features)
    example[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.reshape(example[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [max_seq_length, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;])
    &lt;span class="pl-k"&gt;return&lt;/span&gt; example&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be careful, this will generate a huge TFRecord file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-asynchronous-encoding" class="anchor" aria-hidden="true" href="#asynchronous-encoding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Asynchronous encoding&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example2.py"&gt;be found example2.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;BertClient.encode()&lt;/code&gt; offers a nice synchronous way to get sentence encodes. However,   sometimes we want to do it in an asynchronous manner by feeding all textual data to the server first, fetching the encoded results later. This can be easily done by:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; an endless data stream, generating data in an extremely fast speed&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;text_gen&lt;/span&gt;():
    &lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
        &lt;span class="pl-k"&gt;yield&lt;/span&gt; lst_str  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; yield a batch of text lines&lt;/span&gt;

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get encoded vectors&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; bc.encode_async(text_gen(), &lt;span class="pl-v"&gt;max_num_batch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;received &lt;span class="pl-c1"&gt;%d&lt;/span&gt; x &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (j.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], j.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]))&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-broadcasting-to-multiple-clients" class="anchor" aria-hidden="true" href="#broadcasting-to-multiple-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Broadcasting to multiple clients&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example3.py"&gt;be found in example3.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The encoded result is routed to the client according to its identity. If you have multiple clients with same identity, then they all receive the results! You can use this &lt;em&gt;multicast&lt;/em&gt; feature to do some cool things, e.g. training multiple different models (some using &lt;code&gt;scikit-learn&lt;/code&gt; some using &lt;code&gt;tensorflow&lt;/code&gt;) in multiple separated processes while only call &lt;code&gt;BertServer&lt;/code&gt; once. In the example below, &lt;code&gt;bc&lt;/code&gt; and its two clones will all receive encoded vector.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; clone a client by reusing the identity &lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;client_clone&lt;/span&gt;(&lt;span class="pl-smi"&gt;id&lt;/span&gt;, &lt;span class="pl-smi"&gt;idx&lt;/span&gt;):
    bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;identity&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;id&lt;/span&gt;)
    &lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; bc.listen():
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;clone-client-&lt;span class="pl-c1"&gt;%d&lt;/span&gt;: received &lt;span class="pl-c1"&gt;%d&lt;/span&gt; x &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (idx, j.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], j.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]))

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; start two cloned clients sharing the same identity as bc&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;2&lt;/span&gt;):
    threading.Thread(&lt;span class="pl-v"&gt;target&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;client_clone, &lt;span class="pl-v"&gt;args&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(bc.identity, j)).start()

&lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;3&lt;/span&gt;):
    bc.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-monitoring-the-service-status-in-a-dashboard" class="anchor" aria-hidden="true" href="#monitoring-the-service-status-in-a-dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monitoring the service status in a dashboard&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="plugin/dashboard"&gt;be found in plugin/dashboard/&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a part of the infrastructure, one may also want to monitor the service status and show it in a dashboard. To do that, we can use:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;ip&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;server_ip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

json.dumps(bc.server_status, &lt;span class="pl-v"&gt;ensure_ascii&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives the current status of the server including number of requests, number of clients etc. in JSON format. The only thing remained is to start a HTTP server for returning this JSON to the frontend that renders it.&lt;/p&gt;
&lt;p&gt;Alternatively, one may simply expose an HTTP port when starting a server via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -http_port 8001 -model_dir ...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will allow one to use javascript or &lt;code&gt;curl&lt;/code&gt; to fetch the server status at port 8001.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plugin/dashboard/index.html&lt;/code&gt; shows a simple dashboard based on Bootstrap and Vue.js.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/dashboard.png?raw=true"&gt;&lt;img src=".github/dashboard.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-bert-as-service-to-serve-http-requests-in-json" class="anchor" aria-hidden="true" href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/h3&gt;
&lt;p&gt;Besides calling &lt;code&gt;bert-as-service&lt;/code&gt; from Python, one can also call it via HTTP request in JSON. It is quite useful especially when low transport layer is prohibited. Behind the scene, &lt;code&gt;bert-as-service&lt;/code&gt; spawns a Flask server in a separate process and then reuse a &lt;code&gt;BertClient&lt;/code&gt; instance as a proxy to communicate with the ventilator.&lt;/p&gt;
&lt;p&gt;To enable the build-in HTTP server, we need to first (re)install the server with some extra Python dependencies:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -U bert-serving-server[http]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then simply start the server with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir=/YOUR_MODEL -http_port 8125&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Done! Your server is now listening HTTP and TCP requests at port &lt;code&gt;8125&lt;/code&gt; simultaneously!&lt;/p&gt;
&lt;p&gt;To send a HTTP request, first prepare the payload in JSON as following:&lt;/p&gt;
&lt;div class="highlight highlight-source-json"&gt;&lt;pre&gt;{
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;123&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;texts&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;hello world&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;good day!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;is_tokenized&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;false&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;, where &lt;code&gt;id&lt;/code&gt; is a unique identifier helping you to synchronize the results; &lt;code&gt;is_tokenized&lt;/code&gt; follows the meaning in &lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/client.html#client.BertClient.encode_async" rel="nofollow"&gt;&lt;code&gt;BertClient&lt;/code&gt; API&lt;/a&gt; and &lt;code&gt;false&lt;/code&gt; by default.&lt;/p&gt;
&lt;p&gt;Then simply call the server at &lt;code&gt;/encode&lt;/code&gt; via HTTP POST request. You can use javascript or whatever, here is an example using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;curl -X POST http://xx.xx.xx.xx:8125/encode \
  -H &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;content-type: application/json&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; \
  -d &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;{"id": 123,"texts": ["hello world"], "is_tokenized": false}&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;, which returns a JSON:&lt;/p&gt;
&lt;div class="highlight highlight-source-json"&gt;&lt;pre&gt;{
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;123&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;results&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [[&lt;span class="pl-c1"&gt;768&lt;/span&gt; &lt;span class="pl-ii"&gt;float-list&lt;/span&gt;], [&lt;span class="pl-c1"&gt;768&lt;/span&gt; &lt;span class="pl-ii"&gt;float-list&lt;/span&gt;]],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;status&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;200&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To get the server's status and client's status, you can send GET requests at &lt;code&gt;/status/server&lt;/code&gt; and &lt;code&gt;/status/client&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Finally, one may also config CORS to restrict the public access of the server by specifying &lt;code&gt;-cors&lt;/code&gt; when starting &lt;code&gt;bert-serving-start&lt;/code&gt;. By default &lt;code&gt;-cors=*&lt;/code&gt;, meaning the server is public accessible.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-starting-bertserver-from-python" class="anchor" aria-hidden="true" href="#starting-bertserver-from-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/h3&gt;
&lt;p&gt;Besides shell, one can also start a &lt;code&gt;BertServer&lt;/code&gt; from python. Simply do&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.server.helper &lt;span class="pl-k"&gt;import&lt;/span&gt; get_args_parser
&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.server &lt;span class="pl-k"&gt;import&lt;/span&gt; BertServer
args &lt;span class="pl-k"&gt;=&lt;/span&gt; get_args_parser().parse_args([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-model_dir&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;YOUR_MODEL_PATH_HERE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-port&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;5555&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-port_out&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;5556&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-max_seq_len&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NONE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-mask_cls_sep&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-cpu&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
server &lt;span class="pl-k"&gt;=&lt;/span&gt; BertServer(args)
server.start()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that it's basically mirroring the arg-parsing behavior in CLI, so everything in that &lt;code&gt;.parse_args([])&lt;/code&gt; list should be string, e.g. &lt;code&gt;['-port', '5555']&lt;/code&gt; not &lt;code&gt;['-port', 5555]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To shutdown the server, you may call the static method in &lt;code&gt;BertServer&lt;/code&gt; class via:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;BertServer.shutdown(&lt;span class="pl-v"&gt;port&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;5555&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or via shell CLI:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-terminate -port 5555&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will terminate the server running on localhost at port 5555. You may also use it to terminate a remote server, see &lt;code&gt;bert-serving-terminate --help&lt;/code&gt; for details.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-speech_balloon-faq" class="anchor" aria-hidden="true" href="#speech_balloon-faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;💬&lt;/g-emoji&gt; FAQ&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/faq.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-you-have-a-paper-or-other-written-explanation-to-introduce-your-models-details" class="anchor" aria-hidden="true" href="#q-do-you-have-a-paper-or-other-written-explanation-to-introduce-your-models-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do you have a paper or other written explanation to introduce your model's details?&lt;/h5&gt;
&lt;p&gt;The design philosophy and technical details can be found &lt;a href="https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" rel="nofollow"&gt;in my blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-where-is-the-bert-code-come-from" class="anchor" aria-hidden="true" href="#q-where-is-the-bert-code-come-from"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is the BERT code come from?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; &lt;a href="server/bert_serving/server/bert/"&gt;BERT code of this repo&lt;/a&gt; is forked from the &lt;a href="https://github.com/google-research/bert"&gt;original BERT repo&lt;/a&gt; with necessary modification, &lt;a href="server/bert_serving/server/bert/extract_features.py"&gt;especially in extract_features.py&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-large-is-a-sentence-vector" class="anchor" aria-hidden="true" href="#q-how-large-is-a-sentence-vector"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How large is a sentence vector?&lt;/h5&gt;
&lt;p&gt;In general, each sentence is translated to a 768-dimensional vector. Depending on the pretrained BERT you are using, &lt;code&gt;pooling_strategy&lt;/code&gt; and &lt;code&gt;pooling_layer&lt;/code&gt; the dimensions of the output vector could be different.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-do-you-get-the-fixed-representation-did-you-do-pooling-or-something" class="anchor" aria-hidden="true" href="#q-how-do-you-get-the-fixed-representation-did-you-do-pooling-or-something"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How do you get the fixed representation? Did you do pooling or something?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, pooling is required to get a fixed representation of a sentence. In the default strategy &lt;code&gt;REDUCE_MEAN&lt;/code&gt;, I take the second-to-last hidden layer of all of the tokens in the sentence and do average pooling.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-are-you-suggesting-using-bert-without-fine-tuning" class="anchor" aria-hidden="true" href="#q-are-you-suggesting-using-bert-without-fine-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Are you suggesting using BERT without fine-tuning?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes and no. On the one hand, Google pretrained BERT on Wikipedia data, thus should encode enough prior knowledge of the language into the model. Having such feature is not a bad idea. On the other hand, these prior knowledge is not specific to any particular domain. It should be totally reasonable if the performance is not ideal if you are using it on, for example, classifying legal cases. Nonetheless, you can always first fine-tune your own BERT on the downstream task and then use &lt;code&gt;bert-as-service&lt;/code&gt; to extract the feature vectors efficiently. Keep in mind that &lt;code&gt;bert-as-service&lt;/code&gt; is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-get-a-concatenation-of-several-layers-instead-of-a-single-layer-" class="anchor" aria-hidden="true" href="#q-can-i-get-a-concatenation-of-several-layers-instead-of-a-single-layer-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I get a concatenation of several layers instead of a single layer ?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Sure! Just use a list of the layer you want to concatenate when calling the server. Example:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -pooling_layer -4 -3 -2 -1 -model_dir /tmp/english_L-12_H-768_A-12/&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-are-the-available-pooling-strategies" class="anchor" aria-hidden="true" href="#q-what-are-the-available-pooling-strategies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What are the available pooling strategies?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Here is a table summarizes all pooling strategies I implemented. Choose your favorite one by specifying &lt;code&gt;bert-serving-start -pooling_strategy&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Strategy&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;NONE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;no pooling at all, useful when you want to use word embedding instead of sentence embedding. This will results in a &lt;code&gt;[max_seq_len, 768]&lt;/code&gt; encode matrix for a sequence.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;take the average of the hidden state of encoding layer on the time axis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MAX&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;take the maximum of the hidden state of encoding layer on the time axis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN_MAX&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;do &lt;code&gt;REDUCE_MEAN&lt;/code&gt; and &lt;code&gt;REDUCE_MAX&lt;/code&gt; separately and then concat them together on the last axis, resulting in 1536-dim sentence encodes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;CLS_TOKEN&lt;/code&gt; or &lt;code&gt;FIRST_TOKEN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;get the hidden state corresponding to &lt;code&gt;[CLS]&lt;/code&gt;, i.e. the first token&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;SEP_TOKEN&lt;/code&gt; or &lt;code&gt;LAST_TOKEN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;get the hidden state corresponding to &lt;code&gt;[SEP]&lt;/code&gt;, i.e. the last token&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls" class="anchor" aria-hidden="true" href="#q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why not use the hidden state of the first token as default strategy, i.e. the &lt;code&gt;[CLS]&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Because a pre-trained model is not fine-tuned on any downstream tasks yet. In this case, the hidden state of &lt;code&gt;[CLS]&lt;/code&gt; is not a good sentence representation. If later you fine-tune the model, you may use &lt;code&gt;[CLS]&lt;/code&gt; as well.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-bert-has-1224-layers-so-which-layer-are-you-talking-about" class="anchor" aria-hidden="true" href="#q-bert-has-1224-layers-so-which-layer-are-you-talking-about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; BERT has 12/24 layers, so which layer are you talking about?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; By default this service works on the second last layer, i.e. &lt;code&gt;pooling_layer=-2&lt;/code&gt;. You can change it by setting &lt;code&gt;pooling_layer&lt;/code&gt; to other negative values, e.g. -1 corresponds to the last layer.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-not-the-last-hidden-layer-why-second-to-last" class="anchor" aria-hidden="true" href="#q-why-not-the-last-hidden-layer-why-second-to-last"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why not the last hidden layer? Why second-to-last?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets. If you question about this argument and want to use the last hidden layer anyway, please feel free to set &lt;code&gt;pooling_layer=-1&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-so-which-layer-and-which-pooling-strategy-is-the-best" class="anchor" aria-hidden="true" href="#q-so-which-layer-and-which-pooling-strategy-is-the-best"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; So which layer and which pooling strategy is the best?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It depends. Keep in mind that different BERT layers capture different information. To see that more clearly, here is a visualization on &lt;a href="https://www.kaggle.com/uciml/news-aggregator-dataset" rel="nofollow"&gt;UCI-News Aggregator Dataset&lt;/a&gt;, where I randomly sample 20K news titles; get sentence encodes from different layers and with different pooling strategies, finally reduce it to 2D via PCA (one can of course do t-SNE as well, but that's not my point). There are only four classes of the data, illustrated in red, blue, yellow and green. To reproduce the result, please run &lt;a href="example/example7.py"&gt;example7.py&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pool_mean.png?raw=true"&gt;&lt;img src=".github/pool_mean.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pool_max.png?raw=true"&gt;&lt;img src=".github/pool_max.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Intuitively, &lt;code&gt;pooling_layer=-1&lt;/code&gt; is close to the training output, so it may be biased to the training targets. If you don't fine tune the model, then this could lead to a bad representation. &lt;code&gt;pooling_layer=-12&lt;/code&gt; is close to the word embedding, may preserve the very original word information (with no fancy self-attention etc.). On the other hand, you may achieve the very same performance by simply using a word-embedding only. That said, anything in-between [-1, -12] is then a trade-off.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-could-i-use-other-pooling-techniques" class="anchor" aria-hidden="true" href="#q-could-i-use-other-pooling-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Could I use other pooling techniques?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; For sure. But if you introduce new &lt;code&gt;tf.variables&lt;/code&gt; to the graph, then you need to train those variables before using the model. You may also want to check &lt;a href="https://hanxiao.github.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/#pooling-block" rel="nofollow"&gt;some pooling techniques I mentioned in my blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-to-batch-the-data-before-encode" class="anchor" aria-hidden="true" href="#q-do-i-need-to-batch-the-data-before-encode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need to batch the data before &lt;code&gt;encode()&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;No, not at all. Just do &lt;code&gt;encode&lt;/code&gt; and let the server handles the rest. If the batch is too large, the server will do batching automatically and it is more efficient than doing it by yourself. No matter how many sentences you have, 10K or 100K, as long as you can hold it in client's memory, just send it to the server. Please also read &lt;a href="https://github.com/hanxiao/bert-as-service#speed-wrt-client_batch_size"&gt;the benchmark on the client batch size&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-start-multiple-clients-and-send-requests-to-one-server-simultaneously" class="anchor" aria-hidden="true" href="#q-can-i-start-multiple-clients-and-send-requests-to-one-server-simultaneously"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I start multiple clients and send requests to one server simultaneously?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes! That's the purpose of this repo. In fact you can start as many clients as you want. One server can handle all of them (given enough time).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-many-requests-can-one-service-handle-concurrently" class="anchor" aria-hidden="true" href="#q-how-many-requests-can-one-service-handle-concurrently"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How many requests can one service handle concurrently?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The maximum number of concurrent requests is determined by &lt;code&gt;num_worker&lt;/code&gt; in &lt;code&gt;bert-serving-start&lt;/code&gt;. If you a sending more than &lt;code&gt;num_worker&lt;/code&gt; requests concurrently, the new requests will be temporally stored in a queue until a free worker becomes available.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-so-one-request-means-one-sentence" class="anchor" aria-hidden="true" href="#q-so-one-request-means-one-sentence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; So one request means one sentence?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. One request means a list of sentences sent from a client. Think the size of a request as the batch size. A request may contain 256, 512 or 1024 sentences. The optimal size of a request is often determined empirically. One large request can certainly improve the GPU utilization, yet it also increases the overhead of transmission. You may run &lt;code&gt;python example/example1.py&lt;/code&gt; for a simple benchmark.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-about-the-speed-is-it-fast-enough-for-production" class="anchor" aria-hidden="true" href="#q-how-about-the-speed-is-it-fast-enough-for-production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How about the speed? Is it fast enough for production?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It highly depends on the &lt;code&gt;max_seq_len&lt;/code&gt; and the size of a request. On a single Tesla M40 24GB with &lt;code&gt;max_seq_len=40&lt;/code&gt;, you should get about 470 samples per second using a 12-layer BERT. In general, I'd suggest smaller &lt;code&gt;max_seq_len&lt;/code&gt; (25) and larger request size (512/1024).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-did-you-benchmark-the-efficiency" class="anchor" aria-hidden="true" href="#q-did-you-benchmark-the-efficiency"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Did you benchmark the efficiency?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. See &lt;a href="#zap-benchmark"&gt;Benchmark&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To reproduce the results, please run &lt;code&gt;bert-serving-benchmark&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-is-backend-based-on" class="anchor" aria-hidden="true" href="#q-what-is-backend-based-on"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is backend based on?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; &lt;a href="http://zeromq.org/" rel="nofollow"&gt;ZeroMQ&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-is-the-parallel-processing-model-behind-the-scene" class="anchor" aria-hidden="true" href="#q-what-is-the-parallel-processing-model-behind-the-scene"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is the parallel processing model behind the scene?&lt;/h5&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/bert-parallel-pipeline.png?raw=true"&gt;&lt;img src=".github/bert-parallel-pipeline.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-does-the-server-need-two-ports" class="anchor" aria-hidden="true" href="#q-why-does-the-server-need-two-ports"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why does the server need two ports?&lt;/h5&gt;
&lt;p&gt;One port is for pushing text data into the server, the other port is for publishing the encoded result to the client(s). In this way, we get rid of back-chatter, meaning that at every level recipients never talk back to senders. The overall message flow is strictly one-way, as depicted in the above figure. Killing back-chatter is essential to real scalability, allowing us to use &lt;code&gt;BertClient&lt;/code&gt; in an asynchronous way.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-tensorflow-on-the-client-side" class="anchor" aria-hidden="true" href="#q-do-i-need-tensorflow-on-the-client-side"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need Tensorflow on the client side?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. Think of &lt;code&gt;BertClient&lt;/code&gt; as a general feature extractor, whose output can be fed to &lt;em&gt;any&lt;/em&gt; ML models, e.g. &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;pytorch&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt;. The only file that client need is &lt;a href="service/client.py"&gt;&lt;code&gt;client.py&lt;/code&gt;&lt;/a&gt;. Copy this file to your project and import it, then you are ready to go.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-multilingual-bert-model-provided-by-google" class="anchor" aria-hidden="true" href="#q-can-i-use-multilingual-bert-model-provided-by-google"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use multilingual BERT model provided by Google?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-my-own-fine-tuned-bert-model" class="anchor" aria-hidden="true" href="#q-can-i-use-my-own-fine-tuned-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use my own fine-tuned BERT model?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. In fact, this is suggested. Make sure you have the following three items in &lt;code&gt;model_dir&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-run-it-in-python-2" class="anchor" aria-hidden="true" href="#q-can-i-run-it-in-python-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I run it in python 2?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Server side no, client side yes. This is based on the consideration that python 2.x might still be a major piece in some tech stack. Migrating the whole downstream stack to python 3 for supporting &lt;code&gt;bert-as-service&lt;/code&gt; can take quite some effort. On the other hand, setting up &lt;code&gt;BertServer&lt;/code&gt; is just a one-time thing, which can be even &lt;a href="#run-bert-service-on-nvidia-docker"&gt;run in a docker container&lt;/a&gt;. To ease the integration, we support python 2 on the client side so that you can directly use &lt;code&gt;BertClient&lt;/code&gt; as a part of your python 2 project, whereas the server side should always be hosted with python 3.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-to-do-segmentation-for-chinese" class="anchor" aria-hidden="true" href="#q-do-i-need-to-do-segmentation-for-chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need to do segmentation for Chinese?&lt;/h5&gt;
&lt;p&gt;No, if you are using &lt;a href="https://github.com/google-research/bert#pre-trained-models"&gt;the pretrained Chinese BERT released by Google&lt;/a&gt; you don't need word segmentation. As this Chinese BERT is character-based model. It won't recognize word/phrase even if you intentionally add space in-between. To see that more clearly, this is what the BERT model actually receives after tokenization:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hey you&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;whats up?&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;你好么？&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;我 还 可以&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;tokens: [CLS] hey you [SEP]
input_ids: 101 13153 8357 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] what ##s up ? [SEP]
input_ids: 101 9100 8118 8644 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] 你 好 么 ？ [SEP]
input_ids: 101 872 1962 720 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] 我 还 可 以 [SEP]
input_ids: 101 2769 6820 1377 809 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That means the word embedding is actually the character embedding for Chinese-BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-my-english-word-is-tokenized-to-something" class="anchor" aria-hidden="true" href="#q-why-my-english-word-is-tokenized-to-something"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why my (English) word is tokenized to &lt;code&gt;##something&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;Because your word is out-of-vocabulary (OOV). The tokenizer from Google uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;input&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;unaffable&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
tokenizer_output &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;un&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;##aff&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;##able&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-my-own-tokenizer" class="anchor" aria-hidden="true" href="#q-can-i-use-my-own-tokenizer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use my own tokenizer?&lt;/h5&gt;
&lt;p&gt;Yes. If you already tokenize the sentence on your own, simply send use &lt;code&gt;encode&lt;/code&gt; with &lt;code&gt;List[List[Str]]&lt;/code&gt; as input and turn on &lt;code&gt;is_tokenized&lt;/code&gt;, i.e. &lt;code&gt;bc.encode(texts, is_tokenized=True)&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-i-encounter-zmqerrorzmqerror-operation-cannot-be-accomplished-in-current-state-when-using-bertclient-what-should-i-do" class="anchor" aria-hidden="true" href="#q-i-encounter-zmqerrorzmqerror-operation-cannot-be-accomplished-in-current-state-when-using-bertclient-what-should-i-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; I encounter &lt;code&gt;zmq.error.ZMQError: Operation cannot be accomplished in current state&lt;/code&gt; when using &lt;code&gt;BertClient&lt;/code&gt;, what should I do?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is often due to the misuse of &lt;code&gt;BertClient&lt;/code&gt; in multi-thread/process environment. Note that you can’t reuse one &lt;code&gt;BertClient&lt;/code&gt; among multiple threads/processes, you have to make a separate instance for each thread/process. For example, the following won't work at all:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; BAD example&lt;/span&gt;
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc1/Thread1 scope:&lt;/span&gt;
bc.encode(lst_str)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc2/Thread2 scope:&lt;/span&gt;
bc.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, please do:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc1/Thread1 scope:&lt;/span&gt;
bc1 &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc1.encode(lst_str)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc2/Thread2 scope:&lt;/span&gt;
bc2 &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc2.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-" class="anchor" aria-hidden="true" href="#q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; After running the server, I have several garbage &lt;code&gt;tmpXXXX&lt;/code&gt; folders. How can I change this behavior ?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; These folders are used by ZeroMQ to store sockets. You can choose a different location by setting the environment variable &lt;code&gt;ZEROMQ_SOCK_TMP_DIR&lt;/code&gt; :
&lt;code&gt;export ZEROMQ_SOCK_TMP_DIR=/tmp/&lt;/code&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong" class="anchor" aria-hidden="true" href="#q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; The cosine similarity of two sentence vectors is unreasonably high (e.g. always &amp;gt; 0.8), what's wrong?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; A decent representation for a downstream task doesn't mean that it will be meaningful in terms of cosine distance. Since cosine distance is a linear space where all dimensions are weighted equally. if you want to use cosine distance anyway, then please focus on the rank not the absolute value. Namely, do not use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if cosine(A, B) &amp;gt; 0.9, then A and B are similar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please consider the following instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if cosine(A, B) &amp;gt; cosine(A, C), then A is more similar to B than C.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The graph below illustrates the pairwise similarity of 3000 Chinese sentences randomly sampled from web (char. length &amp;lt; 25). We compute cosine similarity based on the sentence vectors and &lt;a href="https://en.wikipedia.org/wiki/ROUGE_(metric)" rel="nofollow"&gt;Rouge-L&lt;/a&gt; based on the raw text. The diagonal (self-correlation) is removed for the sake of clarity. As one can see, there is some positive correlation between these two metrics.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/cosine-vs-rougel.png?raw=true"&gt;&lt;img src=".github/cosine-vs-rougel.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;   
&lt;h5&gt;&lt;a id="user-content-q-im-getting-bad-performance-what-should-i-do" class="anchor" aria-hidden="true" href="#q-im-getting-bad-performance-what-should-i-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; I'm getting bad performance, what should I do?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This often suggests that the pretrained BERT could not generate a descent representation of your downstream task. Thus, you can fine-tune the model on the downstream task and then use &lt;code&gt;bert-as-service&lt;/code&gt; to serve the fine-tuned BERT. Note that, &lt;code&gt;bert-as-service&lt;/code&gt; is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-run-the-server-side-on-cpu-only-machine" class="anchor" aria-hidden="true" href="#q-can-i-run-the-server-side-on-cpu-only-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I run the server side on CPU-only machine?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, please run &lt;code&gt;bert-serving-start -cpu -max_batch_size 16&lt;/code&gt;. Note that, CPU does not scale as good as GPU on large batches, therefore the &lt;code&gt;max_batch_size&lt;/code&gt; on the server side needs to be smaller, e.g. 16 or 32.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-can-i-choose-num_worker" class="anchor" aria-hidden="true" href="#q-how-can-i-choose-num_worker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How can I choose &lt;code&gt;num_worker&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Generally, the number of workers should be less than or equal to the number of GPU/CPU you have. Otherwise, multiple workers will be allocated to one GPU/CPU, which may not scale well (and may cause out-of-memory on GPU).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-specify-which-gpu-to-use" class="anchor" aria-hidden="true" href="#q-can-i-specify-which-gpu-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I specify which GPU to use?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, you can specifying &lt;code&gt;-device_map&lt;/code&gt; as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -device_map 0 1 4 -num_worker 4 -model_dir ...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will start four workers and allocate them to GPU0, GPU1, GPU4 and again GPU0, respectively. In general, if &lt;code&gt;num_worker&lt;/code&gt; &amp;gt; &lt;code&gt;device_map&lt;/code&gt;, then devices will be reused and shared by the workers (may scale suboptimally or cause OOM); if &lt;code&gt;num_worker&lt;/code&gt; &amp;lt; &lt;code&gt;device_map&lt;/code&gt;, only &lt;code&gt;device_map[:num_worker]&lt;/code&gt; will be used.&lt;/p&gt;
&lt;p&gt;Note, &lt;code&gt;device_map&lt;/code&gt; is ignored when running on CPU.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-zap-benchmark" class="anchor" aria-hidden="true" href="#zap-benchmark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="zap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png"&gt;⚡️&lt;/g-emoji&gt; Benchmark&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/benchmark.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The primary goal of benchmarking is to test the scalability and the speed of this service, which is crucial for using it in a dev/prod environment. Benchmark was done on Tesla M40 24GB, experiments were repeated 10 times and the average value is reported.&lt;/p&gt;
&lt;p&gt;To reproduce the results, please run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-benchmark --help&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Common arguments across all experiments are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;num_worker&lt;/td&gt;
&lt;td&gt;1,2,4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max_seq_len&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;client_batch_size&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max_batch_size&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;num_client&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-max_seq_len" class="anchor" aria-hidden="true" href="#speed-wrt-max_seq_len"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;max_seq_len&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;max_seq_len&lt;/code&gt; is a parameter on the server side, which controls the maximum length of a sequence that a BERT model can handle. Sequences larger than &lt;code&gt;max_seq_len&lt;/code&gt; will be truncated on the left side. Thus, if your client want to send long sequences to the model, please make sure the server can handle them correctly.&lt;/p&gt;
&lt;p&gt;Performance-wise, longer sequences means slower speed and  more chance of OOM, as the multi-head self-attention (the core unit of BERT) needs to do dot products and matrix multiplications between every two symbols in the sequence.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/max_seq_len.png?raw=true"&gt;&lt;img src=".github/max_seq_len.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;max_seq_len&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;903&lt;/td&gt;
&lt;td&gt;1774&lt;/td&gt;
&lt;td&gt;3254&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;td&gt;231&lt;/td&gt;
&lt;td&gt;435&lt;/td&gt;
&lt;td&gt;768&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;td&gt;119&lt;/td&gt;
&lt;td&gt;237&lt;/td&gt;
&lt;td&gt;464&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;108&lt;/td&gt;
&lt;td&gt;212&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-client_batch_size" class="anchor" aria-hidden="true" href="#speed-wrt-client_batch_size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;client_batch_size&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;client_batch_size&lt;/code&gt; is the number of sequences from a client when invoking &lt;code&gt;encode()&lt;/code&gt;. For performance reason, please consider encoding sequences in batch rather than encoding them one by one.&lt;/p&gt;
&lt;p&gt;For example, do:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; prepare your sent in advance&lt;/span&gt;
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
my_sentences &lt;span class="pl-k"&gt;=&lt;/span&gt; [s &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; my_corpus.iter()]
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; doing encoding in one-shot&lt;/span&gt;
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(my_sentences)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;DON'T:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; []
&lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; my_corpus.iter():
    vec.append(bc.encode(s))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It's even worse if you put &lt;code&gt;BertClient()&lt;/code&gt; inside the loop. Don't do that.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/client_batch_size.png?raw=true"&gt;&lt;img src=".github/client_batch_size.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;client_batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;74&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;206&lt;/td&gt;
&lt;td&gt;205&lt;/td&gt;
&lt;td&gt;201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;274&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;267&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;332&lt;/td&gt;
&lt;td&gt;329&lt;/td&gt;
&lt;td&gt;330&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;382&lt;/td&gt;
&lt;td&gt;383&lt;/td&gt;
&lt;td&gt;383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;432&lt;/td&gt;
&lt;td&gt;766&lt;/td&gt;
&lt;td&gt;762&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;862&lt;/td&gt;
&lt;td&gt;1517&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;917&lt;/td&gt;
&lt;td&gt;1681&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4096&lt;/td&gt;
&lt;td&gt;481&lt;/td&gt;
&lt;td&gt;943&lt;/td&gt;
&lt;td&gt;1809&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-num_client" class="anchor" aria-hidden="true" href="#speed-wrt-num_client"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;num_client&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;num_client&lt;/code&gt; represents the number of concurrent clients connected to the server at the same time.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/num_clients.png?raw=true"&gt;&lt;img src=".github/num_clients.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;num_client&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1759&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;261&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;1028&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;133&lt;/td&gt;
&lt;td&gt;267&lt;/td&gt;
&lt;td&gt;533&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As one can observe, 1 clients 1 GPU = 381 seqs/s, 2 clients 2 GPU 402 seqs/s, 4 clients 4 GPU 413 seqs/s. This shows the efficiency of our parallel pipeline and job scheduling, as the service can leverage the GPU time  more exhaustively as concurrent requests increase.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-max_batch_size" class="anchor" aria-hidden="true" href="#speed-wrt-max_batch_size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;max_batch_size&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;max_batch_size&lt;/code&gt; is a parameter on the server side, which controls the maximum number of samples per batch per worker. If a incoming batch from client is larger than &lt;code&gt;max_batch_size&lt;/code&gt;, the server will split it into small batches so that each of them is less or equal than &lt;code&gt;max_batch_size&lt;/code&gt; before sending it to workers.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/max_batch_size.png?raw=true"&gt;&lt;img src=".github/max_batch_size.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;max_batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;450&lt;/td&gt;
&lt;td&gt;887&lt;/td&gt;
&lt;td&gt;1726&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;897&lt;/td&gt;
&lt;td&gt;1759&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;931&lt;/td&gt;
&lt;td&gt;1816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1688&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;464&lt;/td&gt;
&lt;td&gt;866&lt;/td&gt;
&lt;td&gt;1483&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-pooling_layer" class="anchor" aria-hidden="true" href="#speed-wrt-pooling_layer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;pooling_layer&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;pooling_layer&lt;/code&gt; determines the encoding layer that pooling operates on. For example, in a 12-layer BERT model, &lt;code&gt;-1&lt;/code&gt; represents the layer closed to the output, &lt;code&gt;-12&lt;/code&gt; represents the layer closed to the embedding layer. As one can observe below, the depth of the pooling layer affects the speed.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pooling_layer.png?raw=true"&gt;&lt;img src=".github/pooling_layer.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;pooling_layer&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[-1]&lt;/td&gt;
&lt;td&gt;438&lt;/td&gt;
&lt;td&gt;844&lt;/td&gt;
&lt;td&gt;1568&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-2]&lt;/td&gt;
&lt;td&gt;475&lt;/td&gt;
&lt;td&gt;916&lt;/td&gt;
&lt;td&gt;1686&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-3]&lt;/td&gt;
&lt;td&gt;516&lt;/td&gt;
&lt;td&gt;995&lt;/td&gt;
&lt;td&gt;1823&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-4]&lt;/td&gt;
&lt;td&gt;569&lt;/td&gt;
&lt;td&gt;1076&lt;/td&gt;
&lt;td&gt;1986&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-5]&lt;/td&gt;
&lt;td&gt;633&lt;/td&gt;
&lt;td&gt;1193&lt;/td&gt;
&lt;td&gt;2184&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-6]&lt;/td&gt;
&lt;td&gt;711&lt;/td&gt;
&lt;td&gt;1340&lt;/td&gt;
&lt;td&gt;2430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-7]&lt;/td&gt;
&lt;td&gt;820&lt;/td&gt;
&lt;td&gt;1528&lt;/td&gt;
&lt;td&gt;2729&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-8]&lt;/td&gt;
&lt;td&gt;945&lt;/td&gt;
&lt;td&gt;1772&lt;/td&gt;
&lt;td&gt;3104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-9]&lt;/td&gt;
&lt;td&gt;1128&lt;/td&gt;
&lt;td&gt;2047&lt;/td&gt;
&lt;td&gt;3622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-10]&lt;/td&gt;
&lt;td&gt;1392&lt;/td&gt;
&lt;td&gt;2542&lt;/td&gt;
&lt;td&gt;4241&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-11]&lt;/td&gt;
&lt;td&gt;1523&lt;/td&gt;
&lt;td&gt;2737&lt;/td&gt;
&lt;td&gt;4752&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-12]&lt;/td&gt;
&lt;td&gt;1568&lt;/td&gt;
&lt;td&gt;2985&lt;/td&gt;
&lt;td&gt;5303&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt--fp16-and--xla" class="anchor" aria-hidden="true" href="#speed-wrt--fp16-and--xla"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;-fp16&lt;/code&gt; and &lt;code&gt;-xla&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;bert-as-service&lt;/code&gt; supports two additional optimizations: half-precision and XLA, which can be turned on by adding &lt;code&gt;-fp16&lt;/code&gt; and &lt;code&gt;-xla&lt;/code&gt; to &lt;code&gt;bert-serving-start&lt;/code&gt;, respectively. To enable these two options, you have to meet the following requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;your GPU supports FP16 instructions;&lt;/li&gt;
&lt;li&gt;your Tensorflow is self-compiled with XLA and &lt;code&gt;-march=native&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;your CUDA and cudnn are not too old.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On Tesla V100 with &lt;code&gt;tensorflow=1.13.0-rc0&lt;/code&gt; it gives:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/fp16-xla.svg"&gt;&lt;img src=".github/fp16-xla.svg" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FP16 achieves ~1.4x speedup (round-trip) comparing to the FP32 counterpart. To reproduce the result, please run &lt;code&gt;python example/example1.py&lt;/code&gt;.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use bert-as-service in a scientific publication, we would appreciate references to the following BibTex entry:&lt;/p&gt;
&lt;div class="highlight highlight-text-tex-latex"&gt;&lt;pre&gt;@misc{xiao2018bertservice,
  title={bert-as-service},
  author={Xiao, Han},
  howpublished={&lt;span class="pl-c1"&gt;\url&lt;/span&gt;{https://github.com/hanxiao/bert-as-service}},
  year={2018}
}&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hanxiao</author><guid isPermaLink="false">https://github.com/hanxiao/bert-as-service</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>bokeh/bokeh #6 in Python, This week</title><link>https://github.com/bokeh/bokeh</link><description>&lt;p&gt;&lt;i&gt;Interactive Data Visualization in the browser, from  Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a href="https://bokeh.org" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/23c4767f9deb6835e161e7bee64eeaa1125f0721/68747470733a2f2f7374617469632e626f6b65682e6f72672f6c6f676f732f6c6f676f747970652e737667" height="60" width="150" alt="Bokeh logotype" data-canonical-src="https://static.bokeh.org/logos/logotype.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Bokeh is a fiscally sponsored project of &lt;a href="https://numfocus.org" rel="nofollow"&gt;NumFOCUS&lt;/a&gt;, a nonprofit dedicated to supporting the open-source scientific computing community. If you like Bokeh and would like to support our mission, please consider &lt;a href="https://numfocus.org/donate-to-bokeh" rel="nofollow"&gt;making a donation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;Latest Release&lt;/td&gt;
  &lt;td&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/319e5c4665c805f31c4e03601804b746c5d74925/68747470733a2f2f62616467652e667572792e696f2f67682f626f6b6568253246626f6b65682e737667"&gt;&lt;img src="https://camo.githubusercontent.com/319e5c4665c805f31c4e03601804b746c5d74925/68747470733a2f2f62616467652e667572792e696f2f67682f626f6b6568253246626f6b65682e737667" alt="Latest release version" data-canonical-src="https://badge.fury.io/gh/bokeh%2Fbokeh.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://badge.fury.io/js/bokehjs" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/95363bfe2ac7fb7e5eb9885a50868c1dba3a9a70/68747470733a2f2f62616467652e667572792e696f2f6a732f626f6b65686a732e737667" alt="npm version" data-canonical-src="https://badge.fury.io/js/bokehjs.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Conda&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://docs.bokeh.org/en/latest/docs/installation.html" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d0117bc93858f82f1a8ea5ebf5c19cabd97df9fd/68747470733a2f2f707976697a2e6f72672f5f7374617469632f63616368652f626f6b65685f636f6e64615f646f776e6c6f6164735f62616467652e737667" alt="Conda downloads per month" data-canonical-src="https://pyviz.org/_static/cache/bokeh_conda_downloads_badge.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;License&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://github.com/bokeh/bokeh/blob/master/LICENSE.txt"&gt;
    &lt;img src="https://camo.githubusercontent.com/abf7fceca8e58b3842bfed01c2e3c2ee1612fb09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f626f6b65682f626f6b65682e737667" alt="Bokeh license (BSD 3-clause)" data-canonical-src="https://img.shields.io/github/license/bokeh/bokeh.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;PyPI&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://docs.bokeh.org/en/latest/docs/installation.html" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/5ec94c8c7308f7fb219ac302aa09a50b46b18a82/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626f6b65682e737667" alt="PyPI downloads per month" data-canonical-src="https://img.shields.io/pypi/dm/bokeh.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Sponsorship&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="http://numfocus.org" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/28d3beb4213a1bfc61313a5b5a0be78b06e96c05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706f776572656425323062792d4e756d464f4355532d626c61636b2e7376673f7374796c653d666c617426636f6c6f72413d35423542354226636f6c6f72423d303037443841" alt="Powered by NumFOCUS" data-canonical-src="https://img.shields.io/badge/powered%20by-NumFOCUS-black.svg?style=flat&amp;amp;colorA=5B5B5B&amp;amp;colorB=007D8A" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Live Tutorial&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Live Bokeh tutorial notebooks on MyBinder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Build Status&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://travis-ci.org/bokeh/bokeh" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/8335f2a93fe3e1293a3571623b8d23ba262b1d30/68747470733a2f2f7472617669732d63692e6f72672f626f6b65682f626f6b65682e7376673f6272616e63683d6d6173746572" alt="Current TravisCI build status" data-canonical-src="https://travis-ci.org/bokeh/bokeh.svg?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://ci.appveyor.com/project/bokeh-integrations/bokeh" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/91e2de256d6ef59085db8d6fa178f4850fbf6cb3/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f753469646632356468703231396d686f3f7376673d74727565" alt="Current Appveyor build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/u4idf25dhp219mho?svg=true" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Support&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://discourse.bokeh.org" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/4bc630e601229d0ec09ce27647039334bede13ed/68747470733a2f2f696d672e736869656c64732e696f2f646973636f757273652f68747470732f646973636f757273652e626f6b65682e6f72672f706f7374732e737667" alt="Community Support on discourse.bokeh.org" data-canonical-src="https://img.shields.io/discourse/https/discourse.bokeh.org/posts.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Static Analysis&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/0045ca709534292b0c5d2f8fc628b22ca3cbcf11/68747470733a2f2f626574746572636f64656875622e636f6d2f656467652f62616467652f626f6b65682f626f6b65683f6272616e63683d6d6173746572" alt="BetterCodeHub static analysis" data-canonical-src="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Twitter&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://twitter.com/BokehPlots" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/92ffc787e59932d1af96a7126fac375f84816b55/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f626f6b6568706c6f74732e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Follow BokehPlots on Twitter" data-canonical-src="https://img.shields.io/twitter/follow/bokehplots.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;a href="https://bokeh.org" rel="nofollow"&gt;Bokeh&lt;/a&gt; is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics, and affords high-performance interactivity over large or streaming datasets. Bokeh can help anyone who would like to quickly and easily make interactive plots, dashboards, and data applications.&lt;/p&gt;
&lt;p&gt;
&lt;/p&gt;&lt;table cellspacing="10"&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/image.html" rel="nofollow"&gt;
  &lt;img alt="colormapped image plot thumbnail" src="https://camo.githubusercontent.com/f1f586b237dff8683e0ceaa2bdcb1b7e35f13f93/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f696d6167655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/image_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/anscombe.html" rel="nofollow"&gt;
  &lt;img alt="anscombe plot thumbnail" src="https://camo.githubusercontent.com/073188ee921204d9f4741a91ab4562ca196711fe/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f616e73636f6d62655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/anscombe_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/stocks.html" rel="nofollow"&gt;
  &lt;img alt="stocks plot thumbnail" src="https://camo.githubusercontent.com/e616cb26a235f6354269b268ae07eae87bac0c29/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73746f636b735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/stocks_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/lorenz.html" rel="nofollow"&gt;
  &lt;img alt="lorenz attractor plot thumbnail" src="https://camo.githubusercontent.com/7dfd551f33c5b51a099938c4e07349b28ba86387/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f6c6f72656e7a5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/lorenz_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/candlestick.html" rel="nofollow"&gt;
  &lt;img alt="candlestick plot thumbnail" src="https://camo.githubusercontent.com/c44c6fc6939b0db3896efcb4902a68674f9af381/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63616e646c65737469636b5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/candlestick_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/color_scatter.html" rel="nofollow"&gt;
  &lt;img alt="scatter plot thumbnail" src="https://camo.githubusercontent.com/c7dce46ac91e2097b13979e697d42f469c3d3b9c/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f736361747465725f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/scatter_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/iris_splom.html" rel="nofollow"&gt;
  &lt;img alt="SPLOM plot thumbnail" src="https://camo.githubusercontent.com/85ebf47ee37f3ea5093426c73b87ef7122435ba0/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73706c6f6d5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/splom_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/iris.html" rel="nofollow"&gt;
  &lt;img alt="iris dataset plot thumbnail" src="https://camo.githubusercontent.com/0198838e25d20407fc590705827632199daa32a3/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f697269735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/iris_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/histogram.html" rel="nofollow"&gt;
  &lt;img alt="histogram plot thumbnail" src="https://camo.githubusercontent.com/5ca5fb847641949daa5da2c3ebb7afec7029eb96/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f686973746f6772616d5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/histogram_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/periodic.html" rel="nofollow"&gt;
  &lt;img alt="periodic table plot thumbnail" src="https://camo.githubusercontent.com/dc1fa06b214455fd268f67ac0a4ed47a38795b52/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f706572696f6469635f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/periodic_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/texas.html" rel="nofollow"&gt;
  &lt;img alt="choropleth plot thumbnail" src="https://camo.githubusercontent.com/1d1d55f394731332a63780514673c251136f2f63/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63686f726f706c6574685f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/choropleth_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/burtin.html" rel="nofollow"&gt;
  &lt;img alt="burtin antibiotic data plot thumbnail" src="https://camo.githubusercontent.com/f8329c228dabe62292ae4727cf911d783ebe9371/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f62757274696e5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/burtin_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/streamline.html" rel="nofollow"&gt;
  &lt;img alt="streamline plot thumbnail" src="https://camo.githubusercontent.com/2a39c0491917ad87e8c1ed266d85af408953152e/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73747265616d6c696e655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/streamline_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/image_rgba.html" rel="nofollow"&gt;
  &lt;img alt="RGBA image plot thumbnail" src="https://camo.githubusercontent.com/5a0021bc298920caf8f200c8661df4da2dbb908f/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f696d6167655f726762615f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/image_rgba_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/brewer.html" rel="nofollow"&gt;
  &lt;img alt="stacked bars plot thumbnail" src="https://camo.githubusercontent.com/8f4eef019e452e4e04ae69e54d3a603ae20ce861/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f737461636b65645f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/stacked_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/quiver.html" rel="nofollow"&gt;
  &lt;img alt="quiver plot thumbnail" src="https://camo.githubusercontent.com/4f762bcca73dddb3cc86e60b3d43f1521409a838/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f7175697665725f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/quiver_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/elements.html" rel="nofollow"&gt;
  &lt;img alt="elements data plot thumbnail" src="https://camo.githubusercontent.com/1b46c02fcad9fa523b1194f01b7c61821b68bcae/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f656c656d656e74735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/elements_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/boxplot.html" rel="nofollow"&gt;
  &lt;img alt="boxplot thumbnail" src="https://camo.githubusercontent.com/aee991248aed1c36ce01afabea75062560915d91/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f626f78706c6f745f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/boxplot_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/categorical.html" rel="nofollow"&gt;
  &lt;img alt="categorical plot thumbnail" src="https://camo.githubusercontent.com/ad3bf9a45ea612ea5fe36c9aebfd61cdd23f82da/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63617465676f726963616c5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/categorical_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/unemployment.html" rel="nofollow"&gt;
  &lt;img alt="unemployment data plot thumbnail" src="https://camo.githubusercontent.com/a3b8632276b3f5adedd036d1ad6f9e7a02d6c907/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f756e656d706c6f796d656e745f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/unemployment_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/les_mis.html" rel="nofollow"&gt;
  &lt;img alt="Les Mis co-occurrence plot thumbnail" src="https://camo.githubusercontent.com/d50bb70e41286e878e465a38684ace5fb9f8e74e/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f6c65735f6d69735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/les_mis_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;The easiest way to install Bokeh is using the &lt;a href="https://www.anaconda.com/what-is-anaconda/" rel="nofollow"&gt;Anaconda Python distribution&lt;/a&gt; and its included &lt;em&gt;Conda&lt;/em&gt; package management system. To install Bokeh and its required dependencies, enter the following command at a Bash or Windows command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install bokeh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To install using pip, enter the following command at a Bash or Windows command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install bokeh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information, refer to the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#quick-installation" rel="nofollow"&gt;installation documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;p&gt;Once Bokeh is installed, check out the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#getting-started" rel="nofollow"&gt;Getting Started&lt;/a&gt; section of the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html" rel="nofollow"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit the &lt;a href="https://docs.bokeh.org" rel="nofollow"&gt;full documentation site&lt;/a&gt; to view the &lt;a href="https://docs.bokeh.org/en/dev/docs/user_guide.html" rel="nofollow"&gt;User's Guide&lt;/a&gt; or &lt;a href="https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb" rel="nofollow"&gt;launch the Bokeh tutorial&lt;/a&gt; to learn about Bokeh in live Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;Community support is available on the &lt;a href="https://discourse.bokeh.org" rel="nofollow"&gt;Project Discourse&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you would like to contribute to Bokeh, please review the &lt;a href="https://docs.bokeh.org/en/latest/docs/dev_guide.html" rel="nofollow"&gt;Developer Guide&lt;/a&gt; and say hello on the &lt;a href="https://gitter.im/bokeh/bokeh-dev" rel="nofollow"&gt;bokeh-dev chat channel&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-follow-us" class="anchor" aria-hidden="true" href="#follow-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Follow us&lt;/h2&gt;
&lt;p&gt;Follow us on Twitter &lt;a href="https://twitter.com/BokehPlots" rel="nofollow"&gt;@bokehplots&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;The Bokeh project is grateful for &lt;a href="https://numfocus.org/donate-to-bokeh" rel="nofollow"&gt;individual contributions&lt;/a&gt; as well as sponsorship by the organizations and companies below:&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
    &lt;a href="https://www.numfocus.org/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d147c30d8a6d9ca4b5eb060a83c511d7c99e6948/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f6e756d666f6375732e737667" alt="NumFocus Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/numfocus.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.anaconda.com/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/64a2555013bed95305c66c8c823a419dcb1e2754/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f616e61636f6e64612e706e67" alt="Anaconda Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/anaconda.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.nvidia.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d2e3f41f97a9833e323294d46045dc8b0e7fc66a/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f6e76696469612e706e67" alt="NVidia Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/nvidia.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://developer.nvidia.com/rapids" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/16c8825f8ae78640242f57070aab1bf8e664fa9c/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7261706964732e706e67" alt="Rapids Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/rapids.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;table align="center"&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
    &lt;a href="https://www.quansight.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/8b37ec4276ae3506abd62b93028c9a3b5b86459a/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7175616e73696768742e706e67" alt="Quansight Logo" width="100" data-canonical-src="https://static.bokeh.org/sponsor/quansight.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.rexhomes.com/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/723b4a136c3f567e7ee1f9cb71767488eef13a30/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7265782e6a7067" alt="Rex Logo" width="100" data-canonical-src="https://static.bokeh.org/sponsor/rex.jpg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;If your company uses Bokeh and is able to sponsor the project, please contact &lt;a href="info@bokeh.org"&gt;&lt;/a&gt;&lt;a href="mailto:info@bokeh.org"&gt;info@bokeh.org&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bokeh</author><guid isPermaLink="false">https://github.com/bokeh/bokeh</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>bitcoinbook/bitcoinbook #7 in Python, This week</title><link>https://github.com/bitcoinbook/bitcoinbook</link><description>&lt;p&gt;&lt;i&gt;Mastering Bitcoin 2nd Edition - Programming the Open Blockchain&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Code Examples: &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70"&gt;&lt;img src="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70" alt="travis_ci" data-canonical-src="https://travis-ci.org/bitcoinbook/bitcoinbook.svg?branch=develop" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-mastering-bitcoin" class="anchor" aria-hidden="true" href="#mastering-bitcoin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin&lt;/h1&gt;
&lt;p&gt;Mastering Bitcoin is a book for developers, although the first two chapters cover bitcoin at a level that is also approachable to non-programmers. Anyone with a basic understanding of technology can read the first two chapters to get a great understanding of bitcoin.&lt;/p&gt;
&lt;p&gt;This repository contains the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;first edition, second print&lt;/a&gt;, published in December 2014, and the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second edition, second print&lt;/a&gt;, published in July 2017, as published by O'Reilly Media in paperback and ebook formats.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-issues-errors-comments-contributions" class="anchor" aria-hidden="true" href="#issues-errors-comments-contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues, Errors, Comments, Contributions&lt;/h1&gt;
&lt;p&gt;If you know how to make a pull request to contribute a fix, please write the correction and use a pull request to submit it for consideration against the &lt;a href="https://github.com/bitcoinbook/bitcoinbook/tree/develop"&gt;develop branch&lt;/a&gt;. If you are making several changes, please use a separate commit for each to make it easier to cherry-pick or resolve conflicts. Otherwise, please submit an issue, explaining the error or comment. If you would like to contribute extensive changes or new material, please coordinate with the author first; contact information can be found on his website: &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;https://antonopoulos.com/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reading-this-book" class="anchor" aria-hidden="true" href="#reading-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reading this book&lt;/h1&gt;
&lt;p&gt;To read this book, see &lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/book.asciidoc"&gt;book.asciidoc&lt;/a&gt;. Click on each of the chapters to read in your browser. Other parties may choose to release PDFs of the book online.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-chapters" class="anchor" aria-hidden="true" href="#chapters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chapters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 1: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch01.asciidoc"&gt;Introduction&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 2: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch02.asciidoc"&gt;How Bitcoin Works&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 3: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch03.asciidoc"&gt;Bitcoin Core: The Reference Implementation&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 4: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch04.asciidoc"&gt;Keys, Addresses&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 5: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch05.asciidoc"&gt;Wallets&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 6: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch06.asciidoc"&gt;Transactions&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 7: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch07.asciidoc"&gt;Advanced Transactions and Scripting&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 8: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch08.asciidoc"&gt;The Bitcoin Network&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 9: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch09.asciidoc"&gt;The Blockchain&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 10: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch10.asciidoc"&gt;Mining and Consensus&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 11: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch11.asciidoc"&gt;Bitcoin Security&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 12: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch12.asciidoc"&gt;Blockchain Applications&lt;/a&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-published" class="anchor" aria-hidden="true" href="#published"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Published&lt;/h1&gt;
&lt;p&gt;"Mastering Bitcoin (Second Edition, Second Print): Programming the Open Blockchain" is now available in paperback and ebook formats by many booksellers worldwide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Mastering-Bitcoin-Programming-Open-Blockchain/dp/1491954388" rel="nofollow"&gt;Amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mastering Bitcoin (First Edition Second Print) is also published in Japanese, Korean, and Chinese (Simplified) by publishers in the respective countries.&lt;/p&gt;
&lt;p&gt;Mastering Bitcoin (Open Edition), based on the First Edition, has been translated by volunteers into more than a dozen languages. Translations are available for free under CC-BY-SA license at: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-source" class="anchor" aria-hidden="true" href="#source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source&lt;/h1&gt;
&lt;p&gt;The book's source code, found in this repository, is kept synchronized with the print and ebook editions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---first-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---first-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - First Edition&lt;/h2&gt;
&lt;p&gt;The tags &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print1"&gt;Edition1Print1&lt;/a&gt;, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;Edition1Print2&lt;/a&gt; correspond to the two existing prints of Mastering Bitcoin (First Edition) as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - First Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This "Free Culture" compliant license was approved by my publisher O'Reilly Media (&lt;a href="http://oreilly.com" rel="nofollow"&gt;http://oreilly.com&lt;/a&gt;), who understands the value of open source. O'Reilly Media is not just the world's best publisher of technical books, but is also a strong supporter of this open culture and the sharing of knowledge.&lt;/p&gt;
&lt;p&gt;Thank you O'Reilly Media!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---second-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---second-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - Second Edition&lt;/h2&gt;
&lt;p&gt;The tags, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print_1"&gt;second_edition_print_1&lt;/a&gt; and  &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second_edition_print2&lt;/a&gt;, correspond to the first (June 8th, 2017) and second (July 20th, 2017) print of Mastering Bitcoin (Second Edition), as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - Second Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;p&gt;If you are interested in translating this book, please join our team of volunteers at: &lt;a href="https://www.transifex.com/aantonop/mastering-bitcoin" rel="nofollow"&gt;https://www.transifex.com/aantonop/mastering-bitcoin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Free copies of "Mastering Bitcoin Open Edition," translated in many languages, can be downloaded from: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bitcoinbook</author><guid isPermaLink="false">https://github.com/bitcoinbook/bitcoinbook</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>mlflow/mlflow #8 in Python, This week</title><link>https://github.com/mlflow/mlflow</link><description>&lt;p&gt;&lt;i&gt;Open source platform for the machine learning lifecycle&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mlflow-a-machine-learning-lifecycle-platform" class="anchor" aria-hidden="true" href="#mlflow-a-machine-learning-lifecycle-platform"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MLflow: A Machine Learning Lifecycle Platform&lt;/h1&gt;
&lt;p&gt;MLflow is a platform to streamline machine learning development, including tracking experiments, packaging code
into reproducible runs, and sharing and deploying models. MLflow offers a set of lightweight APIs in that can
used with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc), wherever you
currently run ML code (e.g. in notebooks, standalone applications or the cloud). MLflow's current components are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mlflow.org/docs/latest/tracking.html" rel="nofollow"&gt;MLflow Tracking&lt;/a&gt;: An API to log parameters, code, and
results in machine learning experiments and compare them using an interactive UI.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mlflow.org/docs/latest/projects.html" rel="nofollow"&gt;MLflow Projects&lt;/a&gt;: A code packaging format for reproducible
runs using Conda and Docker, so you can share your ML code with others.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mlflow.org/docs/latest/models.html" rel="nofollow"&gt;MLflow Models&lt;/a&gt;: A model packaging format and tools that let
you easily deploy the same model (from any ML library) to batch and real-time scoring on platforms such as
Docker, Apache Spark, Azure ML and AWS SageMaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://mlflow.org/docs/latest/index.html" rel="nofollow"&gt;&lt;img alt="Latest Docs" src="https://camo.githubusercontent.com/8897346570974d9343daaa3d0028f05fba3b2c96/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d737563636573732e737667" data-canonical-src="https://img.shields.io/badge/docs-latest-success.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://travis-ci.org/mlflow/mlflow" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/cc7d1ba99188e8b7c1840ccc124e52dec84524e5/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6d6c666c6f772f6d6c666c6f772e737667" data-canonical-src="https://img.shields.io/travis/mlflow/mlflow.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://pypi.org/project/mlflow/" rel="nofollow"&gt;&lt;img alt="Latest Python Release" src="https://camo.githubusercontent.com/64da7a7404b91748bf7f828013025ed864ad035c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d6c666c6f772e737667" data-canonical-src="https://img.shields.io/pypi/v/mlflow.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/mlflow" rel="nofollow"&gt;&lt;img alt="Latest Conda Release" src="https://camo.githubusercontent.com/433a5d0f4ed3fe6cfa7bc3501c05cb3b68fb149c/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f6d6c666c6f772e737667" data-canonical-src="https://img.shields.io/conda/vn/conda-forge/mlflow.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://cran.r-project.org/package=mlflow" rel="nofollow"&gt;&lt;img alt="Latest CRAN Release" src="https://camo.githubusercontent.com/a0f2cb7d1126f71f1fa04d6d585a800e55c9ec6e/68747470733a2f2f696d672e736869656c64732e696f2f6372616e2f762f6d6c666c6f772e737667" data-canonical-src="https://img.shields.io/cran/v/mlflow.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://mvnrepository.com/artifact/org.mlflow" rel="nofollow"&gt;&lt;img alt="Maven Central" src="https://camo.githubusercontent.com/f65e7489205a7fb500ea5771c6176da3afb74162/68747470733a2f2f696d672e736869656c64732e696f2f6d6176656e2d63656e7472616c2f762f6f72672e6d6c666c6f772f6d6c666c6f772d706172656e742e737667" data-canonical-src="https://img.shields.io/maven-central/v/org.mlflow/mlflow-parent.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://github.com/mlflow/mlflow/blob/master/LICENSE.txt"&gt;&lt;img alt="Apache 2 License" src="https://camo.githubusercontent.com/34c5905ad22fdcb15a03e47e463e8773c815c2fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/license-Apache%202-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-installing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;Install MLflow from PyPi via &lt;code&gt;pip install mlflow&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;MLflow requires &lt;code&gt;conda&lt;/code&gt; to be on the &lt;code&gt;PATH&lt;/code&gt; for the projects feature.&lt;/p&gt;
&lt;p&gt;Nightly snapshots of MLflow master are also available &lt;a href="https://mlflow-snapshots.s3-us-west-2.amazonaws.com/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Official documentation for MLflow can be found at &lt;a href="https://mlflow.org/docs/latest/index.html" rel="nofollow"&gt;https://mlflow.org/docs/latest/index.html&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-community"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-community" class="anchor" aria-hidden="true" href="#community"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community&lt;/h2&gt;
&lt;p&gt;For help or questions about MLflow usage (e.g. "how do I do X?") see the &lt;a href="https://mlflow.org/docs/latest/index.html" rel="nofollow"&gt;docs&lt;/a&gt;
or &lt;a href="https://stackoverflow.com/questions/tagged/mlflow" rel="nofollow"&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To report a bug, file a documentation issue, or submit a feature request, please open a GitHub issue.&lt;/p&gt;
&lt;p&gt;For release announcements and other discussions, please subscribe to our mailing list (&lt;a href="mailto:mlflow-users@googlegroups.com"&gt;mlflow-users@googlegroups.com&lt;/a&gt;)
or join us on Slack at &lt;a href="https://tinyurl.com/mlflow-slack" rel="nofollow"&gt;https://tinyurl.com/mlflow-slack&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-running-a-sample-app-with-the-tracking-api"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-running-a-sample-app-with-the-tracking-api" class="anchor" aria-hidden="true" href="#running-a-sample-app-with-the-tracking-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running a Sample App With the Tracking API&lt;/h2&gt;
&lt;p&gt;The programs in &lt;code&gt;examples&lt;/code&gt; use the MLflow Tracking API. For instance, run:&lt;/p&gt;
&lt;pre&gt;python examples/quickstart/mlflow_tracking.py
&lt;/pre&gt;
&lt;p&gt;This program will use &lt;a href="https://mlflow.org/docs/latest/tracking.html" rel="nofollow"&gt;MLflow Tracking API&lt;/a&gt;,
which logs tracking data in &lt;code&gt;./mlruns&lt;/code&gt;. This can then be viewed with the Tracking UI.&lt;/p&gt;
&lt;a name="user-content-launching-the-tracking-ui"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-launching-the-tracking-ui" class="anchor" aria-hidden="true" href="#launching-the-tracking-ui"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Launching the Tracking UI&lt;/h2&gt;
&lt;p&gt;The MLflow Tracking UI will show runs logged in &lt;code&gt;./mlruns&lt;/code&gt; at &lt;a href="http://localhost:5000" rel="nofollow"&gt;http://localhost:5000&lt;/a&gt;.
Start it with:&lt;/p&gt;
&lt;pre&gt;mlflow ui
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Running &lt;code&gt;mlflow ui&lt;/code&gt; from within a clone of MLflow is not recommended - doing so will
run the dev UI from source. We recommend running the UI from a different working directory,
specifying a backend store via the &lt;code&gt;--backend-store-uri&lt;/code&gt; option. Alternatively, see
instructions for running the dev UI in the &lt;a href="CONTRIBUTING.rst"&gt;contributor guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-running-a-project-from-a-uri"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-running-a-project-from-a-uri" class="anchor" aria-hidden="true" href="#running-a-project-from-a-uri"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running a Project from a URI&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;mlflow run&lt;/code&gt; command lets you run a project packaged with a MLproject file from a local path
or a Git URI:&lt;/p&gt;
&lt;pre&gt;mlflow run examples/sklearn_elasticnet_wine -P alpha=0.4

mlflow run https://github.com/mlflow/mlflow-example.git -P alpha=0.4
&lt;/pre&gt;
&lt;p&gt;See &lt;code&gt;examples/sklearn_elasticnet_wine&lt;/code&gt; for a sample project with an MLproject file.&lt;/p&gt;
&lt;a name="user-content-saving-and-serving-models"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-saving-and-serving-models" class="anchor" aria-hidden="true" href="#saving-and-serving-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Saving and Serving Models&lt;/h2&gt;
&lt;p&gt;To illustrate managing models, the &lt;code&gt;mlflow.sklearn&lt;/code&gt; package can log scikit-learn models as
MLflow artifacts and then load them again for serving. There is an example training application in
&lt;code&gt;examples/sklearn_logistic_regression/train.py&lt;/code&gt; that you can run as follows:&lt;/p&gt;
&lt;pre&gt;$ python examples/sklearn_logistic_regression/train.py
Score: 0.666
Model saved in run &amp;lt;run-id&amp;gt;

$ mlflow models serve --model-uri runs:/&amp;lt;run-id&amp;gt;/model

$ curl -d '{"columns":[0],"index":[0,1],"data":[[1],[-1]]}' -H 'Content-Type: application/json'  localhost:5000/invocations
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We happily welcome contributions to MLflow. Please see our &lt;a href="CONTRIBUTING.rst"&gt;contribution guide&lt;/a&gt;
for details.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>mlflow</author><guid isPermaLink="false">https://github.com/mlflow/mlflow</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>google-research/bert #9 in Python, This week</title><link>https://github.com/google-research/bert</link><description>&lt;p&gt;&lt;i&gt;TensorFlow code and pre-trained models for BERT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bert" class="anchor" aria-hidden="true" href="#bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BERT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;***** New May 31st, 2019: Whole Word Masking Models *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a release of several new models which were the result of an improvement
the pre-processing code.&lt;/p&gt;
&lt;p&gt;In the original pre-processing code, we randomly select WordPiece tokens to
mask. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head&lt;/code&gt;
&lt;code&gt;Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The new technique is called Whole Word Masking. In this case, we always mask
&lt;em&gt;all&lt;/em&gt; of the the tokens corresponding to a word at once. The overall masking
rate remains the same.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The training is identical -- we still predict each masked WordPiece token
independently. The improvement comes from the fact that the original prediction
task was too 'easy' for words that had been split into multiple WordPieces.&lt;/p&gt;
&lt;p&gt;This can be enabled during data generation by passing the flag
&lt;code&gt;--do_whole_word_mask=True&lt;/code&gt; to &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pre-trained models with Whole Word Masking are linked below. The data and
training were otherwise identical, and the models have identical structure and
vocab to the original models. We only include BERT-Large models. When using
these models, please make it clear in the paper that you are using the Whole
Word Masking variant of BERT-Large.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;SQUAD 1.1 F1/EM&lt;/th&gt;
&lt;th align="center"&gt;Multi NLI Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.0/84.3&lt;/td&gt;
&lt;td align="center"&gt;86.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.8/86.7&lt;/td&gt;
&lt;td align="center"&gt;87.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.5/84.8&lt;/td&gt;
&lt;td align="center"&gt;86.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.9/86.7&lt;/td&gt;
&lt;td align="center"&gt;86.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;***** New February 7th, 2019: TfHub Module *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BERT has been uploaded to &lt;a href="https://tfhub.dev" rel="nofollow"&gt;TensorFlow Hub&lt;/a&gt;. See
&lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt; for an example of how to use the TF Hub module,
or run an example in the browser on
&lt;a href="https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="nofollow"&gt;Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 23rd, 2018: Un-normalized multilingual model + Thai +
Mongolian *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We uploaded a new multilingual model which does &lt;em&gt;not&lt;/em&gt; perform any normalization
on the input (no lower casing, accent stripping, or Unicode normalization), and
additionally inclues Thai and Mongolian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is recommended to use this version for developing multilingual models,
especially on languages with non-Latin alphabets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This does not require any code changes, and can be downloaded here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;***** New November 15th, 2018: SOTA SQuAD 2.0 System *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is
currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the
README for details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 5th, 2018: Third-party PyTorch and Chainer versions of
BERT available *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NLP researchers from HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. Sosuke Kobayashi also made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
(Thanks!) We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 3rd, 2018: Multilingual and Chinese models available
*****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have made two new BERT models available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use character-based tokenization for Chinese, and WordPiece tokenization for
all other languages. Both models should work out-of-the-box without any code
changes. We did update the implementation of &lt;code&gt;BasicTokenizer&lt;/code&gt; in
&lt;code&gt;tokenization.py&lt;/code&gt; to support Chinese character tokenization, so please update if
you forked it. However, we did not change the tokenization API.&lt;/p&gt;
&lt;p&gt;For more, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** End new information *****&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;, or &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from
&lt;strong&gt;T&lt;/strong&gt;ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.&lt;/p&gt;
&lt;p&gt;Our academic paper which describes BERT in detail and provides full results on a
number of tasks can be found here:
&lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a few numbers, here are the results on the
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD v1.1&lt;/a&gt; question answering
task:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQuAD v1.1 Leaderboard (Oct 8th 2018)&lt;/th&gt;
&lt;th align="center"&gt;Test EM&lt;/th&gt;
&lt;th align="center"&gt;Test F1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Ensemble - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;93.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Ensemble - nlnet&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;td align="center"&gt;91.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Single Model - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;85.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Single Model - nlnet&lt;/td&gt;
&lt;td align="center"&gt;83.5&lt;/td&gt;
&lt;td align="center"&gt;90.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And several natural language inference tasks:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th align="center"&gt;MultiNLI&lt;/th&gt;
&lt;th align="center"&gt;Question NLI&lt;/th&gt;
&lt;th align="center"&gt;SWAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI GPT (Prev. SOTA)&lt;/td&gt;
&lt;td align="center"&gt;82.2&lt;/td&gt;
&lt;td align="center"&gt;88.1&lt;/td&gt;
&lt;td align="center"&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plus many other tasks.&lt;/p&gt;
&lt;p&gt;Moreover, these results were all obtained with almost no task-specific neural
network architecture design.&lt;/p&gt;
&lt;p&gt;If you already know what BERT is and you just want to get started, you can
&lt;a href="#pre-trained-models"&gt;download the pre-trained models&lt;/a&gt; and
&lt;a href="#fine-tuning-with-bert"&gt;run a state-of-the-art fine-tuning&lt;/a&gt; in only a few
minutes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bert" class="anchor" aria-hidden="true" href="#what-is-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT is a method of pre-training language representations, meaning that we train
a general-purpose "language understanding" model on a large text corpus (like
Wikipedia), and then use that model for downstream NLP tasks that we care about
(like question answering). BERT outperforms previous methods because it is the
first &lt;em&gt;unsupervised&lt;/em&gt;, &lt;em&gt;deeply bidirectional&lt;/em&gt; system for pre-training NLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unsupervised&lt;/em&gt; means that BERT was trained using only a plain text corpus, which
is important because an enormous amount of plain text data is publicly available
on the web in many languages.&lt;/p&gt;
&lt;p&gt;Pre-trained representations can also either be &lt;em&gt;context-free&lt;/em&gt; or &lt;em&gt;contextual&lt;/em&gt;,
and contextual representations can further be &lt;em&gt;unidirectional&lt;/em&gt; or
&lt;em&gt;bidirectional&lt;/em&gt;. Context-free models such as
&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow"&gt;word2vec&lt;/a&gt; or
&lt;a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow"&gt;GloVe&lt;/a&gt; generate a single "word
embedding" representation for each word in the vocabulary, so &lt;code&gt;bank&lt;/code&gt; would have
the same representation in &lt;code&gt;bank deposit&lt;/code&gt; and &lt;code&gt;river bank&lt;/code&gt;. Contextual models
instead generate a representation of each word that is based on the other words
in the sentence.&lt;/p&gt;
&lt;p&gt;BERT was built upon recent work in pre-training contextual representations —
including &lt;a href="https://arxiv.org/abs/1511.01432" rel="nofollow"&gt;Semi-supervised Sequence Learning&lt;/a&gt;,
&lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Generative Pre-Training&lt;/a&gt;,
&lt;a href="https://allennlp.org/elmo" rel="nofollow"&gt;ELMo&lt;/a&gt;, and
&lt;a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="nofollow"&gt;ULMFit&lt;/a&gt;
— but crucially these models are all &lt;em&gt;unidirectional&lt;/em&gt; or &lt;em&gt;shallowly
bidirectional&lt;/em&gt;. This means that each word is only contextualized using the words
to its left (or right). For example, in the sentence &lt;code&gt;I made a bank deposit&lt;/code&gt; the
unidirectional representation of &lt;code&gt;bank&lt;/code&gt; is only based on &lt;code&gt;I made a&lt;/code&gt; but not
&lt;code&gt;deposit&lt;/code&gt;. Some previous work does combine the representations from separate
left-context and right-context models, but only in a "shallow" manner. BERT
represents "bank" using both its left and right context — &lt;code&gt;I made a ... deposit&lt;/code&gt;
— starting from the very bottom of a deep neural network, so it is &lt;em&gt;deeply
bidirectional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;BERT uses a simple approach for this: We mask out 15% of the words in the input,
run the entire sequence through a deep bidirectional
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; encoder, and then predict only
the masked words. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to learn relationships between sentences, we also train on a simple
task which can be generated from any monolingual corpus: Given two sentences &lt;code&gt;A&lt;/code&gt;
and &lt;code&gt;B&lt;/code&gt;, is &lt;code&gt;B&lt;/code&gt; the actual next sentence that comes after &lt;code&gt;A&lt;/code&gt;, or just a random
sentence from the corpus?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then train a large model (12-layer to 24-layer Transformer) on a large corpus
(Wikipedia + &lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt;) for a long time (1M
update steps), and that's BERT.&lt;/p&gt;
&lt;p&gt;Using BERT has two stages: &lt;em&gt;Pre-training&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a
one-time procedure for each language (current models are English-only, but
multilingual models will be released in the near future). We are releasing a
number of pre-trained models from the paper which were pre-trained at Google.
Most NLP researchers will never need to pre-train their own model from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; is inexpensive. All of the results in the paper can be
replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,
starting from the exact same pre-trained model. SQuAD, for example, can be
trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of
91.0%, which is the single system state-of-the-art.&lt;/p&gt;
&lt;p&gt;The other important aspect of BERT is that it can be adapted to many types of
NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on
sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level
(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific
modifications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-has-been-released-in-this-repository" class="anchor" aria-hidden="true" href="#what-has-been-released-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What has been released in this repository?&lt;/h2&gt;
&lt;p&gt;We are releasing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow code for the BERT model architecture (which is mostly a standard
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; architecture).&lt;/li&gt;
&lt;li&gt;Pre-trained checkpoints for both the lowercase and cased version of
&lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; from the paper.&lt;/li&gt;
&lt;li&gt;TensorFlow code for push-button replication of the most important
fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud
TPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained models&lt;/h2&gt;
&lt;p&gt;We are releasing the &lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; models from the paper.
&lt;code&gt;Uncased&lt;/code&gt; means that the text has been lowercased before WordPiece tokenization,
e.g., &lt;code&gt;John Smith&lt;/code&gt; becomes &lt;code&gt;john smith&lt;/code&gt;. The &lt;code&gt;Uncased&lt;/code&gt; model also strips out any
accent markers. &lt;code&gt;Cased&lt;/code&gt; means that the true case and accent markers are
preserved. Typically, the &lt;code&gt;Uncased&lt;/code&gt; model is better unless you know that case
information is important for your task (e.g., Named Entity Recognition or
Part-of-Speech tagging).&lt;/p&gt;
&lt;p&gt;These models are all released under the same license as the source code (Apache
2.0).&lt;/p&gt;
&lt;p&gt;For information about the Multilingual and Chinese model, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When using a cased model, make sure to pass &lt;code&gt;--do_lower=False&lt;/code&gt; to the training
scripts. (Or pass &lt;code&gt;do_lower_case=False&lt;/code&gt; directly to &lt;code&gt;FullTokenizer&lt;/code&gt; if you're
using your own script.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The links to the models are here (right-click, 'Save link as...' on the name):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads , 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased (New, recommended)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Uncased (Orig, not recommended)&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each .zip file contains three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained
weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of
the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fine-tuning-with-bert" class="anchor" aria-hidden="true" href="#fine-tuning-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with BERT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: All results on the paper were fine-tuned on a single Cloud TPU,
which has 64GB of RAM. It is currently not possible to re-produce most of the
&lt;code&gt;BERT-Large&lt;/code&gt; results on the paper using a GPU with 12GB - 16GB of RAM, because
the maximum batch size that can fit in memory is too small. We are working on
adding code to this repository which allows for much larger effective batch size
on the GPU. See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for
more details.&lt;/p&gt;
&lt;p&gt;This code was tested with TensorFlow 1.11.0. It was tested with Python2 and
Python3 (but more thoroughly with Python2, since this is what's used internally
in Google).&lt;/p&gt;
&lt;p&gt;The fine-tuning examples which use &lt;code&gt;BERT-Base&lt;/code&gt; should be able to run on a GPU
that has at least 12GB of RAM using the hyperparameters given.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-with-cloud-tpus" class="anchor" aria-hidden="true" href="#fine-tuning-with-cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with Cloud TPUs&lt;/h3&gt;
&lt;p&gt;Most of the examples below assumes that you will be running training/evaluation
on your local machine, using a GPU like a Titan X or GTX 1080.&lt;/p&gt;
&lt;p&gt;However, if you have access to a Cloud TPU that you want to train on, just add
the following flags to &lt;code&gt;run_classifier.py&lt;/code&gt; or &lt;code&gt;run_squad.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --use_tpu=True \
  --tpu_name=$TPU_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the
&lt;a href="https://cloud.google.com/tpu/docs/tutorials/mnist" rel="nofollow"&gt;Google Cloud TPU tutorial&lt;/a&gt;
for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;On Cloud TPUs, the pretrained model and the output directory will need to be on
Google Cloud Storage. For example, if you have a bucket named &lt;code&gt;some_bucket&lt;/code&gt;, you
might use the following flags instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --output_dir=gs://some_bucket/my_output_dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unzipped pre-trained model files can also be found in the Google Cloud
Storage folder &lt;code&gt;gs://bert_models/2018_10_18&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-sentence-and-sentence-pair-classification-tasks" class="anchor" aria-hidden="true" href="#sentence-and-sentence-pair-classification-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentence (and sentence-pair) classification tasks&lt;/h3&gt;
&lt;p&gt;Before running this example you must download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;. Next, download the &lt;code&gt;BERT-Base&lt;/code&gt;
checkpoint and unzip it to some directory &lt;code&gt;$BERT_BASE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This example code fine-tunes &lt;code&gt;BERT-Base&lt;/code&gt; on the Microsoft Research Paraphrase
Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a
few minutes on most GPUs.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a
high variance in the Dev set accuracy, even when starting from the same
pre-training checkpoint. If you re-run multiple times (making sure to point to
different &lt;code&gt;output_dir&lt;/code&gt;), you should see results between 84% and 88%.&lt;/p&gt;
&lt;p&gt;A few other pre-trained models are implemented off-the-shelf in
&lt;code&gt;run_classifier.py&lt;/code&gt;, so it should be straightforward to follow those examples to
use BERT for any single-sentence or sentence-pair classification task.&lt;/p&gt;
&lt;p&gt;Note: You might see a message &lt;code&gt;Running train on CPU&lt;/code&gt;. This really just means
that it's running on something other than a Cloud TPU, which includes a GPU.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-prediction-from-classifier" class="anchor" aria-hidden="true" href="#prediction-from-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prediction from classifier&lt;/h4&gt;
&lt;p&gt;Once you have trained your classifier you can use it in inference mode by using
the --do_predict=true command. You need to have a file named test.tsv in the
input folder. Output will be created in file called test_results.tsv in the
output folder. Each line will contain output for each sample, columns are the
class probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$TRAINED_CLASSIFIER&lt;/span&gt; \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-squad-11" class="anchor" aria-hidden="true" href="#squad-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 1.1&lt;/h3&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD) is a popular question answering
benchmark dataset. BERT (at the time of the release) obtains state-of-the-art
results on SQuAD with almost no task-specific network architecture modifications
or data augmentation. However, it does require semi-complex data pre-processing
and post-processing to deal with (a) the variable-length nature of SQuAD context
paragraphs, and (b) the character-level answer annotations which are used for
SQuAD training. This processing is implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD, you will first need to download the dataset. The
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD website&lt;/a&gt; does not seem to
link to the v1.1 datasets any longer, but the necessary files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json" rel="nofollow"&gt;train-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json" rel="nofollow"&gt;dev-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"&gt;evaluate-v1.1.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The state-of-the-art SQuAD results from the paper currently cannot be reproduced
on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does
not seem to fit on a 12GB GPU using &lt;code&gt;BERT-Large&lt;/code&gt;). However, a reasonably strong
&lt;code&gt;BERT-Base&lt;/code&gt; model can be trained on the GPU with these hyperparameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dev set predictions will be saved into a file called &lt;code&gt;predictions.json&lt;/code&gt; in
the &lt;code&gt;output_dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ./squad/predictions.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which should produce an output like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 88.41249612335034, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 81.2488174077578}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see a result similar to the 88.5% reported in the paper for
&lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you have access to a Cloud TPU, you can train with &lt;code&gt;BERT-Large&lt;/code&gt;. Here is a
set of hyperparameters (slightly different than the paper) which consistently
obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, one random run with these parameters produces the following Dev
scores:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 90.87081895814865, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 84.38978240302744}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you fine-tune for one epoch on
&lt;a href="http://nlp.cs.washington.edu/triviaqa/" rel="nofollow"&gt;TriviaQA&lt;/a&gt; before this the results will
be even better, but you will need to convert TriviaQA into the SQuAD json
format.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-squad-20" class="anchor" aria-hidden="true" href="#squad-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 2.0&lt;/h3&gt;
&lt;p&gt;This model is also implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD 2.0, you will first need to download the dataset. The necessary
files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json" rel="nofollow"&gt;train-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json" rel="nofollow"&gt;dev-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" rel="nofollow"&gt;evaluate-v2.0.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On Cloud TPU you can run with BERT-Large as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We assume you have copied everything from the output directory to a local
directory called ./squad/. The initial dev set predictions will be at
./squad/predictions.json and the differences between the score of no answer ("")
and the best non-null answer for each question will be in the file
./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Run this script to tune a threshold for predicting null versus non-null answers:&lt;/p&gt;
&lt;p&gt;python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json
./squad/predictions.json --na-prob-file ./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Assume the script outputs "best_f1_thresh" THRESH. (Typical values are between
-1.0 and -5.0). You can now re-run the model to generate predictions with the
derived threshold or alternatively you can extract the appropriate answers from
./squad/nbest_predictions.json.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=False \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True \
  --null_score_diff_threshold=&lt;span class="pl-smi"&gt;$THRESH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-out-of-memory-issues" class="anchor" aria-hidden="true" href="#out-of-memory-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Out-of-memory issues&lt;/h3&gt;
&lt;p&gt;All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of
device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely
to encounter out-of-memory issues if you use the same hyperparameters described
in the paper.&lt;/p&gt;
&lt;p&gt;The factors that affect memory usage are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;max_seq_length&lt;/code&gt;&lt;/strong&gt;: The released models were trained with sequence lengths
up to 512, but you can fine-tune with a shorter max sequence length to save
substantial memory. This is controlled by the &lt;code&gt;max_seq_length&lt;/code&gt; flag in our
example code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;train_batch_size&lt;/code&gt;&lt;/strong&gt;: The memory usage is also directly proportional to
the batch size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model type, &lt;code&gt;BERT-Base&lt;/code&gt; vs. &lt;code&gt;BERT-Large&lt;/code&gt;&lt;/strong&gt;: The &lt;code&gt;BERT-Large&lt;/code&gt; model
requires significantly more memory than &lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: The default optimizer for BERT is Adam, which requires a lot
of extra memory to store the &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; vectors. Switching to a more memory
efficient optimizer can reduce memory usage, but can also affect the
results. We have not experimented with other optimizers for fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the default training scripts (&lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;run_squad.py&lt;/code&gt;), we
benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with
TensorFlow 1.11.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, these max batch sizes for &lt;code&gt;BERT-Large&lt;/code&gt; are so small that they
will actually harm the model accuracy, regardless of the learning rate used. We
are working on adding code to this repository which will allow much larger
effective batch sizes to be used on the GPU. The code will be based on one (or
both) of the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient accumulation&lt;/strong&gt;: The samples in a minibatch are typically
independent with respect to gradient computation (excluding batch
normalization, which is not used here). This means that the gradients of
multiple smaller minibatches can be accumulated before performing the weight
update, and this will be exactly equivalent to a single larger update.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gradient-checkpointing"&gt;&lt;strong&gt;Gradient checkpointing&lt;/strong&gt;&lt;/a&gt;:
The major use of GPU/TPU memory during DNN training is caching the
intermediate activations in the forward pass that are necessary for
efficient computation in the backward pass. "Gradient checkpointing" trades
memory for compute time by re-computing the activations in an intelligent
way.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;However, this is not implemented in the current release.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo" class="anchor" aria-hidden="true" href="#using-bert-to-extract-fixed-feature-vectors-like-elmo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT to extract fixed feature vectors (like ELMo)&lt;/h2&gt;
&lt;p&gt;In certain cases, rather than fine-tuning the entire pre-trained model
end-to-end, it can be beneficial to obtained &lt;em&gt;pre-trained contextual
embeddings&lt;/em&gt;, which are fixed contextual representations of each input token
generated from the hidden layers of the pre-trained model. This should also
mitigate most of the out-of-memory issues.&lt;/p&gt;
&lt;p&gt;As an example, we include the script &lt;code&gt;extract_features.py&lt;/code&gt; which can be used
like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sentence A and Sentence B are separated by the ||| delimiter for sentence&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pair tasks like question answering and entailment.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For single sentence inputs, put one sentence per line and DON'T use the&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; delimiter.&lt;/span&gt;
&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Who was Jim Henson ? ||| Jim Henson was a puppeteer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will create a JSON file (one line per line of input) containing the BERT
activations from each Transformer layer specified by &lt;code&gt;layers&lt;/code&gt; (-1 is the final
hidden layer of the Transformer, etc.)&lt;/p&gt;
&lt;p&gt;Note that this script will produce very large output files (by default, around
15kb for every input token).&lt;/p&gt;
&lt;p&gt;If you need to maintain alignment between the original and tokenized words (for
projecting training labels), see the &lt;a href="#tokenization"&gt;Tokenization&lt;/a&gt; section
below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You may see a message like &lt;code&gt;Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.&lt;/code&gt; This message is expected, it
just means that we are using the &lt;code&gt;init_from_checkpoint()&lt;/code&gt; API rather than the
saved model API. If you don't specify a checkpoint or specify an invalid
checkpoint, this script will complain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.
Just follow the example code in &lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;extract_features.py&lt;/code&gt;.
The basic procedure for sentence-level tasks is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instantiate an instance of &lt;code&gt;tokenizer = tokenization.FullTokenizer&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenize the raw text with &lt;code&gt;tokens = tokenizer.tokenize(raw_text)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncate to the maximum sequence length. (You can use up to 512, but you
probably want to use shorter if possible for memory and speed reasons.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; tokens in the right place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since
you need to maintain alignment between your input text and output text so that
you can project your training labels. SQuAD is a particularly complex example
because the input labels are &lt;em&gt;character&lt;/em&gt;-based, and SQuAD paragraphs are often
longer than our maximum sequence length. See the code in &lt;code&gt;run_squad.py&lt;/code&gt; to show
how we handle this.&lt;/p&gt;
&lt;p&gt;Before we describe the general recipe for handling word-level tasks, it's
important to understand what exactly our tokenizer is doing. It has three main
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text normalization&lt;/strong&gt;: Convert all whitespace characters to spaces, and
(for the &lt;code&gt;Uncased&lt;/code&gt; model) lowercase the input and strip out accent markers.
E.g., &lt;code&gt;John Johanson's, → john johanson's,&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Punctuation splitting&lt;/strong&gt;: Split &lt;em&gt;all&lt;/em&gt; punctuation characters on both sides
(i.e., add whitespace around all punctuation characters). Punctuation
characters are defined as (a) Anything with a &lt;code&gt;P*&lt;/code&gt; Unicode class, (b) any
non-letter/number/space ASCII character (e.g., characters like &lt;code&gt;$&lt;/code&gt; which are
technically not punctuation). E.g., &lt;code&gt;john johanson's, → john johanson ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WordPiece tokenization&lt;/strong&gt;: Apply whitespace tokenization to the output of
the above procedure, and apply
&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py"&gt;WordPiece&lt;/a&gt;
tokenization to each token separately. (Our implementation is directly based
on the one from &lt;code&gt;tensor2tensor&lt;/code&gt;, which is linked). E.g., &lt;code&gt;john johanson ' s , → john johan ##son ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this scheme is that it is "compatible" with most existing
English tokenizers. For example, imagine that you have a part-of-speech tagging
task which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tokenized output will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tokens: john johan ##son ' s house
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Crucially, this would be the same output as if the raw text were &lt;code&gt;John Johanson's house&lt;/code&gt; (with no space before the &lt;code&gt;'s&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have a pre-tokenized representation with word-level annotations, you can
simply tokenize each input word independently, and deterministically maintain an
original-to-tokenized alignment:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Input&lt;/span&gt;
orig_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;John&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Johanson&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;'s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;house&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
labels      &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;POS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Output&lt;/span&gt;
bert_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; []

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Token map will be an int -&amp;gt; int mapping between the `orig_tokens` index and&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the `bert_tokens` index.&lt;/span&gt;
orig_to_tok_map &lt;span class="pl-k"&gt;=&lt;/span&gt; []

tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenization.FullTokenizer(
    &lt;span class="pl-v"&gt;vocab_file&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;vocab_file, &lt;span class="pl-v"&gt;do_lower_case&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[CLS]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;for&lt;/span&gt; orig_token &lt;span class="pl-k"&gt;in&lt;/span&gt; orig_tokens:
  orig_to_tok_map.append(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[SEP]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; orig_to_tok_map == [1, 2, 4, 6]&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;code&gt;orig_to_tok_map&lt;/code&gt; can be used to project &lt;code&gt;labels&lt;/code&gt; to the tokenized
representation.&lt;/p&gt;
&lt;p&gt;There are common English tokenization schemes which will cause a slight mismatch
between how BERT was pre-trained. For example, if your input tokenization splits
off contractions like &lt;code&gt;do n't&lt;/code&gt;, this will cause a mismatch. If it is possible to
do so, you should pre-process your data to convert these back to raw-looking
text, but if it's not possible, this mismatch is likely not a big deal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-training-with-bert" class="anchor" aria-hidden="true" href="#pre-training-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training with BERT&lt;/h2&gt;
&lt;p&gt;We are releasing code to do "masked LM" and "next sentence prediction" on an
arbitrary text corpus. Note that this is &lt;em&gt;not&lt;/em&gt; the exact code that was used for
the paper (the original code was written in C++, and had some additional
complexity), but this code does generate pre-training data as described in the
paper.&lt;/p&gt;
&lt;p&gt;Here's how to run the data generation. The input is a plain text file, with one
sentence per line. (It is important that these be actual sentences for the "next
sentence prediction" task). Documents are delimited by empty lines. The output
is a set of &lt;code&gt;tf.train.Example&lt;/code&gt;s serialized into &lt;code&gt;TFRecord&lt;/code&gt; file format.&lt;/p&gt;
&lt;p&gt;You can perform sentence segmentation with an off-the-shelf NLP toolkit such as
&lt;a href="https://spacy.io/" rel="nofollow"&gt;spaCy&lt;/a&gt;. The &lt;code&gt;create_pretraining_data.py&lt;/code&gt; script will
concatenate segments until they reach the maximum sequence length to minimize
computational waste from padding (see the script for more details). However, you
may want to intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to non-sentential
input during fine-tuning.&lt;/p&gt;
&lt;p&gt;This script stores all of the examples for the entire input file in memory, so
for large data files you should shard the input file and call the script
multiple times. (You can pass in a file glob to &lt;code&gt;run_pretraining.py&lt;/code&gt;, e.g.,
&lt;code&gt;tf_examples.tf_record*&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;max_predictions_per_seq&lt;/code&gt; is the maximum number of masked LM predictions per
sequence. You should set this to around &lt;code&gt;max_seq_length&lt;/code&gt; * &lt;code&gt;masked_lm_prob&lt;/code&gt; (the
script doesn't do that automatically because the exact value needs to be passed
to both scripts).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how to run the pre-training. Do not include &lt;code&gt;init_checkpoint&lt;/code&gt; if you are
pre-training from scratch. The model configuration (including vocab size) is
specified in &lt;code&gt;bert_config_file&lt;/code&gt;. This demo code only pre-trains for a small
number of steps (20), but in practice you will probably want to set
&lt;code&gt;num_train_steps&lt;/code&gt; to 10000 steps or more. The &lt;code&gt;max_seq_length&lt;/code&gt; and
&lt;code&gt;max_predictions_per_seq&lt;/code&gt; parameters passed to &lt;code&gt;run_pretraining.py&lt;/code&gt; must be the
same as &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will produce an output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since our &lt;code&gt;sample_text.txt&lt;/code&gt; file is very small, this example training
will overfit that data in only a few steps and produce unrealistically high
accuracy numbers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-tips-and-caveats" class="anchor" aria-hidden="true" href="#pre-training-tips-and-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training tips and caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If using your own vocabulary, make sure to change &lt;code&gt;vocab_size&lt;/code&gt; in
&lt;code&gt;bert_config.json&lt;/code&gt;. If you use a larger vocabulary without changing this,
you will likely get NaNs when training on GPU or TPU due to unchecked
out-of-bounds access.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If your task has a large domain-specific corpus available (e.g., "movie
reviews" or "scientific papers"), it will likely be beneficial to run
additional steps of pre-training on your corpus, starting from the BERT
checkpoint.&lt;/li&gt;
&lt;li&gt;The learning rate we used in the paper was 1e-4. However, if you are doing
additional steps of pre-training starting from an existing BERT checkpoint,
you should use a smaller learning rate (e.g., 2e-5).&lt;/li&gt;
&lt;li&gt;Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages in the
near future (hopefully by the end of November 2018).&lt;/li&gt;
&lt;li&gt;Longer sequences are disproportionately expensive because attention is
quadratic to the sequence length. In other words, a batch of 64 sequences of
length 512 is much more expensive than a batch of 256 sequences of
length 128. The fully-connected/convolutional cost is the same, but the
attention cost is far greater for the 512-length sequences. Therefore, one
good recipe is to pre-train for, say, 90,000 steps with a sequence length of
128 and then for 10,000 additional steps with a sequence length of 512. The
very long sequences are mostly needed to learn positional embeddings, which
can be learned fairly quickly. Note that this does require generating the
data twice with different values of &lt;code&gt;max_seq_length&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are pre-training from scratch, be prepared that pre-training is
computationally expensive, especially on GPUs. If you are pre-training from
scratch, our recommended recipe is to pre-train a &lt;code&gt;BERT-Base&lt;/code&gt; on a single
&lt;a href="https://cloud.google.com/tpu/docs/pricing" rel="nofollow"&gt;preemptible Cloud TPU v2&lt;/a&gt;, which
takes about 2 weeks at a cost of about $500 USD (based on the pricing in
October 2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-data" class="anchor" aria-hidden="true" href="#pre-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training data&lt;/h3&gt;
&lt;p&gt;We will &lt;strong&gt;not&lt;/strong&gt; be able to release the pre-processed datasets used in the paper.
For Wikipedia, the recommended pre-processing is to download
&lt;a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="nofollow"&gt;the latest dump&lt;/a&gt;,
extract the text with
&lt;a href="https://github.com/attardi/wikiextractor"&gt;&lt;code&gt;WikiExtractor.py&lt;/code&gt;&lt;/a&gt;, and then apply
any necessary cleanup to convert it into plain text.&lt;/p&gt;
&lt;p&gt;Unfortunately the researchers who collected the
&lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt; no longer have it available for
public download. The
&lt;a href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html" rel="nofollow"&gt;Project Guttenberg Dataset&lt;/a&gt;
is a somewhat smaller (200M word) collection of older books that are public
domain.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://commoncrawl.org/" rel="nofollow"&gt;Common Crawl&lt;/a&gt; is another very large collection of
text, but you will likely have to do substantial pre-processing and cleanup to
extract a usable corpus for pre-training BERT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-learning-a-new-wordpiece-vocabulary" class="anchor" aria-hidden="true" href="#learning-a-new-wordpiece-vocabulary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning a new WordPiece vocabulary&lt;/h3&gt;
&lt;p&gt;This repository does not include code for &lt;em&gt;learning&lt;/em&gt; a new WordPiece vocabulary.
The reason is that the code used in the paper was implemented in C++ with
dependencies on Google's internal libraries. For English, it is almost always
better to just start with our vocabulary and pre-trained models. For learning
vocabularies of other languages, there are a number of open source options
available. However, keep in mind that these are not compatible with our
&lt;code&gt;tokenization.py&lt;/code&gt; library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;Google's SentencePiece library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py"&gt;tensor2tensor's WordPiece generation script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsennrich/subword-nmt"&gt;Rico Sennrich's Byte Pair Encoding library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-in-colab" class="anchor" aria-hidden="true" href="#using-bert-in-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT in Colab&lt;/h2&gt;
&lt;p&gt;If you want to use BERT with &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Colab&lt;/a&gt;, you can
get started with the notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".
&lt;strong&gt;At the time of this writing (October 31st, 2018), Colab users can access a
Cloud TPU completely for free.&lt;/strong&gt; Note: One per user, availability limited,
requires a Google Cloud Platform account with storage (although storage may be
purchased with free credit for signing up with GCP), and this capability may not
longer be available in the future. Click on the BERT Colab that was just linked
for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus" class="anchor" aria-hidden="true" href="#is-this-code-compatible-with-cloud-tpus-what-about-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is this code compatible with Cloud TPUs? What about GPUs?&lt;/h4&gt;
&lt;p&gt;Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and
Cloud TPU. However, GPU training is single-GPU only.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-i-am-getting-out-of-memory-errors-what-is-wrong" class="anchor" aria-hidden="true" href="#i-am-getting-out-of-memory-errors-what-is-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I am getting out-of-memory errors, what is wrong?&lt;/h4&gt;
&lt;p&gt;See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for more
information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-pytorch-version-available" class="anchor" aria-hidden="true" href="#is-there-a-pytorch-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a PyTorch version available?&lt;/h4&gt;
&lt;p&gt;There is no official PyTorch implementation. However, NLP researchers from
HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-chainer-version-available" class="anchor" aria-hidden="true" href="#is-there-a-chainer-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a Chainer version available?&lt;/h4&gt;
&lt;p&gt;There is no official Chainer implementation. However, Sosuke Kobayashi made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the Chainer
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-in-other-languages-be-released" class="anchor" aria-hidden="true" href="#will-models-in-other-languages-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models in other languages be released?&lt;/h4&gt;
&lt;p&gt;Yes, we plan to release a multi-lingual BERT model in the near future. We cannot
make promises about exactly which languages will be included, but it will likely
be a single model which includes &lt;em&gt;most&lt;/em&gt; of the languages which have a
significantly-sized Wikipedia.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-larger-than-bert-large-be-released" class="anchor" aria-hidden="true" href="#will-models-larger-than-bert-large-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models larger than &lt;code&gt;BERT-Large&lt;/code&gt; be released?&lt;/h4&gt;
&lt;p&gt;So far we have not attempted to train anything larger than &lt;code&gt;BERT-Large&lt;/code&gt;. It is
possible that we will release larger models if we are able to obtain significant
improvements.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-what-license-is-this-library-released-under" class="anchor" aria-hidden="true" href="#what-license-is-this-library-released-under"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What license is this library released under?&lt;/h4&gt;
&lt;p&gt;All code &lt;em&gt;and&lt;/em&gt; models are released under the Apache 2.0 license. See the
&lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how-do-i-cite-bert" class="anchor" aria-hidden="true" href="#how-do-i-cite-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I cite BERT?&lt;/h4&gt;
&lt;p&gt;For now, cite &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;the Arxiv paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we submit the paper to a conference or journal, we will update the BibTeX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This is not an official Google product.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact-information" class="anchor" aria-hidden="true" href="#contact-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact information&lt;/h2&gt;
&lt;p&gt;For help or issues using BERT, please submit a GitHub issue.&lt;/p&gt;
&lt;p&gt;For personal communication related to BERT, please contact Jacob Devlin
(&lt;code&gt;jacobdevlin@google.com&lt;/code&gt;), Ming-Wei Chang (&lt;code&gt;mingweichang@google.com&lt;/code&gt;), or
Kenton Lee (&lt;code&gt;kentonl@google.com&lt;/code&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/bert</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>521xueweihan/HelloGitHub #10 in Python, This week</title><link>https://github.com/521xueweihan/HelloGitHub</link><description>&lt;p&gt;&lt;i&gt;:octocat: Find pearls on open-source seashore 分享 GitHub 上有趣、入门级的开源项目&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif"&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;中文 | &lt;a href="README_en.md"&gt;English&lt;/a&gt;
  &lt;br&gt;&lt;strong&gt;HelloGitHub&lt;/strong&gt; 一个分享 GitHub 上有趣、入门级的开源项目。&lt;br&gt;兴趣是最好的老师，这里能够帮你找到编程的兴趣！
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://hellogithub.com/weixin.png" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/61343b85520a4714ddb37eb300f8268cc881ae7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54616c6b2d2545352542452541452545342542462541312545372542452541342d627269676874677265656e2e7376673f7374796c653d706f706f75742d737175617265" alt="WeiXin" data-canonical-src="https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/stargazers"&gt;&lt;img src="https://camo.githubusercontent.com/0aec7fa1a5647255bbe8af37a82a007be69d8739/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/issues"&gt;&lt;img src="https://camo.githubusercontent.com/a8367e38e94eccf7e469023edfec05db15132454/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4627590b5d81a690c6c83abaf47f678d70d26e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362542302545362542352541412d576569626f2d7265642e7376673f7374796c653d706f706f75742d737175617265" alt="Sina Weibo" data-canonical-src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;这是一个面向编程新手、热爱编程、对开源社区感兴趣人群的项目，内容&lt;strong&gt;每月 28 号&lt;/strong&gt;以月刊的形式更新发布。内容包括：&lt;strong&gt;流行项目&lt;/strong&gt;、&lt;strong&gt;入门级项目&lt;/strong&gt;、&lt;strong&gt;让生活变得更美好的工具&lt;/strong&gt;、&lt;strong&gt;书籍&lt;/strong&gt;、&lt;strong&gt;学习心得笔记&lt;/strong&gt;、&lt;strong&gt;企业级项目&lt;/strong&gt;等，这些开源项目大多都是非常容易上手、很 Cool，能够让你用很短时间感受到编程的魅力和便捷。从而让大家感受到编程的乐趣，动手开始编程。&lt;/p&gt;
&lt;p&gt;希望通过本项目能够有更多人加入到开源社区、回馈社区。&lt;strong&gt;让有趣、有价值的项目被更多人发现和加入&lt;/strong&gt;。在参与这些项目的过程中，你将得到：&lt;strong&gt;热爱编程的小伙伴&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="man_dancing" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f57a.png"&gt;🕺&lt;/g-emoji&gt; 、&lt;strong&gt;更多编程知识&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt; 、&lt;strong&gt;优秀的编程技巧&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; 、&lt;strong&gt;找到编程的乐趣&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="video_game" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ae.png"&gt;🎮&lt;/g-emoji&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;『每日精选』&lt;/strong&gt; 关注我们的&lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;最惨官微&lt;/a&gt;获取最新项目推荐。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;『讲解开源项目』&lt;/strong&gt; 欢迎开源爱好者给我们投稿&lt;a href="https://github.com/HelloGitHub-Team/Article/blob/master/%E5%88%9B%E4%BD%9C%E9%A1%BB%E7%9F%A5.md"&gt;查看创作须知&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-内容" class="anchor" aria-hidden="true" href="#内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容&lt;/h2&gt;
&lt;p&gt;每月 28 号发布&lt;a href="/content/last.md"&gt;最新一期&lt;/a&gt; | &lt;a href="https://hellogithub.com" rel="nofollow"&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img class="emoji" title=":shipit:" alt=":shipit:" src="https://github.githubassets.com/images/icons/emoji/shipit.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="jack_o_lantern" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f383.png"&gt;🎃&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="beer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37a.png"&gt;🍺&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="fish_cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f365.png"&gt;🍥&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/43/HelloGitHub43.md"&gt;第 43 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/42/HelloGitHub42.md"&gt;第 42 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/41/HelloGitHub41.md"&gt;第 41 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/40/HelloGitHub40.md"&gt;第 40 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/39/HelloGitHub39.md"&gt;第 39 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/38/HelloGitHub38.md"&gt;第 38 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/37/HelloGitHub37.md"&gt;第 37 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/36/HelloGitHub36.md"&gt;第 36 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/35/HelloGitHub35.md"&gt;第 35 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/34/HelloGitHub34.md"&gt;第 34 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/33/HelloGitHub33.md"&gt;第 33 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/32/HelloGitHub32.md"&gt;第 32 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/31/HelloGitHub31.md"&gt;第 31 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/30/HelloGitHub30.md"&gt;第 30 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/29/HelloGitHub29.md"&gt;第 29 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/28/HelloGitHub28.md"&gt;第 28 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/27/HelloGitHub27.md"&gt;第 27 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/26/HelloGitHub26.md"&gt;第 26 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/25/HelloGitHub25.md"&gt;第 25 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/24/HelloGitHub24.md"&gt;第 24 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/23/HelloGitHub23.md"&gt;第 23 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/22/HelloGitHub22.md"&gt;第 22 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/21/HelloGitHub21.md"&gt;第 21 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/20/HelloGitHub20.md"&gt;第 20 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/19/HelloGitHub19.md"&gt;第 19 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/18/HelloGitHub18.md"&gt;第 18 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/17/HelloGitHub17.md"&gt;第 17 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/16/HelloGitHub16.md"&gt;第 16 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/15/HelloGitHub15.md"&gt;第 15 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/14/HelloGitHub14.md"&gt;第 14 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/13/HelloGitHub13.md"&gt;第 13 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/12/HelloGitHub12.md"&gt;第 12 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/11/HelloGitHub11.md"&gt;第 11 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/10/HelloGitHub10.md"&gt;第 10 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/09/HelloGitHub09.md"&gt;第 09 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/08/HelloGitHub08.md"&gt;第 08 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/07/HelloGitHub07.md"&gt;第 07 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/06/HelloGitHub06.md"&gt;第 06 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/05/HelloGitHub05.md"&gt;第 05 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/04/HelloGitHub04.md"&gt;第 04 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/03/HelloGitHub03.md"&gt;第 03 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/02/HelloGitHub02.md"&gt;第 02 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/01/HelloGitHub01.md"&gt;第 01 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;欢迎&lt;a href="https://github.com/521xueweihan/HelloGitHub/issues/new"&gt;推荐或自荐项目&lt;/a&gt;成为 &lt;strong&gt;HelloGitHub&lt;/strong&gt; 的&lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;贡献者&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-贡献者" class="anchor" aria-hidden="true" href="#贡献者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;贡献者&lt;/h2&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/8255800?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;削微寒&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ming995"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/46031112?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;糖醋里脊&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FrontMage"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/17007026?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FrontMage&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/xibinyue"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/14122146?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;xibinyue&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/Eurus-Holmes"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/34226570?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Feiyang Chen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ChungZH"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/42088872?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;ChungZH&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/daixiang0"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/26538619?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;daixiang0&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/nivance"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/3291404?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;nivance&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/hellowHuaairen"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/19610305?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;hellowHuaairen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/17665302?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;更多贡献者&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-合作组织" class="anchor" aria-hidden="true" href="#合作组织"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;合作组织&lt;/h2&gt;
&lt;p&gt;欢迎各种&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;开源组织合作&lt;a href="Mailto:595666367@qq.com"&gt;点击联系我&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FGDBTKD"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40509403?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FGDBTKD&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;AI/ML/DL/NLP&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/d2-projects"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40857578?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;D2 Projects&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Vue/JavaScript&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/doocs"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/43716716?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Doocs&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Technical Knowledge&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;&lt;img alt="知识共享许可协议" src="https://camo.githubusercontent.com/1ae74a56e22c4897b6fbfb9f301bd829c77429a7/68747470733a2f2f6c6963656e7365627574746f6e732e6e65742f6c2f62792d6e632d6e642f342e302f38387833312e706e67" data-canonical-src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;本作品采用 &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 进行许可。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>521xueweihan</author><guid isPermaLink="false">https://github.com/521xueweihan/HelloGitHub</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>KubeOperator/KubeOperator #11 in Python, This week</title><link>https://github.com/KubeOperator/KubeOperator</link><description>&lt;p&gt;&lt;i&gt;KubeOperator 是一个开源项目，通过 Web UI 在 VMware、OpenStack、物理机上一键部署和管理生产级别的 Kubernetes 集群。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kubeoperator---从这里开启您的-kubernetes-之旅" class="anchor" aria-hidden="true" href="#kubeoperator---从这里开启您的-kubernetes-之旅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KubeOperator - 从这里开启您的 Kubernetes 之旅&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/KubeOperatpr/KubeOperatpr/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/7197a397ba1baf73679f3cf0edf68d821c35ae52/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d61706163686525323076322d626c75652e737667" alt="License" data-canonical-src="http://img.shields.io/badge/license-apache%20v2-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.python.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4c11ac2b538cba463dfd1e43d05fe4f30f2d33d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d677265656e2e7376673f7374796c653d706c6173746963" alt="Python3" data-canonical-src="https://img.shields.io/badge/python-3.6-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/35798c7a6bb116ad2e8d420db49766bce91239b1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646a616e676f2d322e312d627269676874677265656e2e7376673f7374796c653d706c6173746963" alt="Django" data-canonical-src="https://img.shields.io/badge/django-2.1-brightgreen.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.ansible.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dbfb9037d993ab109b0dd41252b2aabcd703e4a5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e7369626c652d322e362e352d626c75652e7376673f7374796c653d706c6173746963" alt="Ansible" data-canonical-src="https://img.shields.io/badge/ansible-2.6.5-blue.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.angular.cn/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9829fdfaae3736e19d738b29efaeec4aaf21c61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e67756c61722d372e302e342d7265642e7376673f7374796c653d706c6173746963" alt="Angular" data-canonical-src="https://img.shields.io/badge/angular-7.0.4-red.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator 是一个开源项目，在离线网络环境下，通过可视化 Web UI 在 VMware、Openstack 或者物理机上规划、部署和管理生产级别的 Kubernetes 集群。KubeOperator 是 &lt;a href="https://github.com/jumpserver/jumpserver"&gt;Jumpserver&lt;/a&gt; 明星开源团队在 Kubernetes 领域的的又一全新力作。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/overview.png?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/overview.png?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-web-ui-展示" class="anchor" aria-hidden="true" href="#web-ui-展示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web UI 展示&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;更多功能截屏请查看：&lt;a href="https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot" rel="nofollow"&gt;https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-整体架构" class="anchor" aria-hidden="true" href="#整体架构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;整体架构&lt;/h2&gt;
&lt;p&gt;KubeOperator 使用 Terraform 在 IaaS 平台上自动创建主机（用户也可以自行准备主机，比如物理机或者虚机），通过 Ansible 完成自动化部署和变更操作，支持 Kubernetes 集群 从 Day 0 规划，到 Day 1 部署，到 Day 2 运维及变更的全生命周期管理。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/KubeOperator.jpeg?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/KubeOperator.jpeg?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-技术优势" class="anchor" aria-hidden="true" href="#技术优势"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;技术优势&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;按需创建：调用云平台 API，一键快速创建和部署 Kubernetes 集群 (即 Kubernetes as a Service)；&lt;/li&gt;
&lt;li&gt;按需伸缩：快速伸缩 Kubernetes 集群，优化资源使用效率；&lt;/li&gt;
&lt;li&gt;按需修补：快速升级和修补 Kubernetes 集群，并与社区最新版本同步，保证安全性；&lt;/li&gt;
&lt;li&gt;自我修复：通过重建故障节点确保集群可用性；&lt;/li&gt;
&lt;li&gt;离线部署：持续更新包括 Kubernetes 及常用组件的离线包；&lt;/li&gt;
&lt;li&gt;Multi-AZ 支持：通过把 Kubernetes 集群 Master 节点分布在不同的故障域上确保的高可用；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-demo-视频使用文档" class="anchor" aria-hidden="true" href="#demo-视频使用文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo 视频、使用文档&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubeoperator-1256577600.file.myqcloud.com/video/KubeOperator2.1.mp4" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="tv" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4fa.png"&gt;📺&lt;/g-emoji&gt;8 分钟演示视频&lt;/a&gt;：详细演示 KubeOperator 的功能。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.kubeoperator.io/" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt;安装及使用文档&lt;/a&gt;：包括 KubeOperator 安装文档、使用文档、功能截屏、常见问题等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-kubernetes-离线安装包" class="anchor" aria-hidden="true" href="#kubernetes-离线安装包"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kubernetes 离线安装包&lt;/h2&gt;
&lt;p&gt;KubeOperator 提供完整的离线 Kubernetes 安装包（包括 Kubernetes、Docker、etcd、Dashboard、Promethus、OS 补丁等），每个安装包会被构建成一个独立容器镜像供 KubeOperator 使用，具体信息请参考：&lt;a href="https://github.com/KubeOperator/k8s-package"&gt;k8s-package&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本规划" class="anchor" aria-hidden="true" href="#版本规划"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本规划&lt;/h2&gt;
&lt;p&gt;v1.0 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提供原生 Kubernetes 的离线包仓库；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持一主多节点部署模式；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持离线环境下的一键自动化部署，可视化展示集群部署进展和结果；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 内置 Kubernetes 常用系统应用的安装，包括 Registry、Promethus、Dashboard、Traefik Ingress、Helm 等；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提供简易明了的 Kubernetes 集群运行状况面板；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 NFS 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Flannel 网络插件；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群手动部署模式（自行准备主机和 NFS）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.0 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持调用 VMware vCenter API 自动创建集群主机；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 VMware vSAN 、VMFS/NFS 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Multi AZ，支持多主多节点部署模式；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Calico 网络插件；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 内置 Weave Scope；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持通过 F5 BIG-IP Controller 对外暴露服务（Nodeport mode, 七层和四层服务都支持）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.1 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Openstack 云平台；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Openstack Cinder 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群升级 （Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群扩缩容（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群备份与恢复（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群健康检查与诊断（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 &lt;a href="https://github.com/webkubectl/webkubectl"&gt;webkubectl&lt;/a&gt; ；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.2 （进行中，2019.11.30 发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 日志收集及管理方案；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 离线环境下使用 Sonobuoy 进行 Kubernetes 集群合规检查并可视化展示结果&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 新增概览页面：展示关键信息，比如告警、使用率、异常日志、重复重启容器等信息；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.3 （计划中）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; KubeApps 应用商店；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 国际化支持；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 支持 VMware NSX-T；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-沟通交流" class="anchor" aria-hidden="true" href="#沟通交流"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;沟通交流&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;技术交流 QQ 群：825046920；&lt;/li&gt;
&lt;li&gt;技术支持邮箱：&lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt;；&lt;/li&gt;
&lt;li&gt;微信群： 搜索微信号 wh_it0224，添加好友，备注（城市-github用户名）, 验证通过会加入群聊；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/hashicorp/terraform"&gt;Terraform&lt;/a&gt;: KubeOperator 采用 Terraform 来自动创建虚机；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vmware/clarity/"&gt;Clarity&lt;/a&gt;: KubeOperator 采用 Clarity 作为前端 Web 框架；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ansible/ansible"&gt;Ansible&lt;/a&gt;: KubeOperator 采用 Ansible 作为自动化部署工具；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/easzlab/kubeasz"&gt;kubeasz&lt;/a&gt;: 提供各种 Kubernetes Ansible 脚本；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2014-2019 FIT2CLOUD 飞致云&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.fit2cloud.com" rel="nofollow"&gt;https://www.fit2cloud.com&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator is licensed under the Apache License, Version 2.0.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>KubeOperator</author><guid isPermaLink="false">https://github.com/KubeOperator/KubeOperator</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>trailofbits/algo #12 in Python, This week</title><link>https://github.com/trailofbits/algo</link><description>&lt;p&gt;&lt;i&gt;Set up a personal VPN in the cloud&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-algo-vpn" class="anchor" aria-hidden="true" href="#algo-vpn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algo VPN&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://gitter.im/trailofbits/algo?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c0e33218aa937f681d5088b670c988adf804264/68747470733a2f2f6261646765732e6769747465722e696d2f747261696c6f66626974732f616c676f2e737667" alt="Join the chat at https://gitter.im/trailofbits/algo" data-canonical-src="https://badges.gitter.im/trailofbits/algo.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/AlgoVPN" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a67add962c4c0beeead2da6dd98552fbce611fdb/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f666f6c645f6c6566742e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77253230253430416c676f56504e" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/https/twitter.com/fold_left.svg?style=social&amp;amp;label=Follow%20%40AlgoVPN" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/trailofbits/algo" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/960c464446296d169c0887c1641336b26bc8672f/68747470733a2f2f6170692e7472617669732d63692e6f72672f747261696c6f66626974732f616c676f2e7376673f6272616e63683d6d6173746572" alt="TravisCI Status" data-canonical-src="https://api.travis-ci.org/trailofbits/algo.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Algo VPN is a set of Ansible scripts that simplify the setup of a personal Wireguard and IPSEC VPN. It uses the most secure defaults available, works with common cloud providers, and does not require client software on most devices. See our &lt;a href="https://blog.trailofbits.com/2016/12/12/meet-algo-the-vpn-that-works/" rel="nofollow"&gt;release announcement&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Supports only IKEv2 with strong crypto (AES-GCM, SHA2, and P-256) and &lt;a href="https://www.wireguard.com/" rel="nofollow"&gt;WireGuard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generates Apple profiles to auto-configure iOS and macOS devices&lt;/li&gt;
&lt;li&gt;Includes a helper script to add and remove users&lt;/li&gt;
&lt;li&gt;Blocks ads with a local DNS resolver (optional)&lt;/li&gt;
&lt;li&gt;Sets up limited SSH users for tunneling traffic (optional)&lt;/li&gt;
&lt;li&gt;Based on current versions of Ubuntu and strongSwan&lt;/li&gt;
&lt;li&gt;Installs to DigitalOcean, Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, OpenStack, CloudStack, Hetzner Cloud, or &lt;a href="docs/deploy-to-ubuntu.md"&gt;your own Ubuntu server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-anti-features" class="anchor" aria-hidden="true" href="#anti-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anti-features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Does not support legacy cipher suites or protocols like L2TP, IKEv1, or RSA&lt;/li&gt;
&lt;li&gt;Does not install Tor, OpenVPN, or other risky servers&lt;/li&gt;
&lt;li&gt;Does not depend on the security of &lt;a href="https://tools.ietf.org/html/rfc7457" rel="nofollow"&gt;TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Does not require client software on most platforms&lt;/li&gt;
&lt;li&gt;Does not claim to provide anonymity or censorship avoidance&lt;/li&gt;
&lt;li&gt;Does not claim to protect you from the &lt;a href="https://en.wikipedia.org/wiki/Federal_Security_Service" rel="nofollow"&gt;FSB&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Ministry_of_State_Security_(China)" rel="nofollow"&gt;MSS&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Directorate-General_for_External_Security" rel="nofollow"&gt;DGSE&lt;/a&gt;, or &lt;a href="https://en.wikipedia.org/wiki/Flying_Spaghetti_Monster" rel="nofollow"&gt;FSM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-deploy-the-algo-server" class="anchor" aria-hidden="true" href="#deploy-the-algo-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploy the Algo Server&lt;/h2&gt;
&lt;p&gt;The easiest way to get an Algo server running is to run it on your local system and let it set up a &lt;em&gt;new&lt;/em&gt; virtual machine in the cloud for you.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Setup an account on a cloud hosting provider.&lt;/strong&gt; Algo supports &lt;a href="https://m.do.co/c/4d7f4ff9cfe4" rel="nofollow"&gt;DigitalOcean&lt;/a&gt; (most user friendly), &lt;a href="https://aws.amazon.com/lightsail/" rel="nofollow"&gt;Amazon Lightsail&lt;/a&gt;, &lt;a href="https://aws.amazon.com/" rel="nofollow"&gt;Amazon EC2&lt;/a&gt;, &lt;a href="https://www.vultr.com/" rel="nofollow"&gt;Vultr&lt;/a&gt;, &lt;a href="https://azure.microsoft.com/" rel="nofollow"&gt;Microsoft Azure&lt;/a&gt;, &lt;a href="https://cloud.google.com/compute/" rel="nofollow"&gt;Google Compute Engine&lt;/a&gt;, &lt;a href="https://www.scaleway.com/" rel="nofollow"&gt;Scaleway&lt;/a&gt;, &lt;a href="https://www.dreamhost.com/cloud/computing/" rel="nofollow"&gt;DreamCompute&lt;/a&gt; or other OpenStack-based cloud hosting, &lt;a href="https://www.exoscale.com" rel="nofollow"&gt;Exoscale&lt;/a&gt; or other CloudStack-based cloud hosting,  or &lt;a href="https://www.hetzner.com/" rel="nofollow"&gt;Hetzner Cloud&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Get a copy of Algo.&lt;/strong&gt; The Algo scripts will be installed on your local system. There are two ways to get a copy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Download the &lt;a href="https://github.com/trailofbits/algo/archive/master.zip"&gt;ZIP file&lt;/a&gt;. Unzip the file to create a directory named &lt;code&gt;algo-master&lt;/code&gt; containing the Algo scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the command &lt;code&gt;git clone https://github.com/trailofbits/algo.git&lt;/code&gt; to create a directory named &lt;code&gt;algo&lt;/code&gt; containing the Algo scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Algo's core dependencies.&lt;/strong&gt; Algo requires that &lt;strong&gt;Python 3&lt;/strong&gt; and at least one supporting package are installed on your system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt; Apple does not provide Python 3 with macOS. There are two ways to obtain it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the &lt;a href="https://brew.sh" rel="nofollow"&gt;Homebrew&lt;/a&gt; package manager. After installing Homebrew install Python 3 by running &lt;code&gt;brew install python3&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download and install the latest stable &lt;a href="https://www.python.org/downloads/mac-osx/" rel="nofollow"&gt;Python 3 package&lt;/a&gt;. Be sure to run the included &lt;em&gt;Install Certificates&lt;/em&gt; command from Finder.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once Python 3 is installed on your Mac, from Terminal run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m pip install --upgrade virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Recent releases of Ubuntu, Debian, and Fedora come with Python 3 already installed. Make sure your system is up-to-date and install the supporting package(s):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu and Debian:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo apt install -y python3-virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Fedora:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo dnf install -y python3-virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Red Hat and CentOS 7 and later (for earlier versions see this &lt;a href="docs/deploy-from-redhat-centos6.md"&gt;documentation&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo yum -y install epel-release
sudo yum install -y python36-virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use the Windows Subsystem for Linux (WSL) to create your own copy of Ubuntu running under Windows from which to install and run Algo. See the &lt;a href="docs/deploy-from-windows.md"&gt;Windows documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Algo's remaining dependencies.&lt;/strong&gt; You'll need to run these commands from the Algo directory each time you download a new copy of Algo. In a Terminal window &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;algo-master&lt;/code&gt; (ZIP file) or &lt;code&gt;algo&lt;/code&gt; (&lt;code&gt;git clone&lt;/code&gt;) directory and run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m virtualenv --python=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;command -v python3&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; .env &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
  &lt;span class="pl-c1"&gt;source&lt;/span&gt; .env/bin/activate &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
  python3 -m pip install -U pip virtualenv &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
  python3 -m pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Fedora add the option &lt;code&gt;--system-site-packages&lt;/code&gt; to the first command above. On macOS install the C compiler if prompted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;List the users to create.&lt;/strong&gt; Open the file &lt;code&gt;config.cfg&lt;/code&gt; in your favorite text editor. Specify the users you wish to create in the &lt;code&gt;users&lt;/code&gt; list. Create a unique user for each device you plan to connect to your VPN. If you want to be able to add or delete users later, you &lt;strong&gt;must&lt;/strong&gt; select &lt;code&gt;yes&lt;/code&gt; at the &lt;code&gt;Do you want to retain the keys (PKI)?&lt;/code&gt; prompt during the deployment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Start the deployment.&lt;/strong&gt; Return to your terminal. In the Algo directory, run &lt;code&gt;./algo&lt;/code&gt; and follow the instructions. There are several optional features available. None are required for a fully functional VPN server. These optional features are described in greater detail in &lt;a href="docs/deploy-from-ansible.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it! You will get the message below when the server deployment process completes. Take note of the p12 (user certificate) password and the CA key in case you need them later, &lt;strong&gt;they will only be displayed this time&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You can now set up clients to connect to your VPN. Proceed to &lt;a href="#configure-the-vpn-clients"&gt;Configure the VPN Clients&lt;/a&gt; below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    "#                          Congratulations!                            #"
    "#                     Your Algo server is running.                     #"
    "#    Config files and certificates are in the ./configs/ directory.    #"
    "#              Go to https://whoer.net/ after connecting               #"
    "#        and ensure that all your traffic passes through the VPN.      #"
    "#                     Local DNS resolver 172.16.0.1                    #"
    "#        The p12 and SSH keys password for new users is XXXXXXXX       #"
    "#        The CA key password is XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX       #"
    "#      Shell access: ssh -i configs/algo.pem root@xxx.xxx.xx.xx        #"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-configure-the-vpn-clients" class="anchor" aria-hidden="true" href="#configure-the-vpn-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure the VPN Clients&lt;/h2&gt;
&lt;p&gt;Certificates and configuration files that users will need are placed in the &lt;code&gt;configs&lt;/code&gt; directory. Make sure to secure these files since many contain private keys. All files are saved under a subdirectory named with the IP address of your new Algo VPN server.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-apple-devices" class="anchor" aria-hidden="true" href="#apple-devices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apple Devices&lt;/h3&gt;
&lt;p&gt;WireGuard is used to provide VPN services on Apple devices. Algo generates a WireGuard configuration file, &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.conf&lt;/code&gt;, and a QR code, &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.png&lt;/code&gt;, for each user defined in &lt;code&gt;config.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On iOS, install the &lt;a href="https://itunes.apple.com/us/app/wireguard/id1441195209?mt=8" rel="nofollow"&gt;WireGuard&lt;/a&gt; app from the iOS App Store. Then, use the WireGuard app to scan the QR code or AirDrop the configuration file to the device.&lt;/p&gt;
&lt;p&gt;On macOS Mojave or later, install the &lt;a href="https://itunes.apple.com/us/app/wireguard/id1451685025?mt=12" rel="nofollow"&gt;WireGuard&lt;/a&gt; app from the Mac App Store. WireGuard will appear in the menu bar once you run the app. Click on the WireGuard icon, choose &lt;strong&gt;Import tunnel(s) from file...&lt;/strong&gt;, then select the appropriate WireGuard configuration file.&lt;/p&gt;
&lt;p&gt;On either iOS or macOS, you can enable "Connect on Demand" and/or exclude certain trusted Wi-Fi networks (such as your home or work) by editing the tunnel configuration in the WireGuard app. (Algo can't do this automatically for you.)&lt;/p&gt;
&lt;p&gt;Installing WireGuard is a little more complicated on older version of macOS. See &lt;a href="docs/client-macos-wireguard.md"&gt;Using macOS as a Client with WireGuard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you prefer to use the built-in IPSEC VPN on Apple devices, or need "Connect on Demand" or excluded Wi-Fi networks automatically configured, then see &lt;a href="docs/client-apple-ipsec.md"&gt;Using Apple Devices as a Client with IPSEC&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-android-devices" class="anchor" aria-hidden="true" href="#android-devices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Android Devices&lt;/h3&gt;
&lt;p&gt;WireGuard is used to provide VPN services on Android. Install the &lt;a href="https://play.google.com/store/apps/details?id=com.wireguard.android" rel="nofollow"&gt;WireGuard VPN Client&lt;/a&gt;. Import the corresponding &lt;code&gt;wireguard/&amp;lt;name&amp;gt;.conf&lt;/code&gt; file to your device, then setup a new connection with it. See the &lt;a href="/docs/client-android.md"&gt;Android setup instructions&lt;/a&gt; for more detailed walkthrough.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-windows" class="anchor" aria-hidden="true" href="#windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Windows&lt;/h3&gt;
&lt;p&gt;WireGuard is used to provide VPN services on Windows. Algo generates a WireGuard configuration file, &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.conf&lt;/code&gt;, for each user defined in &lt;code&gt;config.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Install the &lt;a href="https://www.wireguard.com/install/#windows-7-8-81-10-2012-2016-2019" rel="nofollow"&gt;WireGuard VPN Client&lt;/a&gt;. Import the generated &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.conf&lt;/code&gt; file to your device, then setup a new connection with it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-linux-wireguard-clients" class="anchor" aria-hidden="true" href="#linux-wireguard-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linux WireGuard Clients&lt;/h3&gt;
&lt;p&gt;WireGuard works great with Linux clients. See &lt;a href="docs/client-linux-wireguard.md"&gt;this page&lt;/a&gt; for an example of how to configure WireGuard on Ubuntu.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-linux-strongswan-ipsec-clients-eg-openwrt-ubuntu-server-etc" class="anchor" aria-hidden="true" href="#linux-strongswan-ipsec-clients-eg-openwrt-ubuntu-server-etc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linux strongSwan IPsec Clients (e.g., OpenWRT, Ubuntu Server, etc.)&lt;/h3&gt;
&lt;p&gt;Please see &lt;a href="docs/client-linux-ipsec.md"&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-other-devices" class="anchor" aria-hidden="true" href="#other-devices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other Devices&lt;/h3&gt;
&lt;p&gt;Depending on the platform, you may need one or multiple of the following files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ipsec/manual/cacert.pem: CA Certificate&lt;/li&gt;
&lt;li&gt;ipsec/manual/.p12: User Certificate and Private Key (in PKCS#12 format)&lt;/li&gt;
&lt;li&gt;ipsec/manual/.conf: strongSwan client configuration&lt;/li&gt;
&lt;li&gt;ipsec/manual/.secrets: strongSwan client configuration&lt;/li&gt;
&lt;li&gt;ipsec/apple/.mobileconfig: Apple Profile&lt;/li&gt;
&lt;li&gt;wireguard/.conf: WireGuard configuration profile&lt;/li&gt;
&lt;li&gt;wireguard/.png: WireGuard configuration QR code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-setup-an-ssh-tunnel" class="anchor" aria-hidden="true" href="#setup-an-ssh-tunnel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup an SSH Tunnel&lt;/h2&gt;
&lt;p&gt;If you turned on the optional SSH tunneling role, then local user accounts will be created for each user in &lt;code&gt;config.cfg&lt;/code&gt; and SSH authorized_key files for them will be in the &lt;code&gt;configs&lt;/code&gt; directory (user.ssh.pem). SSH user accounts do not have shell access, cannot authenticate with a password, and only have limited tunneling options (e.g., &lt;code&gt;ssh -N&lt;/code&gt; is required). This ensures that SSH users have the least access required to setup a tunnel and can perform no other actions on the Algo server.&lt;/p&gt;
&lt;p&gt;Use the example command below to start an SSH tunnel by replacing &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;ip&lt;/code&gt; with your own. Once the tunnel is setup, you can configure a browser or other application to use 127.0.0.1:1080 as a SOCKS proxy to route traffic through the Algo server.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh -D 127.0.0.1:1080 -f -q -C -N user@ip -i configs/&amp;lt;server_ip&amp;gt;/ssh-tunnel/&amp;lt;user&amp;gt;.pem&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ssh-into-algo-server" class="anchor" aria-hidden="true" href="#ssh-into-algo-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SSH into Algo Server&lt;/h2&gt;
&lt;p&gt;Your Algo server is configured for key-only SSH access for administrative purposes. Open the Terminal app, &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;algo-master&lt;/code&gt; directory where you originally downloaded Algo, and then use the command listed on the success message:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh -i configs/algo.pem user@ip&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;user&lt;/code&gt; is either &lt;code&gt;root&lt;/code&gt; or &lt;code&gt;ubuntu&lt;/code&gt; as listed on the success message, and &lt;code&gt;ip&lt;/code&gt; is the IP address of your Algo server. If you find yourself regularly logging into the server then it will be useful to load your Algo ssh key automatically. Add the following snippet to the bottom of &lt;code&gt;~/.bash_profile&lt;/code&gt; to add it to your shell environment permanently.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh-add ~/.ssh/algo &amp;gt; /dev/null 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-adding-or-removing-users" class="anchor" aria-hidden="true" href="#adding-or-removing-users"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding or Removing Users&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;If you chose to save the CA key during the deploy process,&lt;/em&gt; then Algo's own scripts can easily add and remove users from the VPN server.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Update the &lt;code&gt;users&lt;/code&gt; list in your &lt;code&gt;config.cfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open a terminal, &lt;code&gt;cd&lt;/code&gt; to the algo directory, and activate the virtual environment with &lt;code&gt;source .env/bin/activate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run the command: &lt;code&gt;./algo update-users&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After this process completes, the Algo VPN server will contain only the users listed in the &lt;code&gt;config.cfg&lt;/code&gt; file.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-additional-documentation" class="anchor" aria-hidden="true" href="#additional-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional Documentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/index.md"&gt;Deployment instructions, cloud provider setup instructions, and further client setup instructions available here.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/faq.md"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you read all the documentation and have further questions, &lt;a href="https://gitter.im/trailofbits/algo" rel="nofollow"&gt;join the chat on Gitter&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-endorsements" class="anchor" aria-hidden="true" href="#endorsements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Endorsements&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;I've been ranting about the sorry state of VPN svcs for so long, probably about
time to give a proper talk on the subject. TL;DR: use Algo.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/kennwhite/status/814166603587788800" rel="nofollow"&gt;Kenn White&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Before picking a VPN provider/app, make sure you do some research
&lt;a href="https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf" rel="nofollow"&gt;https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf&lt;/a&gt; ... – or consider Algo&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/TheRegister/status/825076303657177088" rel="nofollow"&gt;The Register&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Algo is really easy and secure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/thegrugq/status/786249040228786176" rel="nofollow"&gt;the grugq&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I played around with Algo VPN, a set of scripts that let you set up a VPN in the cloud in very little time, even if you don’t know much about development. I’ve got to say that I was quite impressed with Trail of Bits’ approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/romaindillet/status/851037243728965632" rel="nofollow"&gt;Romain Dillet&lt;/a&gt; for &lt;a href="https://techcrunch.com/2017/04/09/how-i-made-my-own-vpn-server-in-15-minutes/" rel="nofollow"&gt;TechCrunch&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you’re uncomfortable shelling out the cash to an anonymous, random VPN provider, this is the best solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/kingthor" rel="nofollow"&gt;Thorin Klosowski&lt;/a&gt; for &lt;a href="http://lifehacker.com/how-to-set-up-your-own-completely-free-vpn-in-the-cloud-1794302432" rel="nofollow"&gt;Lifehacker&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-support-algo-vpn" class="anchor" aria-hidden="true" href="#support-algo-vpn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support Algo VPN&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://flattr.com/submit/auto?fid=kxw60j&amp;amp;url=https%3A%2F%2Fgithub.com%2Ftrailofbits%2Falgo" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6b27c2d051d09e13f4009938f0b67aedd4ffd280/68747470733a2f2f627574746f6e2e666c617474722e636f6d2f666c617474722d62616467652d6c617267652e706e67" alt="Flattr" data-canonical-src="https://button.flattr.com/flattr-badge-large.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=CYZZD39GXUJ3E" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e14c85b542e06215f7e56c0763333ef1e9b9f9b7/68747470733a2f2f7777772e70617970616c6f626a656374732e636f6d2f656e5f55532f692f62746e2f62746e5f646f6e6174655f534d2e676966" alt="PayPal" data-canonical-src="https://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.patreon.com/algovpn" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bf653361a158f7645497bf9490a97b697ec18f41/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6261636b5f6f6e2d70617472656f6e2d7265642e737667" alt="Patreon" data-canonical-src="https://img.shields.io/badge/back_on-patreon-red.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.bountysource.com/teams/trailofbits" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/79be76c663eb96142ff6f8d38ec10443107ffe97/68747470733a2f2f696d672e736869656c64732e696f2f626f756e7479736f757263652f7465616d2f747261696c6f66626974732f61637469766974792e737667" alt="Bountysource" data-canonical-src="https://img.shields.io/bountysource/team/trailofbits/activity.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All donations support continued development. Thanks!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We accept donations via &lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=CYZZD39GXUJ3E" rel="nofollow"&gt;PayPal&lt;/a&gt;, &lt;a href="https://www.patreon.com/algovpn" rel="nofollow"&gt;Patreon&lt;/a&gt;, and &lt;a href="https://flattr.com/submit/auto?fid=kxw60j&amp;amp;url=https%3A%2F%2Fgithub.com%2Ftrailofbits%2Falgo" rel="nofollow"&gt;Flattr&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Use our &lt;a href="https://m.do.co/c/4d7f4ff9cfe4" rel="nofollow"&gt;referral code&lt;/a&gt; when you sign up to Digital Ocean for a $10 credit.&lt;/li&gt;
&lt;li&gt;We also accept and appreciate contributions of new code and bugfixes via Github Pull Requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Algo is licensed and distributed under the AGPLv3. If you want to distribute a closed-source modification or service based on Algo, then please consider &lt;a href="mailto:opensource@trailofbits.com"&gt;purchasing an exception&lt;/a&gt; . As with the methods above, this will help support continued development.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>trailofbits</author><guid isPermaLink="false">https://github.com/trailofbits/algo</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>OUCMachineLearning/OUCML #13 in Python, This week</title><link>https://github.com/OUCMachineLearning/OUCML</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-oucml" class="anchor" aria-hidden="true" href="#oucml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OUCML&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/OUCMachineLearning/OUCML/tree/master/One%20Day%20One%20GAN"&gt;ODOG&lt;/a&gt;一天一GAN&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/OUCMachineLearning/OUCML/tree/master/GAN"&gt;GAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/OUCMachineLearning/OUCML/tree/master/AutoML"&gt;AUTOML&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大家编写README.MD文件的时候，可以参考&lt;a href="https://blog.csdn.net/liu537192/article/details/45693529" rel="nofollow"&gt;github上README.md文件如何编写&lt;/a&gt;。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>OUCMachineLearning</author><guid isPermaLink="false">https://github.com/OUCMachineLearning/OUCML</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>jet-admin/jet-bridge #14 in Python, This week</title><link>https://github.com/jet-admin/jet-bridge</link><description>&lt;p&gt;&lt;i&gt;Jet Bridge (Universal) for Jet Admin – API-based Admin Panel Framework for your application&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-jet-bridge--" class="anchor" aria-hidden="true" href="#jet-bridge--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://app.jetadmin.io/demo" rel="nofollow"&gt;Jet Bridge&lt;/a&gt;   &lt;a href="https://twitter.com/intent/tweet?text=Language%20agnostic%20Bridge%20for%20Jet%20%E2%80%93%20Back%20office%20totally%20ready%20to%20run%20your%20service&amp;amp;url=https://github.com/jet-admin/jet-bridge/&amp;amp;via=Jet_Admin&amp;amp;hashtags=admin,interface,backoffice,developers,jetadmin" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" alt="Tweet" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;for Jet Admin – Admin panel framework for your application&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/master/static/overview.gif"&gt;&lt;img src="https://raw.githubusercontent.com/jet-admin/jet-bridge/master/static/overview.gif" alt="Preview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;About Jet Admin: &lt;a href="https://about.jetadmin.io" rel="nofollow"&gt;https://about.jetadmin.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Live Demo&lt;/strong&gt;: &lt;a href="https://app.jetadmin.io/demo" rel="nofollow"&gt;https://app.jetadmin.io/demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Documentation: &lt;a href="https://docs.jetadmin.io/" rel="nofollow"&gt;https://docs.jetadmin.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support: &lt;a href="mailto:support@jetadmin.io"&gt;support@jetadmin.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Jet Admin&lt;/strong&gt; is a SaaS service that automatically generates extendable back office for your application. &lt;br&gt;
&lt;strong&gt;Jet Bridge&lt;/strong&gt; is a standalone app which generates REST API thought which your SQL database is connected to &lt;strong&gt;Jet Admin&lt;/strong&gt;. &lt;br&gt;
This project has been designed to fit requirements of small startups and mature companies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Privacy&lt;/strong&gt;. Jet does not access your data: its transferred directly from browser to your application.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Customizable Interface&lt;/strong&gt;. With WYSIWYG interface customization your can change almost every part of interface.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extendable&lt;/strong&gt;. Flex Features allows you to create your custom Actions, Views, Fields and other.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Works with any technology&lt;/strong&gt;. The interface is generated automatically based on an analysis of the data and data structure of your database.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quick installation&lt;/strong&gt;. All you need is to install Jet Bridge and connect it to your database.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a complete remake of our popular &lt;a href="https://github.com/geex-arts/django-jet"&gt;Django Jet&lt;/a&gt; admin interface.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CRUD (create, read, update, delete)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All common operations to view, create, update or delete data.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/list.jpeg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/list.jpeg" alt="CRUD (create, read, update, delete)" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Search and Filter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Filter data easily by any field with most common lookups and search them by text occurrence. For some specific cases you can create SQL Segment to filter with.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/filters.jpeg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/filters.jpeg" alt="Search and Filter" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Segments allow you to save applied set of filters as a Segment or create it from SQL query for quick use in future.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/segment.jpeg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/segment.jpeg" alt="Segments" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WYSIWYG Interface Customization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can customize almost every part of interface visually – navigation menu, collection list views, record create/update forms.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/customize.jpg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/customize.jpg" alt="WYSIWYG Interface Customization" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;List View layout&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A number of out-of-the-box list layouts except default Table View like Kanban Board and Map with markers.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/kanban.jpeg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/kanban.jpeg" alt="List View layout" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dashboards&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Create different types of charts, tables and other widgets to visualize your KPIs or monitor data without programming – inside your visual interface. Complex data queries can be created with SQL.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/dashboard.jpeg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/dashboard.jpeg" alt="Dashboards" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Teams and Permissions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Invite users to collaborate on a project and assign access rights based on their team.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/users.jpeg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/users.jpeg" alt="Teams and Permissions" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Export&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can export all collection data or part of it into the most common formats like CSV or Excel.&lt;/p&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/export.jpeg"&gt;&lt;img width="300px" src="https://raw.githubusercontent.com/jet-admin/jet-bridge/dev/static/export.jpeg" alt="Export" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Responsive Layout&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The interface is optimized for any device from phones to tablets to desktops.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-extendability" class="anchor" aria-hidden="true" href="#extendability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extendability&lt;/h1&gt;
&lt;p&gt;While we are trying to include most of important features out of the box sometimes its not enough. For any specific cases we offer Flex features to implement functionality not available with standard features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Custom Views&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For very specific pages you can create your own custom FlexView based on React, Angular or any other framework and integrate it in Jet Admin interface. Writing your own custom JS/CSS/HTML has no limits in implementing any page you need.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Custom Actions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If need to run some operations on records or any other business logic inside your Backend you can create FlexActions and run them directly from Jet Admin interface. Passing some additional parameters to your Backend is supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Custom Fields&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes using existing fields is not enough and you need to create custom which can be a combination of multiple fields, use fields from related collections and be result of some calculation. In this case you can use FlexField and write your custom JavaScript function which can format fields data any way you want.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-how-it-works" class="anchor" aria-hidden="true" href="#how-it-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How It Works&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Jet Admin&lt;/strong&gt; is a SaaS frontend application hosted on &lt;strong&gt;Jet Admin&lt;/strong&gt; side that works in your browser. It connects to your project SQL database through open source &lt;strong&gt;Jet Bridge&lt;/strong&gt; backend application which you install on your side. So Integrating &lt;strong&gt;Jet Admin&lt;/strong&gt; with your project requires installing only one component - &lt;strong&gt;Jet Bridge&lt;/strong&gt;. Here how it should look like after installation:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/dc8f70c3b0dad2e289945ce9aeda96b5b568d939/68747470733a2f2f7374617469632e74696c646163646e2e636f6d2f74696c64363233312d363533342d343636352d623033362d3339363333393336363236362f417274626f6172642e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/dc8f70c3b0dad2e289945ce9aeda96b5b568d939/68747470733a2f2f7374617469632e74696c646163646e2e636f6d2f74696c64363233312d363533342d343636352d623033362d3339363333393336363236362f417274626f6172642e706e67" alt="Jet Admin architecture" data-canonical-src="https://static.tildacdn.com/tild6231-6534-4665-b036-396339366266/Artboard.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your App&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Any of your applications which works with your &lt;strong&gt;Database&lt;/strong&gt;. &lt;strong&gt;Jet Admin&lt;/strong&gt; does not interact with it directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Database&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Your database &lt;strong&gt;Jet Admin&lt;/strong&gt; has no direct access to.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jet Bridge&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An open source application installed on your server's side and connected to your database. It automatically generates REST API based on your database structure. &lt;strong&gt;Jet Interface&lt;/strong&gt; works with &lt;strong&gt;Database&lt;/strong&gt; through &lt;strong&gt;Jet Bridge&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jet Interface&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Web application accessible from any browser. Maintaining and updating of this web application is on &lt;strong&gt;Jet Admin&lt;/strong&gt; team side. Your application data is transmitted directly from &lt;strong&gt;Jet Bridge&lt;/strong&gt; to &lt;strong&gt;Jet Interface&lt;/strong&gt; in your browser and remain invisible for the &lt;strong&gt;Jet Admin&lt;/strong&gt; service.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt; 2.7 or 3.4+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Any of the following &lt;strong&gt;SQL Databases&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;SQLite&lt;/li&gt;
&lt;li&gt;Oracle&lt;/li&gt;
&lt;li&gt;Microsoft SQL Server&lt;/li&gt;
&lt;li&gt;Firebird&lt;/li&gt;
&lt;li&gt;Sybase&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;In order to install Jet Admin on your project please follow this guide:&lt;br&gt;
&lt;a href="https://app.jetadmin.io/projects/create" rel="nofollow"&gt;https://app.jetadmin.io/projects/create&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you don't have &lt;strong&gt;Jet&lt;/strong&gt; account yet you will be asked to create one and sign in with the existing account.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;After registering your project you will be redirected to your project and can start working with &lt;strong&gt;Jet&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;p&gt;Feel free to Email us – &lt;a href="mailto:support@jetadmin.io"&gt;support@jetadmin.io&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;This project (Jet Bridge) is &lt;strong&gt;MIT&lt;/strong&gt; licensed - see the LICENCE file for details.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jet-admin</author><guid isPermaLink="false">https://github.com/jet-admin/jet-bridge</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>evilsocket/pwnagotchi #15 in Python, This week</title><link>https://github.com/evilsocket/pwnagotchi</link><description>&lt;p&gt;&lt;i&gt;(⌐■_■) - Deep Reinforcement Learning instrumenting bettercap for WiFi pwning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pwnagotchi" class="anchor" aria-hidden="true" href="#pwnagotchi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pwnagotchi&lt;/h1&gt;
&lt;p align="center"&gt;
    &lt;a href="https://github.com/evilsocket/pwnagotchi/releases/latest"&gt;&lt;img alt="Release" src="https://camo.githubusercontent.com/9ae707a55ead5a1e951d8504ebc47fe6f2259198/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6576696c736f636b65742f70776e61676f746368692e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/github/release/evilsocket/pwnagotchi.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://github.com/evilsocket/pwnagotchi/blob/master/LICENSE.md"&gt;&lt;img alt="Software License" src="https://camo.githubusercontent.com/268d96c6dd81f1fff98b19675ef5867412a2a223/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c332d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://github.com/evilsocket/pwnagotchi/graphs/contributors"&gt;&lt;img alt="Contributors" src="https://camo.githubusercontent.com/929754fc02f162895d1d3ac191ff93d5f0deefb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6576696c736f636b65742f70776e61676f74636869" data-canonical-src="https://img.shields.io/github/contributors/evilsocket/pwnagotchi" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://travis-ci.org/evilsocket/pwnagotchi" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/bb71ac99b3141520709ab3799968d900a4e7d193/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6576696c736f636b65742f70776e61676f746368692f6d61737465722e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/travis/evilsocket/pwnagotchi/master.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://invite.pwnagotchi.ai/" rel="nofollow"&gt;&lt;img alt="Slack" src="https://camo.githubusercontent.com/8b7d4aafb27072069011eadd46bdf045d3139533/68747470733a2f2f696e766974652e70776e61676f746368692e61692f62616467652e737667" data-canonical-src="https://invite.pwnagotchi.ai/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://twitter.com/intent/follow?screen_name=pwnagotchi" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/342b3854fb5df1b53672e419d06fbd99da5a4eb6/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f70776e61676f746368693f7374796c653d736f6369616c266c6f676f3d74776974746572" alt="follow on Twitter" data-canonical-src="https://img.shields.io/twitter/follow/pwnagotchi?style=social&amp;amp;logo=twitter" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pwnagotchi.ai/" rel="nofollow"&gt;Pwnagotchi&lt;/a&gt; is an &lt;a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="nofollow"&gt;A2C&lt;/a&gt;-based "AI" leveraging &lt;a href="https://www.bettercap.org/" rel="nofollow"&gt;bettercap&lt;/a&gt; that learns from its surrounding WiFi environment to maximize the crackable WPA key material it captures (either passively, or by performing authentication and association attacks). This material is collected as PCAP files containing any form of handshake supported by &lt;a href="https://hashcat.net/hashcat/" rel="nofollow"&gt;hashcat&lt;/a&gt;, including &lt;a href="https://www.evilsocket.net/2019/02/13/Pwning-WiFi-networks-with-bettercap-and-the-PMKID-client-less-attack/" rel="nofollow"&gt;PMKIDs&lt;/a&gt;,
full and half WPA handshakes.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e8bbbda8fbeb4da294be75c896026fc777c1a199/68747470733a2f2f692e696d6775722e636f6d2f5836384758726e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e8bbbda8fbeb4da294be75c896026fc777c1a199/68747470733a2f2f692e696d6775722e636f6d2f5836384758726e2e706e67" alt="ui" data-canonical-src="https://i.imgur.com/X68GXrn.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instead of merely playing &lt;a href="https://becominghuman.ai/getting-mario-back-into-the-gym-setting-up-super-mario-bros-in-openais-gym-8e39a96c1e41?gi=c4b66c3d5ced" rel="nofollow"&gt;Super Mario or Atari games&lt;/a&gt; like most reinforcement learning-based "AI" &lt;em&gt;(yawn)&lt;/em&gt;, Pwnagotchi tunes &lt;a href="https://github.com/evilsocket/pwnagotchi/blob/master/pwnagotchi/defaults.yml#L73"&gt;its parameters&lt;/a&gt; over time to &lt;strong&gt;get better at pwning WiFi things to&lt;/strong&gt; in the environments you expose it to.&lt;/p&gt;
&lt;p&gt;More specifically, Pwnagotchi is using an &lt;a href="https://stable-baselines.readthedocs.io/en/master/modules/policies.html#stable_baselines.common.policies.MlpLstmPolicy" rel="nofollow"&gt;LSTM with MLP feature extractor&lt;/a&gt; as its policy network for the &lt;a href="https://stable-baselines.readthedocs.io/en/master/modules/a2c.html" rel="nofollow"&gt;A2C agent&lt;/a&gt;. If you're unfamiliar with A2C, here is &lt;a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="nofollow"&gt;a very good introductory explanation&lt;/a&gt; (in comic form!) of the basic principles behind how Pwnagotchi learns. (You can read more about how Pwnagotchi learns in the &lt;a href="https://www.pwnagotchi.ai/usage/#training-the-ai" rel="nofollow"&gt;Usage&lt;/a&gt; doc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keep in mind:&lt;/strong&gt; Unlike the usual RL simulations, Pwnagotchi learns over time. Time for a Pwnagotchi is measured in epochs; a single epoch can last from a few seconds to minutes, depending on how many access points and client stations are visible. Do not expect your Pwnagotchi to perform amazingly well at the very beginning, as it will be &lt;a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="nofollow"&gt;exploring&lt;/a&gt; several combinations of &lt;a href="https://www.pwnagotchi.ai/usage/#training-the-ai" rel="nofollow"&gt;key parameters&lt;/a&gt; to determine ideal adjustments for pwning the particular environment you are exposing it to during its beginning epochs ... but ** listen to your Pwnagotchi when it tells you it's boring!** Bring it into novel WiFi environments with you and have it observe new networks and capture new handshakes—and you'll see. :)&lt;/p&gt;
&lt;p&gt;Multiple units within close physical proximity can "talk" to each other, advertising their presence to each other by broadcasting custom information elements using a parasite protocol I've built on top of the existing dot11 standard. Over time, two or more units trained together will learn to cooperate upon detecting each other's presence by dividing the available channels among them for optimal pwnage.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.pwnagotchi.ai" rel="nofollow"&gt;https://www.pwnagotchi.ai&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; &lt;/th&gt;
&lt;th&gt;Official Links&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Slack&lt;/td&gt;
&lt;td&gt;&lt;a href="https://invite.pwnagotchi.ai/" rel="nofollow"&gt;pwnagotchi.slack.com&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Twitter&lt;/td&gt;
&lt;td&gt;&lt;a href="https://twitter.com/pwnagotchi" rel="nofollow"&gt;@pwnagotchi&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Subreddit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.reddit.com/r/pwnagotchi/" rel="nofollow"&gt;r/pwnagotchi&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Website&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pwnagotchi.ai/" rel="nofollow"&gt;pwnagotchi.ai&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;pwnagotchi&lt;/code&gt; is made with ♥  by &lt;a href="https://twitter.com/evilsocket" rel="nofollow"&gt;@evilsocket&lt;/a&gt; and the &lt;a href="https://github.com/evilsocket/pwnagotchi/graphs/contributors"&gt;amazing dev team&lt;/a&gt;. It is released under the GPL3 license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>evilsocket</author><guid isPermaLink="false">https://github.com/evilsocket/pwnagotchi</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>swisskyrepo/PayloadsAllTheThings #16 in Python, This week</title><link>https://github.com/swisskyrepo/PayloadsAllTheThings</link><description>&lt;p&gt;&lt;i&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-payloads-all-the-things" class="anchor" aria-hidden="true" href="#payloads-all-the-things"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Payloads All The Things&lt;/h1&gt;
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I &lt;g-emoji class="g-emoji" alias="heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png"&gt;❤️&lt;/g-emoji&gt; pull requests :)&lt;/p&gt;
&lt;p&gt;You can also contribute with a &lt;g-emoji class="g-emoji" alias="beers" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37b.png"&gt;🍻&lt;/g-emoji&gt; IRL&lt;/p&gt;
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;README.md - vulnerability description and how to exploit it&lt;/li&gt;
&lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt;
&lt;li&gt;Images - pictures for the README.md&lt;/li&gt;
&lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/"&gt;Methodology and Resources&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md"&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md"&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md"&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md"&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md"&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md"&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Discovery.md"&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md"&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md"&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md"&gt;Windows - Download and Execute.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Mimikatz.md"&gt;Windows - Mimikatz.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Persistence.md"&gt;Windows - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Post%20Exploitation%20Koadic.md"&gt;Windows - Post Exploitation Koadic.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Privilege%20Escalation.md"&gt;Windows - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Using%20credentials.md"&gt;Windows - Using credentials.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CVE%20Exploits"&gt;CVE Exploits&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Struts 2 CVE-2013-2251 CVE-2017-5638 CVE-2018-11776_.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2017-9805.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2018-11776.py&lt;/li&gt;
&lt;li&gt;Docker API RCE.py&lt;/li&gt;
&lt;li&gt;Drupalgeddon2 CVE-2018-7600.rb&lt;/li&gt;
&lt;li&gt;Heartbleed CVE-2014-0160.py&lt;/li&gt;
&lt;li&gt;JBoss CVE-2015-7501.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2015-8103.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2016-0792.py&lt;/li&gt;
&lt;li&gt;Rails CVE-2019-5420.rb&lt;/li&gt;
&lt;li&gt;Shellshock CVE-2014-6271.py&lt;/li&gt;
&lt;li&gt;Tomcat CVE-2017-12617.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2016-3510.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2017-10271.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2018-2894.py&lt;/li&gt;
&lt;li&gt;WebSphere CVE-2015-7450.py&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/YOUTUBE.md"&gt;Youtube videos&lt;/a&gt; selections.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>swisskyrepo</author><guid isPermaLink="false">https://github.com/swisskyrepo/PayloadsAllTheThings</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>sebastianruder/NLP-progress #17 in Python, This week</title><link>https://github.com/sebastianruder/NLP-progress</link><description>&lt;p&gt;&lt;i&gt;Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tracking-progress-in-natural-language-processing" class="anchor" aria-hidden="true" href="#tracking-progress-in-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tracking Progress in Natural Language Processing&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-english" class="anchor" aria-hidden="true" href="#english"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;English&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="english/automatic_speech_recognition.md"&gt;Automatic speech recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/ccg.md"&gt;CCG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/common_sense.md"&gt;Common sense&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/constituency_parsing.md"&gt;Constituency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/coreference_resolution.md"&gt;Coreference resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/dependency_parsing.md"&gt;Dependency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/dialogue.md"&gt;Dialogue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/domain_adaptation.md"&gt;Domain adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/entity_linking.md"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/grammatical_error_correction.md"&gt;Grammatical error correction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/information_extraction.md"&gt;Information extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/language_modeling.md"&gt;Language modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/lexical_normalization.md"&gt;Lexical normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/machine_translation.md"&gt;Machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/missing_elements.md"&gt;Missing elements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/multi-task_learning.md"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/multimodal.md"&gt;Multi-modal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/named_entity_recognition.md"&gt;Named entity recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/natural_language_inference.md"&gt;Natural language inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/part-of-speech_tagging.md"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/question_answering.md"&gt;Question answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/relation_prediction.md"&gt;Relation prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/relationship_extraction.md"&gt;Relationship extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_textual_similarity.md"&gt;Semantic textual similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_parsing.md"&gt;Semantic parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_role_labeling.md"&gt;Semantic role labeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/sentiment_analysis.md"&gt;Sentiment analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/shallow_syntax.md"&gt;Shallow syntax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/simplification.md"&gt;Simplification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/stance_detection.md"&gt;Stance detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/summarization.md"&gt;Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/taxonomy_learning.md"&gt;Taxonomy learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/temporal_processing.md"&gt;Temporal processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/text_classification.md"&gt;Text classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/word_sense_disambiguation.md"&gt;Word sense disambiguation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-chinese" class="anchor" aria-hidden="true" href="#chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chinese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chinese/chinese.md#entity-linking"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chinese/chinese_word_segmentation.md"&gt;Chinese word segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-hindi" class="anchor" aria-hidden="true" href="#hindi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hindi&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#chunking"&gt;Chunking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#part-of-speech-tagging"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#machine-translation"&gt;Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-vietnamese" class="anchor" aria-hidden="true" href="#vietnamese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vietnamese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#dependency-parsing"&gt;Dependency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#machine-translation"&gt;Machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#named-entity-recognition"&gt;Named entity recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#part-of-speech-tagging"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#word-segmentation"&gt;Word segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-spanish" class="anchor" aria-hidden="true" href="#spanish"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spanish&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="spanish/entity_linking.md#entity-linking"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-portuguese" class="anchor" aria-hidden="true" href="#portuguese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Portuguese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="portuguese/question_answering.md"&gt;Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This document aims to track the progress in Natural Language Processing (NLP) and give an overview
of the state-of-the-art (SOTA) across the most common NLP tasks and their corresponding datasets.&lt;/p&gt;
&lt;p&gt;It aims to cover both traditional and core NLP tasks such as dependency parsing and part-of-speech tagging
as well as more recent ones such as reading comprehension and natural language inference. The main objective
is to provide the reader with a quick overview of benchmark datasets and the state-of-the-art for their
task of interest, which serves as a stepping stone for further research. To this end, if there is a
place where results for a task are already published and regularly maintained, such as a public leaderboard,
the reader will be pointed there.&lt;/p&gt;
&lt;p&gt;If you want to find this document again in the future, just go to &lt;a href="https://nlpprogress.com/" rel="nofollow"&gt;&lt;code&gt;nlpprogress.com&lt;/code&gt;&lt;/a&gt;
or &lt;a href="http://nlpsota.com/" rel="nofollow"&gt;&lt;code&gt;nlpsota.com&lt;/code&gt;&lt;/a&gt; in your browser.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-guidelines" class="anchor" aria-hidden="true" href="#guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Guidelines&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;   Results reported in published papers are preferred; an exception may be made for influential preprints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Datasets&lt;/strong&gt;   Datasets should have been used for evaluation in at least one published paper besides
the one that introduced the dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;   We recommend to add a link to an implementation
if available. You can add a &lt;code&gt;Code&lt;/code&gt; column (see below) to the table if it does not exist.
In the &lt;code&gt;Code&lt;/code&gt; column, indicate an official implementation with &lt;a href="http://link_to_implementation" rel="nofollow"&gt;Official&lt;/a&gt;.
If an unofficial implementation is available, use &lt;a href="http://link_to_implementation" rel="nofollow"&gt;Link&lt;/a&gt; (see below).
If no implementation is available, you can leave the cell empty.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-adding-a-new-result" class="anchor" aria-hidden="true" href="#adding-a-new-result"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding a new result&lt;/h4&gt;
&lt;p&gt;If you would like to add a new result, you can just click on the small edit button in the top-right
corner of the file for the respective task (see below).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/edit_file.png"&gt;&lt;img src="img/edit_file.png" alt="Click on the edit button to add a file" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This allows you to edit the file in Markdown. Simply add a row to the corresponding table in the
same format. Make sure that the table stays sorted (with the best result on top).
After you've made your change, make sure that the table still looks ok by clicking on the
"Preview changes" tab at the top of the page. If everything looks good, go to the bottom of the page,
where you see the below form.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/propose_file_change.png"&gt;&lt;img src="img/propose_file_change.png" alt="Fill out the file change information" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Add a name for your proposed change, an optional description, indicate that you would like to
"Create a new branch for this commit and start a pull request", and click on "Propose file change".&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-adding-a-new-dataset-or-task" class="anchor" aria-hidden="true" href="#adding-a-new-dataset-or-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding a new dataset or task&lt;/h4&gt;
&lt;p&gt;For adding a new dataset or task, you can also follow the steps above. Alternatively, you can fork the repository.
In both cases, follow the steps below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If your task is completely new, create a new file and link to it in the table of contents above.&lt;/li&gt;
&lt;li&gt;If not, add your task or dataset to the respective section of the corresponding file (in alphabetical order).&lt;/li&gt;
&lt;li&gt;Briefly describe the dataset/task and include relevant references.&lt;/li&gt;
&lt;li&gt;Describe the evaluation setting and evaluation metric.&lt;/li&gt;
&lt;li&gt;Show how an annotated example of the dataset/task looks like.&lt;/li&gt;
&lt;li&gt;Add a download link if available.&lt;/li&gt;
&lt;li&gt;Copy the below table and fill in at least two results (including the state-of-the-art)
for your dataset/task (change Score to the metric of your dataset). If your dataset/task
has multiple metrics, add them to the right of &lt;code&gt;Score&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Submit your change as a pull request.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Score&lt;/th&gt;
&lt;th&gt;Paper / Source&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-wish-list" class="anchor" aria-hidden="true" href="#wish-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wish list&lt;/h3&gt;
&lt;p&gt;These are tasks and datasets that are still missing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bilingual dictionary induction&lt;/li&gt;
&lt;li&gt;Discourse parsing&lt;/li&gt;
&lt;li&gt;Keyphrase extraction&lt;/li&gt;
&lt;li&gt;Knowledge base population (KBP)&lt;/li&gt;
&lt;li&gt;More dialogue tasks&lt;/li&gt;
&lt;li&gt;Semi-supervised learning&lt;/li&gt;
&lt;li&gt;Frame-semantic parsing (FrameNet full-sentence analysis)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-exporting-into-a-structured-format" class="anchor" aria-hidden="true" href="#exporting-into-a-structured-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Exporting into a structured format&lt;/h3&gt;
&lt;p&gt;You can extract all the data into a structured, machine-readable JSON format with parsed tasks, descriptions and SOTA tables.&lt;/p&gt;
&lt;p&gt;The instructions are in &lt;a href="structured/README.md"&gt;structured/README.md&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-instructions-for-building-the-site-locally" class="anchor" aria-hidden="true" href="#instructions-for-building-the-site-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Instructions for building the site locally&lt;/h3&gt;
&lt;p&gt;Instructions for building the website locally using Jekyll can be found &lt;a href="jekyll_instructions.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sebastianruder</author><guid isPermaLink="false">https://github.com/sebastianruder/NLP-progress</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>ytdl-org/youtube-dl #18 in Python, This week</title><link>https://github.com/ytdl-org/youtube-dl</link><description>&lt;p&gt;&lt;i&gt;Command-line program to download videos from YouTube.com and other video sites&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/ytdl-org/youtube-dl" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/410a0334f73fc86f47605f8d7c7c581f0202e1be/68747470733a2f2f7472617669732d63692e6f72672f7974646c2d6f72672f796f75747562652d646c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/ytdl-org/youtube-dl.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;youtube-dl - download videos from youtube.com or other video platforms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;INSTALLATION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#description"&gt;DESCRIPTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#options"&gt;OPTIONS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#configuration"&gt;CONFIGURATION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#output-template"&gt;OUTPUT TEMPLATE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#format-selection"&gt;FORMAT SELECTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video-selection"&gt;VIDEO SELECTION&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#developer-instructions"&gt;DEVELOPER INSTRUCTIONS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#embedding-youtube-dl"&gt;EMBEDDING YOUTUBE-DL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bugs"&gt;BUGS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#copyright"&gt;COPYRIGHT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;INSTALLATION&lt;/h1&gt;
&lt;p&gt;To install it right away for all UNIX users (Linux, macOS, etc.), type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not have curl, you can alternatively use a recent wget:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Windows users can &lt;a href="https://yt-dl.org/latest/youtube-dl.exe" rel="nofollow"&gt;download an .exe file&lt;/a&gt; and place it in any location on their &lt;a href="https://en.wikipedia.org/wiki/PATH_%28variable%29" rel="nofollow"&gt;PATH&lt;/a&gt; except for &lt;code&gt;%SYSTEMROOT%\System32&lt;/code&gt; (e.g. &lt;strong&gt;do not&lt;/strong&gt; put in &lt;code&gt;C:\Windows\System32&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;You can also use pip:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo -H pip install --upgrade youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will update youtube-dl if you have already installed it. See the &lt;a href="https://pypi.python.org/pypi/youtube_dl" rel="nofollow"&gt;pypi page&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;macOS users can install youtube-dl with &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or with &lt;a href="https://www.macports.org/" rel="nofollow"&gt;MacPorts&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo port install youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, refer to the &lt;a href="#developer-instructions"&gt;developer instructions&lt;/a&gt; for how to check out and work with the git repository. For further options, including PGP signatures, see the &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;youtube-dl Download Page&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DESCRIPTION&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;youtube-dl&lt;/strong&gt; is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl [OPTIONS] URL [URL...]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-options" class="anchor" aria-hidden="true" href="#options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OPTIONS&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;-h, --help                       Print this help text and exit
--version                        Print program version and exit
-U, --update                     Update this program to latest version. Make
                                 sure that you have sufficient permissions
                                 (run with sudo if needed)
-i, --ignore-errors              Continue on download errors, for example to
                                 skip unavailable videos in a playlist
--abort-on-error                 Abort downloading of further videos (in the
                                 playlist or the command line) if an error
                                 occurs
--dump-user-agent                Display the current browser identification
--list-extractors                List all supported extractors
--extractor-descriptions         Output descriptions of all supported
                                 extractors
--force-generic-extractor        Force extraction to use the generic
                                 extractor
--default-search PREFIX          Use this prefix for unqualified URLs. For
                                 example "gvsearch2:" downloads two videos
                                 from google videos for youtube-dl "large
                                 apple". Use the value "auto" to let
                                 youtube-dl guess ("auto_warning" to emit a
                                 warning when guessing). "error" just throws
                                 an error. The default value "fixup_error"
                                 repairs broken URLs, but emits an error if
                                 this is not possible instead of searching.
--ignore-config                  Do not read configuration files. When given
                                 in the global configuration file
                                 /etc/youtube-dl.conf: Do not read the user
                                 configuration in ~/.config/youtube-
                                 dl/config (%APPDATA%/youtube-dl/config.txt
                                 on Windows)
--config-location PATH           Location of the configuration file; either
                                 the path to the config or its containing
                                 directory.
--flat-playlist                  Do not extract the videos of a playlist,
                                 only list them.
--mark-watched                   Mark videos watched (YouTube only)
--no-mark-watched                Do not mark videos watched (YouTube only)
--no-color                       Do not emit color codes in output
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-network-options" class="anchor" aria-hidden="true" href="#network-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--proxy URL                      Use the specified HTTP/HTTPS/SOCKS proxy.
                                 To enable SOCKS proxy, specify a proper
                                 scheme. For example
                                 socks5://127.0.0.1:1080/. Pass in an empty
                                 string (--proxy "") for direct connection
--socket-timeout SECONDS         Time to wait before giving up, in seconds
--source-address IP              Client-side IP address to bind to
-4, --force-ipv4                 Make all connections via IPv4
-6, --force-ipv6                 Make all connections via IPv6
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-geo-restriction" class="anchor" aria-hidden="true" href="#geo-restriction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Geo Restriction:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--geo-verification-proxy URL     Use this proxy to verify the IP address for
                                 some geo-restricted sites. The default
                                 proxy specified by --proxy (or none, if the
                                 option is not present) is used for the
                                 actual downloading.
--geo-bypass                     Bypass geographic restriction via faking
                                 X-Forwarded-For HTTP header
--no-geo-bypass                  Do not bypass geographic restriction via
                                 faking X-Forwarded-For HTTP header
--geo-bypass-country CODE        Force bypass geographic restriction with
                                 explicitly provided two-letter ISO 3166-2
                                 country code
--geo-bypass-ip-block IP_BLOCK   Force bypass geographic restriction with
                                 explicitly provided IP block in CIDR
                                 notation
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-video-selection" class="anchor" aria-hidden="true" href="#video-selection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Selection:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--playlist-start NUMBER          Playlist video to start at (default is 1)
--playlist-end NUMBER            Playlist video to end at (default is last)
--playlist-items ITEM_SPEC       Playlist video items to download. Specify
                                 indices of the videos in the playlist
                                 separated by commas like: "--playlist-items
                                 1,2,5,8" if you want to download videos
                                 indexed 1, 2, 5, 8 in the playlist. You can
                                 specify range: "--playlist-items
                                 1-3,7,10-13", it will download the videos
                                 at index 1, 2, 3, 7, 10, 11, 12 and 13.
--match-title REGEX              Download only matching titles (regex or
                                 caseless sub-string)
--reject-title REGEX             Skip download for matching titles (regex or
                                 caseless sub-string)
--max-downloads NUMBER           Abort after downloading NUMBER files
--min-filesize SIZE              Do not download any videos smaller than
                                 SIZE (e.g. 50k or 44.6m)
--max-filesize SIZE              Do not download any videos larger than SIZE
                                 (e.g. 50k or 44.6m)
--date DATE                      Download only videos uploaded in this date
--datebefore DATE                Download only videos uploaded on or before
                                 this date (i.e. inclusive)
--dateafter DATE                 Download only videos uploaded on or after
                                 this date (i.e. inclusive)
--min-views COUNT                Do not download any videos with less than
                                 COUNT views
--max-views COUNT                Do not download any videos with more than
                                 COUNT views
--match-filter FILTER            Generic video filter. Specify any key (see
                                 the "OUTPUT TEMPLATE" for a list of
                                 available keys) to match if the key is
                                 present, !key to check if the key is not
                                 present, key &amp;gt; NUMBER (like "comment_count
                                 &amp;gt; 12", also works with &amp;gt;=, &amp;lt;, &amp;lt;=, !=, =) to
                                 compare against a number, key = 'LITERAL'
                                 (like "uploader = 'Mike Smith'", also works
                                 with !=) to match against a string literal
                                 and &amp;amp; to require multiple matches. Values
                                 which are not known are excluded unless you
                                 put a question mark (?) after the operator.
                                 For example, to only match videos that have
                                 been liked more than 100 times and disliked
                                 less than 50 times (or the dislike
                                 functionality is not available at the given
                                 service), but who also have a description,
                                 use --match-filter "like_count &amp;gt; 100 &amp;amp;
                                 dislike_count &amp;lt;? 50 &amp;amp; description" .
--no-playlist                    Download only the video, if the URL refers
                                 to a video and a playlist.
--yes-playlist                   Download the playlist, if the URL refers to
                                 a video and a playlist.
--age-limit YEARS                Download only videos suitable for the given
                                 age
--download-archive FILE          Download only videos not listed in the
                                 archive file. Record the IDs of all
                                 downloaded videos in it.
--include-ads                    Download advertisements as well
                                 (experimental)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-download-options" class="anchor" aria-hidden="true" href="#download-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-r, --limit-rate RATE            Maximum download rate in bytes per second
                                 (e.g. 50K or 4.2M)
-R, --retries RETRIES            Number of retries (default is 10), or
                                 "infinite".
--fragment-retries RETRIES       Number of retries for a fragment (default
                                 is 10), or "infinite" (DASH, hlsnative and
                                 ISM)
--skip-unavailable-fragments     Skip unavailable fragments (DASH, hlsnative
                                 and ISM)
--abort-on-unavailable-fragment  Abort downloading when some fragment is not
                                 available
--keep-fragments                 Keep downloaded fragments on disk after
                                 downloading is finished; fragments are
                                 erased by default
--buffer-size SIZE               Size of download buffer (e.g. 1024 or 16K)
                                 (default is 1024)
--no-resize-buffer               Do not automatically adjust the buffer
                                 size. By default, the buffer size is
                                 automatically resized from an initial value
                                 of SIZE.
--http-chunk-size SIZE           Size of a chunk for chunk-based HTTP
                                 downloading (e.g. 10485760 or 10M) (default
                                 is disabled). May be useful for bypassing
                                 bandwidth throttling imposed by a webserver
                                 (experimental)
--playlist-reverse               Download playlist videos in reverse order
--playlist-random                Download playlist videos in random order
--xattr-set-filesize             Set file xattribute ytdl.filesize with
                                 expected file size
--hls-prefer-native              Use the native HLS downloader instead of
                                 ffmpeg
--hls-prefer-ffmpeg              Use ffmpeg instead of the native HLS
                                 downloader
--hls-use-mpegts                 Use the mpegts container for HLS videos,
                                 allowing to play the video while
                                 downloading (some players may not be able
                                 to play it)
--external-downloader COMMAND    Use the specified external downloader.
                                 Currently supports
                                 aria2c,avconv,axel,curl,ffmpeg,httpie,wget
--external-downloader-args ARGS  Give these arguments to the external
                                 downloader
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-filesystem-options" class="anchor" aria-hidden="true" href="#filesystem-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Filesystem Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-a, --batch-file FILE            File containing URLs to download ('-' for
                                 stdin), one URL per line. Lines starting
                                 with '#', ';' or ']' are considered as
                                 comments and ignored.
--id                             Use only video ID in file name
-o, --output TEMPLATE            Output filename template, see the "OUTPUT
                                 TEMPLATE" for all the info
--autonumber-start NUMBER        Specify the start value for %(autonumber)s
                                 (default is 1)
--restrict-filenames             Restrict filenames to only ASCII
                                 characters, and avoid "&amp;amp;" and spaces in
                                 filenames
-w, --no-overwrites              Do not overwrite files
-c, --continue                   Force resume of partially downloaded files.
                                 By default, youtube-dl will resume
                                 downloads if possible.
--no-continue                    Do not resume partially downloaded files
                                 (restart from beginning)
--no-part                        Do not use .part files - write directly
                                 into output file
--no-mtime                       Do not use the Last-modified header to set
                                 the file modification time
--write-description              Write video description to a .description
                                 file
--write-info-json                Write video metadata to a .info.json file
--write-annotations              Write video annotations to a
                                 .annotations.xml file
--load-info-json FILE            JSON file containing the video information
                                 (created with the "--write-info-json"
                                 option)
--cookies FILE                   File to read cookies from and dump cookie
                                 jar in
--cache-dir DIR                  Location in the filesystem where youtube-dl
                                 can store some downloaded information
                                 permanently. By default
                                 $XDG_CACHE_HOME/youtube-dl or
                                 ~/.cache/youtube-dl . At the moment, only
                                 YouTube player files (for videos with
                                 obfuscated signatures) are cached, but that
                                 may change.
--no-cache-dir                   Disable filesystem caching
--rm-cache-dir                   Delete all filesystem cache files
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-thumbnail-images" class="anchor" aria-hidden="true" href="#thumbnail-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Thumbnail images:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--write-thumbnail                Write thumbnail image to disk
--write-all-thumbnails           Write all thumbnail image formats to disk
--list-thumbnails                Simulate and list all available thumbnail
                                 formats
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-verbosity--simulation-options" class="anchor" aria-hidden="true" href="#verbosity--simulation-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Verbosity / Simulation Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-q, --quiet                      Activate quiet mode
--no-warnings                    Ignore warnings
-s, --simulate                   Do not download the video and do not write
                                 anything to disk
--skip-download                  Do not download the video
-g, --get-url                    Simulate, quiet but print URL
-e, --get-title                  Simulate, quiet but print title
--get-id                         Simulate, quiet but print id
--get-thumbnail                  Simulate, quiet but print thumbnail URL
--get-description                Simulate, quiet but print video description
--get-duration                   Simulate, quiet but print video length
--get-filename                   Simulate, quiet but print output filename
--get-format                     Simulate, quiet but print output format
-j, --dump-json                  Simulate, quiet but print JSON information.
                                 See the "OUTPUT TEMPLATE" for a description
                                 of available keys.
-J, --dump-single-json           Simulate, quiet but print JSON information
                                 for each command-line argument. If the URL
                                 refers to a playlist, dump the whole
                                 playlist information in a single line.
--print-json                     Be quiet and print the video information as
                                 JSON (video is still being downloaded).
--newline                        Output progress bar as new lines
--no-progress                    Do not print progress bar
--console-title                  Display progress in console titlebar
-v, --verbose                    Print various debugging information
--dump-pages                     Print downloaded pages encoded using base64
                                 to debug problems (very verbose)
--write-pages                    Write downloaded intermediary pages to
                                 files in the current directory to debug
                                 problems
--print-traffic                  Display sent and read HTTP traffic
-C, --call-home                  Contact the youtube-dl server for debugging
--no-call-home                   Do NOT contact the youtube-dl server for
                                 debugging
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-workarounds" class="anchor" aria-hidden="true" href="#workarounds"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Workarounds:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--encoding ENCODING              Force the specified encoding (experimental)
--no-check-certificate           Suppress HTTPS certificate validation
--prefer-insecure                Use an unencrypted connection to retrieve
                                 information about the video. (Currently
                                 supported only for YouTube)
--user-agent UA                  Specify a custom user agent
--referer URL                    Specify a custom referer, use if the video
                                 access is restricted to one domain
--add-header FIELD:VALUE         Specify a custom HTTP header and its value,
                                 separated by a colon ':'. You can use this
                                 option multiple times
--bidi-workaround                Work around terminals that lack
                                 bidirectional text support. Requires bidiv
                                 or fribidi executable in PATH
--sleep-interval SECONDS         Number of seconds to sleep before each
                                 download when used alone or a lower bound
                                 of a range for randomized sleep before each
                                 download (minimum possible number of
                                 seconds to sleep) when used along with
                                 --max-sleep-interval.
--max-sleep-interval SECONDS     Upper bound of a range for randomized sleep
                                 before each download (maximum possible
                                 number of seconds to sleep). Must only be
                                 used along with --min-sleep-interval.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-video-format-options" class="anchor" aria-hidden="true" href="#video-format-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Format Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-f, --format FORMAT              Video format code, see the "FORMAT
                                 SELECTION" for all the info
--all-formats                    Download all available video formats
--prefer-free-formats            Prefer free video formats unless a specific
                                 one is requested
-F, --list-formats               List all available formats of requested
                                 videos
--youtube-skip-dash-manifest     Do not download the DASH manifests and
                                 related data on YouTube videos
--merge-output-format FORMAT     If a merge is required (e.g.
                                 bestvideo+bestaudio), output to given
                                 container format. One of mkv, mp4, ogg,
                                 webm, flv. Ignored if no merge is required
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-subtitle-options" class="anchor" aria-hidden="true" href="#subtitle-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Subtitle Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--write-sub                      Write subtitle file
--write-auto-sub                 Write automatically generated subtitle file
                                 (YouTube only)
--all-subs                       Download all the available subtitles of the
                                 video
--list-subs                      List all available subtitles for the video
--sub-format FORMAT              Subtitle format, accepts formats
                                 preference, for example: "srt" or
                                 "ass/srt/best"
--sub-lang LANGS                 Languages of the subtitles to download
                                 (optional) separated by commas, use --list-
                                 subs for available language tags
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-authentication-options" class="anchor" aria-hidden="true" href="#authentication-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authentication Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-u, --username USERNAME          Login with this account ID
-p, --password PASSWORD          Account password. If this option is left
                                 out, youtube-dl will ask interactively.
-2, --twofactor TWOFACTOR        Two-factor authentication code
-n, --netrc                      Use .netrc authentication data
--video-password PASSWORD        Video password (vimeo, smotri, youku)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-adobe-pass-options" class="anchor" aria-hidden="true" href="#adobe-pass-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adobe Pass Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;--ap-mso MSO                     Adobe Pass multiple-system operator (TV
                                 provider) identifier, use --ap-list-mso for
                                 a list of available MSOs
--ap-username USERNAME           Multiple-system operator account login
--ap-password PASSWORD           Multiple-system operator account password.
                                 If this option is left out, youtube-dl will
                                 ask interactively.
--ap-list-mso                    List all supported multiple-system
                                 operators
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-post-processing-options" class="anchor" aria-hidden="true" href="#post-processing-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Post-processing Options:&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;-x, --extract-audio              Convert video files to audio-only files
                                 (requires ffmpeg or avconv and ffprobe or
                                 avprobe)
--audio-format FORMAT            Specify audio format: "best", "aac",
                                 "flac", "mp3", "m4a", "opus", "vorbis", or
                                 "wav"; "best" by default; No effect without
                                 -x
--audio-quality QUALITY          Specify ffmpeg/avconv audio quality, insert
                                 a value between 0 (better) and 9 (worse)
                                 for VBR or a specific bitrate like 128K
                                 (default 5)
--recode-video FORMAT            Encode the video to another format if
                                 necessary (currently supported:
                                 mp4|flv|ogg|webm|mkv|avi)
--postprocessor-args ARGS        Give these arguments to the postprocessor
-k, --keep-video                 Keep the video file on disk after the post-
                                 processing; the video is erased by default
--no-post-overwrites             Do not overwrite post-processed files; the
                                 post-processed files are overwritten by
                                 default
--embed-subs                     Embed subtitles in the video (only for mp4,
                                 webm and mkv videos)
--embed-thumbnail                Embed thumbnail in the audio as cover art
--add-metadata                   Write metadata to the video file
--metadata-from-title FORMAT     Parse additional metadata like song title /
                                 artist from the video title. The format
                                 syntax is the same as --output. Regular
                                 expression with named capture groups may
                                 also be used. The parsed parameters replace
                                 existing values. Example: --metadata-from-
                                 title "%(artist)s - %(title)s" matches a
                                 title like "Coldplay - Paradise". Example
                                 (regex): --metadata-from-title
                                 "(?P&amp;lt;artist&amp;gt;.+?) - (?P&amp;lt;title&amp;gt;.+)"
--xattrs                         Write metadata to the video file's xattrs
                                 (using dublin core and xdg standards)
--fixup POLICY                   Automatically correct known faults of the
                                 file. One of never (do nothing), warn (only
                                 emit a warning), detect_or_warn (the
                                 default; fix file if we can, warn
                                 otherwise)
--prefer-avconv                  Prefer avconv over ffmpeg for running the
                                 postprocessors
--prefer-ffmpeg                  Prefer ffmpeg over avconv for running the
                                 postprocessors (default)
--ffmpeg-location PATH           Location of the ffmpeg/avconv binary;
                                 either the path to the binary or its
                                 containing directory.
--exec CMD                       Execute a command on the file after
                                 downloading, similar to find's -exec
                                 syntax. Example: --exec 'adb push {}
                                 /sdcard/Music/ &amp;amp;&amp;amp; rm {}'
--convert-subs FORMAT            Convert the subtitles to other format
                                 (currently supported: srt|ass|vtt|lrc)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CONFIGURATION&lt;/h1&gt;
&lt;p&gt;You can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at &lt;code&gt;/etc/youtube-dl.conf&lt;/code&gt; and the user wide configuration file at &lt;code&gt;~/.config/youtube-dl/config&lt;/code&gt;. On Windows, the user wide configuration file locations are &lt;code&gt;%APPDATA%\youtube-dl\config.txt&lt;/code&gt; or &lt;code&gt;C:\Users\&amp;lt;user name&amp;gt;\youtube-dl.conf&lt;/code&gt;. Note that by default configuration file may not exist so you may need to create it yourself.&lt;/p&gt;
&lt;p&gt;For example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under &lt;code&gt;Movies&lt;/code&gt; directory in your home directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under Movies directory in your home directory
-o ~/Movies/%(title)s.%(ext)s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that options in configuration file are just the same options aka switches used in regular command line calls thus there &lt;strong&gt;must be no whitespace&lt;/strong&gt; after &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;--&lt;/code&gt;, e.g. &lt;code&gt;-o&lt;/code&gt; or &lt;code&gt;--proxy&lt;/code&gt; but not &lt;code&gt;- o&lt;/code&gt; or &lt;code&gt;-- proxy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can use &lt;code&gt;--ignore-config&lt;/code&gt; if you want to disable the configuration file for a particular youtube-dl run.&lt;/p&gt;
&lt;p&gt;You can also use &lt;code&gt;--config-location&lt;/code&gt; if you want to use custom configuration file for a particular youtube-dl run.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-authentication-with-netrc-file" class="anchor" aria-hidden="true" href="#authentication-with-netrc-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authentication with &lt;code&gt;.netrc&lt;/code&gt; file&lt;/h3&gt;
&lt;p&gt;You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with &lt;code&gt;--username&lt;/code&gt; and &lt;code&gt;--password&lt;/code&gt;) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a &lt;a href="https://stackoverflow.com/tags/.netrc/info" rel="nofollow"&gt;&lt;code&gt;.netrc&lt;/code&gt; file&lt;/a&gt; on a per extractor basis. For that you will need to create a &lt;code&gt;.netrc&lt;/code&gt; file in your &lt;code&gt;$HOME&lt;/code&gt; and restrict permissions to read/write by only you:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;touch $HOME/.netrc
chmod a-rwx,u+rw $HOME/.netrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that you can add credentials for an extractor in the following format, where &lt;em&gt;extractor&lt;/em&gt; is the name of the extractor in lowercase:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine &amp;lt;extractor&amp;gt; login &amp;lt;login&amp;gt; password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To activate authentication with the &lt;code&gt;.netrc&lt;/code&gt; file you should pass &lt;code&gt;--netrc&lt;/code&gt; to youtube-dl or place it in the &lt;a href="#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On Windows you may also need to setup the &lt;code&gt;%HOME%&lt;/code&gt; environment variable manually. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set HOME=%USERPROFILE%
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-output-template" class="anchor" aria-hidden="true" href="#output-template"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OUTPUT TEMPLATE&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;-o&lt;/code&gt; option allows users to indicate a template for the output file names.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="#output-template-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basic usage is not to set any template arguments when downloading a single file, like in &lt;code&gt;youtube-dl -o funny_video.flv "https://some/video"&lt;/code&gt;. However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to &lt;a href="https://docs.python.org/2/library/stdtypes.html#string-formatting" rel="nofollow"&gt;python string formatting operations&lt;/a&gt;. For example, &lt;code&gt;%(NAME)s&lt;/code&gt; or &lt;code&gt;%(NAME)05d&lt;/code&gt;. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations. Allowed names along with sequence type are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt; (string): Video identifier&lt;/li&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt; (string): Video title&lt;/li&gt;
&lt;li&gt;&lt;code&gt;url&lt;/code&gt; (string): Video URL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ext&lt;/code&gt; (string): Video filename extension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alt_title&lt;/code&gt; (string): A secondary title of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;display_id&lt;/code&gt; (string): An alternative identifier for the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uploader&lt;/code&gt; (string): Full name of the video uploader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;license&lt;/code&gt; (string): License name the video is licensed under&lt;/li&gt;
&lt;li&gt;&lt;code&gt;creator&lt;/code&gt; (string): The creator of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;release_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was released&lt;/li&gt;
&lt;li&gt;&lt;code&gt;timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video became available&lt;/li&gt;
&lt;li&gt;&lt;code&gt;upload_date&lt;/code&gt; (string): Video upload date (YYYYMMDD)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uploader_id&lt;/code&gt; (string): Nickname or id of the video uploader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;channel&lt;/code&gt; (string): Full name of the channel the video is uploaded on&lt;/li&gt;
&lt;li&gt;&lt;code&gt;channel_id&lt;/code&gt; (string): Id of the channel&lt;/li&gt;
&lt;li&gt;&lt;code&gt;location&lt;/code&gt; (string): Physical location where the video was filmed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;duration&lt;/code&gt; (numeric): Length of the video in seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;view_count&lt;/code&gt; (numeric): How many users have watched the video on the platform&lt;/li&gt;
&lt;li&gt;&lt;code&gt;like_count&lt;/code&gt; (numeric): Number of positive ratings of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dislike_count&lt;/code&gt; (numeric): Number of negative ratings of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;repost_count&lt;/code&gt; (numeric): Number of reposts of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;average_rating&lt;/code&gt; (numeric): Average rating give by users, the scale used depends on the webpage&lt;/li&gt;
&lt;li&gt;&lt;code&gt;comment_count&lt;/code&gt; (numeric): Number of comments on the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age_limit&lt;/code&gt; (numeric): Age restriction for the video (years)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;is_live&lt;/code&gt; (boolean): Whether this video is a live stream or a fixed-length video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should start, as specified in the URL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should end, as specified in the URL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format&lt;/code&gt; (string): A human-readable description of the format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format_id&lt;/code&gt; (string): Format code specified by &lt;code&gt;--format&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format_note&lt;/code&gt; (string): Additional info about the format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;width&lt;/code&gt; (numeric): Width of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;height&lt;/code&gt; (numeric): Height of the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;resolution&lt;/code&gt; (string): Textual description of width and height&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tbr&lt;/code&gt; (numeric): Average bitrate of audio and video in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;abr&lt;/code&gt; (numeric): Average audio bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acodec&lt;/code&gt; (string): Name of the audio codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;asr&lt;/code&gt; (numeric): Audio sampling rate in Hertz&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vbr&lt;/code&gt; (numeric): Average video bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fps&lt;/code&gt; (numeric): Frame rate&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt; (string): Name of the video codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;container&lt;/code&gt; (string): Name of the container format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filesize&lt;/code&gt; (numeric): The number of bytes, if known in advance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filesize_approx&lt;/code&gt; (numeric): An estimate for the number of bytes&lt;/li&gt;
&lt;li&gt;&lt;code&gt;protocol&lt;/code&gt; (string): The protocol that will be used for the actual download&lt;/li&gt;
&lt;li&gt;&lt;code&gt;extractor&lt;/code&gt; (string): Name of the extractor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;extractor_key&lt;/code&gt; (string): Key name of the extractor&lt;/li&gt;
&lt;li&gt;&lt;code&gt;epoch&lt;/code&gt; (numeric): Unix epoch when creating the file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;autonumber&lt;/code&gt; (numeric): Five-digit number that will be increased with each download, starting at zero&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist&lt;/code&gt; (string): Name or id of the playlist that contains the video&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; (numeric): Index of the video in the playlist padded with leading zeros according to the total length of the playlist&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_id&lt;/code&gt; (string): Playlist identifier&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_title&lt;/code&gt; (string): Playlist title&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_uploader&lt;/code&gt; (string): Full name of the playlist uploader&lt;/li&gt;
&lt;li&gt;&lt;code&gt;playlist_uploader_id&lt;/code&gt; (string): Nickname or id of the playlist uploader&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Available for the video that belongs to some logical chapter or section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;chapter&lt;/code&gt; (string): Name or title of the chapter the video belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;chapter_number&lt;/code&gt; (numeric): Number of the chapter the video belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;chapter_id&lt;/code&gt; (string): Id of the chapter the video belongs to&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Available for the video that is an episode of some series or programme:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;series&lt;/code&gt; (string): Title of the series or programme the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;season&lt;/code&gt; (string): Title of the season the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;season_number&lt;/code&gt; (numeric): Number of the season the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;season_id&lt;/code&gt; (string): Id of the season the video episode belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;episode&lt;/code&gt; (string): Title of the video episode&lt;/li&gt;
&lt;li&gt;&lt;code&gt;episode_number&lt;/code&gt; (numeric): Number of the video episode within a season&lt;/li&gt;
&lt;li&gt;&lt;code&gt;episode_id&lt;/code&gt; (string): Id of the video episode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Available for the media that is a track or a part of a music album:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;track&lt;/code&gt; (string): Title of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;track_number&lt;/code&gt; (numeric): Number of the track within an album or a disc&lt;/li&gt;
&lt;li&gt;&lt;code&gt;track_id&lt;/code&gt; (string): Id of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;artist&lt;/code&gt; (string): Artist(s) of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;genre&lt;/code&gt; (string): Genre(s) of the track&lt;/li&gt;
&lt;li&gt;&lt;code&gt;album&lt;/code&gt; (string): Title of the album the track belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;album_type&lt;/code&gt; (string): Type of the album&lt;/li&gt;
&lt;li&gt;&lt;code&gt;album_artist&lt;/code&gt; (string): List of all artists appeared on the album&lt;/li&gt;
&lt;li&gt;&lt;code&gt;disc_number&lt;/code&gt; (numeric): Number of the disc or other physical medium the track belongs to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;release_year&lt;/code&gt; (numeric): Year (YYYY) when the album was released&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example for &lt;code&gt;-o %(title)s-%(id)s.%(ext)s&lt;/code&gt; and an mp4 video with title &lt;code&gt;youtube-dl test video&lt;/code&gt; and id &lt;code&gt;BaW_jenozKcj&lt;/code&gt;, this will result in a &lt;code&gt;youtube-dl test video-BaW_jenozKcj.mp4&lt;/code&gt; file created in the current directory.&lt;/p&gt;
&lt;p&gt;For numeric sequences you can use numeric related formatting, for example, &lt;code&gt;%(view_count)05d&lt;/code&gt; will result in a string with view count padded with zeros up to 5 characters, like in &lt;code&gt;00042&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Output templates can also contain arbitrary hierarchical path, e.g. &lt;code&gt;-o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s'&lt;/code&gt; which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.&lt;/p&gt;
&lt;p&gt;To use percent literals in an output template use &lt;code&gt;%%&lt;/code&gt;. To output to stdout use &lt;code&gt;-o -&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The current default template is &lt;code&gt;%(title)s-%(id)s.%(ext)s&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In some cases, you don't want special characters such as 中, spaces, or &amp;amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the &lt;code&gt;--restrict-filenames&lt;/code&gt; flag to get a shorter title:&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-output-template-and-windows-batch-files" class="anchor" aria-hidden="true" href="#output-template-and-windows-batch-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Output template and Windows batch files&lt;/h4&gt;
&lt;p&gt;If you are using an output template inside a Windows batch file then you must escape plain percent characters (&lt;code&gt;%&lt;/code&gt;) by doubling, so that &lt;code&gt;-o "%(title)s-%(id)s.%(ext)s"&lt;/code&gt; should become &lt;code&gt;-o "%%(title)s-%%(id)s.%%(ext)s"&lt;/code&gt;. However you should not touch &lt;code&gt;%&lt;/code&gt;'s that are not plain characters, e.g. environment variables for expansion should stay intact: &lt;code&gt;-o "C:\%HOMEPATH%\Desktop\%%(title)s.%%(ext)s"&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-output-template-examples" class="anchor" aria-hidden="true" href="#output-template-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Output template examples&lt;/h4&gt;
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ youtube-dl --get-filename -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; BaW_jenozKc
youtube-dl &lt;span class="pl-c1"&gt;test&lt;/span&gt; video &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;_ä↭𝕐.mp4    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; All kinds of weird characters&lt;/span&gt;

$ youtube-dl --get-filename -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.mp4          &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; A simple file name&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download YouTube playlist videos in separate directory indexed by video order in a playlist&lt;/span&gt;
$ youtube-dl -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; https://www.youtube.com/playlist&lt;span class="pl-k"&gt;?&lt;/span&gt;list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download all playlists of YouTube channel/user keeping each playlist in separate directory:&lt;/span&gt;
$ youtube-dl -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; https://www.youtube.com/user/TheLinuxFoundation/playlists

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home&lt;/span&gt;
$ youtube-dl -u user -p password -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; https://www.udemy.com/java-tutorial/

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download entire series season keeping each series and each season in separate directory under C:/MyVideos&lt;/span&gt;
$ youtube-dl -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; https://videomore.ru/kino_v_detalayah/5_sezon/367617

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Stream the video being downloaded to stdout&lt;/span&gt;
$ youtube-dl -o - BaW_jenozKc&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-format-selection" class="anchor" aria-hidden="true" href="#format-selection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FORMAT SELECTION&lt;/h1&gt;
&lt;p&gt;By default youtube-dl tries to download the best available quality, i.e. if you want the best quality you &lt;strong&gt;don't need&lt;/strong&gt; to pass any special options, youtube-dl will guess it for you by &lt;strong&gt;default&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;But sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called &lt;em&gt;format selection&lt;/em&gt; based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.&lt;/p&gt;
&lt;p&gt;The general syntax for format selection is &lt;code&gt;--format FORMAT&lt;/code&gt; or shorter &lt;code&gt;-f FORMAT&lt;/code&gt; where &lt;code&gt;FORMAT&lt;/code&gt; is a &lt;em&gt;selector expression&lt;/em&gt;, i.e. an expression that describes format or formats you would like to download.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="#format-selection-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The simplest case is requesting a specific format, for example with &lt;code&gt;-f 22&lt;/code&gt; you can download the format with format code equal to 22. You can get the list of available format codes for particular video using &lt;code&gt;--list-formats&lt;/code&gt; or &lt;code&gt;-F&lt;/code&gt;. Note that these format codes are extractor specific.&lt;/p&gt;
&lt;p&gt;You can also use a file extension (currently &lt;code&gt;3gp&lt;/code&gt;, &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;flv&lt;/code&gt;, &lt;code&gt;m4a&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, &lt;code&gt;mp4&lt;/code&gt;, &lt;code&gt;ogg&lt;/code&gt;, &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;webm&lt;/code&gt; are supported) to download the best quality format of a particular file extension served as a single file, e.g. &lt;code&gt;-f webm&lt;/code&gt; will download the best quality format with the &lt;code&gt;webm&lt;/code&gt; extension served as a single file.&lt;/p&gt;
&lt;p&gt;You can also use special names to select particular edge case formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;best&lt;/code&gt;: Select the best quality format represented by a single file with video and audio.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worst&lt;/code&gt;: Select the worst quality format represented by a single file with video and audio.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bestvideo&lt;/code&gt;: Select the best quality video-only format (e.g. DASH video). May not be available.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worstvideo&lt;/code&gt;: Select the worst quality video-only format. May not be available.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bestaudio&lt;/code&gt;: Select the best quality audio only-format. May not be available.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worstaudio&lt;/code&gt;: Select the worst quality audio only-format. May not be available.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, to download the worst quality video-only format you can use &lt;code&gt;-f worstvideo&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example &lt;code&gt;-f 22/17/18&lt;/code&gt; will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.&lt;/p&gt;
&lt;p&gt;If you want to download several formats of the same video use a comma as a separator, e.g. &lt;code&gt;-f 22,17,18&lt;/code&gt; will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: &lt;code&gt;-f 136/137/mp4/bestvideo,140/m4a/bestaudio&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can also filter the video formats by putting a condition in brackets, as in &lt;code&gt;-f "best[height=720]"&lt;/code&gt; (or &lt;code&gt;-f "[filesize&amp;gt;10M]"&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The following numeric meta fields can be used with comparisons &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;!=&lt;/code&gt; (not equals):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: The number of bytes, if known in advance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of the video, if known&lt;/li&gt;
&lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of the video, if known&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Average bitrate of audio and video in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in KBit/s&lt;/li&gt;
&lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sampling rate in Hertz&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Frame rate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also filtering work for comparisons &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;^=&lt;/code&gt; (starts with), &lt;code&gt;$=&lt;/code&gt; (ends with), &lt;code&gt;*=&lt;/code&gt; (contains) and following string meta fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: File extension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Name of the audio codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Name of the video codec in use&lt;/li&gt;
&lt;li&gt;&lt;code&gt;container&lt;/code&gt;: Name of the container format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;protocol&lt;/code&gt;: The protocol that will be used for the actual download, lower-case (&lt;code&gt;http&lt;/code&gt;, &lt;code&gt;https&lt;/code&gt;, &lt;code&gt;rtsp&lt;/code&gt;, &lt;code&gt;rtmp&lt;/code&gt;, &lt;code&gt;rtmpe&lt;/code&gt;, &lt;code&gt;mms&lt;/code&gt;, &lt;code&gt;f4m&lt;/code&gt;, &lt;code&gt;ism&lt;/code&gt;, &lt;code&gt;http_dash_segments&lt;/code&gt;, &lt;code&gt;m3u8&lt;/code&gt;, or &lt;code&gt;m3u8_native&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;format_id&lt;/code&gt;: A short description of the format&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any string comparison may be prefixed with negation &lt;code&gt;!&lt;/code&gt; in order to produce an opposite comparison, e.g. &lt;code&gt;!*=&lt;/code&gt; (does not contain).&lt;/p&gt;
&lt;p&gt;Note that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.&lt;/p&gt;
&lt;p&gt;Formats for which the value is not known are excluded unless you put a question mark (&lt;code&gt;?&lt;/code&gt;) after the operator. You can combine format filters, so &lt;code&gt;-f "[height &amp;lt;=? 720][tbr&amp;gt;500]"&lt;/code&gt; selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.&lt;/p&gt;
&lt;p&gt;You can merge the video and audio of two formats into a single file using &lt;code&gt;-f &amp;lt;video-format&amp;gt;+&amp;lt;audio-format&amp;gt;&lt;/code&gt; (requires ffmpeg or avconv installed), for example &lt;code&gt;-f bestvideo+bestaudio&lt;/code&gt; will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.&lt;/p&gt;
&lt;p&gt;Format selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use &lt;code&gt;-f '(mp4,webm)[height&amp;lt;480]'&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Since the end of April 2015 and version 2015.04.26, youtube-dl uses &lt;code&gt;-f bestvideo+bestaudio/best&lt;/code&gt; as the default format selection (see &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5447"&gt;#5447&lt;/a&gt;, &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/5456"&gt;#5456&lt;/a&gt;). If ffmpeg or avconv are installed this results in downloading &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to &lt;code&gt;best&lt;/code&gt; and results in downloading the best available quality served as a single file. &lt;code&gt;best&lt;/code&gt; is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add &lt;code&gt;-f bestvideo[height&amp;lt;=?1080]+bestaudio/best&lt;/code&gt; to your configuration file. Note that if you use youtube-dl to stream to &lt;code&gt;stdout&lt;/code&gt; (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as &lt;code&gt;-o -&lt;/code&gt;, youtube-dl still uses &lt;code&gt;-f best&lt;/code&gt; format selection in order to start content delivery immediately to your player and not to wait until &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt; are downloaded and muxed.&lt;/p&gt;
&lt;p&gt;If you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with &lt;code&gt;-f best&lt;/code&gt;. You may want to add it to the &lt;a href="#configuration"&gt;configuration file&lt;/a&gt; in order not to type it every time you run youtube-dl.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-format-selection-examples" class="anchor" aria-hidden="true" href="#format-selection-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Format selection examples&lt;/h4&gt;
&lt;p&gt;Note that on Windows you may need to use double quotes instead of single.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best mp4 format available or any other best if no mp4 available&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best format available but no better than 480p&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestvideo[height&amp;lt;=480]+bestaudio/best[height&amp;lt;=480]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best video only format but no bigger than 50 MB&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;best[filesize&amp;lt;50M]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download best format available via direct link over HTTP/HTTPS protocol&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;(bestvideo+bestaudio/best)[protocol^=http]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download the best video format and the best audio format without merging them&lt;/span&gt;
$ youtube-dl -f &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestvideo,bestaudio&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; -o &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;%(title)s.f%(format_id)s.%(ext)s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-video-selection-1" class="anchor" aria-hidden="true" href="#video-selection-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VIDEO SELECTION&lt;/h1&gt;
&lt;p&gt;Videos can be filtered by their upload date using the options &lt;code&gt;--date&lt;/code&gt;, &lt;code&gt;--datebefore&lt;/code&gt; or &lt;code&gt;--dateafter&lt;/code&gt;. They accept dates in two formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Absolute dates: Dates in the format &lt;code&gt;YYYYMMDD&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Relative dates: Dates in the format &lt;code&gt;(now|today)[+-][0-9](day|week|month|year)(s)?&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download only the videos uploaded in the last 6 months&lt;/span&gt;
$ youtube-dl --dateafter now-6months

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download only the videos uploaded on January 1, 1970&lt;/span&gt;
$ youtube-dl --date 19700101

$ &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Download only the videos uploaded in the 200x decade&lt;/span&gt;
$ youtube-dl --dateafter 20000101 --datebefore 20091231&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-update-youtube-dl" class="anchor" aria-hidden="true" href="#how-do-i-update-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I update youtube-dl?&lt;/h3&gt;
&lt;p&gt;If you've followed &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;our manual installation instructions&lt;/a&gt;, you can simply run &lt;code&gt;youtube-dl -U&lt;/code&gt; (or, on Linux, &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have used pip, a simple &lt;code&gt;sudo pip install -U youtube-dl&lt;/code&gt; is sufficient to update.&lt;/p&gt;
&lt;p&gt;If you have installed youtube-dl using a package manager like &lt;em&gt;apt-get&lt;/em&gt; or &lt;em&gt;yum&lt;/em&gt;, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to &lt;a href="https://yt-dl.org" rel="nofollow"&gt;https://yt-dl.org&lt;/a&gt; to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.&lt;/p&gt;
&lt;p&gt;As a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get remove -y youtube-dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Afterwards, simply follow &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;our manual installation instructions&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo wget https://yt-dl.org/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+x /usr/local/bin/youtube-dl
hash -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, from then on you'll be able to update with &lt;code&gt;sudo youtube-dl -U&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-youtube-dl-is-extremely-slow-to-start-on-windows" class="anchor" aria-hidden="true" href="#youtube-dl-is-extremely-slow-to-start-on-windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;youtube-dl is extremely slow to start on Windows&lt;/h3&gt;
&lt;p&gt;Add a file exclusion for &lt;code&gt;youtube-dl.exe&lt;/code&gt; in Windows Defender settings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-im-getting-an-error-unable-to-extract-opengraph-title-on-youtube-playlists" class="anchor" aria-hidden="true" href="#im-getting-an-error-unable-to-extract-opengraph-title-on-youtube-playlists"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I'm getting an error &lt;code&gt;Unable to extract OpenGraph title&lt;/code&gt; on YouTube playlists&lt;/h3&gt;
&lt;p&gt;YouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.&lt;/p&gt;
&lt;p&gt;If you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to &lt;a href="https://bugs.launchpad.net/ubuntu/+source/youtube-dl/+filebug" rel="nofollow"&gt;report bugs&lt;/a&gt; to the &lt;a href="mailto:ubuntu-motu@lists.ubuntu.com?subject=outdated%20version%20of%20youtube-dl"&gt;Ubuntu packaging people&lt;/a&gt; - all they have to do is update the package to a somewhat recent version. See above for a way to update.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-im-getting-an-error-when-trying-to-use-output-template-error-using-output-template-conflicts-with-using-title-video-id-or-auto-number" class="anchor" aria-hidden="true" href="#im-getting-an-error-when-trying-to-use-output-template-error-using-output-template-conflicts-with-using-title-video-id-or-auto-number"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I'm getting an error when trying to use output template: &lt;code&gt;error: using output template conflicts with using title, video ID or auto number&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Make sure you are not using &lt;code&gt;-o&lt;/code&gt; with any of these options &lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--title&lt;/code&gt;, &lt;code&gt;--id&lt;/code&gt;, &lt;code&gt;-A&lt;/code&gt; or &lt;code&gt;--auto-number&lt;/code&gt; set in command line or in a configuration file. Remove the latter if any.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-do-i-always-have-to-pass--citw" class="anchor" aria-hidden="true" href="#do-i-always-have-to-pass--citw"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do I always have to pass &lt;code&gt;-citw&lt;/code&gt;?&lt;/h3&gt;
&lt;p&gt;By default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, &lt;a href="https://yt-dl.org/bug" rel="nofollow"&gt;please file an issue where you explain that&lt;/a&gt;). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of &lt;code&gt;-citw&lt;/code&gt; that is regularly useful is &lt;code&gt;-i&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-can-you-please-put-the--b-option-back" class="anchor" aria-hidden="true" href="#can-you-please-put-the--b-option-back"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can you please put the &lt;code&gt;-b&lt;/code&gt; option back?&lt;/h3&gt;
&lt;p&gt;Most people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the &lt;code&gt;-b&lt;/code&gt; option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the &lt;code&gt;-f&lt;/code&gt; option and youtube-dl will try to download it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-get-http-error-402-when-trying-to-download-a-video-whats-this" class="anchor" aria-hidden="true" href="#i-get-http-error-402-when-trying-to-download-a-video-whats-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I get HTTP error 402 when trying to download a video. What's this?&lt;/h3&gt;
&lt;p&gt;Apparently YouTube requires you to pass a CAPTCHA test if you download too much. We're &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/154"&gt;considering to provide a way to let you solve the CAPTCHA&lt;/a&gt;, but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-do-i-need-any-other-programs" class="anchor" aria-hidden="true" href="#do-i-need-any-other-programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do I need any other programs?&lt;/h3&gt;
&lt;p&gt;youtube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need &lt;a href="https://libav.org/" rel="nofollow"&gt;avconv&lt;/a&gt; or &lt;a href="https://www.ffmpeg.org/" rel="nofollow"&gt;ffmpeg&lt;/a&gt;. On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.&lt;/p&gt;
&lt;p&gt;Videos or video formats streamed via RTMP protocol can only be downloaded when &lt;a href="https://rtmpdump.mplayerhq.hu/" rel="nofollow"&gt;rtmpdump&lt;/a&gt; is installed. Downloading MMS and RTSP videos requires either &lt;a href="https://mplayerhq.hu/" rel="nofollow"&gt;mplayer&lt;/a&gt; or &lt;a href="https://mpv.io/" rel="nofollow"&gt;mpv&lt;/a&gt; to be installed.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-have-downloaded-a-video-but-how-can-i-play-it" class="anchor" aria-hidden="true" href="#i-have-downloaded-a-video-but-how-can-i-play-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I have downloaded a video but how can I play it?&lt;/h3&gt;
&lt;p&gt;Once the video is fully downloaded, use any video player, such as &lt;a href="https://mpv.io/" rel="nofollow"&gt;mpv&lt;/a&gt;, &lt;a href="https://www.videolan.org/" rel="nofollow"&gt;vlc&lt;/a&gt; or &lt;a href="https://www.mplayerhq.hu/" rel="nofollow"&gt;mplayer&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-extracted-a-video-url-with--g-but-it-does-not-play-on-another-machine--in-my-web-browser" class="anchor" aria-hidden="true" href="#i-extracted-a-video-url-with--g-but-it-does-not-play-on-another-machine--in-my-web-browser"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I extracted a video URL with &lt;code&gt;-g&lt;/code&gt;, but it does not play on another machine / in my web browser.&lt;/h3&gt;
&lt;p&gt;It depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the &lt;code&gt;--cookies&lt;/code&gt; option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use &lt;code&gt;--dump-user-agent&lt;/code&gt; to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with &lt;code&gt;--dump-json&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.&lt;/p&gt;
&lt;p&gt;Please bear in mind that some URL protocols are &lt;strong&gt;not&lt;/strong&gt; supported by browsers out of the box, including RTMP. If you are using &lt;code&gt;-g&lt;/code&gt;, your own downloader must support these as well.&lt;/p&gt;
&lt;p&gt;If you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use &lt;code&gt;-o -&lt;/code&gt; to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-error-no-fmt_url_map-or-conn-information-found-in-video-info" class="anchor" aria-hidden="true" href="#error-no-fmt_url_map-or-conn-information-found-in-video-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ERROR: no fmt_url_map or conn information found in video info&lt;/h3&gt;
&lt;p&gt;YouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See &lt;a href="#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-error-unable-to-download-video" class="anchor" aria-hidden="true" href="#error-unable-to-download-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ERROR: unable to download video&lt;/h3&gt;
&lt;p&gt;YouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See &lt;a href="#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-video-url-contains-an-ampersand-and-im-getting-some-strange-output-1-2839-or-v-is-not-recognized-as-an-internal-or-external-command" class="anchor" aria-hidden="true" href="#video-url-contains-an-ampersand-and-im-getting-some-strange-output-1-2839-or-v-is-not-recognized-as-an-internal-or-external-command"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video URL contains an ampersand and I'm getting some strange output &lt;code&gt;[1] 2839&lt;/code&gt; or &lt;code&gt;'v' is not recognized as an internal or external command&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).&lt;/p&gt;
&lt;p&gt;For example if your URL is &lt;a href="https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc" rel="nofollow"&gt;https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc&lt;/a&gt; you should end up with following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;youtube-dl 'https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;code&gt;youtube-dl https://www.youtube.com/watch?t=4\&amp;amp;v=BaW_jenozKc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For Windows you have to use the double quotes:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;youtube-dl "https://www.youtube.com/watch?t=4&amp;amp;v=BaW_jenozKc"&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-extractorerror-could-not-find-js-function-uof" class="anchor" aria-hidden="true" href="#extractorerror-could-not-find-js-function-uof"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ExtractorError: Could not find JS function u'OF'&lt;/h3&gt;
&lt;p&gt;In February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See &lt;a href="#how-do-i-update-youtube-dl"&gt;above&lt;/a&gt; for how to update youtube-dl.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-http-error-429-too-many-requests-or-402-payment-required" class="anchor" aria-hidden="true" href="#http-error-429-too-many-requests-or-402-payment-required"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTTP Error 429: Too Many Requests or 402: Payment Required&lt;/h3&gt;
&lt;p&gt;These two error codes indicate that the service is blocking your IP address because of overuse. Contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the &lt;a href="#network-options"&gt;&lt;code&gt;--proxy&lt;/code&gt; or &lt;code&gt;--source-address&lt;/code&gt; options&lt;/a&gt; to select another IP address.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-syntaxerror-non-ascii-character" class="anchor" aria-hidden="true" href="#syntaxerror-non-ascii-character"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SyntaxError: Non-ASCII character&lt;/h3&gt;
&lt;p&gt;The error&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;File "youtube-dl", line 2
SyntaxError: Non-ASCII character '\x93' ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;means you're using an outdated version of Python. Please update to Python 2.6 or 2.7.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-is-this-binary-file-where-has-the-code-gone" class="anchor" aria-hidden="true" href="#what-is-this-binary-file-where-has-the-code-gone"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this binary file? Where has the code gone?&lt;/h3&gt;
&lt;p&gt;Since June 2012 (&lt;a href="https://github.com/ytdl-org/youtube-dl/issues/342"&gt;#342&lt;/a&gt;) youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to &lt;code&gt;youtube-dl.zip&lt;/code&gt; first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the &lt;code&gt;__main__.py&lt;/code&gt; file. To recompile the executable, run &lt;code&gt;make youtube-dl&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-the-exe-throws-an-error-due-to-missing-msvcr100dll" class="anchor" aria-hidden="true" href="#the-exe-throws-an-error-due-to-missing-msvcr100dll"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The exe throws an error due to missing &lt;code&gt;MSVCR100.dll&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;To run the exe you need to install first the &lt;a href="https://www.microsoft.com/en-US/download/details.aspx?id=5555" rel="nofollow"&gt;Microsoft Visual C++ 2010 Redistributable Package (x86)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-on-windows-how-should-i-set-up-ffmpeg-and-youtube-dl-where-should-i-put-the-exe-files" class="anchor" aria-hidden="true" href="#on-windows-how-should-i-set-up-ffmpeg-and-youtube-dl-where-should-i-put-the-exe-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?&lt;/h3&gt;
&lt;p&gt;If you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.&lt;/p&gt;
&lt;p&gt;To make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, &lt;code&gt;C:\bin&lt;/code&gt;, or &lt;code&gt;C:\Users\&amp;lt;User name&amp;gt;\bin&lt;/code&gt;), put all the executables directly in there, and then &lt;a href="https://www.java.com/en/download/help/path.xml" rel="nofollow"&gt;set your PATH environment variable&lt;/a&gt; to include that directory.&lt;/p&gt;
&lt;p&gt;From then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing &lt;code&gt;youtube-dl&lt;/code&gt; or &lt;code&gt;ffmpeg&lt;/code&gt;, no matter what directory you're in.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-put-downloads-into-a-specific-folder" class="anchor" aria-hidden="true" href="#how-do-i-put-downloads-into-a-specific-folder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I put downloads into a specific folder?&lt;/h3&gt;
&lt;p&gt;Use the &lt;code&gt;-o&lt;/code&gt; to specify an &lt;a href="#output-template"&gt;output template&lt;/a&gt;, for example &lt;code&gt;-o "/home/user/videos/%(title)s-%(id)s.%(ext)s"&lt;/code&gt;. If you want this for all of your downloads, put the option into your &lt;a href="#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-download-a-video-starting-with-a--" class="anchor" aria-hidden="true" href="#how-do-i-download-a-video-starting-with-a--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I download a video starting with a &lt;code&gt;-&lt;/code&gt;?&lt;/h3&gt;
&lt;p&gt;Either prepend &lt;code&gt;https://www.youtube.com/watch?v=&lt;/code&gt; or separate the ID from the options with &lt;code&gt;--&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl -- -wNyEUrxzFU
youtube-dl "https://www.youtube.com/watch?v=-wNyEUrxzFU"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-pass-cookies-to-youtube-dl" class="anchor" aria-hidden="true" href="#how-do-i-pass-cookies-to-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I pass cookies to youtube-dl?&lt;/h3&gt;
&lt;p&gt;Use the &lt;code&gt;--cookies&lt;/code&gt; option, for example &lt;code&gt;--cookies /path/to/cookies/file.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, &lt;a href="https://chrome.google.com/webstore/detail/cookiestxt/njabckikapfpffapmjgojcnbfjonfjfg" rel="nofollow"&gt;cookies.txt&lt;/a&gt; (for Chrome) or &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/" rel="nofollow"&gt;cookies.txt&lt;/a&gt; (for Firefox).&lt;/p&gt;
&lt;p&gt;Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either &lt;code&gt;# HTTP Cookie File&lt;/code&gt; or &lt;code&gt;# Netscape HTTP Cookie File&lt;/code&gt;. Make sure you have correct &lt;a href="https://en.wikipedia.org/wiki/Newline" rel="nofollow"&gt;newline format&lt;/a&gt; in the cookies file and convert newlines if necessary to correspond with your OS, namely &lt;code&gt;CRLF&lt;/code&gt; (&lt;code&gt;\r\n&lt;/code&gt;) for Windows and &lt;code&gt;LF&lt;/code&gt; (&lt;code&gt;\n&lt;/code&gt;) for Unix and Unix-like systems (Linux, macOS, etc.). &lt;code&gt;HTTP Error 400: Bad Request&lt;/code&gt; when using &lt;code&gt;--cookies&lt;/code&gt; is a good sign of invalid newline format.&lt;/p&gt;
&lt;p&gt;Passing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around &lt;a href="https://en.wikipedia.org/wiki/CAPTCHA" rel="nofollow"&gt;CAPTCHA&lt;/a&gt; some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-stream-directly-to-media-player" class="anchor" aria-hidden="true" href="#how-do-i-stream-directly-to-media-player"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I stream directly to media player?&lt;/h3&gt;
&lt;p&gt;You will first need to tell youtube-dl to stream media to stdout with &lt;code&gt;-o -&lt;/code&gt;, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to &lt;a href="https://www.videolan.org/" rel="nofollow"&gt;vlc&lt;/a&gt; can be achieved with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl -o - "https://www.youtube.com/watch?v=BaW_jenozKcj" | vlc -
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-how-do-i-download-only-new-videos-from-a-playlist" class="anchor" aria-hidden="true" href="#how-do-i-download-only-new-videos-from-a-playlist"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I download only new videos from a playlist?&lt;/h3&gt;
&lt;p&gt;Use download-archive feature. With this feature you should initially download the complete playlist with &lt;code&gt;--download-archive /path/to/download/archive/file.txt&lt;/code&gt; that will record identifiers of all the videos in a special file. Each subsequent run with the same &lt;code&gt;--download-archive&lt;/code&gt; will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.&lt;/p&gt;
&lt;p&gt;For example, at first,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will download the complete &lt;code&gt;PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&lt;/code&gt; playlist and create a file &lt;code&gt;archive.txt&lt;/code&gt;. Each subsequent run will only download new videos if any:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-should-i-add---hls-prefer-native-into-my-config" class="anchor" aria-hidden="true" href="#should-i-add---hls-prefer-native-into-my-config"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Should I add &lt;code&gt;--hls-prefer-native&lt;/code&gt; into my config?&lt;/h3&gt;
&lt;p&gt;When youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.&lt;/p&gt;
&lt;p&gt;When youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.&lt;/p&gt;
&lt;p&gt;In particular, the generic extractor (used when your website is not in the &lt;a href="https://ytdl-org.github.io/youtube-dl/supportedsites.html" rel="nofollow"&gt;list of supported sites by youtube-dl&lt;/a&gt; cannot mandate one specific downloader.&lt;/p&gt;
&lt;p&gt;If you put either &lt;code&gt;--hls-prefer-native&lt;/code&gt; or &lt;code&gt;--hls-prefer-ffmpeg&lt;/code&gt; into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to &lt;a href="https://yt-dl.org/bug" rel="nofollow"&gt;file an issue&lt;/a&gt; or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free" class="anchor" aria-hidden="true" href="#can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Can you add support for this anime video site, or site which shows current movies for free?&lt;/h3&gt;
&lt;p&gt;As a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.&lt;/p&gt;
&lt;p&gt;A note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should &lt;strong&gt;not&lt;/strong&gt; be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A "fair use" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.&lt;/p&gt;
&lt;p&gt;Support requests for services that &lt;strong&gt;do&lt;/strong&gt; purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-can-i-speed-up-work-on-my-issue" class="anchor" aria-hidden="true" href="#how-can-i-speed-up-work-on-my-issue"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How can I speed up work on my issue?&lt;/h3&gt;
&lt;p&gt;(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:&lt;/p&gt;
&lt;p&gt;First of all, please do report the issue &lt;a href="https://yt-dl.org/bugs" rel="nofollow"&gt;at our issue tracker&lt;/a&gt;. That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.&lt;/p&gt;
&lt;p&gt;Please read the &lt;a href="#bugs"&gt;bug reporting instructions&lt;/a&gt; below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.&lt;/p&gt;
&lt;p&gt;If nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).&lt;/p&gt;
&lt;p&gt;Feel free to bump the issue from time to time by writing a small comment ("Issue is still present in youtube-dl version ...from France, but fixed from Belgium"), but please not more than once a month. Please do not declare your issue as &lt;code&gt;important&lt;/code&gt; or &lt;code&gt;urgent&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-can-i-detect-whether-a-given-url-is-supported-by-youtube-dl" class="anchor" aria-hidden="true" href="#how-can-i-detect-whether-a-given-url-is-supported-by-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How can I detect whether a given URL is supported by youtube-dl?&lt;/h3&gt;
&lt;p&gt;For one, have a look at the &lt;a href="docs/supportedsites.md"&gt;list of supported sites&lt;/a&gt;. Note that it can sometimes happen that the site changes its URL scheme (say, from &lt;a href="https://example.com/video/1234567" rel="nofollow"&gt;https://example.com/video/1234567&lt;/a&gt; to &lt;a href="https://example.com/v/1234567" rel="nofollow"&gt;https://example.com/v/1234567&lt;/a&gt; ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.&lt;/p&gt;
&lt;p&gt;It is &lt;em&gt;not&lt;/em&gt; possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches &lt;strong&gt;all&lt;/strong&gt; URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.&lt;/p&gt;
&lt;p&gt;If you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an &lt;code&gt;UnsupportedError&lt;/code&gt; exception if you run it from a Python program.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-why-do-i-need-to-go-through-that-much-red-tape-when-filing-bugs" class="anchor" aria-hidden="true" href="#why-do-i-need-to-go-through-that-much-red-tape-when-filing-bugs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why do I need to go through that much red tape when filing bugs?&lt;/h1&gt;
&lt;p&gt;Before we had the issue template, despite our extensive &lt;a href="#bugs"&gt;bug reporting instructions&lt;/a&gt;, about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said "please install ffmpeg", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.&lt;/p&gt;
&lt;p&gt;youtube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of &lt;code&gt;youtube-dl -v YOUR_URL_HERE&lt;/code&gt; is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-developer-instructions" class="anchor" aria-hidden="true" href="#developer-instructions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DEVELOPER INSTRUCTIONS&lt;/h1&gt;
&lt;p&gt;Most users do not need to build youtube-dl and can &lt;a href="https://ytdl-org.github.io/youtube-dl/download.html" rel="nofollow"&gt;download the builds&lt;/a&gt; or get them from their distribution.&lt;/p&gt;
&lt;p&gt;To run youtube-dl as a developer, you don't need to build anything either. Simply execute&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m youtube_dl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m unittest discover
python test/test_download.py
nosetests
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See item 6 of &lt;a href="#adding-support-for-a-new-site"&gt;new extractor tutorial&lt;/a&gt; for how to run extractor specific test cases.&lt;/p&gt;
&lt;p&gt;If you want to create a build of youtube-dl yourself, you'll need&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;python&lt;/li&gt;
&lt;li&gt;make (only GNU make is supported)&lt;/li&gt;
&lt;li&gt;pandoc&lt;/li&gt;
&lt;li&gt;zip&lt;/li&gt;
&lt;li&gt;nosetests&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-adding-support-for-a-new-site" class="anchor" aria-hidden="true" href="#adding-support-for-a-new-site"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding support for a new site&lt;/h3&gt;
&lt;p&gt;If you want to add support for a new site, first of all &lt;strong&gt;make sure&lt;/strong&gt; this site is &lt;strong&gt;not dedicated to &lt;a href="README.md#can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free"&gt;copyright infringement&lt;/a&gt;&lt;/strong&gt;. youtube-dl does &lt;strong&gt;not support&lt;/strong&gt; such sites thus pull requests adding support for them &lt;strong&gt;will be rejected&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called &lt;code&gt;yourextractor&lt;/code&gt;):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/fork"&gt;Fork this repository&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check out the source code with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; git clone git@github.com:YOUR_GITHUB_USERNAME/youtube-dl.git
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start a new git branch with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; cd youtube-dl
 git checkout -b yourextractor
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start with this simple template and save it to &lt;code&gt;youtube_dl/extractor/yourextractor.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; coding: utf-8&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;__future__&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; unicode_literals

&lt;span class="pl-k"&gt;from&lt;/span&gt; .common &lt;span class="pl-k"&gt;import&lt;/span&gt; InfoExtractor


&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;YourExtractorIE&lt;/span&gt;(&lt;span class="pl-e"&gt;InfoExtractor&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;_VALID_URL&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https&lt;span class="pl-k"&gt;?&lt;/span&gt;://&lt;span class="pl-c1"&gt;(?:&lt;/span&gt;www&lt;span class="pl-cce"&gt;\.&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-k"&gt;?&lt;/span&gt;yourextractor&lt;span class="pl-cce"&gt;\.&lt;/span&gt;com/watch/&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-ent"&gt;?P&amp;lt;id&amp;gt;&lt;/span&gt;[&lt;span class="pl-c1"&gt;0-9&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-c1"&gt;_TEST&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; {
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;url&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://yourextractor.com/watch/42&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;md5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;TODO: md5 sum of the first 10241 bytes of the video file (use --test)&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;info_dict&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;id&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;42&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ext&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mp4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Video title goes here&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thumbnail&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;re:&lt;span class="pl-c1"&gt;^&lt;/span&gt;https&lt;span class="pl-k"&gt;?&lt;/span&gt;://&lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-cce"&gt;\.&lt;/span&gt;jpg&lt;span class="pl-c1"&gt;$&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &lt;span class="pl-k"&gt;TODO&lt;/span&gt; more properties, either as:&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * A value&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * MD5 checksum; start the string with md5:&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * A regular expression; start the string with re:&lt;/span&gt;
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; * Any Python type (for example int or float)&lt;/span&gt;
        }
    }

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_real_extract&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;url&lt;/span&gt;):
        video_id &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._match_id(url)
        webpage &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._download_webpage(url, video_id)

        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &lt;span class="pl-k"&gt;TODO&lt;/span&gt; more code goes here, for example ...&lt;/span&gt;
        title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_regex(&lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;h1&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;span class="pl-k"&gt;+?&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/h1&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

        &lt;span class="pl-k"&gt;return&lt;/span&gt; {
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;id&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: video_id,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: title,
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._og_search_description(webpage),
            &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;uploader&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(&lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;div[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;id="uploader"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;uploader&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;fatal&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;),
            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &lt;span class="pl-k"&gt;TODO&lt;/span&gt; more properties (see youtube_dl/extractor/common.py)&lt;/span&gt;
        }&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add an import in &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/extractors.py"&gt;&lt;code&gt;youtube_dl/extractor/extractors.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;python test/test_download.py TestDownload.test_YourExtractor&lt;/code&gt;. This &lt;em&gt;should fail&lt;/em&gt; at first, but you can continually re-run it until you're done. If you decide to add more than one test, then rename &lt;code&gt;_TEST&lt;/code&gt; to &lt;code&gt;_TESTS&lt;/code&gt; and make it into a list of dictionaries. The tests will then be named &lt;code&gt;TestDownload.test_YourExtractor&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_1&lt;/code&gt;, &lt;code&gt;TestDownload.test_YourExtractor_2&lt;/code&gt;, etc. Note that tests with &lt;code&gt;only_matching&lt;/code&gt; key in test's dict are not counted in.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/extractor/common.py"&gt;&lt;code&gt;youtube_dl/extractor/common.py&lt;/code&gt;&lt;/a&gt; for possible helper methods and a &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;detailed description of what your extractor should and may return&lt;/a&gt;. Add tests and code for as many as you want.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure your code follows &lt;a href="#youtube-dl-coding-conventions"&gt;youtube-dl coding conventions&lt;/a&gt; and check the code with &lt;a href="http://flake8.pycqa.org/en/latest/index.html#quickstart" rel="nofollow"&gt;flake8&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; $ flake8 youtube_dl/extractor/yourextractor.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure your code works under all &lt;a href="https://www.python.org/" rel="nofollow"&gt;Python&lt;/a&gt; versions claimed supported by youtube-dl, namely 2.6, 2.7, and 3.2+.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When the tests pass, &lt;a href="https://git-scm.com/docs/git-add" rel="nofollow"&gt;add&lt;/a&gt; the new files and &lt;a href="https://git-scm.com/docs/git-commit" rel="nofollow"&gt;commit&lt;/a&gt; them and &lt;a href="https://git-scm.com/docs/git-push" rel="nofollow"&gt;push&lt;/a&gt; the result, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git add youtube_dl/extractor/extractors.py
$ git add youtube_dl/extractor/yourextractor.py
$ git commit -m '[yourextractor] Add new extractor'
$ git push origin yourextractor
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, &lt;a href="https://help.github.com/articles/creating-a-pull-request"&gt;create a pull request&lt;/a&gt;. We'll then review and merge it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In any case, thank you very much for your contributions!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-youtube-dl-coding-conventions" class="anchor" aria-hidden="true" href="#youtube-dl-coding-conventions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;youtube-dl coding conventions&lt;/h2&gt;
&lt;p&gt;This section introduces a guide lines for writing idiomatic, robust and future-proof extractor code.&lt;/p&gt;
&lt;p&gt;Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-mandatory-and-optional-metafields" class="anchor" aria-hidden="true" href="#mandatory-and-optional-metafields"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mandatory and optional metafields&lt;/h3&gt;
&lt;p&gt;For extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303"&gt;information dictionary&lt;/a&gt; or simply &lt;em&gt;info dict&lt;/em&gt;. Only the following meta fields in the &lt;em&gt;info dict&lt;/em&gt; are considered mandatory for a successful extraction process by youtube-dl:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt; (media identifier)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt; (media title)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;url&lt;/code&gt; (media download URL) or &lt;code&gt;formats&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ytdl-org/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L188-L303"&gt;Any field&lt;/a&gt; apart from the aforementioned ones are considered &lt;strong&gt;optional&lt;/strong&gt;. That means that extraction should be &lt;strong&gt;tolerant&lt;/strong&gt; to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and &lt;strong&gt;future-proof&lt;/strong&gt; in order not to break the extraction of general purpose mandatory fields.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example" class="anchor" aria-hidden="true" href="#example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Say you have some source dictionary &lt;code&gt;meta&lt;/code&gt; that you've fetched as JSON with HTTP request and it has a key &lt;code&gt;summary&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;meta &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._download_json(url, video_id)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Assume at this point &lt;code&gt;meta&lt;/code&gt;'s layout is:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;{
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;summary&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;some fancy summary text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Assume you want to extract &lt;code&gt;summary&lt;/code&gt; and put it into the resulting info dict as &lt;code&gt;description&lt;/code&gt;. Since &lt;code&gt;description&lt;/code&gt; is an optional meta field you should be ready that this key may be missing from the &lt;code&gt;meta&lt;/code&gt; dict, so that you should extract it like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; meta.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; correct&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and not like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; meta[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; incorrect&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The latter will break extraction process with &lt;code&gt;KeyError&lt;/code&gt; if &lt;code&gt;summary&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; at some later time but with the former approach extraction will just go ahead with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt; which is perfectly fine (remember &lt;code&gt;None&lt;/code&gt; is equivalent to the absence of data).&lt;/p&gt;
&lt;p&gt;Similarly, you should pass &lt;code&gt;fatal=False&lt;/code&gt; when extracting optional data from a webpage with &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt; or similar methods, for instance:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;id="title"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;fatal&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With &lt;code&gt;fatal&lt;/code&gt; set to &lt;code&gt;False&lt;/code&gt; if &lt;code&gt;_search_regex&lt;/code&gt; fails to extract &lt;code&gt;description&lt;/code&gt; it will emit a warning and continue extraction.&lt;/p&gt;
&lt;p&gt;You can also pass &lt;code&gt;default=&amp;lt;some fallback value&amp;gt;&lt;/code&gt;, for example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;id="title"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On failure this code will silently continue the extraction with &lt;code&gt;description&lt;/code&gt; set to &lt;code&gt;None&lt;/code&gt;. That is useful for metafields that may or may not be present.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-provide-fallbacks" class="anchor" aria-hidden="true" href="#provide-fallbacks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Provide fallbacks&lt;/h3&gt;
&lt;p&gt;When extracting metadata try to do so from multiple sources. For example if &lt;code&gt;title&lt;/code&gt; is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-1" class="anchor" aria-hidden="true" href="#example-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Say &lt;code&gt;meta&lt;/code&gt; from the previous example has a &lt;code&gt;title&lt;/code&gt; and you are about to extract it. Since &lt;code&gt;title&lt;/code&gt; is a mandatory meta field you should end up with something like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; meta[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If &lt;code&gt;title&lt;/code&gt; disappears from &lt;code&gt;meta&lt;/code&gt; in future due to some changes on the hoster's side the extraction would fail since &lt;code&gt;title&lt;/code&gt; is mandatory. That's expected.&lt;/p&gt;
&lt;p&gt;Assume that you have some another source you can extract &lt;code&gt;title&lt;/code&gt; from, for example &lt;code&gt;og:title&lt;/code&gt; HTML meta of a &lt;code&gt;webpage&lt;/code&gt;. In this case you can provide a fallback scenario:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; meta.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._og_search_title(webpage)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code will try to extract from &lt;code&gt;meta&lt;/code&gt; first and if it fails it will try extracting &lt;code&gt;og:title&lt;/code&gt; from a &lt;code&gt;webpage&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-regular-expressions" class="anchor" aria-hidden="true" href="#regular-expressions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Regular expressions&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-dont-capture-groups-you-dont-use" class="anchor" aria-hidden="true" href="#dont-capture-groups-you-dont-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Don't capture groups you don't use&lt;/h4&gt;
&lt;p&gt;Capturing group must be an indication that it's used somewhere in the code. Any group that is not used must be non capturing.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-example-2" class="anchor" aria-hidden="true" href="#example-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h5&gt;
&lt;p&gt;Don't capture id attribute name here since you can't use it for anything anyway.&lt;/p&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;(?:id|ID)=(?P&amp;lt;id&amp;gt;\d+)&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;(id|ID)=(?P&amp;lt;id&amp;gt;\d+)&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-make-regular-expressions-relaxed-and-flexible" class="anchor" aria-hidden="true" href="#make-regular-expressions-relaxed-and-flexible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Make regular expressions relaxed and flexible&lt;/h4&gt;
&lt;p&gt;When using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-example-3" class="anchor" aria-hidden="true" href="#example-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h5&gt;
&lt;p&gt;Say you need to extract &lt;code&gt;title&lt;/code&gt; from the following HTML code:&lt;/p&gt;
&lt;div class="highlight highlight-text-html-basic"&gt;&lt;pre&gt;&amp;lt;&lt;span class="pl-ent"&gt;span&lt;/span&gt; &lt;span class="pl-e"&gt;style&lt;/span&gt;=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s1"&gt;&lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;position&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;absolute&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;left&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;910&lt;span class="pl-k"&gt;px&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;width&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;90&lt;span class="pl-k"&gt;px&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;float&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;right&lt;/span&gt;; &lt;span class="pl-c1"&gt;&lt;span class="pl-c1"&gt;z-index&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;9999&lt;/span&gt;;&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-e"&gt;class&lt;/span&gt;=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;title&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&amp;gt;some fancy title&amp;lt;/&lt;span class="pl-ent"&gt;span&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The code for that task should look similar to:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;class="title"[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or even better:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;class=&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-c1"&gt;"&lt;/span&gt;&lt;span class="pl-cce"&gt;\'&lt;/span&gt;]&lt;span class="pl-c1"&gt;)&lt;/span&gt;title&lt;span class="pl-ent"&gt;\1&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;gt;&lt;/span&gt;]&lt;span class="pl-k"&gt;*&lt;/span&gt;&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-ent"&gt;?P&amp;lt;title&amp;gt;&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;group&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how you tolerate potential changes in the &lt;code&gt;style&lt;/code&gt; attribute's value or switch from using double quotes to single for &lt;code&gt;class&lt;/code&gt; attribute:&lt;/p&gt;
&lt;p&gt;The code definitely should not look like:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._search_regex(
    &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;span style="position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;" class="title"&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;&lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;span class="pl-k"&gt;*?&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/span&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;group&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-long-lines-policy" class="anchor" aria-hidden="true" href="#long-lines-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Long lines policy&lt;/h3&gt;
&lt;p&gt;There is a soft limit to keep lines of code under 80 characters long. This means it should be respected if possible and if it does not make readability and code maintenance worse.&lt;/p&gt;
&lt;p&gt;For example, you should &lt;strong&gt;never&lt;/strong&gt; split long string literals like URLs or some other often copied entities over multiple lines to fit this limit:&lt;/p&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=FqZTN594JQw&amp;amp;list=&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-inline-values" class="anchor" aria-hidden="true" href="#inline-values"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inline values&lt;/h3&gt;
&lt;p&gt;Extracting variables is acceptable for reducing code duplication and improving readability of complex expressions. However, you should avoid extracting variables used only once and moving them to opposite parts of the extractor file, which makes reading the linear flow difficult.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-4" class="anchor" aria-hidden="true" href="#example-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_regex(&lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;title&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/title&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;TITLE_RE&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-sr"&gt;&lt;span class="pl-k"&gt;r&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;lt;title&amp;gt;&lt;span class="pl-c1"&gt;(&lt;/span&gt;[&lt;span class="pl-k"&gt;^&lt;/span&gt;&lt;span class="pl-c1"&gt;&amp;lt;&lt;/span&gt;]&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;)&lt;/span&gt;&amp;lt;/title&amp;gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...some lines of code...&lt;/span&gt;
title &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_regex(&lt;span class="pl-c1"&gt;TITLE_RE&lt;/span&gt;, webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;title&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-collapse-fallbacks" class="anchor" aria-hidden="true" href="#collapse-fallbacks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Collapse fallbacks&lt;/h3&gt;
&lt;p&gt;Multiple fallback values can quickly become unwieldy. Collapse multiple fallback values into a single expression via a list of patterns.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-5" class="anchor" aria-hidden="true" href="#example-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Good:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_meta(
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;og:description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;twitter:description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
    webpage, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Unwieldy:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; (
    &lt;span class="pl-c1"&gt;self&lt;/span&gt;._og_search_description(webpage, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)
    &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_meta(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)
    &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;._html_search_meta(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;twitter:description&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, webpage, &lt;span class="pl-v"&gt;default&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Methods supporting list of patterns are: &lt;code&gt;_search_regex&lt;/code&gt;, &lt;code&gt;_html_search_regex&lt;/code&gt;, &lt;code&gt;_og_search_property&lt;/code&gt;, &lt;code&gt;_html_search_meta&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-trailing-parentheses" class="anchor" aria-hidden="true" href="#trailing-parentheses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trailing parentheses&lt;/h3&gt;
&lt;p&gt;Always move trailing parentheses after the last argument.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-example-6" class="anchor" aria-hidden="true" href="#example-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h4&gt;
&lt;p&gt;Correct:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ResultSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrlSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
    &lt;span class="pl-c1"&gt;list&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Incorrect:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ResultSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrlSet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;VideoUrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
    &lt;span class="pl-c1"&gt;list&lt;/span&gt;,
)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-use-convenience-conversion-and-parsing-functions" class="anchor" aria-hidden="true" href="#use-convenience-conversion-and-parsing-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use convenience conversion and parsing functions&lt;/h3&gt;
&lt;p&gt;Wrap all extracted numeric data into safe functions from &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt;: &lt;code&gt;int_or_none&lt;/code&gt;, &lt;code&gt;float_or_none&lt;/code&gt;. Use them for string to number conversions as well.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;url_or_none&lt;/code&gt; for safe URL processing.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;try_get&lt;/code&gt; for safe metadata extraction from parsed JSON.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;unified_strdate&lt;/code&gt; for uniform &lt;code&gt;upload_date&lt;/code&gt; or any &lt;code&gt;YYYYMMDD&lt;/code&gt; meta field extraction, &lt;code&gt;unified_timestamp&lt;/code&gt; for uniform &lt;code&gt;timestamp&lt;/code&gt; extraction, &lt;code&gt;parse_filesize&lt;/code&gt; for &lt;code&gt;filesize&lt;/code&gt; extraction, &lt;code&gt;parse_count&lt;/code&gt; for count meta fields extraction, &lt;code&gt;parse_resolution&lt;/code&gt;, &lt;code&gt;parse_duration&lt;/code&gt; for &lt;code&gt;duration&lt;/code&gt; extraction, &lt;code&gt;parse_age_limit&lt;/code&gt; for &lt;code&gt;age_limit&lt;/code&gt; extraction.&lt;/p&gt;
&lt;p&gt;Explore &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/youtube_dl/utils.py"&gt;&lt;code&gt;youtube_dl/utils.py&lt;/code&gt;&lt;/a&gt; for more useful convenience functions.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-more-examples" class="anchor" aria-hidden="true" href="#more-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More examples&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-safely-extract-optional-description-from-parsed-json" class="anchor" aria-hidden="true" href="#safely-extract-optional-description-from-parsed-json"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Safely extract optional description from parsed JSON&lt;/h5&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;description &lt;span class="pl-k"&gt;=&lt;/span&gt; try_get(response, &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;video&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], compat_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-safely-extract-more-optional-metadata" class="anchor" aria-hidden="true" href="#safely-extract-more-optional-metadata"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Safely extract more optional metadata&lt;/h5&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;video &lt;span class="pl-k"&gt;=&lt;/span&gt; try_get(response, &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;result&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;video&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-c1"&gt;dict&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; {}
description &lt;span class="pl-k"&gt;=&lt;/span&gt; video.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;summary&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
duration &lt;span class="pl-k"&gt;=&lt;/span&gt; float_or_none(video.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;durationMs&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;), &lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1000&lt;/span&gt;)
view_count &lt;span class="pl-k"&gt;=&lt;/span&gt; int_or_none(video.get(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;views&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;))&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-embedding-youtube-dl" class="anchor" aria-hidden="true" href="#embedding-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;EMBEDDING YOUTUBE-DL&lt;/h1&gt;
&lt;p&gt;youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/new"&gt;create a report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From a Python program, you can embed youtube-dl in a more powerful fashion, like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;__future__&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; unicode_literals
&lt;span class="pl-k"&gt;import&lt;/span&gt; youtube_dl

ydl_opts &lt;span class="pl-k"&gt;=&lt;/span&gt; {}
&lt;span class="pl-k"&gt;with&lt;/span&gt; youtube_dl.YoutubeDL(ydl_opts) &lt;span class="pl-k"&gt;as&lt;/span&gt; ydl:
    ydl.download([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Most likely, you'll want to use various options. For a list of options available, have a look at &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/3e4cedf9e8cd3157df2457df7274d0c842421945/youtube_dl/YoutubeDL.py#L137-L312"&gt;&lt;code&gt;youtube_dl/YoutubeDL.py&lt;/code&gt;&lt;/a&gt;. For a start, if you want to intercept youtube-dl's output, set a &lt;code&gt;logger&lt;/code&gt; object.&lt;/p&gt;
&lt;p&gt;Here's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; &lt;span class="pl-c1"&gt;__future__&lt;/span&gt; &lt;span class="pl-k"&gt;import&lt;/span&gt; unicode_literals
&lt;span class="pl-k"&gt;import&lt;/span&gt; youtube_dl


&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;MyLogger&lt;/span&gt;(&lt;span class="pl-c1"&gt;object&lt;/span&gt;):
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;debug&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;msg&lt;/span&gt;):
        &lt;span class="pl-k"&gt;pass&lt;/span&gt;

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;warning&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;msg&lt;/span&gt;):
        &lt;span class="pl-k"&gt;pass&lt;/span&gt;

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;error&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;msg&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(msg)


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;my_hook&lt;/span&gt;(&lt;span class="pl-smi"&gt;d&lt;/span&gt;):
    &lt;span class="pl-k"&gt;if&lt;/span&gt; d[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;status&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;finished&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Done downloading, now converting ...&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)


ydl_opts &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;format&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bestaudio/best&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;postprocessors&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [{
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;key&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;FFmpegExtractAudio&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;preferredcodec&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mp3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;preferredquality&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;192&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    }],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;logger&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: MyLogger(),
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;progress_hooks&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [my_hook],
}
&lt;span class="pl-k"&gt;with&lt;/span&gt; youtube_dl.YoutubeDL(ydl_opts) &lt;span class="pl-k"&gt;as&lt;/span&gt; ydl:
    ydl.download([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-bugs" class="anchor" aria-hidden="true" href="#bugs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BUGS&lt;/h1&gt;
&lt;p&gt;Bugs and suggestions should be reported at: &lt;a href="https://github.com/ytdl-org/youtube-dl/issues"&gt;https://github.com/ytdl-org/youtube-dl/issues&lt;/a&gt;. Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel #youtube-dl on freenode (&lt;a href="https://webchat.freenode.net/?randomnick=1&amp;amp;channels=youtube-dl" rel="nofollow"&gt;webchat&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please include the full output of youtube-dl when run with &lt;code&gt;-v&lt;/code&gt;&lt;/strong&gt;, i.e. &lt;strong&gt;add&lt;/strong&gt; &lt;code&gt;-v&lt;/code&gt; flag to &lt;strong&gt;your command line&lt;/strong&gt;, copy the &lt;strong&gt;whole&lt;/strong&gt; output and post it in the issue body wrapped in ``` for better formatting. It should look similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ youtube-dl -v &amp;lt;your command line&amp;gt;
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2015.12.06
[debug] Git HEAD: 135392e
[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Do not post screenshots of verbose logs; only plain text is acceptable.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.&lt;/p&gt;
&lt;p&gt;Please re-read your issue once again to avoid a couple of common mistakes (you can and should use this as a checklist):&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-the-description-of-the-issue-itself-sufficient" class="anchor" aria-hidden="true" href="#is-the-description-of-the-issue-itself-sufficient"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is the description of the issue itself sufficient?&lt;/h3&gt;
&lt;p&gt;We often get issue reports that we cannot really decipher. While in most cases we eventually get the required information after asking back multiple times, this poses an unnecessary drain on our resources. Many contributors, including myself, are also not native speakers, so we may misread some parts.&lt;/p&gt;
&lt;p&gt;So please elaborate on what feature you are requesting, or what bug you want to be fixed. Make sure that it's obvious&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What the problem is&lt;/li&gt;
&lt;li&gt;How it could be fixed&lt;/li&gt;
&lt;li&gt;How your proposed solution would look like&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.&lt;/p&gt;
&lt;p&gt;For bug reports, this means that your report should contain the &lt;em&gt;complete&lt;/em&gt; output of youtube-dl when called with the &lt;code&gt;-v&lt;/code&gt; flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.&lt;/p&gt;
&lt;p&gt;If your server has multiple IPs or you suspect censorship, adding &lt;code&gt;--call-home&lt;/code&gt; may be a good idea to get more diagnostics. If the error is &lt;code&gt;ERROR: Unable to extract ...&lt;/code&gt; and you cannot reproduce it from multiple countries, add &lt;code&gt;--dump-pages&lt;/code&gt; (warning: this will yield a rather large output, redirect it to the file &lt;code&gt;log.txt&lt;/code&gt; by adding &lt;code&gt;&amp;gt;log.txt 2&amp;gt;&amp;amp;1&lt;/code&gt; to your command-line) or upload the &lt;code&gt;.dump&lt;/code&gt; files you get when you add &lt;code&gt;--write-pages&lt;/code&gt; &lt;a href="https://gist.github.com/"&gt;somewhere&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Site support requests must contain an example URL&lt;/strong&gt;. An example URL is a URL you might want to download, like &lt;code&gt;https://www.youtube.com/watch?v=BaW_jenozKc&lt;/code&gt;. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. &lt;code&gt;https://www.youtube.com/&lt;/code&gt;) is &lt;em&gt;not&lt;/em&gt; an example URL.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-are-you-using-the-latest-version" class="anchor" aria-hidden="true" href="#are-you-using-the-latest-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Are you using the latest version?&lt;/h3&gt;
&lt;p&gt;Before reporting any issue, type &lt;code&gt;youtube-dl -U&lt;/code&gt;. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-the-issue-already-documented" class="anchor" aria-hidden="true" href="#is-the-issue-already-documented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is the issue already documented?&lt;/h3&gt;
&lt;p&gt;Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the &lt;a href="https://github.com/ytdl-org/youtube-dl/search?type=Issues"&gt;GitHub Issues&lt;/a&gt; of this repository. If there is an issue, feel free to write something along the lines of "This affects me as well, with version 2015.01.01. Here is some more information on the issue: ...". While some issues may be old, a new post into them often spurs rapid activity.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-why-are-existing-options-not-enough" class="anchor" aria-hidden="true" href="#why-are-existing-options-not-enough"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why are existing options not enough?&lt;/h3&gt;
&lt;p&gt;Before requesting a new feature, please have a quick peek at &lt;a href="https://github.com/ytdl-org/youtube-dl/blob/master/README.md#options"&gt;the list of supported options&lt;/a&gt;. Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do &lt;em&gt;not&lt;/em&gt; solve your problem.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-there-enough-context-in-your-bug-report" class="anchor" aria-hidden="true" href="#is-there-enough-context-in-your-bug-report"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there enough context in your bug report?&lt;/h3&gt;
&lt;p&gt;People want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).&lt;/p&gt;
&lt;p&gt;We are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-does-the-issue-involve-one-problem-and-one-problem-only" class="anchor" aria-hidden="true" href="#does-the-issue-involve-one-problem-and-one-problem-only"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Does the issue involve one problem, and one problem only?&lt;/h3&gt;
&lt;p&gt;Some of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.&lt;/p&gt;
&lt;p&gt;In particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-anyone-going-to-need-the-feature" class="anchor" aria-hidden="true" href="#is-anyone-going-to-need-the-feature"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is anyone going to need the feature?&lt;/h3&gt;
&lt;p&gt;Only post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-is-your-question-about-youtube-dl" class="anchor" aria-hidden="true" href="#is-your-question-about-youtube-dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is your question about youtube-dl?&lt;/h3&gt;
&lt;p&gt;It may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;COPYRIGHT&lt;/h1&gt;
&lt;p&gt;youtube-dl is released into the public domain by the copyright holders.&lt;/p&gt;
&lt;p&gt;This README file was originally written by &lt;a href="https://github.com/dbbolton"&gt;Daniel Bolton&lt;/a&gt; and is likewise released into the public domain.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ytdl-org</author><guid isPermaLink="false">https://github.com/ytdl-org/youtube-dl</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>tensorflow/models #19 in Python, This week</title><link>https://github.com/tensorflow/models</link><description>&lt;p&gt;&lt;i&gt;Models and examples built with TensorFlow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-models" class="anchor" aria-hidden="true" href="#tensorflow-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Models&lt;/h1&gt;
&lt;p&gt;This repository contains a number of different models implemented in &lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;The &lt;a href="official"&gt;official models&lt;/a&gt; are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/tensorflow/models/tree/master/research"&gt;research models&lt;/a&gt; are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.&lt;/p&gt;
&lt;p&gt;The &lt;a href="samples"&gt;samples folder&lt;/a&gt; contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.&lt;/p&gt;
&lt;p&gt;The &lt;a href="tutorials"&gt;tutorials folder&lt;/a&gt; is a collection of models described in the &lt;a href="https://www.tensorflow.org/tutorials/" rel="nofollow"&gt;TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to models, be sure to review the &lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/models</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>pytorch/examples #20 in Python, This week</title><link>https://github.com/pytorch/examples</link><description>&lt;p&gt;&lt;i&gt;A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-examples" class="anchor" aria-hidden="true" href="#pytorch-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Examples&lt;/h1&gt;
&lt;p&gt;A repository showcasing examples of using &lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mnist"&gt;Image classification (MNIST) using Convnets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="word_language_model"&gt;Word level Language Modeling using LSTM RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="imagenet"&gt;Training Imagenet Classifiers with Residual Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="dcgan"&gt;Generative Adversarial Networks (DCGAN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vae"&gt;Variational Auto-Encoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="super_resolution"&gt;Superresolution using an efficient sub-pixel convolutional neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mnist_hogwild"&gt;Hogwild training of shared ConvNets across multiple processes on MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning"&gt;Training a CartPole to balance in OpenAI Gym with actor-critic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="snli"&gt;Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="time_sequence_prediction"&gt;Time sequence prediction - use an LSTM to learn Sine waves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="fast_neural_style"&gt;Implement the Neural Style Transfer algorithm on images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="cpp"&gt;Several examples illustrating the C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, a list of good examples hosted in their own repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenNMT/OpenNMT-py"&gt;Neural Machine Translation using sequence-to-sequence RNN with attention (OpenNMT)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/examples</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>getsentry/sentry #21 in Python, This week</title><link>https://github.com/getsentry/sentry</link><description>&lt;p&gt;&lt;i&gt;Sentry is cross-platform application monitoring, with a focus on error reporting.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://sentry.io/?utm_source=github&amp;amp;utm_medium=logo" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/2dfeafbee0904d6df16ddf7200993dace1629e60/68747470733a2f2f73656e7472792d6272616e642e73746f726167652e676f6f676c65617069732e636f6d2f73656e7472792d6c6f676f2d626c61636b2e706e67" alt="Sentry" height="72" data-canonical-src="https://sentry-brand.storage.googleapis.com/sentry-logo-black.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align="center"&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;a name="user-content-what-s-sentry"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-whats-sentry" class="anchor" aria-hidden="true" href="#whats-sentry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's Sentry?&lt;/h2&gt;
&lt;p&gt;Sentry fundamentally is a service that helps you monitor and fix crashes in realtime.
The server is in Python, but it contains a full API for sending events from any
language, in any application.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;&lt;a name="user-content-official-sentry-sdks"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-official-sentry-sdks" class="anchor" aria-hidden="true" href="#official-sentry-sdks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official Sentry SDKs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-javascript"&gt;JavaScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/react-native-sentry"&gt;React-Native&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/raven-ruby"&gt;Ruby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-php"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-go"&gt;Go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-java"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-cocoa"&gt;Objective-C/Swift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-dotnet"&gt;C#&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/perl-raven"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-elixir"&gt;Elixir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-laravel"&gt;Laravel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-resources"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.sentry.io/" rel="nofollow"&gt;Community&lt;/a&gt; (Bugs, feature requests, general questions)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/internal/contributing/" rel="nofollow"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry/issues"&gt;Bug Tracker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRC  (irc.freenode.net, #sentry)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.transifex.com/getsentry/sentry/" rel="nofollow"&gt;Transifex&lt;/a&gt; (Translate Sentry!)&lt;/li&gt;
&lt;/ul&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>getsentry</author><guid isPermaLink="false">https://github.com/getsentry/sentry</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>scikit-learn/scikit-learn #22 in Python, This week</title><link>https://github.com/scikit-learn/scikit-learn</link><description>&lt;p&gt;&lt;i&gt;scikit-learn: machine learning in Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img alt="Azure" src="https://camo.githubusercontent.com/bfe67a3604768c16e941f3331709bf55507a4b57/68747470733a2f2f6465762e617a7572652e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f5f617069732f6275696c642f7374617475732f7363696b69742d6c6561726e2e7363696b69742d6c6561726e3f6272616e63684e616d653d6d6173746572" data-canonical-src="https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/590475799489c962f111c9fc5c1432ecbc577578/68747470733a2f2f6170692e7472617669732d63692e6f72672f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/scikit-learn/scikit-learn?branch=master" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/58a0b06906ca5d106ec090fe8a1ac85a092b81c2/68747470733a2f2f636f6465636f762e696f2f6769746875622f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" data-canonical-src="https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&amp;amp;service=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d2913194913f85128f908483a265e64dcd6d31e4/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d3a636972636c652d746f6b656e" data-canonical-src="https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&amp;amp;circle-token=:circle-token" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="Python35" src="https://camo.githubusercontent.com/53aa0b9151bc545b404852175644228ee0efffe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e352d626c75652e737667" data-canonical-src="https://img.shields.io/badge/python-3.5-blue.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/9f0ed32d05350afa18a801573e4da7f4a240e181/68747470733a2f2f62616467652e667572792e696f2f70792f7363696b69742d6c6561726e2e737667" data-canonical-src="https://badge.fury.io/py/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="DOI" src="https://camo.githubusercontent.com/73c63e44b8bee62df142664048c58f83ec8ad95c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f32313336392f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e737667" data-canonical-src="https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-scikit-learn"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-scikit-learn" class="anchor" aria-hidden="true" href="#scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;scikit-learn&lt;/h2&gt;
&lt;p&gt;scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;It is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;a name="user-content-dependencies"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;p&gt;scikit-learn requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python (&amp;gt;= 3.5)&lt;/li&gt;
&lt;li&gt;NumPy (&amp;gt;= 1.11.0)&lt;/li&gt;
&lt;li&gt;SciPy (&amp;gt;= 0.17.0)&lt;/li&gt;
&lt;li&gt;joblib (&amp;gt;= 0.11)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.&lt;/strong&gt;
scikit-learn 0.21 and later require Python 3.5 or newer.&lt;/p&gt;
&lt;p&gt;Scikit-learn plotting capabilities (i.e., functions start with "&lt;a href="#id2"&gt;&lt;span id="user-content-id3"&gt;plot_&lt;/span&gt;&lt;/a&gt;"
and classes end with "Display") require Matplotlib (&amp;gt;= 1.5.1). For running the
examples Matplotlib &amp;gt;= 1.5.1 is required. A few examples require
scikit-image &amp;gt;= 0.12.3, a few examples require pandas &amp;gt;= 0.18.0.&lt;/p&gt;
&lt;a name="user-content-user-installation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-user-installation" class="anchor" aria-hidden="true" href="#user-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User installation&lt;/h4&gt;
&lt;p&gt;If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;pip install -U scikit-learn
&lt;/pre&gt;
&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;conda install scikit-learn
&lt;/pre&gt;
&lt;p&gt;The documentation includes more detailed &lt;a href="http://scikit-learn.org/stable/install.html" rel="nofollow"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-changelog"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h3&gt;
&lt;p&gt;See the &lt;a href="http://scikit-learn.org/dev/whats_new.html" rel="nofollow"&gt;changelog&lt;/a&gt;
for a history of notable changes to scikit-learn.&lt;/p&gt;
&lt;a name="user-content-development"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
&lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;Development Guide&lt;/a&gt;
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.&lt;/p&gt;
&lt;a name="user-content-important-links"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-important-links" class="anchor" aria-hidden="true" href="#important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important links&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Official source code repo: &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download releases: &lt;a href="https://pypi.org/project/scikit-learn/" rel="nofollow"&gt;https://pypi.org/project/scikit-learn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/scikit-learn/scikit-learn/issues"&gt;https://github.com/scikit-learn/scikit-learn/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-source-code"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-source-code" class="anchor" aria-hidden="true" href="#source-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source code&lt;/h4&gt;
&lt;p&gt;You can check the latest sources with the command:&lt;/p&gt;
&lt;pre&gt;git clone https://github.com/scikit-learn/scikit-learn.git
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h4&gt;
&lt;p&gt;To learn more about making a contribution to scikit-learn, please see our
&lt;a href="https://scikit-learn.org/dev/developers/contributing.html" rel="nofollow"&gt;Contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-testing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h4&gt;
&lt;p&gt;After installation, you can launch the test suite from outside the
source directory (you will need to have &lt;code&gt;pytest&lt;/code&gt; &amp;gt;= 3.3.0 installed):&lt;/p&gt;
&lt;pre&gt;pytest sklearn
&lt;/pre&gt;
&lt;p&gt;See the web page &lt;a href="http://scikit-learn.org/dev/developers/advanced_installation.html#testing" rel="nofollow"&gt;http://scikit-learn.org/dev/developers/advanced_installation.html#testing&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;blockquote&gt;
Random number generation can be controlled during testing by setting
the &lt;code&gt;SKLEARN_SEED&lt;/code&gt; environment variable.&lt;/blockquote&gt;
&lt;a name="user-content-submitting-a-pull-request"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-submitting-a-pull-request" class="anchor" aria-hidden="true" href="#submitting-a-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Submitting a Pull Request&lt;/h4&gt;
&lt;p&gt;Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: &lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;http://scikit-learn.org/stable/developers/index.html&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-project-history"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-project-history" class="anchor" aria-hidden="true" href="#project-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project History&lt;/h3&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the  &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;The project is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: scikit-learn was previously referred to as scikits.learn.&lt;/p&gt;
&lt;a name="user-content-help-and-support"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-help-and-support" class="anchor" aria-hidden="true" href="#help-and-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help and Support&lt;/h3&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HTML documentation (stable release): &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HTML documentation (development version): &lt;a href="http://scikit-learn.org/dev/" rel="nofollow"&gt;http://scikit-learn.org/dev/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FAQ: &lt;a href="http://scikit-learn.org/stable/faq.html" rel="nofollow"&gt;http://scikit-learn.org/stable/faq.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-communication"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href="https://mail.python.org/mailman/listinfo/scikit-learn" rel="nofollow"&gt;https://mail.python.org/mailman/listinfo/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRC channel: &lt;code&gt;#scikit-learn&lt;/code&gt; at &lt;code&gt;webchat.freenode.net&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stack Overflow: &lt;a href="https://stackoverflow.com/questions/tagged/scikit-learn" rel="nofollow"&gt;https://stackoverflow.com/questions/tagged/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-citation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h4&gt;
&lt;p&gt;If you use scikit-learn in a scientific publication, we would appreciate citations: &lt;a href="http://scikit-learn.org/stable/about.html#citing-scikit-learn" rel="nofollow"&gt;http://scikit-learn.org/stable/about.html#citing-scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>scikit-learn</author><guid isPermaLink="false">https://github.com/scikit-learn/scikit-learn</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>CorentinJ/Real-Time-Voice-Cloning #23 in Python, This week</title><link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link><description>&lt;p&gt;&lt;i&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-time-voice-cloning" class="anchor" aria-hidden="true" href="#real-time-voice-cloning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real-Time Voice Cloning&lt;/h1&gt;
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. Feel free to check &lt;a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow"&gt;my thesis&lt;/a&gt; if you're curious or if you're looking for info I haven't documented yet (don't hesitate to make an issue for that too). Mostly I would recommend giving a quick look to the figures beyond the introduction.&lt;/p&gt;
&lt;p&gt;SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9c33f78be8afe656503da974c478ea2ba2647db7/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-papers-implemented" class="anchor" aria-hidden="true" href="#papers-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers implemented&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Designation&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Implementation source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow"&gt;1802.08435&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;WaveRNN (vocoder)&lt;/td&gt;
&lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;1712.05884&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tacotron 2 (synthesizer)&lt;/td&gt;
&lt;td&gt;Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Rayhane-mamah/Tacotron-2"&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow"&gt;1710.10467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GE2E (encoder)&lt;/td&gt;
&lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;20/08/19:&lt;/strong&gt; I'm working on &lt;a href="https://github.com/resemble-ai/Resemblyzer"&gt;resemblyzer&lt;/a&gt;, an independent package for the voice encoder. You can use your trained encoder models from this repo with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;06/07/19:&lt;/strong&gt; Need to run within a docker container on a remote server? See &lt;a href="https://sean.lane.sh/posts/2019/07/Running-the-Real-Time-Voice-Cloning-project-in-Docker/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;25/06/19:&lt;/strong&gt; Experimental support for low-memory GPUs (~2gb) added for the synthesizer. Pass &lt;code&gt;--low_mem&lt;/code&gt; to &lt;code&gt;demo_cli.py&lt;/code&gt; or &lt;code&gt;demo_toolbox.py&lt;/code&gt; to enable it. It adds a big overhead, so it's not recommended if you have enough VRAM.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;You will need the following whether you plan to use the toolbox only or to retrain the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 3.7&lt;/strong&gt;. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the necessary packages. Additionally you will need &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch&lt;/a&gt; (&amp;gt;=1.0.1).&lt;/p&gt;
&lt;p&gt;A GPU is mandatory, but you don't necessarily need a high tier GPU if you only want to use the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained models&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary" class="anchor" aria-hidden="true" href="#preliminary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary&lt;/h3&gt;
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h3&gt;
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="http://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-toolbox" class="anchor" aria-hidden="true" href="#toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toolbox&lt;/h3&gt;
&lt;p&gt;You can then try the toolbox:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt;
or&lt;br&gt;
&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wiki&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it all works&lt;/strong&gt; (WIP - &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works"&gt;stub&lt;/a&gt;, you might be better off reading my thesis until it's done)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training"&gt;&lt;strong&gt;Training models yourself&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training with other data/languages&lt;/strong&gt; (WIP - see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-507864097"&gt;here&lt;/a&gt; for now)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/TODO-&amp;amp;-planned-features"&gt;&lt;strong&gt;TODO and planned features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;I'm working full-time as of June 2019. Replying to issues is very time-consuming, I can't always do it. I won't be making progress of my own on this repo, but I will still gladly merge PRs and accept contributions to the wiki. Don't hesitate to send me an email if you wish to contribute.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CorentinJ</author><guid isPermaLink="false">https://github.com/CorentinJ/Real-Time-Voice-Cloning</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>ansible/ansible #24 in Python, This week</title><link>https://github.com/ansible/ansible</link><description>&lt;p&gt;&lt;i&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications — automate in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com/ansible/&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible" rel="nofollow"&gt;&lt;img alt="PyPI version" src="https://camo.githubusercontent.com/1700ed8e65665052f4e72ba6ae9e1f1d7fddc6c6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f616e7369626c652e737667" data-canonical-src="https://img.shields.io/pypi/v/ansible.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/" rel="nofollow"&gt;&lt;img alt="Docs badge" src="https://camo.githubusercontent.com/dc37b81ae5ef1245837ee1f1547892e8345ccd4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/docs-latest-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;&lt;img alt="Chat badge" src="https://camo.githubusercontent.com/a36ab54aea33fa40f9d063c8804b9bbf1b6fbd47/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d4952432d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/chat-IRC-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://app.shippable.com/projects/573f79d02a8192902e20e34b" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/c4dd185960fb101604717a4c8965ac9ba2725e69/68747470733a2f2f6170692e736869707061626c652e636f6d2f70726f6a656374732f3537336637396430326138313932393032653230653334622f62616467653f6272616e63683d646576656c" data-canonical-src="https://api.shippable.com/projects/573f79d02a8192902e20e34b/badge?branch=devel" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/code_of_conduct.html" rel="nofollow"&gt;&lt;img alt="Ansible Code of Conduct" src="https://camo.githubusercontent.com/412f4c0b8d7289e25a69d8568bd02c1bf976f9fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532306f66253230636f6e647563742d416e7369626c652d73696c7665722e737667" data-canonical-src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information" rel="nofollow"&gt;&lt;img alt="Ansible mailing lists" src="https://camo.githubusercontent.com/74dd4958c493abf9d3105cbcd020e55aa5df90c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696c696e672532306c697374732d416e7369626c652d6f72616e67652e737667" data-canonical-src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="COPYING"&gt;&lt;img alt="Repository License" src="https://camo.githubusercontent.com/0ac7552afd56fbe0c2ce6722d54f68857aa92b82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c25323076332e302d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-ansible"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-ansible" class="anchor" aria-hidden="true" href="#ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ansible&lt;/h2&gt;
&lt;p&gt;Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on &lt;a href="https://ansible.com/" rel="nofollow"&gt;the Ansible website&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-design-principles"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-design-principles" class="anchor" aria-hidden="true" href="#design-principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Design Principles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Have a dead simple setup process and a minimal learning curve.&lt;/li&gt;
&lt;li&gt;Manage machines very quickly and in parallel.&lt;/li&gt;
&lt;li&gt;Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.&lt;/li&gt;
&lt;li&gt;Describe infrastructure in a language that is both machine and human
friendly.&lt;/li&gt;
&lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt;
&lt;li&gt;Manage new remote machines instantly, without bootstrapping any
software.&lt;/li&gt;
&lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt;
&lt;li&gt;Be usable as non-root.&lt;/li&gt;
&lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-use-ansible"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-use-ansible" class="anchor" aria-hidden="true" href="#use-ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use Ansible&lt;/h3&gt;
&lt;p&gt;You can install a released version of Ansible via &lt;code&gt;pip&lt;/code&gt;, a package manager, or
our &lt;a href="https://releases.ansible.com/ansible/" rel="nofollow"&gt;release repository&lt;/a&gt;. See our
&lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html" rel="nofollow"&gt;installation guide&lt;/a&gt; for details on installing Ansible
on a variety of platforms.&lt;/p&gt;
&lt;p&gt;Red Hat offers supported builds of &lt;a href="https://www.ansible.com/ansible-engine" rel="nofollow"&gt;Ansible Engine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved
in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt;
&lt;a name="user-content-get-involved"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-get-involved" class="anchor" aria-hidden="true" href="#get-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Involved&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/latest/community" rel="nofollow"&gt;Community
Information&lt;/a&gt; for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.&lt;/li&gt;
&lt;li&gt;Join a &lt;a href="https://github.com/ansible/community/wiki"&gt;Working Group&lt;/a&gt;, an organized community devoted to a specific technology domain or platform.&lt;/li&gt;
&lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt;
&lt;li&gt;Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.&lt;/li&gt;
&lt;li&gt;For a list of email lists, IRC channels and Working Groups, see the
&lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;Communication page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-branch-info"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-branch-info" class="anchor" aria-hidden="true" href="#branch-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Branch Info&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt;
&lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.html#common-environment-setup" rel="nofollow"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt;
&lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/release_and_maintenance.html" rel="nofollow"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-roadmap"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-roadmap" class="anchor" aria-hidden="true" href="#roadmap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Roadmap&lt;/h3&gt;
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/" rel="nofollow"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt;
&lt;a name="user-content-authors"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h3&gt;
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt;
and has contributions from over 4600 users (and growing). Thanks everyone!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ansible.com" rel="nofollow"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com" rel="nofollow"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-license"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;GNU General Public License v3.0&lt;/p&gt;
&lt;p&gt;See &lt;a href="COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>ansible</author><guid isPermaLink="false">https://github.com/ansible/ansible</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item><item><title>psf/black #25 in Python, This week</title><link>https://github.com/psf/black</link><description>&lt;p&gt;&lt;i&gt;The uncompromising Python code formatter&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png"&gt;&lt;img src="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png" alt="Black Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-the-uncompromising-code-formatter" class="anchor" aria-hidden="true" href="#the-uncompromising-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Uncompromising Code Formatter&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;a href="https://travis-ci.com/psf/black" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/0d62c6ce125db151bb0bc13cbc834c0d0522ed88/68747470733a2f2f7472617669732d63692e636f6d2f7073662f626c61636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/psf/black.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://black.readthedocs.io/en/stable/?badge=stable" rel="nofollow"&gt;&lt;img alt="Documentation Status" src="https://camo.githubusercontent.com/ab197284ad0cd9cae552157b75f815b640eba827/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626c61636b2f62616467652f3f76657273696f6e3d737461626c65" data-canonical-src="https://readthedocs.org/projects/black/badge/?version=stable" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/psf/black?branch=master" rel="nofollow"&gt;&lt;img alt="Coverage Status" src="https://camo.githubusercontent.com/b77842b75f031fbf9bbf690eb55585ae55a8321d/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7073662f626c61636b2f62616467652e7376673f6272616e63683d6d6173746572" data-canonical-src="https://coveralls.io/repos/github/psf/black/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black/blob/master/LICENSE"&gt;&lt;img alt="License: MIT" src="https://camo.githubusercontent.com/14a9abb7e83098f2949f26d2190e04fb1bd52c06/68747470733a2f2f626c61636b2e72656164746865646f63732e696f2f656e2f737461626c652f5f7374617469632f6c6963656e73652e737667" data-canonical-src="https://black.readthedocs.io/en/stable/_static/license.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/black/" rel="nofollow"&gt;&lt;img alt="PyPI" src="https://camo.githubusercontent.com/5f6551edb3716d9b0a5659caf441d987bb1527a6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626c61636b" data-canonical-src="https://img.shields.io/pypi/v/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/black" rel="nofollow"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/7b3026c6e1fecc9574cb4b93a3925e392bee087d/68747470733a2f2f706570792e746563682f62616467652f626c61636b" data-canonical-src="https://pepy.tech/badge/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Any color you like.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed,
determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time
and mental energy for more important matters.&lt;/p&gt;
&lt;p&gt;Blackened code looks the same regardless of the project you're reading. Formatting
becomes transparent after a while and you can focus on the content instead.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt;
&lt;p&gt;Try it out now using the &lt;a href="https://black.now.sh" rel="nofollow"&gt;Black Playground&lt;/a&gt;. Watch the
&lt;a href="https://youtu.be/esZLCuWs_2Y" rel="nofollow"&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Contents:&lt;/em&gt; &lt;strong&gt;&lt;a href="#installation-and-usage"&gt;Installation and usage&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#the-black-code-style"&gt;Code style&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#pyprojecttoml"&gt;pyproject.toml&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#editor-integration"&gt;Editor integration&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#blackd"&gt;blackd&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#version-control-integration"&gt;Version control integration&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#ignoring-unmodified-files"&gt;Ignoring unmodified files&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#used-by"&gt;Used by&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#testimonials"&gt;Testimonials&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#show-your-style"&gt;Show your style&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#contributing-to-black"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#change-log"&gt;Change Log&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.6.0+ to
run but you can reformat Python 2 code with it, too.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-command-line-options" class="anchor" aria-hidden="true" href="#command-line-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command line options&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; doesn't provide many options. You can list them by running &lt;code&gt;black --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;black [OPTIONS] [SRC]...

Options:
  -c, --code TEXT                 Format the code passed in as a string.
  -l, --line-length INTEGER       How many characters per line to allow.
                                  [default: 88]
  -t, --target-version [py27|py33|py34|py35|py36|py37|py38]
                                  Python versions that should be supported by
                                  Black's output. [default: per-file auto-
                                  detection]
  --py36                          Allow using Python 3.6-only syntax on all
                                  input files.  This will put trailing commas
                                  in function signatures and calls also after
                                  *args and **kwargs. Deprecated; use
                                  --target-version instead. [default: per-file
                                  auto-detection]
  --pyi                           Format all input files like typing stubs
                                  regardless of file extension (useful when
                                  piping source on standard input).
  -S, --skip-string-normalization
                                  Don't normalize string quotes or prefixes.
  --check                         Don't write the files back, just return the
                                  status.  Return code 0 means nothing would
                                  change.  Return code 1 means some files
                                  would be reformatted.  Return code 123 means
                                  there was an internal error.
  --diff                          Don't write the files back, just output a
                                  diff for each file on stdout.
  --fast / --safe                 If --fast given, skip temporary sanity
                                  checks. [default: --safe]
  --include TEXT                  A regular expression that matches files and
                                  directories that should be included on
                                  recursive searches.  An empty value means
                                  all files are included regardless of the
                                  name.  Use forward slashes for directories
                                  on all platforms (Windows, too).  Exclusions
                                  are calculated first, inclusions later.
                                  [default: \.pyi?$]
  --exclude TEXT                  A regular expression that matches files and
                                  directories that should be excluded on
                                  recursive searches.  An empty value means no
                                  paths are excluded. Use forward slashes for
                                  directories on all platforms (Windows, too).
                                  Exclusions are calculated first, inclusions
                                  later.  [default: /(\.eggs|\.git|\.hg|\.mypy
                                  _cache|\.nox|\.tox|\.venv|_build|buck-
                                  out|build|dist)/]
  -q, --quiet                     Don't emit non-error messages to stderr.
                                  Errors are still emitted, silence those with
                                  2&amp;gt;/dev/null.
  -v, --verbose                   Also emit messages to stderr about files
                                  that were not changed or were ignored due to
                                  --exclude=.
  --version                       Show the version and exit.
  --config PATH                   Read configuration from PATH.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a well-behaved Unix-style command-line tool:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it does nothing if no sources are passed to it;&lt;/li&gt;
&lt;li&gt;it will read from standard input and write to standard output if &lt;code&gt;-&lt;/code&gt; is used as the
filename;&lt;/li&gt;
&lt;li&gt;it only outputs messages to users on standard error;&lt;/li&gt;
&lt;li&gt;exits with code 0 unless an internal error occurred (or &lt;code&gt;--check&lt;/code&gt; was used).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-note-this-is-a-beta-product" class="anchor" aria-hidden="true" href="#note-this-is-a-beta-product"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NOTE: This is a beta product&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href="#used-by"&gt;successfully used&lt;/a&gt; by many projects, small and big. It
also sports a decent test suite. However, it is still very new. Things will probably be
wonky for a while. This is made explicit by the "Beta" trove classifier, as well as by
the "b" in the version number. What this means for you is that &lt;strong&gt;until the formatter
becomes stable, you should expect some formatting to change in the future&lt;/strong&gt;. That being
said, no drastic stylistic changes are planned, mostly responses to bug reports.&lt;/p&gt;
&lt;p&gt;Also, as a temporary safety measure, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still
produces a valid AST that is equivalent to the original. This slows it down. If you're
feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-black-code-style" class="anchor" aria-hidden="true" href="#the-black-code-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; reformats entire files in place. It is not configurable. It doesn't take
previous formatting into account. It doesn't reformat blocks that start with
&lt;code&gt;# fmt: off&lt;/code&gt; and end with &lt;code&gt;# fmt: on&lt;/code&gt;. &lt;code&gt;# fmt: on/off&lt;/code&gt; have to be on the same level of
indentation. It also recognizes &lt;a href="https://github.com/google/yapf"&gt;YAPF&lt;/a&gt;'s block comments
to the same effect, as a courtesy for straddling code.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-black-wraps-lines" class="anchor" aria-hidden="true" href="#how-black-wraps-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How &lt;em&gt;Black&lt;/em&gt; wraps lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; ignores previous formatting and applies uniform horizontal and vertical
whitespace to your code. The rules for horizontal whitespace can be summarized as: do
whatever makes &lt;code&gt;pycodestyle&lt;/code&gt; happy. The coding style used by &lt;em&gt;Black&lt;/em&gt; can be viewed as a
strict subset of PEP 8.&lt;/p&gt;
&lt;p&gt;As for vertical whitespace, &lt;em&gt;Black&lt;/em&gt; tries to render one full expression or simple
statement per line. If this fits the allotted line length, great.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;2&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;3&lt;/span&gt;,
]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If not, &lt;em&gt;Black&lt;/em&gt; will look at the contents of the first outer matching brackets and put
that in a separate indented line.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

ImportantClass.important_method(exc, limit, lookup_lines, capture_locals, extra_argument)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

ImportantClass.important_method(
    exc, limit, lookup_lines, capture_locals, extra_argument
)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If that still doesn't fit the bill, it will decompose the internal expression further
using the same rule, indenting matching brackets every time. If the contents of the
matching brackets pair are comma-separated (like an argument list, or a dict literal,
and so on) then &lt;em&gt;Black&lt;/em&gt; will first try to keep them on the same line with the matching
brackets. If that doesn't work, it will put all of them in separate lines.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(&lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;, &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike, &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;w&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(
    &lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike,
    &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;,
):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;w&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You might have noticed that closing brackets are always dedented and that a trailing
comma is always added. Such formatting produces smaller diffs; when you add or remove an
element, it's always just one line. Also, having the closing bracket dedented provides a
clear delimiter between two distinct sections of the code that otherwise share the same
indentation level (like the arguments list and the docstring in the example above).&lt;/p&gt;
&lt;p&gt;If a data structure literal (tuple, list, set, dict) or a line of "from" imports cannot
fit in the allotted length, it's always split into one element per line. This minimizes
diffs as well as enables readers of code to find which commit introduced a particular
entry. This also makes &lt;em&gt;Black&lt;/em&gt; compatible with &lt;a href="https://pypi.org/p/isort/" rel="nofollow"&gt;isort&lt;/a&gt; with
the following configuration.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;A compatible `.isort.cfg`&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;[settings]
multi_line_output=3
include_trailing_comma=True
force_grid_wrap=0
use_parentheses=True
line_length=88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equivalent command line is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ isort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88 [ file.py ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-line-length" class="anchor" aria-hidden="true" href="#line-length"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line length&lt;/h3&gt;
&lt;p&gt;You probably noticed the peculiar default line length. &lt;em&gt;Black&lt;/em&gt; defaults to 88 characters
per line, which happens to be 10% over 80. This number was found to produce
significantly shorter files than sticking with 80 (the most popular), or even 79 (used
by the standard library). In general,
&lt;a href="https://youtu.be/wf-BqAjZb8M?t=260" rel="nofollow"&gt;90-ish seems like the wise choice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're paid by the line of code you write, you can pass &lt;code&gt;--line-length&lt;/code&gt; with a lower
number. &lt;em&gt;Black&lt;/em&gt; will try to respect that. However, sometimes it won't be able to without
breaking other rules. In those rare cases, auto-formatted code will exceed your allotted
limit.&lt;/p&gt;
&lt;p&gt;You can also increase it, but remember that people with sight disabilities find it
harder to work with line lengths exceeding 100 characters. It also adversely affects
side-by-side diff review on typical screen resolutions. Long lines also make it harder
to present code neatly in documentation or talk slides.&lt;/p&gt;
&lt;p&gt;If you're using Flake8, you can bump &lt;code&gt;max-line-length&lt;/code&gt; to 88 and forget about it.
Alternatively, use &lt;a href="https://github.com/PyCQA/flake8-bugbear"&gt;Bugbear&lt;/a&gt;'s B950 warning
instead of E501 and keep the max line length at 80 which you are probably already using.
You'd do it like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-ini"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;[flake8]&lt;/span&gt;
&lt;span class="pl-k"&gt;max-line-length&lt;/span&gt; = 80
...
&lt;span class="pl-k"&gt;select&lt;/span&gt; = C,E,F,W,B,B950
&lt;span class="pl-k"&gt;ignore&lt;/span&gt; = E203, E501, W503&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll find &lt;em&gt;Black&lt;/em&gt;'s own .flake8 config file is configured like this. Explanation of
why W503 and E203 are disabled can be found further in this documentation. And if you're
curious about the reasoning behind B950,
&lt;a href="https://github.com/PyCQA/flake8-bugbear#opinionated-warnings"&gt;Bugbear's documentation&lt;/a&gt;
explains it. The tl;dr is "it's like highway speed limits, we won't bother you if you
overdo it by a few km/h".&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-empty-lines" class="anchor" aria-hidden="true" href="#empty-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Empty lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; avoids spurious vertical whitespace. This is in the spirit of PEP 8 which says
that in-function vertical whitespace should only be used sparingly.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will allow single empty lines inside functions, and single and double empty
lines on module level left by the original editors, except when they're within
parenthesized expressions. Since such expressions are always reformatted to fit minimal
space, this whitespace is lost.&lt;/p&gt;
&lt;p&gt;It will also insert proper spacing before and after function definitions. It's one line
before and after inner functions and two lines before and after module-level functions
and classes. &lt;em&gt;Black&lt;/em&gt; will not put empty lines between function/class definitions and
standalone comments that immediately precede the given function/class.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will enforce single empty lines between a class-level docstring and the first
following field or method. This conforms to
&lt;a href="https://www.python.org/dev/peps/pep-0257/#multi-line-docstrings" rel="nofollow"&gt;PEP 257&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; won't insert empty lines after function docstrings unless that empty line is
required due to an inner function starting immediately after.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-trailing-commas" class="anchor" aria-hidden="true" href="#trailing-commas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trailing commas&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will add trailing commas to expressions that are split by comma where each
element is on its own line. This includes function signatures.&lt;/p&gt;
&lt;p&gt;Unnecessary trailing commas are removed if an expression fits in one line. This makes it
1% more likely that your line won't exceed the allotted line length limit. Moreover, in
this scenario, if you added another argument to your call, you'd probably fit it in the
same line anyway. That doesn't make diffs any larger.&lt;/p&gt;
&lt;p&gt;One exception to removing trailing commas is tuple expressions with just one element. In
this case &lt;em&gt;Black&lt;/em&gt; won't touch the single trailing comma as this would unexpectedly
change the underlying data type. Note that this is also the case when commas are used
while indexing. This is a tuple in disguise: &lt;code&gt;numpy_array[3, ]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One exception to adding trailing commas is function signatures containing &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;*args&lt;/code&gt;,
or &lt;code&gt;**kwargs&lt;/code&gt;. In this case a trailing comma is only safe to use on Python 3.6. &lt;em&gt;Black&lt;/em&gt;
will detect if your file is already 3.6+ only and use trailing commas in this situation.
If you wonder how it knows, it looks for f-strings and existing use of trailing commas
in function signatures that have stars in them. In other words, if you'd like a trailing
comma in this situation and &lt;em&gt;Black&lt;/em&gt; didn't recognize it was safe to do so, put it there
manually and &lt;em&gt;Black&lt;/em&gt; will keep it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-strings" class="anchor" aria-hidden="true" href="#strings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Strings&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; prefers double quotes (&lt;code&gt;"&lt;/code&gt; and &lt;code&gt;"""&lt;/code&gt;) over single quotes (&lt;code&gt;'&lt;/code&gt; and &lt;code&gt;'''&lt;/code&gt;). It
will replace the latter with the former as long as it does not result in more backslash
escapes than before.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; also standardizes string prefixes, making them always lowercase. On top of that,
if your code is already Python 3.6+ only or it's using the &lt;code&gt;unicode_literals&lt;/code&gt; future
import, &lt;em&gt;Black&lt;/em&gt; will remove &lt;code&gt;u&lt;/code&gt; from the string prefix as it is meaningless in those
scenarios.&lt;/p&gt;
&lt;p&gt;The main reason to standardize on a single form of quotes is aesthetics. Having one kind
of quotes everywhere reduces reader distraction. It will also enable a future version of
&lt;em&gt;Black&lt;/em&gt; to merge consecutive string literals that ended up on the same line (see
&lt;a href="https://github.com/psf/black/issues/26"&gt;#26&lt;/a&gt; for details).&lt;/p&gt;
&lt;p&gt;Why settle on double quotes? They anticipate apostrophes in English text. They match the
docstring standard described in
&lt;a href="https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring" rel="nofollow"&gt;PEP 257&lt;/a&gt;. An empty
string in double quotes (&lt;code&gt;""&lt;/code&gt;) is impossible to confuse with a one double-quote
regardless of fonts and syntax highlighting used. On top of this, double quotes for
strings are consistent with C which Python interacts a lot with.&lt;/p&gt;
&lt;p&gt;On certain keyboard layouts like US English, typing single quotes is a bit easier than
double quotes. The latter requires use of the Shift key. My recommendation here is to
keep using whatever is faster to type and let &lt;em&gt;Black&lt;/em&gt; handle the transformation.&lt;/p&gt;
&lt;p&gt;If you are adopting &lt;em&gt;Black&lt;/em&gt; in a large project with pre-existing string conventions
(like the popular
&lt;a href="https://stackoverflow.com/a/56190" rel="nofollow"&gt;"single quotes for data, double quotes for human-readable strings"&lt;/a&gt;),
you can pass &lt;code&gt;--skip-string-normalization&lt;/code&gt; on the command line. This is meant as an
adoption helper, avoid using this for new projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-numeric-literals" class="anchor" aria-hidden="true" href="#numeric-literals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numeric literals&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; standardizes most numeric literals to use lowercase letters for the syntactic
parts and uppercase letters for the digits themselves: &lt;code&gt;0xAB&lt;/code&gt; instead of &lt;code&gt;0XAB&lt;/code&gt; and
&lt;code&gt;1e10&lt;/code&gt; instead of &lt;code&gt;1E10&lt;/code&gt;. Python 2 long literals are styled as &lt;code&gt;2L&lt;/code&gt; instead of &lt;code&gt;2l&lt;/code&gt; to
avoid confusion between &lt;code&gt;l&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-line-breaks--binary-operators" class="anchor" aria-hidden="true" href="#line-breaks--binary-operators"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line breaks &amp;amp; binary operators&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will break a line before a binary operator when splitting a block of code over
multiple lines. This is so that &lt;em&gt;Black&lt;/em&gt; is compliant with the recent changes in the
&lt;a href="https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator" rel="nofollow"&gt;PEP 8&lt;/a&gt;
style guide, which emphasizes that this approach improves readability.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;W503 line break before binary operator&lt;/code&gt; warnings in style
guide enforcement tools like Flake8. Since &lt;code&gt;W503&lt;/code&gt; is not PEP 8 compliant, you should
tell Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-slices" class="anchor" aria-hidden="true" href="#slices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slices&lt;/h3&gt;
&lt;p&gt;PEP 8
&lt;a href="https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements" rel="nofollow"&gt;recommends&lt;/a&gt;
to treat &lt;code&gt;:&lt;/code&gt; in slices as a binary operator with the lowest priority, and to leave an
equal amount of space on either side, except if a parameter is omitted (e.g.
&lt;code&gt;ham[1 + 1 :]&lt;/code&gt;). It also states that for extended slices, both &lt;code&gt;:&lt;/code&gt; operators have to
have the same amount of spacing, except if a parameter is omitted (&lt;code&gt;ham[1 + 1 ::]&lt;/code&gt;).
&lt;em&gt;Black&lt;/em&gt; enforces these rules consistently.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;E203 whitespace before ':'&lt;/code&gt; warnings in style guide
enforcement tools like Flake8. Since &lt;code&gt;E203&lt;/code&gt; is not PEP 8 compliant, you should tell
Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-parentheses" class="anchor" aria-hidden="true" href="#parentheses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parentheses&lt;/h3&gt;
&lt;p&gt;Some parentheses are optional in the Python grammar. Any expression can be wrapped in a
pair of parentheses to form an atom. There are a few interesting cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;while (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;for (...) in (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;assert (...), (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from X import (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;assignments like:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;target = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;target: type = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;some, *un, packing = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;augmented += (...)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In those cases, parentheses are removed when the entire statement fits in one line, or
if the inner expression doesn't have any delimiters to further split on. If there is
only a single delimiter and the expression starts or ends with a bracket, the
parenthesis can also be successfully omitted since the existing bracket pair will
organize the expression neatly anyway. Otherwise, the parentheses are added.&lt;/p&gt;
&lt;p&gt;Please note that &lt;em&gt;Black&lt;/em&gt; does not add or remove any additional nested parentheses that
you might want to have for clarity or further code organization. For example those
parentheses are not going to be removed:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; (this &lt;span class="pl-k"&gt;or&lt;/span&gt; that)
decision &lt;span class="pl-k"&gt;=&lt;/span&gt; (maybe.this() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; (maybe.that() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-call-chains" class="anchor" aria-hidden="true" href="#call-chains"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Call chains&lt;/h3&gt;
&lt;p&gt;Some popular APIs, like ORMs, use call chaining. This API style is known as a
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interface&lt;/a&gt;. &lt;em&gt;Black&lt;/em&gt; formats
those by treating dots that follow a call or an indexing operation like a very low
priority delimiter. It's easier to show the behavior than to explain it. Look at the
example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;example&lt;/span&gt;(&lt;span class="pl-smi"&gt;session&lt;/span&gt;):
    result &lt;span class="pl-k"&gt;=&lt;/span&gt; (
        session.query(models.Customer.id)
        .filter(
            models.Customer.account_id &lt;span class="pl-k"&gt;==&lt;/span&gt; account_id,
            models.Customer.email &lt;span class="pl-k"&gt;==&lt;/span&gt; email_address,
        )
        .order_by(models.Customer.id.asc())
        .all()
    )&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typing-stub-files" class="anchor" aria-hidden="true" href="#typing-stub-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typing stub files&lt;/h3&gt;
&lt;p&gt;PEP 484 describes the syntax for type hints in Python. One of the use cases for typing
is providing type annotations for modules which cannot contain them directly (they might
be written in C, or they might be third-party, or their implementation may be overly
dynamic, and so on).&lt;/p&gt;
&lt;p&gt;To solve this,
&lt;a href="https://www.python.org/dev/peps/pep-0484/#stub-files" rel="nofollow"&gt;stub files with the &lt;code&gt;.pyi&lt;/code&gt; file extension&lt;/a&gt;
can be used to describe typing information for an external module. Those stub files omit
the implementation of classes and functions they describe, instead they only contain the
structure of the file (listing globals, functions, and classes with their members). The
recommended code style for those files is more terse than PEP 8:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; on the same line as the class/function signature;&lt;/li&gt;
&lt;li&gt;avoid vertical whitespace between consecutive module-level functions, names, or
methods and fields within a single class;&lt;/li&gt;
&lt;li&gt;use a single blank line between top-level class definitions, or none if the classes
are very small.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; enforces the above rules. There are additional guidelines for formatting &lt;code&gt;.pyi&lt;/code&gt;
file that are not enforced yet but might be in a future version of the formatter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all function bodies should be empty (contain &lt;code&gt;...&lt;/code&gt; instead of the body);&lt;/li&gt;
&lt;li&gt;do not use docstrings;&lt;/li&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; over &lt;code&gt;pass&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;for arguments with a default, use &lt;code&gt;...&lt;/code&gt; instead of the actual default;&lt;/li&gt;
&lt;li&gt;avoid using string literals in type annotations, stub files support forward references
natively (like Python 3.7 code with &lt;code&gt;from __future__ import annotations&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;use variable annotations instead of type comments, even for stubs that target older
versions of Python;&lt;/li&gt;
&lt;li&gt;for arguments that default to &lt;code&gt;None&lt;/code&gt;, use &lt;code&gt;Optional[]&lt;/code&gt; explicitly;&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;float&lt;/code&gt; instead of &lt;code&gt;Union[int, float]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pyprojecttoml" class="anchor" aria-hidden="true" href="#pyprojecttoml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyproject.toml&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options
from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom
&lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; patterns for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you're asking yourself "Do I need to configure anything?" the answer is
"No". &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-on-earth-is-a-pyprojecttoml-file" class="anchor" aria-hidden="true" href="#what-on-earth-is-a-pyprojecttoml-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What on Earth is a &lt;code&gt;pyproject.toml&lt;/code&gt; file?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.python.org/dev/peps/pep-0518/" rel="nofollow"&gt;PEP 518&lt;/a&gt; defines &lt;code&gt;pyproject.toml&lt;/code&gt; as a
configuration file to store build system requirements for Python projects. With the help
of tools like &lt;a href="https://poetry.eustace.io/" rel="nofollow"&gt;Poetry&lt;/a&gt; or
&lt;a href="https://flit.readthedocs.io/en/latest/" rel="nofollow"&gt;Flit&lt;/a&gt; it can fully replace the need for
&lt;code&gt;setup.py&lt;/code&gt; and &lt;code&gt;setup.cfg&lt;/code&gt; files.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-where-black-looks-for-the-file" class="anchor" aria-hidden="true" href="#where-black-looks-for-the-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where &lt;em&gt;Black&lt;/em&gt; looks for the file&lt;/h3&gt;
&lt;p&gt;By default &lt;em&gt;Black&lt;/em&gt; looks for &lt;code&gt;pyproject.toml&lt;/code&gt; starting from the common base directory of
all files and directories passed on the command line. If it's not there, it looks in
parent directories. It stops looking when it finds the file, or a &lt;code&gt;.git&lt;/code&gt; directory, or a
&lt;code&gt;.hg&lt;/code&gt; directory, or the root of the file system, whichever comes first.&lt;/p&gt;
&lt;p&gt;If you're formatting standard input, &lt;em&gt;Black&lt;/em&gt; will look for configuration starting from
the current working directory.&lt;/p&gt;
&lt;p&gt;You can also explicitly specify the path to a particular file that you want with
&lt;code&gt;--config&lt;/code&gt;. In this situation &lt;em&gt;Black&lt;/em&gt; will not look for any other file.&lt;/p&gt;
&lt;p&gt;If you're running with &lt;code&gt;--verbose&lt;/code&gt;, you will see a blue message if a file was found and
used.&lt;/p&gt;
&lt;p&gt;Please note &lt;code&gt;blackd&lt;/code&gt; will not use &lt;code&gt;pyproject.toml&lt;/code&gt; configuration.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-configuration-format" class="anchor" aria-hidden="true" href="#configuration-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration format&lt;/h3&gt;
&lt;p&gt;As the file extension suggests, &lt;code&gt;pyproject.toml&lt;/code&gt; is a
&lt;a href="https://github.com/toml-lang/toml"&gt;TOML&lt;/a&gt; file. It contains separate sections for
different tools. &lt;em&gt;Black&lt;/em&gt; is using the &lt;code&gt;[tool.black]&lt;/code&gt; section. The option keys are the
same as long names of options on the command line.&lt;/p&gt;
&lt;p&gt;Note that you have to use single-quoted strings in TOML for regular expressions. It's
the equivalent of r-strings in Python. Multiline strings are treated as verbose regular
expressions by Black. Use &lt;code&gt;[ ]&lt;/code&gt; to denote a significant space character.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example `pyproject.toml`&lt;/summary&gt;
&lt;div class="highlight highlight-source-toml"&gt;&lt;pre&gt;[&lt;span class="pl-en"&gt;tool&lt;/span&gt;.&lt;span class="pl-en"&gt;black&lt;/span&gt;]
&lt;span class="pl-smi"&gt;line-length&lt;/span&gt; = &lt;span class="pl-c1"&gt;88&lt;/span&gt;
&lt;span class="pl-smi"&gt;target-version&lt;/span&gt; = [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;py37&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
&lt;span class="pl-smi"&gt;include&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;\.pyi?$&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-smi"&gt;exclude&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;(&lt;/span&gt;
&lt;span class="pl-s"&gt;  /(&lt;/span&gt;
&lt;span class="pl-s"&gt;      \.eggs         # exclude a few common directories in the&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.git          # root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.hg&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.mypy_cache&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.tox&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.venv&lt;/span&gt;
&lt;span class="pl-s"&gt;    | _build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | buck-out&lt;/span&gt;
&lt;span class="pl-s"&gt;    | build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | dist&lt;/span&gt;
&lt;span class="pl-s"&gt;  )/&lt;/span&gt;
&lt;span class="pl-s"&gt;  | foo.py           # also separately exclude a file named foo.py in&lt;/span&gt;
&lt;span class="pl-s"&gt;                     # the root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;)&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-lookup-hierarchy" class="anchor" aria-hidden="true" href="#lookup-hierarchy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lookup hierarchy&lt;/h3&gt;
&lt;p&gt;Command-line options have defaults that you can see in &lt;code&gt;--help&lt;/code&gt;. A &lt;code&gt;pyproject.toml&lt;/code&gt; can
override those defaults. Finally, options provided by the user on the command line
override both.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will only ever use one &lt;code&gt;pyproject.toml&lt;/code&gt; file during an entire run. It doesn't
look for multiple files, and doesn't compose configuration from different levels of the
file hierarchy.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-editor-integration" class="anchor" aria-hidden="true" href="#editor-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Editor integration&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-emacs" class="anchor" aria-hidden="true" href="#emacs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Emacs&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/proofit404/blacken"&gt;proofit404/blacken&lt;/a&gt; or
&lt;a href="https://github.com/jorgenschaefer/elpy"&gt;Elpy&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pycharmintellij-idea" class="anchor" aria-hidden="true" href="#pycharmintellij-idea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyCharm/IntelliJ IDEA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Locate your &lt;code&gt;black&lt;/code&gt; installation folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS / Linux / BSD:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;which black&lt;/span&gt;
&lt;span class="pl-c1"&gt;/usr/local/bin/black  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Windows:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;where black&lt;/span&gt;
&lt;span class="pl-c1"&gt;%LocalAppData%\Programs\Python\Python36-32\Scripts\black.exe  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Open External tools in PyCharm/IntelliJ IDEA&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PyCharm -&amp;gt; Preferences -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;On Windows / Linux / BSD:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;File -&amp;gt; Settings -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;p&gt;Click the + icon to add a new external tool with the following values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;Description: Black is the uncompromising Python code formatter.&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;"$FilePath$"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Format the currently opened file by selecting &lt;code&gt;Tools -&amp;gt; External Tools -&amp;gt; black&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, you can set a keyboard shortcut by navigating to
&lt;code&gt;Preferences or Settings -&amp;gt; Keymap -&amp;gt; External Tools -&amp;gt; External Tools - Black&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optionally, run &lt;em&gt;Black&lt;/em&gt; on every file save:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have the
&lt;a href="https://plugins.jetbrains.com/plugin/7177-file-watchers" rel="nofollow"&gt;File Watcher&lt;/a&gt; plugin
installed.&lt;/li&gt;
&lt;li&gt;Go to &lt;code&gt;Preferences or Settings -&amp;gt; Tools -&amp;gt; File Watchers&lt;/code&gt; and click &lt;code&gt;+&lt;/code&gt; to add a
new watcher:
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;File type: Python&lt;/li&gt;
&lt;li&gt;Scope: Project Files&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Output paths to refresh: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Working directory: &lt;code&gt;$ProjectFileDir$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Uncheck "Auto-save edited files to trigger the watcher"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-wing-ide" class="anchor" aria-hidden="true" href="#wing-ide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wing IDE&lt;/h3&gt;
&lt;p&gt;Wing supports black via the OS Commands tool, as explained in the Wing documentation on
&lt;a href="https://wingware.com/doc/edit/pep8" rel="nofollow"&gt;pep8 formatting&lt;/a&gt;. The detailed procedure is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Make sure it runs from the command line, e.g.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;black --help&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;In Wing IDE, activate the &lt;strong&gt;OS Commands&lt;/strong&gt; panel and define the command &lt;strong&gt;black&lt;/strong&gt; to
execute black on the currently selected file:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use the Tools -&amp;gt; OS Commands menu selection&lt;/li&gt;
&lt;li&gt;click on &lt;strong&gt;+&lt;/strong&gt; in &lt;strong&gt;OS Commands&lt;/strong&gt; -&amp;gt; New: Command line..
&lt;ul class="contains-task-list"&gt;
&lt;li&gt;Title: black&lt;/li&gt;
&lt;li&gt;Command Line: black %s&lt;/li&gt;
&lt;li&gt;I/O Encoding: Use Default&lt;/li&gt;
&lt;li&gt;Key Binding: F1&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Raise OS Commands when executed&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Auto-save files before execution&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Line mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Select a file in the editor and press &lt;strong&gt;F1&lt;/strong&gt; , or whatever key binding you selected
in step 3, to reformat the file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-vim" class="anchor" aria-hidden="true" href="#vim"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vim&lt;/h3&gt;
&lt;p&gt;Commands and shortcuts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;:Black&lt;/code&gt; to format the entire file (ranges not supported);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackUpgrade&lt;/code&gt; to upgrade &lt;em&gt;Black&lt;/em&gt; inside the virtualenv;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackVersion&lt;/code&gt; to get the current version of &lt;em&gt;Black&lt;/em&gt; inside the virtualenv.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;g:black_fast&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_linelength&lt;/code&gt; (defaults to &lt;code&gt;88&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_skip_string_normalization&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_virtualenv&lt;/code&gt; (defaults to &lt;code&gt;~/.vim/black&lt;/code&gt; or &lt;code&gt;~/.local/share/nvim/black&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install with &lt;a href="https://github.com/junegunn/vim-plug"&gt;vim-plug&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plug 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with &lt;a href="https://github.com/VundleVim/Vundle.vim"&gt;Vundle&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plugin 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can copy the plugin from
&lt;a href="https://github.com/psf/black/tree/master/plugin/black.vim"&gt;plugin/black.vim&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/.vim/pack/python/start/black/plugin
curl https://raw.githubusercontent.com/psf/black/master/plugin/black.vim -o ~/.vim/pack/python/start/black/plugin/black.vim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me know if this requires any changes to work with Vim 8's builtin &lt;code&gt;packadd&lt;/code&gt;, or
Pathogen, and so on.&lt;/p&gt;
&lt;p&gt;This plugin &lt;strong&gt;requires Vim 7.0+ built with Python 3.6+ support&lt;/strong&gt;. It needs Python 3.6 to
be able to run &lt;em&gt;Black&lt;/em&gt; inside the Vim process which is much faster than calling an
external command.&lt;/p&gt;
&lt;p&gt;On first run, the plugin creates its own virtualenv using the right Python version and
automatically installs &lt;em&gt;Black&lt;/em&gt;. You can upgrade it later by calling &lt;code&gt;:BlackUpgrade&lt;/code&gt; and
restarting Vim.&lt;/p&gt;
&lt;p&gt;If you need to do anything special to make your virtualenv work and install &lt;em&gt;Black&lt;/em&gt; (for
example you want to run a version from master), create a virtualenv manually and point
&lt;code&gt;g:black_virtualenv&lt;/code&gt; to it. The plugin will use it.&lt;/p&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on save, add the following line to &lt;code&gt;.vimrc&lt;/code&gt; or &lt;code&gt;init.vim&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autocmd BufWritePre *.py execute ':Black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on a key press (e.g. F9 below), add this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nnoremap &amp;lt;F9&amp;gt; :Black&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;How to get Vim with Python 3.6?&lt;/strong&gt; On Ubuntu 17.10 Vim comes with Python 3.6 by
default. On macOS with Homebrew run: &lt;code&gt;brew install vim --with-python3&lt;/code&gt;. When building
Vim from source, use: &lt;code&gt;./configure --enable-python3interp=yes&lt;/code&gt;. There's many guides
online how to do this.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h3&gt;
&lt;p&gt;Use the
&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python" rel="nofollow"&gt;Python extension&lt;/a&gt;
(&lt;a href="https://code.visualstudio.com/docs/python/editing#_formatting" rel="nofollow"&gt;instructions&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sublimetext-3" class="anchor" aria-hidden="true" href="#sublimetext-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SublimeText 3&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/jgirardet/sublack"&gt;sublack plugin&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jupyter-notebook-magic" class="anchor" aria-hidden="true" href="#jupyter-notebook-magic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook Magic&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/csurfer/blackcellmagic"&gt;blackcellmagic&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-language-server" class="anchor" aria-hidden="true" href="#python-language-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Language Server&lt;/h3&gt;
&lt;p&gt;If your editor supports the &lt;a href="https://langserver.org/" rel="nofollow"&gt;Language Server Protocol&lt;/a&gt; (Atom,
Sublime Text, Visual Studio Code and many more), you can use the
&lt;a href="https://github.com/palantir/python-language-server"&gt;Python Language Server&lt;/a&gt; with the
&lt;a href="https://github.com/rupert/pyls-black"&gt;pyls-black&lt;/a&gt; plugin.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-atomnuclide" class="anchor" aria-hidden="true" href="#atomnuclide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Atom/Nuclide&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://atom.io/packages/python-black" rel="nofollow"&gt;python-black&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-kakoune" class="anchor" aria-hidden="true" href="#kakoune"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kakoune&lt;/h3&gt;
&lt;p&gt;Add the following hook to your kakrc, then run black with &lt;code&gt;:format&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hook global WinSetOption filetype=python %{
    set-option window formatcmd 'black -q  -'
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-other-editors" class="anchor" aria-hidden="true" href="#other-editors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other editors&lt;/h3&gt;
&lt;p&gt;Other editors will require external contributions.&lt;/p&gt;
&lt;p&gt;Patches welcome! &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;🍰&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;Any tool that can pipe code through &lt;em&gt;Black&lt;/em&gt; using its stdio mode (just
&lt;a href="https://www.tldp.org/LDP/abs/html/special-chars.html#DASHREF2" rel="nofollow"&gt;use &lt;code&gt;-&lt;/code&gt; as the file name&lt;/a&gt;).
The formatted code will be returned on stdout (unless &lt;code&gt;--check&lt;/code&gt; was passed). &lt;em&gt;Black&lt;/em&gt;
will still emit messages on stderr but that shouldn't affect your use case.&lt;/p&gt;
&lt;p&gt;This can be used for example with PyCharm's or IntelliJ's
&lt;a href="https://www.jetbrains.com/help/pycharm/file-watchers.html" rel="nofollow"&gt;File Watchers&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-blackd" class="anchor" aria-hidden="true" href="#blackd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;blackd&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is a small HTTP server that exposes &lt;em&gt;Black&lt;/em&gt;'s functionality over a simple
protocol. The main benefit of using it is to avoid paying the cost of starting up a new
&lt;em&gt;Black&lt;/em&gt; process every time you want to blacken a file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage-1" class="anchor" aria-hidden="true" href="#usage-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is not packaged alongside &lt;em&gt;Black&lt;/em&gt; by default because it has additional
dependencies. You will need to do &lt;code&gt;pip install black[d]&lt;/code&gt; to install it.&lt;/p&gt;
&lt;p&gt;You can start the server on the default port, binding only to the local interface by
running &lt;code&gt;blackd&lt;/code&gt;. You will see a single line mentioning the server's version, and the
host and port it's listening on. &lt;code&gt;blackd&lt;/code&gt; will then print an access log similar to most
web servers on standard output, merged with any exception traces caused by invalid
formatting requests.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; provides even less options than &lt;em&gt;Black&lt;/em&gt;. You can see them by running
&lt;code&gt;blackd --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;Usage: blackd [OPTIONS]

Options:
  --bind-host TEXT                Address to bind the server to.
  --bind-port INTEGER             Port to listen on
  --version                       Show the version and exit.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no official blackd client tool (yet!). You can test that blackd is working
using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blackd --bind-port 9090 &amp;amp;  # or let blackd choose a port
curl -s -XPOST "localhost:9090" -d "print('valid')"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-protocol" class="anchor" aria-hidden="true" href="#protocol"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Protocol&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; only accepts &lt;code&gt;POST&lt;/code&gt; requests at the &lt;code&gt;/&lt;/code&gt; path. The body of the request should
contain the python source code to be formatted, encoded according to the &lt;code&gt;charset&lt;/code&gt; field
in the &lt;code&gt;Content-Type&lt;/code&gt; request header. If no &lt;code&gt;charset&lt;/code&gt; is specified, &lt;code&gt;blackd&lt;/code&gt; assumes
&lt;code&gt;UTF-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are a few HTTP headers that control how the source is formatted. These correspond
to command line flags for &lt;em&gt;Black&lt;/em&gt;. There is one exception to this: &lt;code&gt;X-Protocol-Version&lt;/code&gt;
which if present, should have the value &lt;code&gt;1&lt;/code&gt;, otherwise the request is rejected with
&lt;code&gt;HTTP 501&lt;/code&gt; (Not Implemented).&lt;/p&gt;
&lt;p&gt;The headers controlling how code is formatted are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X-Line-Length&lt;/code&gt;: corresponds to the &lt;code&gt;--line-length&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Skip-String-Normalization&lt;/code&gt;: corresponds to the &lt;code&gt;--skip-string-normalization&lt;/code&gt;
command line flag. If present and its value is not the empty string, no string
normalization will be performed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Fast-Or-Safe&lt;/code&gt;: if set to &lt;code&gt;fast&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--fast&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Python-Variant&lt;/code&gt;: if set to &lt;code&gt;pyi&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--pyi&lt;/code&gt; command line flag. Otherwise, its value must correspond to a Python version or
a set of comma-separated Python versions, optionally prefixed with &lt;code&gt;py&lt;/code&gt;. For example,
to request code that is compatible with Python 3.5 and 3.6, set the header to
&lt;code&gt;py3.5,py3.6&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Diff&lt;/code&gt;: corresponds to the &lt;code&gt;--diff&lt;/code&gt; command line flag. If present, a diff of the
formats will be output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of these headers are set to invalid values, &lt;code&gt;blackd&lt;/code&gt; returns a &lt;code&gt;HTTP 400&lt;/code&gt; error
response, mentioning the name of the problematic header in the message body.&lt;/p&gt;
&lt;p&gt;Apart from the above, &lt;code&gt;blackd&lt;/code&gt; can produce the following response codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;HTTP 204&lt;/code&gt;: If the input is already well-formatted. The response body is empty.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 200&lt;/code&gt;: If formatting was needed on the input. The response body contains the
blackened Python code, and the &lt;code&gt;Content-Type&lt;/code&gt; header is set accordingly.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 400&lt;/code&gt;: If the input contains a syntax error. Details of the error are returned in
the response body.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 500&lt;/code&gt;: If there was any kind of error while trying to format the input. The
response body contains a textual representation of the error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The response headers include a &lt;code&gt;X-Black-Version&lt;/code&gt; header containing the version of
&lt;em&gt;Black&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-version-control-integration" class="anchor" aria-hidden="true" href="#version-control-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Version control integration&lt;/h2&gt;
&lt;p&gt;Use &lt;a href="https://pre-commit.com/" rel="nofollow"&gt;pre-commit&lt;/a&gt;. Once you
&lt;a href="https://pre-commit.com/#install" rel="nofollow"&gt;have it installed&lt;/a&gt;, add this to the
&lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; in your repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-yaml"&gt;&lt;pre&gt;&lt;span class="pl-ent"&gt;repos&lt;/span&gt;:
  - &lt;span class="pl-ent"&gt;repo&lt;/span&gt;: &lt;span class="pl-s"&gt;https://github.com/psf/black&lt;/span&gt;
    &lt;span class="pl-ent"&gt;rev&lt;/span&gt;: &lt;span class="pl-s"&gt;stable&lt;/span&gt;
    &lt;span class="pl-ent"&gt;hooks&lt;/span&gt;:
      - &lt;span class="pl-ent"&gt;id&lt;/span&gt;: &lt;span class="pl-s"&gt;black&lt;/span&gt;
        &lt;span class="pl-ent"&gt;language_version&lt;/span&gt;: &lt;span class="pl-s"&gt;python3.6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then run &lt;code&gt;pre-commit install&lt;/code&gt; and you're ready to go.&lt;/p&gt;
&lt;p&gt;Avoid using &lt;code&gt;args&lt;/code&gt; in the hook. Instead, store necessary configuration in
&lt;code&gt;pyproject.toml&lt;/code&gt; so that editors and command-line usage of Black all behave consistently
for your project. See &lt;em&gt;Black&lt;/em&gt;'s own &lt;a href="/pyproject.toml"&gt;pyproject.toml&lt;/a&gt; for an example.&lt;/p&gt;
&lt;p&gt;If you're already using Python 3.7, switch the &lt;code&gt;language_version&lt;/code&gt; accordingly. Finally,
&lt;code&gt;stable&lt;/code&gt; is a tag that is pinned to the latest release on PyPI. If you'd rather run on
master, this is also an option.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ignoring-unmodified-files" class="anchor" aria-hidden="true" href="#ignoring-unmodified-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ignoring unmodified files&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; remembers files it has already formatted, unless the &lt;code&gt;--diff&lt;/code&gt; flag is used or
code is passed via standard input. This information is stored per-user. The exact
location of the file depends on the &lt;em&gt;Black&lt;/em&gt; version and the system on which &lt;em&gt;Black&lt;/em&gt; is
run. The file is non-portable. The standard location on common operating systems is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows:
&lt;code&gt;C:\\Users\&amp;lt;username&amp;gt;\AppData\Local\black\black\Cache\&amp;lt;version&amp;gt;\cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;macOS:
&lt;code&gt;/Users/&amp;lt;username&amp;gt;/Library/Caches/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Linux:
&lt;code&gt;/home/&amp;lt;username&amp;gt;/.cache/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;file-mode&lt;/code&gt; is an int flag that determines whether the file was formatted as 3.6+ only,
as .pyi, and whether string normalization was omitted.&lt;/p&gt;
&lt;p&gt;To override the location of these files on macOS or Linux, set the environment variable
&lt;code&gt;XDG_CACHE_HOME&lt;/code&gt; to your preferred location. For example, if you want to put the cache
in the directory you're running &lt;em&gt;Black&lt;/em&gt; from, set &lt;code&gt;XDG_CACHE_HOME=.cache&lt;/code&gt;. &lt;em&gt;Black&lt;/em&gt; will
then write the above files to &lt;code&gt;.cache/black/&amp;lt;version&amp;gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-used-by" class="anchor" aria-hidden="true" href="#used-by"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Used by&lt;/h2&gt;
&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent
code style: pytest, tox, Pyramid, Django Channels, Hypothesis, attrs, SQLAlchemy,
Poetry, PyPA applications (Warehouse, Pipenv, virtualenv), pandas, Pillow, every Datadog
Agent Integration.&lt;/p&gt;
&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testimonials" class="anchor" aria-hidden="true" href="#testimonials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testimonials&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;,
&lt;a href="https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips" rel="nofollow"&gt;writer&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don't have to be.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href="https://www.attrs.org/" rel="nofollow"&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of
Twisted and CPython:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An auto-formatter that doesn't suck is all I want for Xmas!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;Django&lt;/a&gt; core developer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At least the name is good.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href="http://python-requests.org/" rel="nofollow"&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and
&lt;a href="https://docs.pipenv.org/" rel="nofollow"&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-show-your-style" class="anchor" aria-hidden="true" href="#show-your-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Show your style&lt;/h2&gt;
&lt;p&gt;Use the badge in your project's README.md:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;[![&lt;span class="pl-e"&gt;Code style: black&lt;/span&gt;](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like this:
&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-black" class="anchor" aria-hidden="true" href="#contributing-to-black"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to &lt;em&gt;Black&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;In terms of inspiration, &lt;em&gt;Black&lt;/em&gt; is about as configurable as &lt;em&gt;gofmt&lt;/em&gt;. This is
deliberate.&lt;/p&gt;
&lt;p&gt;Bug reports and fixes are always welcome! However, before you suggest a new feature or
configuration knob, ask yourself why you want it. If it enables better integration with
some workflow, fixes an inconsistency, speeds things up, and so on - go for it! On the
other hand, if your answer is "because I don't like a particular formatting" then you're
not ready to embrace &lt;em&gt;Black&lt;/em&gt; yet. Such changes are unlikely to get accepted. You can
still try but prepare to be disappointed.&lt;/p&gt;
&lt;p&gt;More details can be found in &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1910b0" class="anchor" aria-hidden="true" href="#1910b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.10b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 572 assignment expressions (#711)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 570 positional-only arguments (#943)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for async generators (#593)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for pre-splitting collections by putting an explicit trailing comma
inside (#826)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;black -c&lt;/code&gt; as a way to format code passed from the command line (#761)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;--safe now works with Python 2 code (#840)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed grammar selection for Python 2-specific code (#765)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed feature detection for trailing commas in function definitions and call sites
(#763)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt;/&lt;code&gt;# fmt: on&lt;/code&gt; comment pairs placed multiple times within the same block of
code now behave correctly (#1005)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Windows machines with more than 61 cores (#838)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on standalone comments prepended with a backslash (#767)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on &lt;code&gt;from&lt;/code&gt; ... &lt;code&gt;import&lt;/code&gt; blocks with comments (#829)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Python 3.7 on some platform configurations (#494)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails on comments in from-imports (#671)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails when the file starts with a backslash (#922)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer merges regular comments with type comments (#1027)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer splits long lines that contain type comments (#997)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;removed unnecessary parentheses around &lt;code&gt;yield&lt;/code&gt; expressions (#834)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around long tuples in unpacking assignments (#832)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around complex powers when they are prefixed by a unary operator
(#646)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed bug that led &lt;em&gt;Black&lt;/em&gt; format some code with a line length target of 1 (#762)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer introduces quotes in f-string subexpressions on string boundaries
(#863)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if &lt;em&gt;Black&lt;/em&gt; puts parenthesis around a single expression, it moves comments to the
wrapped expression instead of after the brackets (#872)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now returns the version of &lt;em&gt;Black&lt;/em&gt; in the response headers (#1013)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; can now output the diff of formats on source code when the &lt;code&gt;X-Diff&lt;/code&gt; header is
provided (#969)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-193b0" class="anchor" aria-hidden="true" href="#193b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.3b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;new option &lt;code&gt;--target-version&lt;/code&gt; to control which Python versions &lt;em&gt;Black&lt;/em&gt;-formatted code
should target (#618)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;deprecated &lt;code&gt;--py36&lt;/code&gt; (use &lt;code&gt;--target-version=py36&lt;/code&gt; instead) (#724)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer normalizes numeric literals to include &lt;code&gt;_&lt;/code&gt; separators (#696)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;long &lt;code&gt;del&lt;/code&gt; statements are now split into multiple lines (#698)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;type comments are no longer mangled in function signatures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;improved performance of formatting deeply nested data structures (#509)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now properly formats multiple files in parallel on Windows (#632)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now creates cache files atomically which allows it to be used in parallel
pipelines (like &lt;code&gt;xargs -P8&lt;/code&gt;) (#673)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now correctly indents comments in files that were previously formatted with
tabs (#262)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now supports CORS (#622)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-189b0" class="anchor" aria-hidden="true" href="#189b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.9b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are now formatted by &lt;em&gt;Black&lt;/em&gt; (#452, #461, #464, #469):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are normalized to include &lt;code&gt;_&lt;/code&gt; separators on Python 3.6+ code&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-numeric-underscore-normalization&lt;/code&gt; to disable the above behavior and
leave numeric underscores as they were in the input&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code with &lt;code&gt;_&lt;/code&gt; in numeric literals is recognized as Python 3.6+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;most letters in numeric literals are lowercased (e.g., in &lt;code&gt;1e10&lt;/code&gt;, &lt;code&gt;0x01&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hexadecimal digits are always uppercased (e.g. &lt;code&gt;0xBADC0DE&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;blackd&lt;/code&gt;, see &lt;a href="#blackd"&gt;its documentation&lt;/a&gt; for more info (#349)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjacent string literals are now correctly split into multiple lines (#463)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;trailing comma is now added to single imports that don't fit on a line (#250)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cache is now populated when &lt;code&gt;--check&lt;/code&gt; is successful for a file which speeds up
consecutive checks of properly formatted unmodified files (#448)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whitespace at the beginning of the file is now removed (#399)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed mangling &lt;a href="http://mpastell.com/pweave/" rel="nofollow"&gt;pweave&lt;/a&gt; and
&lt;a href="https://pythonhosted.org/spyder/" rel="nofollow"&gt;Spyder IDE&lt;/a&gt; special comments (#532)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when unpacking big tuples (#267)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of &lt;code&gt;__future__&lt;/code&gt; imports with renames (#389)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed scope of &lt;code&gt;# fmt: off&lt;/code&gt; when directly preceding &lt;code&gt;yield&lt;/code&gt; and other nodes (#385)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of lambda expressions with default arguments (#468)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;async for&lt;/code&gt; statements: &lt;em&gt;Black&lt;/em&gt; no longer breaks them into separate lines (#372)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;note: the Vim plugin stopped registering &lt;code&gt;,=&lt;/code&gt; as a default chord as it turned out to
be a bad idea (#415)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b4" class="anchor" aria-hidden="true" href="#186b4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hotfix: don't freeze when multiple comments directly precede &lt;code&gt;# fmt: off&lt;/code&gt; (#371)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b3" class="anchor" aria-hidden="true" href="#186b3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) now have blank lines added after constants (#340)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are now much more dependable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they now work also within bracket pairs (#329)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now correctly work across function/class boundaries (#335)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now work when an indentation block starts with empty lines or misaligned
comments (#334)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;made Click not fail on invalid environments; note that Click is right but the
likelihood we'll need to access non-ASCII file paths when dealing with Python source
code is low (#277)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper formatting of f-strings with quotes inside interpolated expressions
(#322)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown when long list literals where found in a file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown on AST nodes with very many siblings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed cannibalizing backslashes during string normalization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed a crash due to symbolic links pointing outside of the project directory (#338)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b2" class="anchor" aria-hidden="true" href="#186b2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--config&lt;/code&gt; (#65)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;-h&lt;/code&gt; equivalent to &lt;code&gt;--help&lt;/code&gt; (#316)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper unmodified file caching when &lt;code&gt;-S&lt;/code&gt; was used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra space in string unpacking (#305)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of empty triple quoted strings (#313)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown in comment placement calculation on lines without comments&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b1" class="anchor" aria-hidden="true" href="#186b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output human-facing information on stdout (#299)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output cake emoji on non-zero return code (#300)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b0" class="anchor" aria-hidden="true" href="#186b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; (#270)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-string-normalization&lt;/code&gt; (#118)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--verbose&lt;/code&gt; (#283)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the header output in &lt;code&gt;--diff&lt;/code&gt; now actually conforms to the unified diff spec&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed long trivial assignments being wrapped in unnecessary parentheses (#273)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary parentheses when a line contained multiline strings (#232)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed stdin handling not working correctly if an old version of Click was used (#276)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now preserves line endings when formatting a file in place (#258)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b1" class="anchor" aria-hidden="true" href="#185b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--pyi&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--py36&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python grammar pickle caches are stored with the formatting caches, making &lt;em&gt;Black&lt;/em&gt;
work in environments where site-packages is not user-writable (#192)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now enforces a PEP 257 empty line after a class-level docstring (and/or
fields) and the first method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when standalone comments were present in a trailer that
was omitted from line splitting on a large expression (#237)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed optional parentheses being removed within &lt;code&gt;# fmt: off&lt;/code&gt; sections (#224)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when stars in very long imports were incorrectly wrapped
in optional parentheses (#234)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when inline comments were moved around in a trailer that was
omitted from line splitting on a large expression (#238)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a class declaration and the first method if no class
docstring or fields are present (#219)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a function signature and an inner function or inner
class (#196)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b0" class="anchor" aria-hidden="true" href="#185b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;call chains are now formatted according to the
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interfaces&lt;/a&gt; style (#67)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;data structure literals (tuples, lists, dictionaries, and sets) are now also always
exploded like imports when they don't fit in a single line (#152)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;slices are now formatted according to PEP 8 (#178)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;parentheses are now also managed automatically on the right-hand side of assignments
and return statements (#140)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;math operators now use their respective priorities for delimiting multiline
expressions (#148)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;optional parentheses are now omitted on expressions that start or end with a bracket
and only contain a single operator (#177)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;empty parentheses in a class definition are now removed (#145, #180)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;string prefixes are now standardized to lowercase and &lt;code&gt;u&lt;/code&gt; is removed on Python 3.6+
only code and Python 2.7+ code with the &lt;code&gt;unicode_literals&lt;/code&gt; future import (#188, #198,
#199)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) are now formatted in a style that is consistent with PEP
484 (#207, #210)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;progress when reformatting many files is now reported incrementally&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed trailers (content with brackets) being unnecessarily exploded into their own
lines after a dedented closing bracket (#119)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed an invalid trailing comma sometimes left in imports (#185)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed non-deterministic formatting when multiple pairs of removable parentheses were
used (#183)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed multiline strings being unnecessarily wrapped in optional parentheses in long
assignments (#215)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed not splitting long from-imports with only a single name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed Python 3.6+ file discovery by also looking at function calls with unpacking.
This fixed non-deterministic formatting if trailing commas where used both in function
signatures with stars and function calls with stars but the former would be
reformatted to a single line.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash on dealing with optional parentheses (#193)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed "is", "is not", "in", and "not in" not considered operators for splitting
purposes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash when dead symlinks where encountered&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a4" class="anchor" aria-hidden="true" href="#184a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;don't populate the cache on &lt;code&gt;--check&lt;/code&gt; (#175)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a3" class="anchor" aria-hidden="true" href="#184a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added a "cache"; files already reformatted that haven't changed on disk won't be
reformatted again (#109)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;--check&lt;/code&gt; and &lt;code&gt;--diff&lt;/code&gt; are no longer mutually exclusive (#149)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generalized star expression handling, including double stars; this fixes
multiplication making expressions "unsafe" for trailing commas (#132)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer enforces putting empty lines behind control flow statements (#90)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now splits imports like "Mode 3 + trailing comma" of isort (#127)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed comment indentation when a standalone comment closes a block (#16, #32)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed standalone comments receiving extra empty lines if immediately preceding a
class, def, or decorator (#56, #154)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;--diff&lt;/code&gt; not showing entire path (#130)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of complex expressions after star and double stars in function calls
(#2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting on comma in lambda arguments (#133)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing splits of ternary expressions (#141)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a2" class="anchor" aria-hidden="true" href="#184a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of unaligned standalone comments (#99, #112)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed placement of dictionary unpacking inside dictionary literals (#111)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vim plugin now works on Windows, too&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when encountering unnecessarily escaped quotes in a string
(#120)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a1" class="anchor" aria-hidden="true" href="#184a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--quiet&lt;/code&gt; (#78)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added automatic parentheses management (#4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;a href="https://pre-commit.com" rel="nofollow"&gt;pre-commit&lt;/a&gt; integration (#103, #104)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed reporting on &lt;code&gt;--check&lt;/code&gt; with multiple files (#101, #102)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed removing backslash escapes from raw strings (#100, #105)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a0" class="anchor" aria-hidden="true" href="#184a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--diff&lt;/code&gt; (#87)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;add line breaks before all delimiters, except in cases like commas, to better comply
with PEP 8 (#73)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;standardize string literals to use double quotes (almost) everywhere (#75)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed handling of standalone comments within nested bracketed expressions; &lt;em&gt;Black&lt;/em&gt;
will no longer produce super long lines or put all standalone comments at the end of
the expression (#22)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: don't crash and burn on empty lines with trailing whitespace
(#80)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: &lt;code&gt;# yapf: disable&lt;/code&gt; usage as trailing comment would cause
&lt;em&gt;Black&lt;/em&gt; to not emit the rest of the file (#95)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when CTRL+C is pressed while formatting many files, &lt;em&gt;Black&lt;/em&gt; no longer freaks out with
a flurry of asyncio-related exceptions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only allow up to two empty lines on module level and only single empty lines within
functions (#74)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a4" class="anchor" aria-hidden="true" href="#183a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are implemented (#5)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;automatic detection of deprecated Python 2 forms of print statements and exec
statements in the formatted file (#49)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use proper spaces for complex expressions in default values of typed function
arguments (#60)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only return exit code 1 when --check is used (#50)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't remove single trailing commas from square bracket indexing (#59)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't omit whitespace if the previous factor leaf wasn't a math operator (#55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in kwarg unpacking if it's the first argument (#46)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in
&lt;a href="http://www.sphinx-doc.org/en/stable/ext/autodoc.html#directive-autoattribute" rel="nofollow"&gt;Sphinx auto-attribute comments&lt;/a&gt;
(#68)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a3" class="anchor" aria-hidden="true" href="#183a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;don't remove single empty lines outside of bracketed expressions (#19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added ability to pipe formatting from stdin to stdin (#25)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;restored ability to format code with legacy usage of &lt;code&gt;async&lt;/code&gt; as a name (#20, #42)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;even better handling of numpy-style array indexing (#33, again)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a2" class="anchor" aria-hidden="true" href="#183a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;changed positioning of binary operators to occur at beginning of lines instead of at
the end, following
&lt;a href="https://github.com/python/peps/commit/c59c4376ad233a62ca4b3a6060c81368bd21e85b"&gt;a recent change to PEP 8&lt;/a&gt;
(#21)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ignore empty bracket pairs while splitting. This avoids very weirdly looking
formattings (#34, #35)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;remove a trailing comma if there is a single argument to a call&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if top level functions were separated by a comment, don't put four empty lines after
the upper function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting of newlines with imports&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unintentional folding of post scriptum standalone comments into last statement
if it was a simple statement (#18, #28)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing space in numpy-style array indexing (#33)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after star-based unary expressions (#31)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a1" class="anchor" aria-hidden="true" href="#183a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--check&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only put trailing commas in function signatures and calls if it's safe to do so. If
the file is Python 3.6+ it's always safe, otherwise only safe if there are no &lt;code&gt;*args&lt;/code&gt;
or &lt;code&gt;**kwargs&lt;/code&gt; used in the signature or call. (#8)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid spacing of dots in relative imports (#6, #13)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting after comma on unpacked variables in for-loops (#23)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space in parenthesized set expressions (#7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after opening parentheses and in default arguments (#14, #17)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after unary operators when the operand was a complex expression
(#15)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a0" class="anchor" aria-hidden="true" href="#183a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;first published version, Happy &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;🍰&lt;/g-emoji&gt; Day 2018!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;alpha quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date-versioned (see: &lt;a href="https://calver.org/" rel="nofollow"&gt;https://calver.org/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;Glued together by &lt;a href="mailto:lukasz@langa.pl"&gt;Łukasz Langa&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Maintained with &lt;a href="mailto:carolcode@willingconsulting.com"&gt;Carol Willing&lt;/a&gt;,
&lt;a href="mailto:carl@oddbird.net"&gt;Carl Meyer&lt;/a&gt;,
&lt;a href="mailto:jelle.zijlstra@gmail.com"&gt;Jelle Zijlstra&lt;/a&gt;,
&lt;a href="mailto:mail@autophagy.io"&gt;Mika Naylor&lt;/a&gt;, and
&lt;a href="mailto:zsol.zsol@gmail.com"&gt;Zsolt Dollenstein&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Multiple contributions by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mailto:cryptolabour@gmail.com"&gt;Abdur-Rahmaan Janhangeer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@adamj.eu"&gt;Adam Johnson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@grande.coffee"&gt;Alexander Huynh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andrew.thorp.dev@gmail.com"&gt;Andrew Thorp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:dyuuus@yandex.ru"&gt;Andrey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andy@andyfreeland.net"&gt;Andy Freeland&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asottile@umich.edu"&gt;Anthony Sottile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:arjaan.buijk@gmail.com"&gt;Arjaan Buijk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:proofit404@gmail.com"&gt;Artem Malyshev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asgerdrewsen@gmail.com"&gt;Asger Hautop Drewsen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:raf@durin42.com"&gt;Augie Fackler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:aviskarkc10@gmail.com"&gt;Aviskar KC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@benjam.info"&gt;Benjamin Woodruff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:brandtbucher@gmail.com"&gt;Brandt Bucher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Charles Reid&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:christian@python.org"&gt;Christian Heimes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:chuck.wooters@microsoft.com"&gt;Chuck Wooters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@thequod.de"&gt;Daniel Hahler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:polycitizen@gmail.com"&gt;Daniel M. Capella&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Daniele Esposti&lt;/li&gt;
&lt;li&gt;dylanjblack&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:eli@treuherz.com"&gt;Eli Treuherz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:fthiery@gmail.com"&gt;Florent Thiery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hauntsaninja&lt;/li&gt;
&lt;li&gt;Hugo van Kemenade&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ivan.katanic@gmail.com"&gt;Ivan Katanić&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@jasonfried.info"&gt;Jason Fried&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ijkl@netc.fr"&gt;jgirardet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jma353@cornell.edu"&gt;Joe Antonakakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jon.dufresne@gmail.com"&gt;Jon Dufresne&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ojiidotch@gmail.com"&gt;Jonas Obrist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:joshbode@fastmail.com"&gt;Josh Bode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hello@juanlu.space"&gt;Juan Luis Cano Rodríguez&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:katie@glasnt.com"&gt;Katie McLaughlin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lawrence Chan&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@linusgroh.de"&gt;Linus Groh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:luka.sterbic@gmail.com"&gt;Luka Sterbic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mariatta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:vaneseltine@gmail.com"&gt;Matt VanEseltine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael.flaxman@gmail.com"&gt;Michael Flaxman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sully@msully.net"&gt;Michael J. Sullivan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael@mcclimon.org"&gt;Michael McClimon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miggaiowski@gmail.com"&gt;Miguel Gaiowski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:roshi@fedoraproject.org"&gt;Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:minho42@gmail.com"&gt;Min ho Kim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miroslav@miki725.com"&gt;Miroslav Shubernetskiy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:neraste.herr10@gmail.com"&gt;Neraste&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ofekmeister@gmail.com"&gt;Ofek Lev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:osaetindaniel@gmail.com"&gt;Osaetin Daniel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:Pablogsal@gmail.com"&gt;Pablo Galindo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@peterbe.com"&gt;Peter Bengtsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pmacosta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:rishijha424@gmail.com"&gt;Rishikesh Jha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hi@stavros.io"&gt;Stavros Korokithakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sirosen@globus.org"&gt;Stephen Rosen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:snlkapil@gmail.com"&gt;Sunil Kapil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:thomas.c.lu@gmail.com"&gt;Thom Lu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:tom@tomchristie.com"&gt;Tom Christie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:uranusjr@gmail.com"&gt;Tzu-ping Chung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ukshah2@illinois.edu"&gt;Utsav Shah&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vezeli&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sharma.vishwas88@gmail.com"&gt;Vishwas B Sharma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:yngve@hoiseth.net"&gt;Yngve Høiseth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:1998uriyyo@gmail.com"&gt;Yurii Karabas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>psf</author><guid isPermaLink="false">https://github.com/psf/black</guid><pubDate>Fri, 01 Nov 2019 00:00:00 GMT</pubDate></item></channel></rss>