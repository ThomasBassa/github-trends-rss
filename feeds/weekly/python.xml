<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, This week</title><link>https://github.com/trending/python?since=weekly</link><description>The top repositories on GitHub for python, measured weekly</description><pubDate>Tue, 05 Nov 2019 01:06:21 GMT</pubDate><lastBuildDate>Tue, 05 Nov 2019 01:06:21 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>iGhibli/iOS-DeviceSupport #1 in Python, This week</title><link>https://github.com/iGhibli/iOS-DeviceSupport</link><description>&lt;p&gt;&lt;i&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ios-devicesupport" class="anchor" aria-hidden="true" href="#ios-devicesupport"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;iOS-DeviceSupport&lt;/h1&gt;
&lt;p&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;See docs: &lt;a href="https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/" rel="nofollow"&gt;https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Below command will try to unzip all new device support files to &lt;code&gt;/Applications/Xcode.app&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can use &lt;code&gt;-t&lt;/code&gt; if your Xcode is not in &lt;code&gt;/Applications/&lt;/code&gt; or has different name.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py -t /Applications/Xcode&lt;span class="pl-cce"&gt;\ &lt;/span&gt;9.app&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  -t TARGET   The path &lt;span class="pl-k"&gt;for&lt;/span&gt; Xcode&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-supported-versions" class="anchor" aria-hidden="true" href="#supported-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported versions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;iOS8
&lt;ul&gt;
&lt;li&gt;8.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.4 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS9
&lt;ul&gt;
&lt;li&gt;9.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS10
&lt;ul&gt;
&lt;li&gt;10.0 (14A345) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 (14B72) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 (14C92) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 (14E269) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS11
&lt;ul&gt;
&lt;li&gt;11.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 (15B87) &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 (15C107) &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5167d) &lt;code&gt;2018/01/30&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5201e) &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5037c) &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5061c) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F79) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS12
&lt;ul&gt;
&lt;li&gt;12.0 (16A5288q) &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5308d) &lt;code&gt;2018/06/19&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5318d) &lt;code&gt;2018/06/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5327d) &lt;code&gt;2018/07/20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5339e) &lt;code&gt;2018/07/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5354b) &lt;code&gt;2018/08/15&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A366) &lt;code&gt;2018/09/18&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5059d) &lt;code&gt;2018/09/21&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5068g) &lt;code&gt;2018/10/08&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B91) &lt;code&gt;2018/10/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5181e) &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5212e) &lt;code&gt;2019/03/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E226) &lt;code&gt;2019/03/27&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.3 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (16G73) &lt;code&gt;2019/07/22&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS13
&lt;ul&gt;
&lt;li&gt;13.0 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.0 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.1 &lt;code&gt;2019/08/28&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.2 &lt;code&gt;2019/10/02&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iGhibli</author><guid isPermaLink="false">https://github.com/iGhibli/iOS-DeviceSupport</guid><pubDate>Tue, 05 Nov 2019 00:01:00 GMT</pubDate></item><item><title>google-research/google-research #2 in Python, This week</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Tue, 05 Nov 2019 00:02:00 GMT</pubDate></item><item><title>bokeh/bokeh #3 in Python, This week</title><link>https://github.com/bokeh/bokeh</link><description>&lt;p&gt;&lt;i&gt;Interactive Data Visualization in the browser, from  Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a href="https://bokeh.org" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/23c4767f9deb6835e161e7bee64eeaa1125f0721/68747470733a2f2f7374617469632e626f6b65682e6f72672f6c6f676f732f6c6f676f747970652e737667" height="60" width="150" alt="Bokeh logotype" data-canonical-src="https://static.bokeh.org/logos/logotype.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Bokeh is a fiscally sponsored project of &lt;a href="https://numfocus.org" rel="nofollow"&gt;NumFOCUS&lt;/a&gt;, a nonprofit dedicated to supporting the open-source scientific computing community. If you like Bokeh and would like to support our mission, please consider &lt;a href="https://numfocus.org/donate-to-bokeh" rel="nofollow"&gt;making a donation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;Latest Release&lt;/td&gt;
  &lt;td&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/319e5c4665c805f31c4e03601804b746c5d74925/68747470733a2f2f62616467652e667572792e696f2f67682f626f6b6568253246626f6b65682e737667"&gt;&lt;img src="https://camo.githubusercontent.com/319e5c4665c805f31c4e03601804b746c5d74925/68747470733a2f2f62616467652e667572792e696f2f67682f626f6b6568253246626f6b65682e737667" alt="Latest release version" data-canonical-src="https://badge.fury.io/gh/bokeh%2Fbokeh.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://badge.fury.io/js/bokehjs" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/95363bfe2ac7fb7e5eb9885a50868c1dba3a9a70/68747470733a2f2f62616467652e667572792e696f2f6a732f626f6b65686a732e737667" alt="npm version" data-canonical-src="https://badge.fury.io/js/bokehjs.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Conda&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://docs.bokeh.org/en/latest/docs/installation.html" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d0117bc93858f82f1a8ea5ebf5c19cabd97df9fd/68747470733a2f2f707976697a2e6f72672f5f7374617469632f63616368652f626f6b65685f636f6e64615f646f776e6c6f6164735f62616467652e737667" alt="Conda downloads per month" data-canonical-src="https://pyviz.org/_static/cache/bokeh_conda_downloads_badge.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;License&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://github.com/bokeh/bokeh/blob/master/LICENSE.txt"&gt;
    &lt;img src="https://camo.githubusercontent.com/abf7fceca8e58b3842bfed01c2e3c2ee1612fb09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f626f6b65682f626f6b65682e737667" alt="Bokeh license (BSD 3-clause)" data-canonical-src="https://img.shields.io/github/license/bokeh/bokeh.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;PyPI&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://docs.bokeh.org/en/latest/docs/installation.html" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/5ec94c8c7308f7fb219ac302aa09a50b46b18a82/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626f6b65682e737667" alt="PyPI downloads per month" data-canonical-src="https://img.shields.io/pypi/dm/bokeh.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Sponsorship&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="http://numfocus.org" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/28d3beb4213a1bfc61313a5b5a0be78b06e96c05/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706f776572656425323062792d4e756d464f4355532d626c61636b2e7376673f7374796c653d666c617426636f6c6f72413d35423542354226636f6c6f72423d303037443841" alt="Powered by NumFOCUS" data-canonical-src="https://img.shields.io/badge/powered%20by-NumFOCUS-black.svg?style=flat&amp;amp;colorA=5B5B5B&amp;amp;colorB=007D8A" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Live Tutorial&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Live Bokeh tutorial notebooks on MyBinder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Build Status&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://travis-ci.org/bokeh/bokeh" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/8335f2a93fe3e1293a3571623b8d23ba262b1d30/68747470733a2f2f7472617669732d63692e6f72672f626f6b65682f626f6b65682e7376673f6272616e63683d6d6173746572" alt="Current TravisCI build status" data-canonical-src="https://travis-ci.org/bokeh/bokeh.svg?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://ci.appveyor.com/project/bokeh-integrations/bokeh" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/91e2de256d6ef59085db8d6fa178f4850fbf6cb3/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f753469646632356468703231396d686f3f7376673d74727565" alt="Current Appveyor build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/u4idf25dhp219mho?svg=true" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Support&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://discourse.bokeh.org" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/4bc630e601229d0ec09ce27647039334bede13ed/68747470733a2f2f696d672e736869656c64732e696f2f646973636f757273652f68747470732f646973636f757273652e626f6b65682e6f72672f706f7374732e737667" alt="Community Support on discourse.bokeh.org" data-canonical-src="https://img.shields.io/discourse/https/discourse.bokeh.org/posts.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Static Analysis&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/0045ca709534292b0c5d2f8fc628b22ca3cbcf11/68747470733a2f2f626574746572636f64656875622e636f6d2f656467652f62616467652f626f6b65682f626f6b65683f6272616e63683d6d6173746572" alt="BetterCodeHub static analysis" data-canonical-src="https://bettercodehub.com/edge/badge/bokeh/bokeh?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;Twitter&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://twitter.com/BokehPlots" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/92ffc787e59932d1af96a7126fac375f84816b55/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f626f6b6568706c6f74732e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Follow BokehPlots on Twitter" data-canonical-src="https://img.shields.io/twitter/follow/bokehplots.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;a href="https://bokeh.org" rel="nofollow"&gt;Bokeh&lt;/a&gt; is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics, and affords high-performance interactivity over large or streaming datasets. Bokeh can help anyone who would like to quickly and easily make interactive plots, dashboards, and data applications.&lt;/p&gt;
&lt;p&gt;
&lt;/p&gt;&lt;table cellspacing="10"&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/image.html" rel="nofollow"&gt;
  &lt;img alt="colormapped image plot thumbnail" src="https://camo.githubusercontent.com/f1f586b237dff8683e0ceaa2bdcb1b7e35f13f93/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f696d6167655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/image_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/anscombe.html" rel="nofollow"&gt;
  &lt;img alt="anscombe plot thumbnail" src="https://camo.githubusercontent.com/073188ee921204d9f4741a91ab4562ca196711fe/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f616e73636f6d62655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/anscombe_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/stocks.html" rel="nofollow"&gt;
  &lt;img alt="stocks plot thumbnail" src="https://camo.githubusercontent.com/e616cb26a235f6354269b268ae07eae87bac0c29/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73746f636b735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/stocks_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/lorenz.html" rel="nofollow"&gt;
  &lt;img alt="lorenz attractor plot thumbnail" src="https://camo.githubusercontent.com/7dfd551f33c5b51a099938c4e07349b28ba86387/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f6c6f72656e7a5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/lorenz_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/candlestick.html" rel="nofollow"&gt;
  &lt;img alt="candlestick plot thumbnail" src="https://camo.githubusercontent.com/c44c6fc6939b0db3896efcb4902a68674f9af381/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63616e646c65737469636b5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/candlestick_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/color_scatter.html" rel="nofollow"&gt;
  &lt;img alt="scatter plot thumbnail" src="https://camo.githubusercontent.com/c7dce46ac91e2097b13979e697d42f469c3d3b9c/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f736361747465725f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/scatter_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/iris_splom.html" rel="nofollow"&gt;
  &lt;img alt="SPLOM plot thumbnail" src="https://camo.githubusercontent.com/85ebf47ee37f3ea5093426c73b87ef7122435ba0/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73706c6f6d5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/splom_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/iris.html" rel="nofollow"&gt;
  &lt;img alt="iris dataset plot thumbnail" src="https://camo.githubusercontent.com/0198838e25d20407fc590705827632199daa32a3/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f697269735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/iris_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/histogram.html" rel="nofollow"&gt;
  &lt;img alt="histogram plot thumbnail" src="https://camo.githubusercontent.com/5ca5fb847641949daa5da2c3ebb7afec7029eb96/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f686973746f6772616d5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/histogram_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/periodic.html" rel="nofollow"&gt;
  &lt;img alt="periodic table plot thumbnail" src="https://camo.githubusercontent.com/dc1fa06b214455fd268f67ac0a4ed47a38795b52/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f706572696f6469635f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/periodic_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/texas.html" rel="nofollow"&gt;
  &lt;img alt="choropleth plot thumbnail" src="https://camo.githubusercontent.com/1d1d55f394731332a63780514673c251136f2f63/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63686f726f706c6574685f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/choropleth_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/burtin.html" rel="nofollow"&gt;
  &lt;img alt="burtin antibiotic data plot thumbnail" src="https://camo.githubusercontent.com/f8329c228dabe62292ae4727cf911d783ebe9371/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f62757274696e5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/burtin_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/streamline.html" rel="nofollow"&gt;
  &lt;img alt="streamline plot thumbnail" src="https://camo.githubusercontent.com/2a39c0491917ad87e8c1ed266d85af408953152e/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f73747265616d6c696e655f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/streamline_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/image_rgba.html" rel="nofollow"&gt;
  &lt;img alt="RGBA image plot thumbnail" src="https://camo.githubusercontent.com/5a0021bc298920caf8f200c8661df4da2dbb908f/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f696d6167655f726762615f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/image_rgba_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/brewer.html" rel="nofollow"&gt;
  &lt;img alt="stacked bars plot thumbnail" src="https://camo.githubusercontent.com/8f4eef019e452e4e04ae69e54d3a603ae20ce861/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f737461636b65645f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/stacked_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/quiver.html" rel="nofollow"&gt;
  &lt;img alt="quiver plot thumbnail" src="https://camo.githubusercontent.com/4f762bcca73dddb3cc86e60b3d43f1521409a838/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f7175697665725f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/quiver_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/elements.html" rel="nofollow"&gt;
  &lt;img alt="elements data plot thumbnail" src="https://camo.githubusercontent.com/1b46c02fcad9fa523b1194f01b7c61821b68bcae/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f656c656d656e74735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/elements_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/boxplot.html" rel="nofollow"&gt;
  &lt;img alt="boxplot thumbnail" src="https://camo.githubusercontent.com/aee991248aed1c36ce01afabea75062560915d91/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f626f78706c6f745f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/boxplot_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/categorical.html" rel="nofollow"&gt;
  &lt;img alt="categorical plot thumbnail" src="https://camo.githubusercontent.com/ad3bf9a45ea612ea5fe36c9aebfd61cdd23f82da/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f63617465676f726963616c5f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/categorical_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/unemployment.html" rel="nofollow"&gt;
  &lt;img alt="unemployment data plot thumbnail" src="https://camo.githubusercontent.com/a3b8632276b3f5adedd036d1ad6f9e7a02d6c907/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f756e656d706c6f796d656e745f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/unemployment_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
  &lt;a href="https://docs.bokeh.org/en/latest/docs/gallery/les_mis.html" rel="nofollow"&gt;
  &lt;img alt="Les Mis co-occurrence plot thumbnail" src="https://camo.githubusercontent.com/d50bb70e41286e878e465a38684ace5fb9f8e74e/68747470733a2f2f646f63732e626f6b65682e6f72672f656e2f6c61746573742f5f696d616765732f6c65735f6d69735f742e706e67" data-canonical-src="https://docs.bokeh.org/en/latest/_images/les_mis_t.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;The easiest way to install Bokeh is using the &lt;a href="https://www.anaconda.com/what-is-anaconda/" rel="nofollow"&gt;Anaconda Python distribution&lt;/a&gt; and its included &lt;em&gt;Conda&lt;/em&gt; package management system. To install Bokeh and its required dependencies, enter the following command at a Bash or Windows command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install bokeh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To install using pip, enter the following command at a Bash or Windows command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install bokeh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more information, refer to the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#quick-installation" rel="nofollow"&gt;installation documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;p&gt;Once Bokeh is installed, check out the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html#getting-started" rel="nofollow"&gt;Getting Started&lt;/a&gt; section of the &lt;a href="https://docs.bokeh.org/en/latest/docs/user_guide/quickstart.html" rel="nofollow"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit the &lt;a href="https://docs.bokeh.org" rel="nofollow"&gt;full documentation site&lt;/a&gt; to view the &lt;a href="https://docs.bokeh.org/en/dev/docs/user_guide.html" rel="nofollow"&gt;User's Guide&lt;/a&gt; or &lt;a href="https://mybinder.org/v2/gh/bokeh/bokeh-notebooks/master?filepath=tutorial%2F00%20-%20Introduction%20and%20Setup.ipynb" rel="nofollow"&gt;launch the Bokeh tutorial&lt;/a&gt; to learn about Bokeh in live Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;Community support is available on the &lt;a href="https://discourse.bokeh.org" rel="nofollow"&gt;Project Discourse&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you would like to contribute to Bokeh, please review the &lt;a href="https://docs.bokeh.org/en/latest/docs/dev_guide.html" rel="nofollow"&gt;Developer Guide&lt;/a&gt; and say hello on the &lt;a href="https://gitter.im/bokeh/bokeh-dev" rel="nofollow"&gt;bokeh-dev chat channel&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-follow-us" class="anchor" aria-hidden="true" href="#follow-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Follow us&lt;/h2&gt;
&lt;p&gt;Follow us on Twitter &lt;a href="https://twitter.com/BokehPlots" rel="nofollow"&gt;@bokehplots&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;The Bokeh project is grateful for &lt;a href="https://numfocus.org/donate-to-bokeh" rel="nofollow"&gt;individual contributions&lt;/a&gt; as well as sponsorship by the organizations and companies below:&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
    &lt;a href="https://www.numfocus.org/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d147c30d8a6d9ca4b5eb060a83c511d7c99e6948/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f6e756d666f6375732e737667" alt="NumFocus Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/numfocus.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.anaconda.com/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/64a2555013bed95305c66c8c823a419dcb1e2754/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f616e61636f6e64612e706e67" alt="Anaconda Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/anaconda.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.nvidia.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/d2e3f41f97a9833e323294d46045dc8b0e7fc66a/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f6e76696469612e706e67" alt="NVidia Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/nvidia.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://developer.nvidia.com/rapids" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/16c8825f8ae78640242f57070aab1bf8e664fa9c/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7261706964732e706e67" alt="Rapids Logo" width="200" data-canonical-src="https://static.bokeh.org/sponsor/rapids.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;table align="center"&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;
    &lt;a href="https://www.quansight.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/8b37ec4276ae3506abd62b93028c9a3b5b86459a/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7175616e73696768742e706e67" alt="Quansight Logo" width="100" data-canonical-src="https://static.bokeh.org/sponsor/quansight.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://www.rexhomes.com/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/723b4a136c3f567e7ee1f9cb71767488eef13a30/68747470733a2f2f7374617469632e626f6b65682e6f72672f73706f6e736f722f7265782e6a7067" alt="Rex Logo" width="100" data-canonical-src="https://static.bokeh.org/sponsor/rex.jpg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;If your company uses Bokeh and is able to sponsor the project, please contact &lt;a href="info@bokeh.org"&gt;&lt;/a&gt;&lt;a href="mailto:info@bokeh.org"&gt;info@bokeh.org&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bokeh</author><guid isPermaLink="false">https://github.com/bokeh/bokeh</guid><pubDate>Tue, 05 Nov 2019 00:03:00 GMT</pubDate></item><item><title>bitcoinbook/bitcoinbook #4 in Python, This week</title><link>https://github.com/bitcoinbook/bitcoinbook</link><description>&lt;p&gt;&lt;i&gt;Mastering Bitcoin 2nd Edition - Programming the Open Blockchain&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Code Examples: &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70"&gt;&lt;img src="https://camo.githubusercontent.com/57c2d755f3c0c1f750bb5dcd687aa2b5d640aa84/68747470733a2f2f7472617669732d63692e6f72672f626974636f696e626f6f6b2f626974636f696e626f6f6b2e7376673f6272616e63683d646576656c6f70" alt="travis_ci" data-canonical-src="https://travis-ci.org/bitcoinbook/bitcoinbook.svg?branch=develop" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-mastering-bitcoin" class="anchor" aria-hidden="true" href="#mastering-bitcoin"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin&lt;/h1&gt;
&lt;p&gt;Mastering Bitcoin is a book for developers, although the first two chapters cover bitcoin at a level that is also approachable to non-programmers. Anyone with a basic understanding of technology can read the first two chapters to get a great understanding of bitcoin.&lt;/p&gt;
&lt;p&gt;This repository contains the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;first edition, second print&lt;/a&gt;, published in December 2014, and the complete &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second edition, second print&lt;/a&gt;, published in July 2017, as published by O'Reilly Media in paperback and ebook formats.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-issues-errors-comments-contributions" class="anchor" aria-hidden="true" href="#issues-errors-comments-contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues, Errors, Comments, Contributions&lt;/h1&gt;
&lt;p&gt;If you know how to make a pull request to contribute a fix, please write the correction and use a pull request to submit it for consideration against the &lt;a href="https://github.com/bitcoinbook/bitcoinbook/tree/develop"&gt;develop branch&lt;/a&gt;. If you are making several changes, please use a separate commit for each to make it easier to cherry-pick or resolve conflicts. Otherwise, please submit an issue, explaining the error or comment. If you would like to contribute extensive changes or new material, please coordinate with the author first; contact information can be found on his website: &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;https://antonopoulos.com/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reading-this-book" class="anchor" aria-hidden="true" href="#reading-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reading this book&lt;/h1&gt;
&lt;p&gt;To read this book, see &lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/book.asciidoc"&gt;book.asciidoc&lt;/a&gt;. Click on each of the chapters to read in your browser. Other parties may choose to release PDFs of the book online.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-chapters" class="anchor" aria-hidden="true" href="#chapters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chapters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Chapter 1: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch01.asciidoc"&gt;Introduction&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 2: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch02.asciidoc"&gt;How Bitcoin Works&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 3: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch03.asciidoc"&gt;Bitcoin Core: The Reference Implementation&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 4: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch04.asciidoc"&gt;Keys, Addresses&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 5: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch05.asciidoc"&gt;Wallets&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 6: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch06.asciidoc"&gt;Transactions&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 7: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch07.asciidoc"&gt;Advanced Transactions and Scripting&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 8: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch08.asciidoc"&gt;The Bitcoin Network&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 9: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch09.asciidoc"&gt;The Blockchain&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 10: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch10.asciidoc"&gt;Mining and Consensus&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 11: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch11.asciidoc"&gt;Bitcoin Security&lt;/a&gt;'&lt;/li&gt;
&lt;li&gt;Chapter 12: '&lt;a href="https://github.com/bitcoinbook/bitcoinbook/blob/develop/ch12.asciidoc"&gt;Blockchain Applications&lt;/a&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-published" class="anchor" aria-hidden="true" href="#published"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Published&lt;/h1&gt;
&lt;p&gt;"Mastering Bitcoin (Second Edition, Second Print): Programming the Open Blockchain" is now available in paperback and ebook formats by many booksellers worldwide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Mastering-Bitcoin-Programming-Open-Blockchain/dp/1491954388" rel="nofollow"&gt;Amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mastering Bitcoin (First Edition Second Print) is also published in Japanese, Korean, and Chinese (Simplified) by publishers in the respective countries.&lt;/p&gt;
&lt;p&gt;Mastering Bitcoin (Open Edition), based on the First Edition, has been translated by volunteers into more than a dozen languages. Translations are available for free under CC-BY-SA license at: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-source" class="anchor" aria-hidden="true" href="#source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source&lt;/h1&gt;
&lt;p&gt;The book's source code, found in this repository, is kept synchronized with the print and ebook editions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---first-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---first-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - First Edition&lt;/h2&gt;
&lt;p&gt;The tags &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print1"&gt;Edition1Print1&lt;/a&gt;, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/Edition1Print2"&gt;Edition1Print2&lt;/a&gt; correspond to the two existing prints of Mastering Bitcoin (First Edition) as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - First Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This "Free Culture" compliant license was approved by my publisher O'Reilly Media (&lt;a href="http://oreilly.com" rel="nofollow"&gt;http://oreilly.com&lt;/a&gt;), who understands the value of open source. O'Reilly Media is not just the world's best publisher of technical books, but is also a strong supporter of this open culture and the sharing of knowledge.&lt;/p&gt;
&lt;p&gt;Thank you O'Reilly Media!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-mastering-bitcoin---second-edition" class="anchor" aria-hidden="true" href="#mastering-bitcoin---second-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mastering Bitcoin - Second Edition&lt;/h2&gt;
&lt;p&gt;The tags, &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print_1"&gt;second_edition_print_1&lt;/a&gt; and  &lt;a href="https://github.com/bitcoinbook/bitcoinbook/releases/tag/second_edition_print2"&gt;second_edition_print2&lt;/a&gt;, correspond to the first (June 8th, 2017) and second (July 20th, 2017) print of Mastering Bitcoin (Second Edition), as published by O'Reilly Media.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/e170e276291254896665fa8f612b99fe5b7dd005/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d73612f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Mastering Bitcoin - Second Edition&lt;/span&gt; by &lt;a href="https://antonopoulos.com/" rel="nofollow"&gt;Andreas M. Antonopoulos LLC&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;p&gt;If you are interested in translating this book, please join our team of volunteers at: &lt;a href="https://www.transifex.com/aantonop/mastering-bitcoin" rel="nofollow"&gt;https://www.transifex.com/aantonop/mastering-bitcoin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Free copies of "Mastering Bitcoin Open Edition," translated in many languages, can be downloaded from: &lt;a href="https://bitcoinbook.info" rel="nofollow"&gt;https://bitcoinbook.info&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bitcoinbook</author><guid isPermaLink="false">https://github.com/bitcoinbook/bitcoinbook</guid><pubDate>Tue, 05 Nov 2019 00:04:00 GMT</pubDate></item><item><title>eriklindernoren/ML-From-Scratch #5 in Python, This week</title><link>https://github.com/eriklindernoren/ML-From-Scratch</link><description>&lt;p&gt;&lt;i&gt;Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-from-scratch" class="anchor" aria-hidden="true" href="#machine-learning-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning From Scratch&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.&lt;/p&gt;
&lt;p&gt;The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine-learning-from-scratch"&gt;Machine Learning From Scratch&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about"&gt;About&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#examples"&gt;Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#polynomial-regression"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classification-with-cnn"&gt;Classification With CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#density-based-clustering"&gt;Density-Based Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generating-handwritten-digits"&gt;Generating Handwritten Digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-reinforcement-learning"&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-reconstruction-with-rbm"&gt;Image Reconstruction With RBM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#evolutionary-evolved-neural-network"&gt;Evolutionary Evolved Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#genetic-algorithm"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#association-analysis"&gt;Association Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-polynomial-regression" class="anchor" aria-hidden="true" href="#polynomial-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Polynomial Regression&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/polynomial_regression.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d82416364e7916546886f94027e2652d3247e8ab/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f705f7265672e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/p_reg.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in Linköping, Sweden 2016.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-classification-with-cnn" class="anchor" aria-hidden="true" href="#classification-with-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Classification With CNN&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c2bca09f5d1ce2b72f33fe61464408607797caa3/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f636e6e312e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_cnn1.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-density-based-clustering" class="anchor" aria-hidden="true" href="#density-based-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Density-Based Clustering&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/dbscan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/eaf413b6e8cbf3f8fd048f3a63984482ffd7350e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64627363616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dbscan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generating-handwritten-digits" class="anchor" aria-hidden="true" href="#generating-handwritten-digits"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Handwritten Digits&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966"&gt;&lt;img src="https://camo.githubusercontent.com/15ad5010011227a7ab8c6c77d19b7cc625cced30/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f67616e5f6d6e697374352e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/gan_mnist5.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-reinforcement-learning" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Reinforcement Learning&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/c605134f41b739121c4710f3d5c6e8370a592e0c/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6d6c66735f64716c312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/mlfs_dql1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-image-reconstruction-with-rbm" class="anchor" aria-hidden="true" href="#image-reconstruction-with-rbm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Reconstruction With RBM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/restricted_boltzmann_machine.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966"&gt;&lt;img src="https://camo.githubusercontent.com/d209d42aed9e8e32a10eaec9b76f141319a2b0d7/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f72626d5f646967697473312e676966" width="640" data-canonical-src="http://eriklindernoren.se/images/rbm_digits1.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-evolutionary-evolved-neural-network" class="anchor" aria-hidden="true" href="#evolutionary-evolved-neural-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evolutionary Evolved Neural Network&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1a8abe4882d0195b8f8bd4c6f24caab639291e6e/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f65766f5f6e6e342e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/evo_nn4.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-genetic-algorithm" class="anchor" aria-hidden="true" href="#genetic-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Genetic Algorithm&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-association-analysis" class="anchor" aria-hidden="true" href="#association-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Association Analysis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -&amp;gt; 2 (support: 0.43, confidence: 1.0)
    4 -&amp;gt; 2 (support: 0.57, confidence: 0.8)
    [1, 4] -&amp;gt; 2 (support: 0.29, confidence: 1.0)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-supervised-learning" class="anchor" aria-hidden="true" href="#supervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/adaboost.py"&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/bayesian_regression.py"&gt;Bayesian Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/decision_tree.py"&gt;Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Elastic Net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/gradient_boosting.py"&gt;Gradient Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/k_nearest_neighbors.py"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Lasso Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/linear_discriminant_analysis.py"&gt;Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/logistic_regression.py"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multi_class_lda.py"&gt;Multi-class Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/naive_bayes.py"&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/neuroevolution.py"&gt;Neuroevolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/particle_swarm_optimization.py"&gt;Particle Swarm Optimization of Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/perceptron.py"&gt;Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/random_forest.py"&gt;Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/regression.py"&gt;Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/support_vector_machine.py"&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/supervised_learning/xgboost.py"&gt;XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-unsupervised-learning" class="anchor" aria-hidden="true" href="#unsupervised-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/apriori.py"&gt;Apriori&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/autoencoder.py"&gt;Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/dbscan.py"&gt;DBSCAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/fp_growth.py"&gt;FP-Growth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/gaussian_mixture_model.py"&gt;Gaussian Mixture Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/generative_adversarial_network.py"&gt;Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/genetic_algorithm.py"&gt;Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/k_means.py"&gt;K-Means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/partitioning_around_medoids.py"&gt;Partitioning Around Medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/principal_component_analysis.py"&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py"&gt;Restricted Boltzmann Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/reinforcement_learning/deep_q_network.py"&gt;Deep Q-Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/neural_network.py"&gt;Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/deep_learning/layers.py"&gt;Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Activation Layer&lt;/li&gt;
&lt;li&gt;Average Pooling Layer&lt;/li&gt;
&lt;li&gt;Batch Normalization Layer&lt;/li&gt;
&lt;li&gt;Constant Padding Layer&lt;/li&gt;
&lt;li&gt;Convolutional Layer&lt;/li&gt;
&lt;li&gt;Dropout Layer&lt;/li&gt;
&lt;li&gt;Flatten Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected (Dense) Layer&lt;/li&gt;
&lt;li&gt;Fully-Connected RNN Layer&lt;/li&gt;
&lt;li&gt;Max Pooling Layer&lt;/li&gt;
&lt;li&gt;Reshape Layer&lt;/li&gt;
&lt;li&gt;Up Sampling Layer&lt;/li&gt;
&lt;li&gt;Zero Padding Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Model Types
&lt;ul&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/convolutional_neural_network.py"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/multilayer_perceptron.py"&gt;Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mlfromscratch/examples/recurrent_neural_network.py"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If there's some implementation you would like to see here or if you're just feeling social,
feel free to &lt;a href="mailto:eriklindernoren@gmail.com"&gt;email&lt;/a&gt; me or connect with me on &lt;a href="https://www.linkedin.com/in/eriklindernoren/" rel="nofollow"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/ML-From-Scratch</guid><pubDate>Tue, 05 Nov 2019 00:05:00 GMT</pubDate></item><item><title>521xueweihan/HelloGitHub #6 in Python, This week</title><link>https://github.com/521xueweihan/HelloGitHub</link><description>&lt;p&gt;&lt;i&gt;:octocat: Find pearls on open-source seashore 分享 GitHub 上有趣、入门级的开源项目&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif"&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;中文 | &lt;a href="README_en.md"&gt;English&lt;/a&gt;
  &lt;br&gt;&lt;strong&gt;HelloGitHub&lt;/strong&gt; 一个分享 GitHub 上有趣、入门级的开源项目。&lt;br&gt;兴趣是最好的老师，这里能够帮你找到编程的兴趣！
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://hellogithub.com/weixin.png" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/61343b85520a4714ddb37eb300f8268cc881ae7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54616c6b2d2545352542452541452545342542462541312545372542452541342d627269676874677265656e2e7376673f7374796c653d706f706f75742d737175617265" alt="WeiXin" data-canonical-src="https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/stargazers"&gt;&lt;img src="https://camo.githubusercontent.com/0aec7fa1a5647255bbe8af37a82a007be69d8739/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/issues"&gt;&lt;img src="https://camo.githubusercontent.com/a8367e38e94eccf7e469023edfec05db15132454/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4627590b5d81a690c6c83abaf47f678d70d26e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362542302545362542352541412d576569626f2d7265642e7376673f7374796c653d706f706f75742d737175617265" alt="Sina Weibo" data-canonical-src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;这是一个面向编程新手、热爱编程、对开源社区感兴趣人群的项目，内容&lt;strong&gt;每月 28 号&lt;/strong&gt;以月刊的形式更新发布。内容包括：&lt;strong&gt;流行项目&lt;/strong&gt;、&lt;strong&gt;入门级项目&lt;/strong&gt;、&lt;strong&gt;让生活变得更美好的工具&lt;/strong&gt;、&lt;strong&gt;书籍&lt;/strong&gt;、&lt;strong&gt;学习心得笔记&lt;/strong&gt;、&lt;strong&gt;企业级项目&lt;/strong&gt;等，这些开源项目大多都是非常容易上手、很 Cool，能够让你用很短时间感受到编程的魅力和便捷。从而让大家感受到编程的乐趣，动手开始编程。&lt;/p&gt;
&lt;p&gt;希望通过本项目能够有更多人加入到开源社区、回馈社区。&lt;strong&gt;让有趣、有价值的项目被更多人发现和加入&lt;/strong&gt;。在参与这些项目的过程中，你将得到：&lt;strong&gt;热爱编程的小伙伴&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="man_dancing" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f57a.png"&gt;🕺&lt;/g-emoji&gt; 、&lt;strong&gt;更多编程知识&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt; 、&lt;strong&gt;优秀的编程技巧&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; 、&lt;strong&gt;找到编程的乐趣&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="video_game" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ae.png"&gt;🎮&lt;/g-emoji&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;『每日精选』&lt;/strong&gt; 关注我们的&lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;最惨官微&lt;/a&gt;获取最新项目推荐。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;『讲解开源项目』&lt;/strong&gt; 欢迎开源爱好者给我们投稿&lt;a href="https://github.com/HelloGitHub-Team/Article/blob/master/%E5%88%9B%E4%BD%9C%E9%A1%BB%E7%9F%A5.md"&gt;查看创作须知&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-内容" class="anchor" aria-hidden="true" href="#内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容&lt;/h2&gt;
&lt;p&gt;每月 28 号发布&lt;a href="/content/last.md"&gt;最新一期&lt;/a&gt; | &lt;a href="https://hellogithub.com" rel="nofollow"&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img class="emoji" title=":shipit:" alt=":shipit:" src="https://github.githubassets.com/images/icons/emoji/shipit.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="jack_o_lantern" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f383.png"&gt;🎃&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="beer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37a.png"&gt;🍺&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="fish_cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f365.png"&gt;🍥&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/43/HelloGitHub43.md"&gt;第 43 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/42/HelloGitHub42.md"&gt;第 42 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/41/HelloGitHub41.md"&gt;第 41 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/40/HelloGitHub40.md"&gt;第 40 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/39/HelloGitHub39.md"&gt;第 39 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/38/HelloGitHub38.md"&gt;第 38 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/37/HelloGitHub37.md"&gt;第 37 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/36/HelloGitHub36.md"&gt;第 36 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/35/HelloGitHub35.md"&gt;第 35 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/34/HelloGitHub34.md"&gt;第 34 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/33/HelloGitHub33.md"&gt;第 33 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/32/HelloGitHub32.md"&gt;第 32 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/31/HelloGitHub31.md"&gt;第 31 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/30/HelloGitHub30.md"&gt;第 30 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/29/HelloGitHub29.md"&gt;第 29 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/28/HelloGitHub28.md"&gt;第 28 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/27/HelloGitHub27.md"&gt;第 27 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/26/HelloGitHub26.md"&gt;第 26 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/25/HelloGitHub25.md"&gt;第 25 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/24/HelloGitHub24.md"&gt;第 24 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/23/HelloGitHub23.md"&gt;第 23 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/22/HelloGitHub22.md"&gt;第 22 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/21/HelloGitHub21.md"&gt;第 21 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/20/HelloGitHub20.md"&gt;第 20 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/19/HelloGitHub19.md"&gt;第 19 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/18/HelloGitHub18.md"&gt;第 18 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/17/HelloGitHub17.md"&gt;第 17 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/16/HelloGitHub16.md"&gt;第 16 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/15/HelloGitHub15.md"&gt;第 15 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/14/HelloGitHub14.md"&gt;第 14 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/13/HelloGitHub13.md"&gt;第 13 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/12/HelloGitHub12.md"&gt;第 12 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/11/HelloGitHub11.md"&gt;第 11 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/10/HelloGitHub10.md"&gt;第 10 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/09/HelloGitHub09.md"&gt;第 09 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/08/HelloGitHub08.md"&gt;第 08 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/07/HelloGitHub07.md"&gt;第 07 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/06/HelloGitHub06.md"&gt;第 06 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/05/HelloGitHub05.md"&gt;第 05 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/04/HelloGitHub04.md"&gt;第 04 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/03/HelloGitHub03.md"&gt;第 03 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/02/HelloGitHub02.md"&gt;第 02 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/01/HelloGitHub01.md"&gt;第 01 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;欢迎&lt;a href="https://github.com/521xueweihan/HelloGitHub/issues/new"&gt;推荐或自荐项目&lt;/a&gt;成为 &lt;strong&gt;HelloGitHub&lt;/strong&gt; 的&lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;贡献者&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-贡献者" class="anchor" aria-hidden="true" href="#贡献者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;贡献者&lt;/h2&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/8255800?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;削微寒&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ming995"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/46031112?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;糖醋里脊&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FrontMage"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/17007026?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FrontMage&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/xibinyue"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/14122146?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;xibinyue&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/Eurus-Holmes"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/34226570?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Feiyang Chen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ChungZH"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/42088872?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;ChungZH&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/daixiang0"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/26538619?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;daixiang0&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/nivance"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/3291404?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;nivance&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/hellowHuaairen"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/19610305?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;hellowHuaairen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/17665302?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;更多贡献者&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-合作组织" class="anchor" aria-hidden="true" href="#合作组织"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;合作组织&lt;/h2&gt;
&lt;p&gt;欢迎各种&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;开源组织合作&lt;a href="Mailto:595666367@qq.com"&gt;点击联系我&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FGDBTKD"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40509403?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FGDBTKD&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;AI/ML/DL/NLP&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/d2-projects"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40857578?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;D2 Projects&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Vue/JavaScript&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/doocs"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/43716716?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Doocs&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Technical Knowledge&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;&lt;img alt="知识共享许可协议" src="https://camo.githubusercontent.com/1ae74a56e22c4897b6fbfb9f301bd829c77429a7/68747470733a2f2f6c6963656e7365627574746f6e732e6e65742f6c2f62792d6e632d6e642f342e302f38387833312e706e67" data-canonical-src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;本作品采用 &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 进行许可。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>521xueweihan</author><guid isPermaLink="false">https://github.com/521xueweihan/HelloGitHub</guid><pubDate>Tue, 05 Nov 2019 00:06:00 GMT</pubDate></item><item><title>KubeOperator/KubeOperator #7 in Python, This week</title><link>https://github.com/KubeOperator/KubeOperator</link><description>&lt;p&gt;&lt;i&gt;KubeOperator 是一个开源项目，通过 Web UI 在 VMware、OpenStack、物理机上一键部署和管理生产级别的 Kubernetes 集群。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kubeoperator---从这里开启您的-kubernetes-之旅" class="anchor" aria-hidden="true" href="#kubeoperator---从这里开启您的-kubernetes-之旅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KubeOperator - 从这里开启您的 Kubernetes 之旅&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/KubeOperatpr/KubeOperatpr/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/7197a397ba1baf73679f3cf0edf68d821c35ae52/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d61706163686525323076322d626c75652e737667" alt="License" data-canonical-src="http://img.shields.io/badge/license-apache%20v2-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.python.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4c11ac2b538cba463dfd1e43d05fe4f30f2d33d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d677265656e2e7376673f7374796c653d706c6173746963" alt="Python3" data-canonical-src="https://img.shields.io/badge/python-3.6-green.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/35798c7a6bb116ad2e8d420db49766bce91239b1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646a616e676f2d322e312d627269676874677265656e2e7376673f7374796c653d706c6173746963" alt="Django" data-canonical-src="https://img.shields.io/badge/django-2.1-brightgreen.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.ansible.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dbfb9037d993ab109b0dd41252b2aabcd703e4a5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e7369626c652d322e362e352d626c75652e7376673f7374796c653d706c6173746963" alt="Ansible" data-canonical-src="https://img.shields.io/badge/ansible-2.6.5-blue.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.angular.cn/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9829fdfaae3736e19d738b29efaeec4aaf21c61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616e67756c61722d372e302e342d7265642e7376673f7374796c653d706c6173746963" alt="Angular" data-canonical-src="https://img.shields.io/badge/angular-7.0.4-red.svg?style=plastic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator 是一个开源项目，在离线网络环境下，通过可视化 Web UI 在 VMware、Openstack 或者物理机上规划、部署和管理生产级别的 Kubernetes 集群。KubeOperator 是 &lt;a href="https://github.com/jumpserver/jumpserver"&gt;Jumpserver&lt;/a&gt; 明星开源团队在 Kubernetes 领域的的又一全新力作。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/overview.png?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/overview.png?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-web-ui-展示" class="anchor" aria-hidden="true" href="#web-ui-展示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web UI 展示&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/KubeOperator/website/master/images/kubeoperator-ui.jpg" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;更多功能截屏请查看：&lt;a href="https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot" rel="nofollow"&gt;https://docs.kubeoperator.io/kubeoperator-v2.1/screenshot&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-整体架构" class="anchor" aria-hidden="true" href="#整体架构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;整体架构&lt;/h2&gt;
&lt;p&gt;KubeOperator 使用 Terraform 在 IaaS 平台上自动创建主机（用户也可以自行准备主机，比如物理机或者虚机），通过 Ansible 完成自动化部署和变更操作，支持 Kubernetes 集群 从 Day 0 规划，到 Day 1 部署，到 Day 2 运维及变更的全生命周期管理。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/KubeOperator/docs/blob/master/website/static/img/KubeOperator.jpeg?raw=true"&gt;&lt;img src="https://github.com/KubeOperator/docs/raw/master/website/static/img/KubeOperator.jpeg?raw=true" alt="overview" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-技术优势" class="anchor" aria-hidden="true" href="#技术优势"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;技术优势&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;按需创建：调用云平台 API，一键快速创建和部署 Kubernetes 集群 (即 Kubernetes as a Service)；&lt;/li&gt;
&lt;li&gt;按需伸缩：快速伸缩 Kubernetes 集群，优化资源使用效率；&lt;/li&gt;
&lt;li&gt;按需修补：快速升级和修补 Kubernetes 集群，并与社区最新版本同步，保证安全性；&lt;/li&gt;
&lt;li&gt;自我修复：通过重建故障节点确保集群可用性；&lt;/li&gt;
&lt;li&gt;离线部署：持续更新包括 Kubernetes 及常用组件的离线包；&lt;/li&gt;
&lt;li&gt;Multi-AZ 支持：通过把 Kubernetes 集群 Master 节点分布在不同的故障域上确保的高可用；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-demo-视频使用文档" class="anchor" aria-hidden="true" href="#demo-视频使用文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Demo 视频、使用文档&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://kubeoperator-1256577600.file.myqcloud.com/video/KubeOperator2.1.mp4" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="tv" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4fa.png"&gt;📺&lt;/g-emoji&gt;8 分钟演示视频&lt;/a&gt;：详细演示 KubeOperator 的功能。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.kubeoperator.io/" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt;安装及使用文档&lt;/a&gt;：包括 KubeOperator 安装文档、使用文档、功能截屏、常见问题等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-kubernetes-离线安装包" class="anchor" aria-hidden="true" href="#kubernetes-离线安装包"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kubernetes 离线安装包&lt;/h2&gt;
&lt;p&gt;KubeOperator 提供完整的离线 Kubernetes 安装包（包括 Kubernetes、Docker、etcd、Dashboard、Promethus、OS 补丁等），每个安装包会被构建成一个独立容器镜像供 KubeOperator 使用，具体信息请参考：&lt;a href="https://github.com/KubeOperator/k8s-package"&gt;k8s-package&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本规划" class="anchor" aria-hidden="true" href="#版本规划"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本规划&lt;/h2&gt;
&lt;p&gt;v1.0 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提供原生 Kubernetes 的离线包仓库；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持一主多节点部署模式；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持离线环境下的一键自动化部署，可视化展示集群部署进展和结果；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 内置 Kubernetes 常用系统应用的安装，包括 Registry、Promethus、Dashboard、Traefik Ingress、Helm 等；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 提供简易明了的 Kubernetes 集群运行状况面板；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 NFS 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Flannel 网络插件；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群手动部署模式（自行准备主机和 NFS）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.0 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持调用 VMware vCenter API 自动创建集群主机；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 VMware vSAN 、VMFS/NFS 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Multi AZ，支持多主多节点部署模式；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Calico 网络插件；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 内置 Weave Scope；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持通过 F5 BIG-IP Controller 对外暴露服务（Nodeport mode, 七层和四层服务都支持）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.1 （已发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Openstack 云平台；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Openstack Cinder 作为持久化存储；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群升级 （Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群扩缩容（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群备份与恢复（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 Kubernetes 集群健康检查与诊断（Day 2）；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; 支持 &lt;a href="https://github.com/webkubectl/webkubectl"&gt;webkubectl&lt;/a&gt; ；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.2 （进行中，2019.11.30 发布）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; K8s 日志收集及管理方案；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; KubeOperator 自身的系统日志收集和管理；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 新增概览页面：展示关键信息，比如状态、容量、TOP 使用率、异常日志、异常容器等信息；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;v2.3 （计划中）&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; KubeApps 应用商店；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 离线环境下使用 Sonobuoy 进行 Kubernetes 集群合规检查并可视化展示结果；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 国际化支持；&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 支持 VMware NSX-T；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-沟通交流" class="anchor" aria-hidden="true" href="#沟通交流"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;沟通交流&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;技术交流 QQ 群：825046920；&lt;/li&gt;
&lt;li&gt;技术支持邮箱：&lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt;；&lt;/li&gt;
&lt;li&gt;微信群： 搜索微信号 wh_it0224，添加好友，备注（城市-github用户名）, 验证通过会加入群聊；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/hashicorp/terraform"&gt;Terraform&lt;/a&gt;: KubeOperator 采用 Terraform 来自动创建虚机；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vmware/clarity/"&gt;Clarity&lt;/a&gt;: KubeOperator 采用 Clarity 作为前端 Web 框架；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ansible/ansible"&gt;Ansible&lt;/a&gt;: KubeOperator 采用 Ansible 作为自动化部署工具；&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/easzlab/kubeasz"&gt;kubeasz&lt;/a&gt;: 提供各种 Kubernetes Ansible 脚本；&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2014-2019 FIT2CLOUD 飞致云&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.fit2cloud.com" rel="nofollow"&gt;https://www.fit2cloud.com&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;KubeOperator is licensed under the Apache License, Version 2.0.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>KubeOperator</author><guid isPermaLink="false">https://github.com/KubeOperator/KubeOperator</guid><pubDate>Tue, 05 Nov 2019 00:07:00 GMT</pubDate></item><item><title>google-research/bert #8 in Python, This week</title><link>https://github.com/google-research/bert</link><description>&lt;p&gt;&lt;i&gt;TensorFlow code and pre-trained models for BERT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bert" class="anchor" aria-hidden="true" href="#bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BERT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;***** New May 31st, 2019: Whole Word Masking Models *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a release of several new models which were the result of an improvement
the pre-processing code.&lt;/p&gt;
&lt;p&gt;In the original pre-processing code, we randomly select WordPiece tokens to
mask. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head&lt;/code&gt;
&lt;code&gt;Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The new technique is called Whole Word Masking. In this case, we always mask
&lt;em&gt;all&lt;/em&gt; of the the tokens corresponding to a word at once. The overall masking
rate remains the same.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The training is identical -- we still predict each masked WordPiece token
independently. The improvement comes from the fact that the original prediction
task was too 'easy' for words that had been split into multiple WordPieces.&lt;/p&gt;
&lt;p&gt;This can be enabled during data generation by passing the flag
&lt;code&gt;--do_whole_word_mask=True&lt;/code&gt; to &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pre-trained models with Whole Word Masking are linked below. The data and
training were otherwise identical, and the models have identical structure and
vocab to the original models. We only include BERT-Large models. When using
these models, please make it clear in the paper that you are using the Whole
Word Masking variant of BERT-Large.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;SQUAD 1.1 F1/EM&lt;/th&gt;
&lt;th align="center"&gt;Multi NLI Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.0/84.3&lt;/td&gt;
&lt;td align="center"&gt;86.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.8/86.7&lt;/td&gt;
&lt;td align="center"&gt;87.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.5/84.8&lt;/td&gt;
&lt;td align="center"&gt;86.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.9/86.7&lt;/td&gt;
&lt;td align="center"&gt;86.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;***** New February 7th, 2019: TfHub Module *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BERT has been uploaded to &lt;a href="https://tfhub.dev" rel="nofollow"&gt;TensorFlow Hub&lt;/a&gt;. See
&lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt; for an example of how to use the TF Hub module,
or run an example in the browser on
&lt;a href="https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="nofollow"&gt;Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 23rd, 2018: Un-normalized multilingual model + Thai +
Mongolian *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We uploaded a new multilingual model which does &lt;em&gt;not&lt;/em&gt; perform any normalization
on the input (no lower casing, accent stripping, or Unicode normalization), and
additionally inclues Thai and Mongolian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is recommended to use this version for developing multilingual models,
especially on languages with non-Latin alphabets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This does not require any code changes, and can be downloaded here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;***** New November 15th, 2018: SOTA SQuAD 2.0 System *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is
currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the
README for details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 5th, 2018: Third-party PyTorch and Chainer versions of
BERT available *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NLP researchers from HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. Sosuke Kobayashi also made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
(Thanks!) We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 3rd, 2018: Multilingual and Chinese models available
*****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have made two new BERT models available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use character-based tokenization for Chinese, and WordPiece tokenization for
all other languages. Both models should work out-of-the-box without any code
changes. We did update the implementation of &lt;code&gt;BasicTokenizer&lt;/code&gt; in
&lt;code&gt;tokenization.py&lt;/code&gt; to support Chinese character tokenization, so please update if
you forked it. However, we did not change the tokenization API.&lt;/p&gt;
&lt;p&gt;For more, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** End new information *****&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;, or &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from
&lt;strong&gt;T&lt;/strong&gt;ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.&lt;/p&gt;
&lt;p&gt;Our academic paper which describes BERT in detail and provides full results on a
number of tasks can be found here:
&lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a few numbers, here are the results on the
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD v1.1&lt;/a&gt; question answering
task:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQuAD v1.1 Leaderboard (Oct 8th 2018)&lt;/th&gt;
&lt;th align="center"&gt;Test EM&lt;/th&gt;
&lt;th align="center"&gt;Test F1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Ensemble - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;93.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Ensemble - nlnet&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;td align="center"&gt;91.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Single Model - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;85.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Single Model - nlnet&lt;/td&gt;
&lt;td align="center"&gt;83.5&lt;/td&gt;
&lt;td align="center"&gt;90.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And several natural language inference tasks:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th align="center"&gt;MultiNLI&lt;/th&gt;
&lt;th align="center"&gt;Question NLI&lt;/th&gt;
&lt;th align="center"&gt;SWAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI GPT (Prev. SOTA)&lt;/td&gt;
&lt;td align="center"&gt;82.2&lt;/td&gt;
&lt;td align="center"&gt;88.1&lt;/td&gt;
&lt;td align="center"&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plus many other tasks.&lt;/p&gt;
&lt;p&gt;Moreover, these results were all obtained with almost no task-specific neural
network architecture design.&lt;/p&gt;
&lt;p&gt;If you already know what BERT is and you just want to get started, you can
&lt;a href="#pre-trained-models"&gt;download the pre-trained models&lt;/a&gt; and
&lt;a href="#fine-tuning-with-bert"&gt;run a state-of-the-art fine-tuning&lt;/a&gt; in only a few
minutes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bert" class="anchor" aria-hidden="true" href="#what-is-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT is a method of pre-training language representations, meaning that we train
a general-purpose "language understanding" model on a large text corpus (like
Wikipedia), and then use that model for downstream NLP tasks that we care about
(like question answering). BERT outperforms previous methods because it is the
first &lt;em&gt;unsupervised&lt;/em&gt;, &lt;em&gt;deeply bidirectional&lt;/em&gt; system for pre-training NLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unsupervised&lt;/em&gt; means that BERT was trained using only a plain text corpus, which
is important because an enormous amount of plain text data is publicly available
on the web in many languages.&lt;/p&gt;
&lt;p&gt;Pre-trained representations can also either be &lt;em&gt;context-free&lt;/em&gt; or &lt;em&gt;contextual&lt;/em&gt;,
and contextual representations can further be &lt;em&gt;unidirectional&lt;/em&gt; or
&lt;em&gt;bidirectional&lt;/em&gt;. Context-free models such as
&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow"&gt;word2vec&lt;/a&gt; or
&lt;a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow"&gt;GloVe&lt;/a&gt; generate a single "word
embedding" representation for each word in the vocabulary, so &lt;code&gt;bank&lt;/code&gt; would have
the same representation in &lt;code&gt;bank deposit&lt;/code&gt; and &lt;code&gt;river bank&lt;/code&gt;. Contextual models
instead generate a representation of each word that is based on the other words
in the sentence.&lt;/p&gt;
&lt;p&gt;BERT was built upon recent work in pre-training contextual representations —
including &lt;a href="https://arxiv.org/abs/1511.01432" rel="nofollow"&gt;Semi-supervised Sequence Learning&lt;/a&gt;,
&lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Generative Pre-Training&lt;/a&gt;,
&lt;a href="https://allennlp.org/elmo" rel="nofollow"&gt;ELMo&lt;/a&gt;, and
&lt;a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="nofollow"&gt;ULMFit&lt;/a&gt;
— but crucially these models are all &lt;em&gt;unidirectional&lt;/em&gt; or &lt;em&gt;shallowly
bidirectional&lt;/em&gt;. This means that each word is only contextualized using the words
to its left (or right). For example, in the sentence &lt;code&gt;I made a bank deposit&lt;/code&gt; the
unidirectional representation of &lt;code&gt;bank&lt;/code&gt; is only based on &lt;code&gt;I made a&lt;/code&gt; but not
&lt;code&gt;deposit&lt;/code&gt;. Some previous work does combine the representations from separate
left-context and right-context models, but only in a "shallow" manner. BERT
represents "bank" using both its left and right context — &lt;code&gt;I made a ... deposit&lt;/code&gt;
— starting from the very bottom of a deep neural network, so it is &lt;em&gt;deeply
bidirectional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;BERT uses a simple approach for this: We mask out 15% of the words in the input,
run the entire sequence through a deep bidirectional
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; encoder, and then predict only
the masked words. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to learn relationships between sentences, we also train on a simple
task which can be generated from any monolingual corpus: Given two sentences &lt;code&gt;A&lt;/code&gt;
and &lt;code&gt;B&lt;/code&gt;, is &lt;code&gt;B&lt;/code&gt; the actual next sentence that comes after &lt;code&gt;A&lt;/code&gt;, or just a random
sentence from the corpus?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then train a large model (12-layer to 24-layer Transformer) on a large corpus
(Wikipedia + &lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt;) for a long time (1M
update steps), and that's BERT.&lt;/p&gt;
&lt;p&gt;Using BERT has two stages: &lt;em&gt;Pre-training&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a
one-time procedure for each language (current models are English-only, but
multilingual models will be released in the near future). We are releasing a
number of pre-trained models from the paper which were pre-trained at Google.
Most NLP researchers will never need to pre-train their own model from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; is inexpensive. All of the results in the paper can be
replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,
starting from the exact same pre-trained model. SQuAD, for example, can be
trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of
91.0%, which is the single system state-of-the-art.&lt;/p&gt;
&lt;p&gt;The other important aspect of BERT is that it can be adapted to many types of
NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on
sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level
(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific
modifications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-has-been-released-in-this-repository" class="anchor" aria-hidden="true" href="#what-has-been-released-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What has been released in this repository?&lt;/h2&gt;
&lt;p&gt;We are releasing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow code for the BERT model architecture (which is mostly a standard
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; architecture).&lt;/li&gt;
&lt;li&gt;Pre-trained checkpoints for both the lowercase and cased version of
&lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; from the paper.&lt;/li&gt;
&lt;li&gt;TensorFlow code for push-button replication of the most important
fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud
TPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained models&lt;/h2&gt;
&lt;p&gt;We are releasing the &lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; models from the paper.
&lt;code&gt;Uncased&lt;/code&gt; means that the text has been lowercased before WordPiece tokenization,
e.g., &lt;code&gt;John Smith&lt;/code&gt; becomes &lt;code&gt;john smith&lt;/code&gt;. The &lt;code&gt;Uncased&lt;/code&gt; model also strips out any
accent markers. &lt;code&gt;Cased&lt;/code&gt; means that the true case and accent markers are
preserved. Typically, the &lt;code&gt;Uncased&lt;/code&gt; model is better unless you know that case
information is important for your task (e.g., Named Entity Recognition or
Part-of-Speech tagging).&lt;/p&gt;
&lt;p&gt;These models are all released under the same license as the source code (Apache
2.0).&lt;/p&gt;
&lt;p&gt;For information about the Multilingual and Chinese model, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When using a cased model, make sure to pass &lt;code&gt;--do_lower=False&lt;/code&gt; to the training
scripts. (Or pass &lt;code&gt;do_lower_case=False&lt;/code&gt; directly to &lt;code&gt;FullTokenizer&lt;/code&gt; if you're
using your own script.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The links to the models are here (right-click, 'Save link as...' on the name):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads , 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased (New, recommended)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Uncased (Orig, not recommended)&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each .zip file contains three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained
weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of
the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fine-tuning-with-bert" class="anchor" aria-hidden="true" href="#fine-tuning-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with BERT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: All results on the paper were fine-tuned on a single Cloud TPU,
which has 64GB of RAM. It is currently not possible to re-produce most of the
&lt;code&gt;BERT-Large&lt;/code&gt; results on the paper using a GPU with 12GB - 16GB of RAM, because
the maximum batch size that can fit in memory is too small. We are working on
adding code to this repository which allows for much larger effective batch size
on the GPU. See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for
more details.&lt;/p&gt;
&lt;p&gt;This code was tested with TensorFlow 1.11.0. It was tested with Python2 and
Python3 (but more thoroughly with Python2, since this is what's used internally
in Google).&lt;/p&gt;
&lt;p&gt;The fine-tuning examples which use &lt;code&gt;BERT-Base&lt;/code&gt; should be able to run on a GPU
that has at least 12GB of RAM using the hyperparameters given.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-with-cloud-tpus" class="anchor" aria-hidden="true" href="#fine-tuning-with-cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with Cloud TPUs&lt;/h3&gt;
&lt;p&gt;Most of the examples below assumes that you will be running training/evaluation
on your local machine, using a GPU like a Titan X or GTX 1080.&lt;/p&gt;
&lt;p&gt;However, if you have access to a Cloud TPU that you want to train on, just add
the following flags to &lt;code&gt;run_classifier.py&lt;/code&gt; or &lt;code&gt;run_squad.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --use_tpu=True \
  --tpu_name=$TPU_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the
&lt;a href="https://cloud.google.com/tpu/docs/tutorials/mnist" rel="nofollow"&gt;Google Cloud TPU tutorial&lt;/a&gt;
for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;On Cloud TPUs, the pretrained model and the output directory will need to be on
Google Cloud Storage. For example, if you have a bucket named &lt;code&gt;some_bucket&lt;/code&gt;, you
might use the following flags instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --output_dir=gs://some_bucket/my_output_dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unzipped pre-trained model files can also be found in the Google Cloud
Storage folder &lt;code&gt;gs://bert_models/2018_10_18&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-sentence-and-sentence-pair-classification-tasks" class="anchor" aria-hidden="true" href="#sentence-and-sentence-pair-classification-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentence (and sentence-pair) classification tasks&lt;/h3&gt;
&lt;p&gt;Before running this example you must download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;. Next, download the &lt;code&gt;BERT-Base&lt;/code&gt;
checkpoint and unzip it to some directory &lt;code&gt;$BERT_BASE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This example code fine-tunes &lt;code&gt;BERT-Base&lt;/code&gt; on the Microsoft Research Paraphrase
Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a
few minutes on most GPUs.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a
high variance in the Dev set accuracy, even when starting from the same
pre-training checkpoint. If you re-run multiple times (making sure to point to
different &lt;code&gt;output_dir&lt;/code&gt;), you should see results between 84% and 88%.&lt;/p&gt;
&lt;p&gt;A few other pre-trained models are implemented off-the-shelf in
&lt;code&gt;run_classifier.py&lt;/code&gt;, so it should be straightforward to follow those examples to
use BERT for any single-sentence or sentence-pair classification task.&lt;/p&gt;
&lt;p&gt;Note: You might see a message &lt;code&gt;Running train on CPU&lt;/code&gt;. This really just means
that it's running on something other than a Cloud TPU, which includes a GPU.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-prediction-from-classifier" class="anchor" aria-hidden="true" href="#prediction-from-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prediction from classifier&lt;/h4&gt;
&lt;p&gt;Once you have trained your classifier you can use it in inference mode by using
the --do_predict=true command. You need to have a file named test.tsv in the
input folder. Output will be created in file called test_results.tsv in the
output folder. Each line will contain output for each sample, columns are the
class probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$TRAINED_CLASSIFIER&lt;/span&gt; \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-squad-11" class="anchor" aria-hidden="true" href="#squad-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 1.1&lt;/h3&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD) is a popular question answering
benchmark dataset. BERT (at the time of the release) obtains state-of-the-art
results on SQuAD with almost no task-specific network architecture modifications
or data augmentation. However, it does require semi-complex data pre-processing
and post-processing to deal with (a) the variable-length nature of SQuAD context
paragraphs, and (b) the character-level answer annotations which are used for
SQuAD training. This processing is implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD, you will first need to download the dataset. The
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD website&lt;/a&gt; does not seem to
link to the v1.1 datasets any longer, but the necessary files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json" rel="nofollow"&gt;train-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json" rel="nofollow"&gt;dev-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"&gt;evaluate-v1.1.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The state-of-the-art SQuAD results from the paper currently cannot be reproduced
on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does
not seem to fit on a 12GB GPU using &lt;code&gt;BERT-Large&lt;/code&gt;). However, a reasonably strong
&lt;code&gt;BERT-Base&lt;/code&gt; model can be trained on the GPU with these hyperparameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dev set predictions will be saved into a file called &lt;code&gt;predictions.json&lt;/code&gt; in
the &lt;code&gt;output_dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ./squad/predictions.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which should produce an output like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 88.41249612335034, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 81.2488174077578}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see a result similar to the 88.5% reported in the paper for
&lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you have access to a Cloud TPU, you can train with &lt;code&gt;BERT-Large&lt;/code&gt;. Here is a
set of hyperparameters (slightly different than the paper) which consistently
obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, one random run with these parameters produces the following Dev
scores:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 90.87081895814865, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 84.38978240302744}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you fine-tune for one epoch on
&lt;a href="http://nlp.cs.washington.edu/triviaqa/" rel="nofollow"&gt;TriviaQA&lt;/a&gt; before this the results will
be even better, but you will need to convert TriviaQA into the SQuAD json
format.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-squad-20" class="anchor" aria-hidden="true" href="#squad-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 2.0&lt;/h3&gt;
&lt;p&gt;This model is also implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD 2.0, you will first need to download the dataset. The necessary
files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json" rel="nofollow"&gt;train-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json" rel="nofollow"&gt;dev-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" rel="nofollow"&gt;evaluate-v2.0.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On Cloud TPU you can run with BERT-Large as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We assume you have copied everything from the output directory to a local
directory called ./squad/. The initial dev set predictions will be at
./squad/predictions.json and the differences between the score of no answer ("")
and the best non-null answer for each question will be in the file
./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Run this script to tune a threshold for predicting null versus non-null answers:&lt;/p&gt;
&lt;p&gt;python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json
./squad/predictions.json --na-prob-file ./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Assume the script outputs "best_f1_thresh" THRESH. (Typical values are between
-1.0 and -5.0). You can now re-run the model to generate predictions with the
derived threshold or alternatively you can extract the appropriate answers from
./squad/nbest_predictions.json.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=False \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True \
  --null_score_diff_threshold=&lt;span class="pl-smi"&gt;$THRESH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-out-of-memory-issues" class="anchor" aria-hidden="true" href="#out-of-memory-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Out-of-memory issues&lt;/h3&gt;
&lt;p&gt;All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of
device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely
to encounter out-of-memory issues if you use the same hyperparameters described
in the paper.&lt;/p&gt;
&lt;p&gt;The factors that affect memory usage are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;max_seq_length&lt;/code&gt;&lt;/strong&gt;: The released models were trained with sequence lengths
up to 512, but you can fine-tune with a shorter max sequence length to save
substantial memory. This is controlled by the &lt;code&gt;max_seq_length&lt;/code&gt; flag in our
example code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;train_batch_size&lt;/code&gt;&lt;/strong&gt;: The memory usage is also directly proportional to
the batch size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model type, &lt;code&gt;BERT-Base&lt;/code&gt; vs. &lt;code&gt;BERT-Large&lt;/code&gt;&lt;/strong&gt;: The &lt;code&gt;BERT-Large&lt;/code&gt; model
requires significantly more memory than &lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: The default optimizer for BERT is Adam, which requires a lot
of extra memory to store the &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; vectors. Switching to a more memory
efficient optimizer can reduce memory usage, but can also affect the
results. We have not experimented with other optimizers for fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the default training scripts (&lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;run_squad.py&lt;/code&gt;), we
benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with
TensorFlow 1.11.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, these max batch sizes for &lt;code&gt;BERT-Large&lt;/code&gt; are so small that they
will actually harm the model accuracy, regardless of the learning rate used. We
are working on adding code to this repository which will allow much larger
effective batch sizes to be used on the GPU. The code will be based on one (or
both) of the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient accumulation&lt;/strong&gt;: The samples in a minibatch are typically
independent with respect to gradient computation (excluding batch
normalization, which is not used here). This means that the gradients of
multiple smaller minibatches can be accumulated before performing the weight
update, and this will be exactly equivalent to a single larger update.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gradient-checkpointing"&gt;&lt;strong&gt;Gradient checkpointing&lt;/strong&gt;&lt;/a&gt;:
The major use of GPU/TPU memory during DNN training is caching the
intermediate activations in the forward pass that are necessary for
efficient computation in the backward pass. "Gradient checkpointing" trades
memory for compute time by re-computing the activations in an intelligent
way.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;However, this is not implemented in the current release.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo" class="anchor" aria-hidden="true" href="#using-bert-to-extract-fixed-feature-vectors-like-elmo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT to extract fixed feature vectors (like ELMo)&lt;/h2&gt;
&lt;p&gt;In certain cases, rather than fine-tuning the entire pre-trained model
end-to-end, it can be beneficial to obtained &lt;em&gt;pre-trained contextual
embeddings&lt;/em&gt;, which are fixed contextual representations of each input token
generated from the hidden layers of the pre-trained model. This should also
mitigate most of the out-of-memory issues.&lt;/p&gt;
&lt;p&gt;As an example, we include the script &lt;code&gt;extract_features.py&lt;/code&gt; which can be used
like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sentence A and Sentence B are separated by the ||| delimiter for sentence&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pair tasks like question answering and entailment.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For single sentence inputs, put one sentence per line and DON'T use the&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; delimiter.&lt;/span&gt;
&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Who was Jim Henson ? ||| Jim Henson was a puppeteer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will create a JSON file (one line per line of input) containing the BERT
activations from each Transformer layer specified by &lt;code&gt;layers&lt;/code&gt; (-1 is the final
hidden layer of the Transformer, etc.)&lt;/p&gt;
&lt;p&gt;Note that this script will produce very large output files (by default, around
15kb for every input token).&lt;/p&gt;
&lt;p&gt;If you need to maintain alignment between the original and tokenized words (for
projecting training labels), see the &lt;a href="#tokenization"&gt;Tokenization&lt;/a&gt; section
below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You may see a message like &lt;code&gt;Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.&lt;/code&gt; This message is expected, it
just means that we are using the &lt;code&gt;init_from_checkpoint()&lt;/code&gt; API rather than the
saved model API. If you don't specify a checkpoint or specify an invalid
checkpoint, this script will complain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.
Just follow the example code in &lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;extract_features.py&lt;/code&gt;.
The basic procedure for sentence-level tasks is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instantiate an instance of &lt;code&gt;tokenizer = tokenization.FullTokenizer&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenize the raw text with &lt;code&gt;tokens = tokenizer.tokenize(raw_text)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncate to the maximum sequence length. (You can use up to 512, but you
probably want to use shorter if possible for memory and speed reasons.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; tokens in the right place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since
you need to maintain alignment between your input text and output text so that
you can project your training labels. SQuAD is a particularly complex example
because the input labels are &lt;em&gt;character&lt;/em&gt;-based, and SQuAD paragraphs are often
longer than our maximum sequence length. See the code in &lt;code&gt;run_squad.py&lt;/code&gt; to show
how we handle this.&lt;/p&gt;
&lt;p&gt;Before we describe the general recipe for handling word-level tasks, it's
important to understand what exactly our tokenizer is doing. It has three main
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text normalization&lt;/strong&gt;: Convert all whitespace characters to spaces, and
(for the &lt;code&gt;Uncased&lt;/code&gt; model) lowercase the input and strip out accent markers.
E.g., &lt;code&gt;John Johanson's, → john johanson's,&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Punctuation splitting&lt;/strong&gt;: Split &lt;em&gt;all&lt;/em&gt; punctuation characters on both sides
(i.e., add whitespace around all punctuation characters). Punctuation
characters are defined as (a) Anything with a &lt;code&gt;P*&lt;/code&gt; Unicode class, (b) any
non-letter/number/space ASCII character (e.g., characters like &lt;code&gt;$&lt;/code&gt; which are
technically not punctuation). E.g., &lt;code&gt;john johanson's, → john johanson ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WordPiece tokenization&lt;/strong&gt;: Apply whitespace tokenization to the output of
the above procedure, and apply
&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py"&gt;WordPiece&lt;/a&gt;
tokenization to each token separately. (Our implementation is directly based
on the one from &lt;code&gt;tensor2tensor&lt;/code&gt;, which is linked). E.g., &lt;code&gt;john johanson ' s , → john johan ##son ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this scheme is that it is "compatible" with most existing
English tokenizers. For example, imagine that you have a part-of-speech tagging
task which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tokenized output will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tokens: john johan ##son ' s house
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Crucially, this would be the same output as if the raw text were &lt;code&gt;John Johanson's house&lt;/code&gt; (with no space before the &lt;code&gt;'s&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have a pre-tokenized representation with word-level annotations, you can
simply tokenize each input word independently, and deterministically maintain an
original-to-tokenized alignment:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Input&lt;/span&gt;
orig_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;John&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Johanson&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;'s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;house&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
labels      &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;POS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Output&lt;/span&gt;
bert_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; []

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Token map will be an int -&amp;gt; int mapping between the `orig_tokens` index and&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the `bert_tokens` index.&lt;/span&gt;
orig_to_tok_map &lt;span class="pl-k"&gt;=&lt;/span&gt; []

tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenization.FullTokenizer(
    &lt;span class="pl-v"&gt;vocab_file&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;vocab_file, &lt;span class="pl-v"&gt;do_lower_case&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[CLS]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;for&lt;/span&gt; orig_token &lt;span class="pl-k"&gt;in&lt;/span&gt; orig_tokens:
  orig_to_tok_map.append(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[SEP]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; orig_to_tok_map == [1, 2, 4, 6]&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;code&gt;orig_to_tok_map&lt;/code&gt; can be used to project &lt;code&gt;labels&lt;/code&gt; to the tokenized
representation.&lt;/p&gt;
&lt;p&gt;There are common English tokenization schemes which will cause a slight mismatch
between how BERT was pre-trained. For example, if your input tokenization splits
off contractions like &lt;code&gt;do n't&lt;/code&gt;, this will cause a mismatch. If it is possible to
do so, you should pre-process your data to convert these back to raw-looking
text, but if it's not possible, this mismatch is likely not a big deal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-training-with-bert" class="anchor" aria-hidden="true" href="#pre-training-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training with BERT&lt;/h2&gt;
&lt;p&gt;We are releasing code to do "masked LM" and "next sentence prediction" on an
arbitrary text corpus. Note that this is &lt;em&gt;not&lt;/em&gt; the exact code that was used for
the paper (the original code was written in C++, and had some additional
complexity), but this code does generate pre-training data as described in the
paper.&lt;/p&gt;
&lt;p&gt;Here's how to run the data generation. The input is a plain text file, with one
sentence per line. (It is important that these be actual sentences for the "next
sentence prediction" task). Documents are delimited by empty lines. The output
is a set of &lt;code&gt;tf.train.Example&lt;/code&gt;s serialized into &lt;code&gt;TFRecord&lt;/code&gt; file format.&lt;/p&gt;
&lt;p&gt;You can perform sentence segmentation with an off-the-shelf NLP toolkit such as
&lt;a href="https://spacy.io/" rel="nofollow"&gt;spaCy&lt;/a&gt;. The &lt;code&gt;create_pretraining_data.py&lt;/code&gt; script will
concatenate segments until they reach the maximum sequence length to minimize
computational waste from padding (see the script for more details). However, you
may want to intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to non-sentential
input during fine-tuning.&lt;/p&gt;
&lt;p&gt;This script stores all of the examples for the entire input file in memory, so
for large data files you should shard the input file and call the script
multiple times. (You can pass in a file glob to &lt;code&gt;run_pretraining.py&lt;/code&gt;, e.g.,
&lt;code&gt;tf_examples.tf_record*&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;max_predictions_per_seq&lt;/code&gt; is the maximum number of masked LM predictions per
sequence. You should set this to around &lt;code&gt;max_seq_length&lt;/code&gt; * &lt;code&gt;masked_lm_prob&lt;/code&gt; (the
script doesn't do that automatically because the exact value needs to be passed
to both scripts).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how to run the pre-training. Do not include &lt;code&gt;init_checkpoint&lt;/code&gt; if you are
pre-training from scratch. The model configuration (including vocab size) is
specified in &lt;code&gt;bert_config_file&lt;/code&gt;. This demo code only pre-trains for a small
number of steps (20), but in practice you will probably want to set
&lt;code&gt;num_train_steps&lt;/code&gt; to 10000 steps or more. The &lt;code&gt;max_seq_length&lt;/code&gt; and
&lt;code&gt;max_predictions_per_seq&lt;/code&gt; parameters passed to &lt;code&gt;run_pretraining.py&lt;/code&gt; must be the
same as &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will produce an output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since our &lt;code&gt;sample_text.txt&lt;/code&gt; file is very small, this example training
will overfit that data in only a few steps and produce unrealistically high
accuracy numbers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-tips-and-caveats" class="anchor" aria-hidden="true" href="#pre-training-tips-and-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training tips and caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If using your own vocabulary, make sure to change &lt;code&gt;vocab_size&lt;/code&gt; in
&lt;code&gt;bert_config.json&lt;/code&gt;. If you use a larger vocabulary without changing this,
you will likely get NaNs when training on GPU or TPU due to unchecked
out-of-bounds access.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If your task has a large domain-specific corpus available (e.g., "movie
reviews" or "scientific papers"), it will likely be beneficial to run
additional steps of pre-training on your corpus, starting from the BERT
checkpoint.&lt;/li&gt;
&lt;li&gt;The learning rate we used in the paper was 1e-4. However, if you are doing
additional steps of pre-training starting from an existing BERT checkpoint,
you should use a smaller learning rate (e.g., 2e-5).&lt;/li&gt;
&lt;li&gt;Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages in the
near future (hopefully by the end of November 2018).&lt;/li&gt;
&lt;li&gt;Longer sequences are disproportionately expensive because attention is
quadratic to the sequence length. In other words, a batch of 64 sequences of
length 512 is much more expensive than a batch of 256 sequences of
length 128. The fully-connected/convolutional cost is the same, but the
attention cost is far greater for the 512-length sequences. Therefore, one
good recipe is to pre-train for, say, 90,000 steps with a sequence length of
128 and then for 10,000 additional steps with a sequence length of 512. The
very long sequences are mostly needed to learn positional embeddings, which
can be learned fairly quickly. Note that this does require generating the
data twice with different values of &lt;code&gt;max_seq_length&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are pre-training from scratch, be prepared that pre-training is
computationally expensive, especially on GPUs. If you are pre-training from
scratch, our recommended recipe is to pre-train a &lt;code&gt;BERT-Base&lt;/code&gt; on a single
&lt;a href="https://cloud.google.com/tpu/docs/pricing" rel="nofollow"&gt;preemptible Cloud TPU v2&lt;/a&gt;, which
takes about 2 weeks at a cost of about $500 USD (based on the pricing in
October 2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-data" class="anchor" aria-hidden="true" href="#pre-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training data&lt;/h3&gt;
&lt;p&gt;We will &lt;strong&gt;not&lt;/strong&gt; be able to release the pre-processed datasets used in the paper.
For Wikipedia, the recommended pre-processing is to download
&lt;a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="nofollow"&gt;the latest dump&lt;/a&gt;,
extract the text with
&lt;a href="https://github.com/attardi/wikiextractor"&gt;&lt;code&gt;WikiExtractor.py&lt;/code&gt;&lt;/a&gt;, and then apply
any necessary cleanup to convert it into plain text.&lt;/p&gt;
&lt;p&gt;Unfortunately the researchers who collected the
&lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt; no longer have it available for
public download. The
&lt;a href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html" rel="nofollow"&gt;Project Guttenberg Dataset&lt;/a&gt;
is a somewhat smaller (200M word) collection of older books that are public
domain.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://commoncrawl.org/" rel="nofollow"&gt;Common Crawl&lt;/a&gt; is another very large collection of
text, but you will likely have to do substantial pre-processing and cleanup to
extract a usable corpus for pre-training BERT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-learning-a-new-wordpiece-vocabulary" class="anchor" aria-hidden="true" href="#learning-a-new-wordpiece-vocabulary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning a new WordPiece vocabulary&lt;/h3&gt;
&lt;p&gt;This repository does not include code for &lt;em&gt;learning&lt;/em&gt; a new WordPiece vocabulary.
The reason is that the code used in the paper was implemented in C++ with
dependencies on Google's internal libraries. For English, it is almost always
better to just start with our vocabulary and pre-trained models. For learning
vocabularies of other languages, there are a number of open source options
available. However, keep in mind that these are not compatible with our
&lt;code&gt;tokenization.py&lt;/code&gt; library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;Google's SentencePiece library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py"&gt;tensor2tensor's WordPiece generation script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsennrich/subword-nmt"&gt;Rico Sennrich's Byte Pair Encoding library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-in-colab" class="anchor" aria-hidden="true" href="#using-bert-in-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT in Colab&lt;/h2&gt;
&lt;p&gt;If you want to use BERT with &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Colab&lt;/a&gt;, you can
get started with the notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".
&lt;strong&gt;At the time of this writing (October 31st, 2018), Colab users can access a
Cloud TPU completely for free.&lt;/strong&gt; Note: One per user, availability limited,
requires a Google Cloud Platform account with storage (although storage may be
purchased with free credit for signing up with GCP), and this capability may not
longer be available in the future. Click on the BERT Colab that was just linked
for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus" class="anchor" aria-hidden="true" href="#is-this-code-compatible-with-cloud-tpus-what-about-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is this code compatible with Cloud TPUs? What about GPUs?&lt;/h4&gt;
&lt;p&gt;Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and
Cloud TPU. However, GPU training is single-GPU only.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-i-am-getting-out-of-memory-errors-what-is-wrong" class="anchor" aria-hidden="true" href="#i-am-getting-out-of-memory-errors-what-is-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I am getting out-of-memory errors, what is wrong?&lt;/h4&gt;
&lt;p&gt;See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for more
information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-pytorch-version-available" class="anchor" aria-hidden="true" href="#is-there-a-pytorch-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a PyTorch version available?&lt;/h4&gt;
&lt;p&gt;There is no official PyTorch implementation. However, NLP researchers from
HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-chainer-version-available" class="anchor" aria-hidden="true" href="#is-there-a-chainer-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a Chainer version available?&lt;/h4&gt;
&lt;p&gt;There is no official Chainer implementation. However, Sosuke Kobayashi made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the Chainer
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-in-other-languages-be-released" class="anchor" aria-hidden="true" href="#will-models-in-other-languages-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models in other languages be released?&lt;/h4&gt;
&lt;p&gt;Yes, we plan to release a multi-lingual BERT model in the near future. We cannot
make promises about exactly which languages will be included, but it will likely
be a single model which includes &lt;em&gt;most&lt;/em&gt; of the languages which have a
significantly-sized Wikipedia.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-larger-than-bert-large-be-released" class="anchor" aria-hidden="true" href="#will-models-larger-than-bert-large-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models larger than &lt;code&gt;BERT-Large&lt;/code&gt; be released?&lt;/h4&gt;
&lt;p&gt;So far we have not attempted to train anything larger than &lt;code&gt;BERT-Large&lt;/code&gt;. It is
possible that we will release larger models if we are able to obtain significant
improvements.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-what-license-is-this-library-released-under" class="anchor" aria-hidden="true" href="#what-license-is-this-library-released-under"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What license is this library released under?&lt;/h4&gt;
&lt;p&gt;All code &lt;em&gt;and&lt;/em&gt; models are released under the Apache 2.0 license. See the
&lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how-do-i-cite-bert" class="anchor" aria-hidden="true" href="#how-do-i-cite-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I cite BERT?&lt;/h4&gt;
&lt;p&gt;For now, cite &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;the Arxiv paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we submit the paper to a conference or journal, we will update the BibTeX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This is not an official Google product.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact-information" class="anchor" aria-hidden="true" href="#contact-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact information&lt;/h2&gt;
&lt;p&gt;For help or issues using BERT, please submit a GitHub issue.&lt;/p&gt;
&lt;p&gt;For personal communication related to BERT, please contact Jacob Devlin
(&lt;code&gt;jacobdevlin@google.com&lt;/code&gt;), Ming-Wei Chang (&lt;code&gt;mingweichang@google.com&lt;/code&gt;), or
Kenton Lee (&lt;code&gt;kentonl@google.com&lt;/code&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/bert</guid><pubDate>Tue, 05 Nov 2019 00:08:00 GMT</pubDate></item><item><title>sebastianruder/NLP-progress #9 in Python, This week</title><link>https://github.com/sebastianruder/NLP-progress</link><description>&lt;p&gt;&lt;i&gt;Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tracking-progress-in-natural-language-processing" class="anchor" aria-hidden="true" href="#tracking-progress-in-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tracking Progress in Natural Language Processing&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-english" class="anchor" aria-hidden="true" href="#english"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;English&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="english/automatic_speech_recognition.md"&gt;Automatic speech recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/ccg.md"&gt;CCG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/common_sense.md"&gt;Common sense&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/constituency_parsing.md"&gt;Constituency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/coreference_resolution.md"&gt;Coreference resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/dependency_parsing.md"&gt;Dependency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/dialogue.md"&gt;Dialogue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/domain_adaptation.md"&gt;Domain adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/entity_linking.md"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/grammatical_error_correction.md"&gt;Grammatical error correction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/information_extraction.md"&gt;Information extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/language_modeling.md"&gt;Language modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/lexical_normalization.md"&gt;Lexical normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/machine_translation.md"&gt;Machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/missing_elements.md"&gt;Missing elements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/multi-task_learning.md"&gt;Multi-task learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/multimodal.md"&gt;Multi-modal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/named_entity_recognition.md"&gt;Named entity recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/natural_language_inference.md"&gt;Natural language inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/part-of-speech_tagging.md"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/question_answering.md"&gt;Question answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/relation_prediction.md"&gt;Relation prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/relationship_extraction.md"&gt;Relationship extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_textual_similarity.md"&gt;Semantic textual similarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_parsing.md"&gt;Semantic parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/semantic_role_labeling.md"&gt;Semantic role labeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/sentiment_analysis.md"&gt;Sentiment analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/shallow_syntax.md"&gt;Shallow syntax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/simplification.md"&gt;Simplification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/stance_detection.md"&gt;Stance detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/summarization.md"&gt;Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/taxonomy_learning.md"&gt;Taxonomy learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/temporal_processing.md"&gt;Temporal processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/text_classification.md"&gt;Text classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="english/word_sense_disambiguation.md"&gt;Word sense disambiguation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-chinese" class="anchor" aria-hidden="true" href="#chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chinese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chinese/chinese.md#entity-linking"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chinese/chinese_word_segmentation.md"&gt;Chinese word segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-hindi" class="anchor" aria-hidden="true" href="#hindi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hindi&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#chunking"&gt;Chunking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#part-of-speech-tagging"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hindi/hindi.md#machine-translation"&gt;Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-vietnamese" class="anchor" aria-hidden="true" href="#vietnamese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vietnamese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#dependency-parsing"&gt;Dependency parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#machine-translation"&gt;Machine translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#named-entity-recognition"&gt;Named entity recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#part-of-speech-tagging"&gt;Part-of-speech tagging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vietnamese/vietnamese.md#word-segmentation"&gt;Word segmentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-spanish" class="anchor" aria-hidden="true" href="#spanish"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spanish&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="spanish/entity_linking.md#entity-linking"&gt;Entity linking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-portuguese" class="anchor" aria-hidden="true" href="#portuguese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Portuguese&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="portuguese/question_answering.md"&gt;Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This document aims to track the progress in Natural Language Processing (NLP) and give an overview
of the state-of-the-art (SOTA) across the most common NLP tasks and their corresponding datasets.&lt;/p&gt;
&lt;p&gt;It aims to cover both traditional and core NLP tasks such as dependency parsing and part-of-speech tagging
as well as more recent ones such as reading comprehension and natural language inference. The main objective
is to provide the reader with a quick overview of benchmark datasets and the state-of-the-art for their
task of interest, which serves as a stepping stone for further research. To this end, if there is a
place where results for a task are already published and regularly maintained, such as a public leaderboard,
the reader will be pointed there.&lt;/p&gt;
&lt;p&gt;If you want to find this document again in the future, just go to &lt;a href="https://nlpprogress.com/" rel="nofollow"&gt;&lt;code&gt;nlpprogress.com&lt;/code&gt;&lt;/a&gt;
or &lt;a href="http://nlpsota.com/" rel="nofollow"&gt;&lt;code&gt;nlpsota.com&lt;/code&gt;&lt;/a&gt; in your browser.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-guidelines" class="anchor" aria-hidden="true" href="#guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Guidelines&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;   Results reported in published papers are preferred; an exception may be made for influential preprints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Datasets&lt;/strong&gt;   Datasets should have been used for evaluation in at least one published paper besides
the one that introduced the dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;   We recommend to add a link to an implementation
if available. You can add a &lt;code&gt;Code&lt;/code&gt; column (see below) to the table if it does not exist.
In the &lt;code&gt;Code&lt;/code&gt; column, indicate an official implementation with &lt;a href="http://link_to_implementation" rel="nofollow"&gt;Official&lt;/a&gt;.
If an unofficial implementation is available, use &lt;a href="http://link_to_implementation" rel="nofollow"&gt;Link&lt;/a&gt; (see below).
If no implementation is available, you can leave the cell empty.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-adding-a-new-result" class="anchor" aria-hidden="true" href="#adding-a-new-result"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding a new result&lt;/h4&gt;
&lt;p&gt;If you would like to add a new result, you can just click on the small edit button in the top-right
corner of the file for the respective task (see below).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/edit_file.png"&gt;&lt;img src="img/edit_file.png" alt="Click on the edit button to add a file" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This allows you to edit the file in Markdown. Simply add a row to the corresponding table in the
same format. Make sure that the table stays sorted (with the best result on top).
After you've made your change, make sure that the table still looks ok by clicking on the
"Preview changes" tab at the top of the page. If everything looks good, go to the bottom of the page,
where you see the below form.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/propose_file_change.png"&gt;&lt;img src="img/propose_file_change.png" alt="Fill out the file change information" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Add a name for your proposed change, an optional description, indicate that you would like to
"Create a new branch for this commit and start a pull request", and click on "Propose file change".&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-adding-a-new-dataset-or-task" class="anchor" aria-hidden="true" href="#adding-a-new-dataset-or-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding a new dataset or task&lt;/h4&gt;
&lt;p&gt;For adding a new dataset or task, you can also follow the steps above. Alternatively, you can fork the repository.
In both cases, follow the steps below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If your task is completely new, create a new file and link to it in the table of contents above.&lt;/li&gt;
&lt;li&gt;If not, add your task or dataset to the respective section of the corresponding file (in alphabetical order).&lt;/li&gt;
&lt;li&gt;Briefly describe the dataset/task and include relevant references.&lt;/li&gt;
&lt;li&gt;Describe the evaluation setting and evaluation metric.&lt;/li&gt;
&lt;li&gt;Show how an annotated example of the dataset/task looks like.&lt;/li&gt;
&lt;li&gt;Add a download link if available.&lt;/li&gt;
&lt;li&gt;Copy the below table and fill in at least two results (including the state-of-the-art)
for your dataset/task (change Score to the metric of your dataset). If your dataset/task
has multiple metrics, add them to the right of &lt;code&gt;Score&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Submit your change as a pull request.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Score&lt;/th&gt;
&lt;th&gt;Paper / Source&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-wish-list" class="anchor" aria-hidden="true" href="#wish-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wish list&lt;/h3&gt;
&lt;p&gt;These are tasks and datasets that are still missing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bilingual dictionary induction&lt;/li&gt;
&lt;li&gt;Discourse parsing&lt;/li&gt;
&lt;li&gt;Keyphrase extraction&lt;/li&gt;
&lt;li&gt;Knowledge base population (KBP)&lt;/li&gt;
&lt;li&gt;More dialogue tasks&lt;/li&gt;
&lt;li&gt;Semi-supervised learning&lt;/li&gt;
&lt;li&gt;Frame-semantic parsing (FrameNet full-sentence analysis)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-exporting-into-a-structured-format" class="anchor" aria-hidden="true" href="#exporting-into-a-structured-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Exporting into a structured format&lt;/h3&gt;
&lt;p&gt;You can extract all the data into a structured, machine-readable JSON format with parsed tasks, descriptions and SOTA tables.&lt;/p&gt;
&lt;p&gt;The instructions are in &lt;a href="structured/README.md"&gt;structured/README.md&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-instructions-for-building-the-site-locally" class="anchor" aria-hidden="true" href="#instructions-for-building-the-site-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Instructions for building the site locally&lt;/h3&gt;
&lt;p&gt;Instructions for building the website locally using Jekyll can be found &lt;a href="jekyll_instructions.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sebastianruder</author><guid isPermaLink="false">https://github.com/sebastianruder/NLP-progress</guid><pubDate>Tue, 05 Nov 2019 00:09:00 GMT</pubDate></item><item><title>NVIDIA/DeepLearningExamples #10 in Python, This week</title><link>https://github.com/NVIDIA/DeepLearningExamples</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nvidia-deep-learning-examples-for-tensor-cores" class="anchor" aria-hidden="true" href="#nvidia-deep-learning-examples-for-tensor-cores"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA Deep Learning Examples for Tensor Cores&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This repository provides the latest deep learning example networks for training.  These examples focus on achieving the best performance and convergence from NVIDIA Volta Tensor Cores.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-gpu-cloud-ngc-container-registry" class="anchor" aria-hidden="true" href="#nvidia-gpu-cloud-ngc-container-registry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA GPU Cloud (NGC) Container Registry&lt;/h2&gt;
&lt;p&gt;These examples, along with our NVIDIA deep learning software stack, are provided in a monthly updated Docker container on the NGC container registry (&lt;a href="https://ngc.nvidia.com" rel="nofollow"&gt;https://ngc.nvidia.com&lt;/a&gt;). These containers include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest NVIDIA examples from this repository&lt;/li&gt;
&lt;li&gt;The latest NVIDIA contributions shared upstream to the respective framework&lt;/li&gt;
&lt;li&gt;The latest NVIDIA Deep Learning software libraries, such as cuDNN, NCCL, cuBLAS, etc. which have all been through a rigorous monthly quality assurance process to ensure that they provide the best possible performance&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/dgx/index.html#nvidia-optimized-frameworks-release-notes" rel="nofollow"&gt;Monthly release notes&lt;/a&gt; for each of the NVIDIA optimized containers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-directory-structure" class="anchor" aria-hidden="true" href="#directory-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directory structure&lt;/h2&gt;
&lt;p&gt;The examples are organized first by framework, such as TensorFlow, PyTorch, etc. and second by use case, such as computer vision, natural language processing, etc. We hope this structure enables you to quickly locate the example networks that best suit your needs. Here are the currently supported models:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-computer-vision" class="anchor" aria-hidden="true" href="#computer-vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResNet-50&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/MxNet/Classification/RN50v1.5"&gt;MXNet&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/RN50v1.5"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Classification/RN50v1.5"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Detection/SSD"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(industrial)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Industrial"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(medical)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Medical"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GNMT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/GNMT"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Translation/GNMT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/Transformer"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT"&gt;PyTorch&lt;/a&gt;][&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recommender-systems" class="anchor" aria-hidden="true" href="#recommender-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommender Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NCF&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/NCF"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-text-to-speech" class="anchor" aria-hidden="true" href="#text-to-speech"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text to Speech&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tacotron &amp;amp; WaveGlow&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-speech-recognition" class="anchor" aria-hidden="true" href="#speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech Recognition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jasper&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-support" class="anchor" aria-hidden="true" href="#nvidia-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA support&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate the level of support that will be provided. The range is from ongoing updates and improvements to a point-in-time release for thought leadership.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-feedback--contributions" class="anchor" aria-hidden="true" href="#feedback--contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback / Contributions&lt;/h2&gt;
&lt;p&gt;We're posting these examples on GitHub to better support the community, facilitate feedback, as well as collect and implement contributions using GitHub Issues and pull requests. We welcome all contributions!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known issues&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate any known issues and encourage the community to provide feedback.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/DeepLearningExamples</guid><pubDate>Tue, 05 Nov 2019 00:10:00 GMT</pubDate></item><item><title>ddbourgin/numpy-ml #11 in Python, This week</title><link>https://github.com/ddbourgin/numpy-ml</link><description>&lt;p&gt;&lt;i&gt;Machine learning, in numpy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-numpy-ml" class="anchor" aria-hidden="true" href="#numpy-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;numpy-ml&lt;/h1&gt;
&lt;p&gt;Ever wish you had an inefficient but somewhat legible collection of machine
learning algorithms implemented exclusively in numpy? No?&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;To see all of the available models, take a look at the &lt;a href="https://numpy-ml.readthedocs.io/" rel="nofollow"&gt;project documentation&lt;/a&gt; or see &lt;a href="https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Am I missing your favorite model? Is there something that could be cleaner /
less confusing? Did I mess something up? Submit a PR! The only requirement is
that your models are written with just the &lt;a href="https://docs.python.org/3/library/" rel="nofollow"&gt;Python standard
library&lt;/a&gt; and &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt;. The
&lt;a href="https://scipy.github.io/devdocs/" rel="nofollow"&gt;SciPy library&lt;/a&gt; is also permitted under special
circumstances ;)&lt;/p&gt;
&lt;p&gt;See full contributing guidelines &lt;a href="./CONTRIBUTING.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ddbourgin</author><guid isPermaLink="false">https://github.com/ddbourgin/numpy-ml</guid><pubDate>Tue, 05 Nov 2019 00:11:00 GMT</pubDate></item><item><title>CorentinJ/Real-Time-Voice-Cloning #12 in Python, This week</title><link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link><description>&lt;p&gt;&lt;i&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-time-voice-cloning" class="anchor" aria-hidden="true" href="#real-time-voice-cloning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real-Time Voice Cloning&lt;/h1&gt;
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. Feel free to check &lt;a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow"&gt;my thesis&lt;/a&gt; if you're curious or if you're looking for info I haven't documented yet (don't hesitate to make an issue for that too). Mostly I would recommend giving a quick look to the figures beyond the introduction.&lt;/p&gt;
&lt;p&gt;SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9c33f78be8afe656503da974c478ea2ba2647db7/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-papers-implemented" class="anchor" aria-hidden="true" href="#papers-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers implemented&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Designation&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Implementation source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow"&gt;1802.08435&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;WaveRNN (vocoder)&lt;/td&gt;
&lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;1712.05884&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tacotron 2 (synthesizer)&lt;/td&gt;
&lt;td&gt;Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Rayhane-mamah/Tacotron-2"&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow"&gt;1710.10467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GE2E (encoder)&lt;/td&gt;
&lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;20/08/19:&lt;/strong&gt; I'm working on &lt;a href="https://github.com/resemble-ai/Resemblyzer"&gt;resemblyzer&lt;/a&gt;, an independent package for the voice encoder. You can use your trained encoder models from this repo with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;06/07/19:&lt;/strong&gt; Need to run within a docker container on a remote server? See &lt;a href="https://sean.lane.sh/posts/2019/07/Running-the-Real-Time-Voice-Cloning-project-in-Docker/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;25/06/19:&lt;/strong&gt; Experimental support for low-memory GPUs (~2gb) added for the synthesizer. Pass &lt;code&gt;--low_mem&lt;/code&gt; to &lt;code&gt;demo_cli.py&lt;/code&gt; or &lt;code&gt;demo_toolbox.py&lt;/code&gt; to enable it. It adds a big overhead, so it's not recommended if you have enough VRAM.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;You will need the following whether you plan to use the toolbox only or to retrain the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 3.7&lt;/strong&gt;. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the necessary packages. Additionally you will need &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch&lt;/a&gt; (&amp;gt;=1.0.1).&lt;/p&gt;
&lt;p&gt;A GPU is mandatory, but you don't necessarily need a high tier GPU if you only want to use the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained models&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary" class="anchor" aria-hidden="true" href="#preliminary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary&lt;/h3&gt;
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h3&gt;
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="http://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-toolbox" class="anchor" aria-hidden="true" href="#toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toolbox&lt;/h3&gt;
&lt;p&gt;You can then try the toolbox:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt;
or&lt;br&gt;
&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wiki&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it all works&lt;/strong&gt; (WIP - &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works"&gt;stub&lt;/a&gt;, you might be better off reading my thesis until it's done)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training"&gt;&lt;strong&gt;Training models yourself&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training with other data/languages&lt;/strong&gt; (WIP - see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-507864097"&gt;here&lt;/a&gt; for now)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/TODO-&amp;amp;-planned-features"&gt;&lt;strong&gt;TODO and planned features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;I'm working full-time as of June 2019. Replying to issues is very time-consuming, I can't always do it. I won't be making progress of my own on this repo, but I will still gladly merge PRs and accept contributions to the wiki. Don't hesitate to send me an email if you wish to contribute.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CorentinJ</author><guid isPermaLink="false">https://github.com/CorentinJ/Real-Time-Voice-Cloning</guid><pubDate>Tue, 05 Nov 2019 00:12:00 GMT</pubDate></item><item><title>geekcomputers/Python #13 in Python, This week</title><link>https://github.com/geekcomputers/Python</link><description>&lt;p&gt;&lt;i&gt;My Python Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-my-python-examples" class="anchor" aria-hidden="true" href="#my-python-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;My Python Examples&lt;/h1&gt;
&lt;p&gt;I do not consider myself a programmer. I create these little programs as experiments to play with the language, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me at &lt;a href="mailto:craig@geekcomputers.co.uk"&gt;craig@geekcomputers.co.uk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These scripts contain important functions which help reduce human workload.
Code documentation is aligned correctly when the files are viewed in &lt;a href="https://notepad-plus-plus.org/" rel="nofollow"&gt;Notepad++&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/batch_file_rename.py"&gt;batch_file_rename.py&lt;/a&gt; - This batch renames a group of files in a given directory, once you pass the current and the new extensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/create_dir_if_not_there.py"&gt;create_dir_if_not_there.py&lt;/a&gt; - Checks to see if a directory exists in the users home directory, if not then create it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube-downloader%20fast.py"&gt;Fast Youtube Downloader&lt;/a&gt; - Downloads YouTube videos quickly with parallel threads using aria2c&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/tree/master/Google_Image_Downloader"&gt;Google Image Downloader&lt;/a&gt; - Query a given term and retrieve images from the Google Image database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/dir_test.py"&gt;dir_test.py&lt;/a&gt; - Tests to see if the directory &lt;code&gt;testdir&lt;/code&gt; exists, if not it will create the directory for you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/env_check.py"&gt;env_check.py&lt;/a&gt; - This script will check to see if all of the environment variables required are set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Ratna04priya/Python/blob/master/BlackJack_game/blackjack.py"&gt;blackjack.py&lt;/a&gt; - This script contains the Casino BlackJack-21 Game in Python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/fileinfo.py"&gt;fileinfo.py&lt;/a&gt; - Shows file information for a given file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/folder_size.py"&gt;folder_size.py&lt;/a&gt; - Scans the current directory and all subdirectories and displays the size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/logs.py"&gt;logs.py&lt;/a&gt; - This script will search for all &lt;code&gt;*.log&lt;/code&gt; files in the given directory, zip them using the program you specify, and then date stamp them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/move_files_over_x_days.py"&gt;move_files_over_x_days.py&lt;/a&gt; - Moves all files over a specified age (in days) from the source directory to the destination directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/nslookup_check.py"&gt;nslookup_check.py&lt;/a&gt; - This simple script opens the file &lt;code&gt;server_list.txt&lt;/code&gt; and then does an nslookup for each one to check the DNS entry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/osinfo.py"&gt;osinfo.py&lt;/a&gt; - Displays some information about the OS on which you are running this script.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_servers.py"&gt;ping_servers.py&lt;/a&gt; - This script, depending on the arguments supplied, will ping the servers associated with that application group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_subnet.py"&gt;ping_subnet.py&lt;/a&gt; - After supplying the first 3 octets this file scans the final range for available addresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/powerdown_startup.py"&gt;powerdown_startup.py&lt;/a&gt; - This file goes through the server list and pings the machine, if it is up it will load the putty session, if it is not it will notify you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/puttylogs.py"&gt;puttylogs.py&lt;/a&gt; -  This file zips up all the logs in the given directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_count.py"&gt;script_count.py&lt;/a&gt; - This file scans the scripts directory and gives a count of the different types of scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[get_youtube_view.py] - This is very simple python script to get more views for your youtube videos.Some times I use for repeating my favorite songs by this scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_listing.py"&gt;script_listing.py&lt;/a&gt; - This file will list all the files in the given directory, and go through all the subdirectories as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/testlines.py"&gt;testlines.py&lt;/a&gt; - This simple script opens a file and prints out 100 lines of whatever is the set for the line variable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/tweeter.py"&gt;tweeter.py&lt;/a&gt; - Allows you to tweet text or a picture from the terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/serial_scanner.py"&gt;serial_scanner.py&lt;/a&gt; contains a method called ListAvailablePorts which returns a list with the names of the serial ports that are in use in the computer. This method works only on Linux and Windows (can be extended for mac osx). If no port is found, an empty list is returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/get_youtube_view.py"&gt;get_youtube_view.py&lt;/a&gt; - A simple python script to get more views for your YouTube videos. Useful for repeating songs on YouTube.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacter.py"&gt;CountMillionCharacter.py&lt;/a&gt; And &lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacters-2.0.py"&gt;CountMillionCharacter2.0&lt;/a&gt;.py - Gets character count of a text file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/xkcd_downloader.py"&gt;xkcd_downloader.py&lt;/a&gt; - Downloads the latest XKCD comic and places them in a new folder called "comics".&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/timymodule.py"&gt;timymodule.py&lt;/a&gt; - A great alternative to Pythons 'timeit' module and easier to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/calculator.py"&gt;calculator.py&lt;/a&gt; - Uses Python's eval() function to implement a calculator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Google_News.py"&gt;Google_News.py&lt;/a&gt; - Uses BeautifulSoup to provide Latest news headline along with news link.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Cricket_score.py"&gt;cricket_live_score&lt;/a&gt; - Uses BeautifulSoup to provide live cricket score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube.py"&gt;youtube.py&lt;/a&gt; - Takes a song name as input and fetches the YouTube URL of the best matching song and plays it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/site_health.py"&gt;site_health.py&lt;/a&gt; - Checks the health of a remote server&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/SimpleStopWatch.py"&gt;SimpleStopWatch.py&lt;/a&gt; - Simple Stop Watch implementation using Python's time module.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/changemac.py"&gt;Changemac.py&lt;/a&gt; - This script change your MAC address , generate random MAC address or enter input as new MAC address in your linux(Successfully Tested in Ubuntu 18.04).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/whatsapp-monitor.py"&gt;whatsapp-monitor.py&lt;/a&gt; - Uses Selenium to give online status about your contacts when your contacts become online in whatsapp you will get an update about it on terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/subahanii/whatsapp-Chat-Analyzer"&gt;whatsapp-chat-analyzer.py&lt;/a&gt; - This is whatsapp group/individual chat analyzer .
This script is able to analyse all activity happened in whatsapp group and visualize all thing through matplotlib library(In Graph form).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>geekcomputers</author><guid isPermaLink="false">https://github.com/geekcomputers/Python</guid><pubDate>Tue, 05 Nov 2019 00:13:00 GMT</pubDate></item><item><title>scikit-learn/scikit-learn #14 in Python, This week</title><link>https://github.com/scikit-learn/scikit-learn</link><description>&lt;p&gt;&lt;i&gt;scikit-learn: machine learning in Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img alt="Azure" src="https://camo.githubusercontent.com/bfe67a3604768c16e941f3331709bf55507a4b57/68747470733a2f2f6465762e617a7572652e636f6d2f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f5f617069732f6275696c642f7374617475732f7363696b69742d6c6561726e2e7363696b69742d6c6561726e3f6272616e63684e616d653d6d6173746572" data-canonical-src="https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://travis-ci.org/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/590475799489c962f111c9fc5c1432ecbc577578/68747470733a2f2f6170692e7472617669732d63692e6f72672f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/scikit-learn/scikit-learn?branch=master" rel="nofollow"&gt;&lt;img alt="Codecov" src="https://camo.githubusercontent.com/58a0b06906ca5d106ec090fe8a1ac85a092b81c2/68747470733a2f2f636f6465636f762e696f2f6769746875622f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562" data-canonical-src="https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master&amp;amp;service=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d2913194913f85128f908483a265e64dcd6d31e4/68747470733a2f2f636972636c6563692e636f6d2f67682f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2f747265652f6d61737465722e7376673f7374796c653d736869656c6426636972636c652d746f6b656e3d3a636972636c652d746f6b656e" data-canonical-src="https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield&amp;amp;circle-token=:circle-token" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://img.shields.io/pypi/pyversions/scikit-learn.svg" rel="nofollow"&gt;&lt;img alt="PythonVersion" src="https://camo.githubusercontent.com/45416807fdec5b0d83acca16b2b9f08fe7d32bf1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7363696b69742d6c6561726e2e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://badge.fury.io/py/scikit-learn" rel="nofollow"&gt;&lt;img alt="PyPi" src="https://camo.githubusercontent.com/9f0ed32d05350afa18a801573e4da7f4a240e181/68747470733a2f2f62616467652e667572792e696f2f70792f7363696b69742d6c6561726e2e737667" data-canonical-src="https://badge.fury.io/py/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn" rel="nofollow"&gt;&lt;img alt="DOI" src="https://camo.githubusercontent.com/73c63e44b8bee62df142664048c58f83ec8ad95c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f32313336392f7363696b69742d6c6561726e2f7363696b69742d6c6561726e2e737667" data-canonical-src="https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-scikit-learn"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-scikit-learn" class="anchor" aria-hidden="true" href="#scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;scikit-learn&lt;/h2&gt;
&lt;p&gt;scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;It is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;a name="user-content-dependencies"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h4&gt;
&lt;p&gt;scikit-learn requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python (&amp;gt;= 3.5)&lt;/li&gt;
&lt;li&gt;NumPy (&amp;gt;= 1.11.0)&lt;/li&gt;
&lt;li&gt;SciPy (&amp;gt;= 0.17.0)&lt;/li&gt;
&lt;li&gt;joblib (&amp;gt;= 0.11)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.&lt;/strong&gt;
scikit-learn 0.21 and later require Python 3.5 or newer.&lt;/p&gt;
&lt;p&gt;Scikit-learn plotting capabilities (i.e., functions start with "&lt;a href="#id2"&gt;&lt;span id="user-content-id3"&gt;plot_&lt;/span&gt;&lt;/a&gt;"
and classes end with "Display") require Matplotlib (&amp;gt;= 1.5.1). For running the
examples Matplotlib &amp;gt;= 1.5.1 is required. A few examples require
scikit-image &amp;gt;= 0.12.3, a few examples require pandas &amp;gt;= 0.18.0.&lt;/p&gt;
&lt;a name="user-content-user-installation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-user-installation" class="anchor" aria-hidden="true" href="#user-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;User installation&lt;/h4&gt;
&lt;p&gt;If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;pip install -U scikit-learn
&lt;/pre&gt;
&lt;p&gt;or &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;conda install scikit-learn
&lt;/pre&gt;
&lt;p&gt;The documentation includes more detailed &lt;a href="http://scikit-learn.org/stable/install.html" rel="nofollow"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-changelog"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h3&gt;
&lt;p&gt;See the &lt;a href="http://scikit-learn.org/dev/whats_new.html" rel="nofollow"&gt;changelog&lt;/a&gt;
for a history of notable changes to scikit-learn.&lt;/p&gt;
&lt;a name="user-content-development"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
&lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;Development Guide&lt;/a&gt;
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.&lt;/p&gt;
&lt;a name="user-content-important-links"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-important-links" class="anchor" aria-hidden="true" href="#important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Important links&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Official source code repo: &lt;a href="https://github.com/scikit-learn/scikit-learn"&gt;https://github.com/scikit-learn/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download releases: &lt;a href="https://pypi.org/project/scikit-learn/" rel="nofollow"&gt;https://pypi.org/project/scikit-learn/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Issue tracker: &lt;a href="https://github.com/scikit-learn/scikit-learn/issues"&gt;https://github.com/scikit-learn/scikit-learn/issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-source-code"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-source-code" class="anchor" aria-hidden="true" href="#source-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source code&lt;/h4&gt;
&lt;p&gt;You can check the latest sources with the command:&lt;/p&gt;
&lt;pre&gt;git clone https://github.com/scikit-learn/scikit-learn.git
&lt;/pre&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h4&gt;
&lt;p&gt;To learn more about making a contribution to scikit-learn, please see our
&lt;a href="https://scikit-learn.org/dev/developers/contributing.html" rel="nofollow"&gt;Contributing guide&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-testing"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h4&gt;
&lt;p&gt;After installation, you can launch the test suite from outside the
source directory (you will need to have &lt;code&gt;pytest&lt;/code&gt; &amp;gt;= 3.3.0 installed):&lt;/p&gt;
&lt;pre&gt;pytest sklearn
&lt;/pre&gt;
&lt;p&gt;See the web page &lt;a href="http://scikit-learn.org/dev/developers/advanced_installation.html#testing" rel="nofollow"&gt;http://scikit-learn.org/dev/developers/advanced_installation.html#testing&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;blockquote&gt;
Random number generation can be controlled during testing by setting
the &lt;code&gt;SKLEARN_SEED&lt;/code&gt; environment variable.&lt;/blockquote&gt;
&lt;a name="user-content-submitting-a-pull-request"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-submitting-a-pull-request" class="anchor" aria-hidden="true" href="#submitting-a-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Submitting a Pull Request&lt;/h4&gt;
&lt;p&gt;Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: &lt;a href="http://scikit-learn.org/stable/developers/index.html" rel="nofollow"&gt;http://scikit-learn.org/stable/developers/index.html&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-project-history"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-project-history" class="anchor" aria-hidden="true" href="#project-history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project History&lt;/h3&gt;
&lt;p&gt;The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the  &lt;a href="http://scikit-learn.org/dev/about.html#authors" rel="nofollow"&gt;About us&lt;/a&gt; page
for a list of core contributors.&lt;/p&gt;
&lt;p&gt;The project is currently maintained by a team of volunteers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: scikit-learn was previously referred to as scikits.learn.&lt;/p&gt;
&lt;a name="user-content-help-and-support"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-help-and-support" class="anchor" aria-hidden="true" href="#help-and-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Help and Support&lt;/h3&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HTML documentation (stable release): &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HTML documentation (development version): &lt;a href="http://scikit-learn.org/dev/" rel="nofollow"&gt;http://scikit-learn.org/dev/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FAQ: &lt;a href="http://scikit-learn.org/stable/faq.html" rel="nofollow"&gt;http://scikit-learn.org/stable/faq.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-communication"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-communication" class="anchor" aria-hidden="true" href="#communication"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href="https://mail.python.org/mailman/listinfo/scikit-learn" rel="nofollow"&gt;https://mail.python.org/mailman/listinfo/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRC channel: &lt;code&gt;#scikit-learn&lt;/code&gt; at &lt;code&gt;webchat.freenode.net&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stack Overflow: &lt;a href="https://stackoverflow.com/questions/tagged/scikit-learn" rel="nofollow"&gt;https://stackoverflow.com/questions/tagged/scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Website: &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;http://scikit-learn.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-citation"&gt;&lt;/a&gt;
&lt;h4&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h4&gt;
&lt;p&gt;If you use scikit-learn in a scientific publication, we would appreciate citations: &lt;a href="http://scikit-learn.org/stable/about.html#citing-scikit-learn" rel="nofollow"&gt;http://scikit-learn.org/stable/about.html#citing-scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>scikit-learn</author><guid isPermaLink="false">https://github.com/scikit-learn/scikit-learn</guid><pubDate>Tue, 05 Nov 2019 00:14:00 GMT</pubDate></item><item><title>swisskyrepo/PayloadsAllTheThings #15 in Python, This week</title><link>https://github.com/swisskyrepo/PayloadsAllTheThings</link><description>&lt;p&gt;&lt;i&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-payloads-all-the-things" class="anchor" aria-hidden="true" href="#payloads-all-the-things"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Payloads All The Things&lt;/h1&gt;
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I &lt;g-emoji class="g-emoji" alias="heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png"&gt;❤️&lt;/g-emoji&gt; pull requests :)&lt;/p&gt;
&lt;p&gt;You can also contribute with a &lt;g-emoji class="g-emoji" alias="beers" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37b.png"&gt;🍻&lt;/g-emoji&gt; IRL&lt;/p&gt;
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;README.md - vulnerability description and how to exploit it&lt;/li&gt;
&lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt;
&lt;li&gt;Images - pictures for the README.md&lt;/li&gt;
&lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/"&gt;Methodology and Resources&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md"&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md"&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md"&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md"&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md"&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md"&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Discovery.md"&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md"&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md"&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md"&gt;Windows - Download and Execute.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Mimikatz.md"&gt;Windows - Mimikatz.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Persistence.md"&gt;Windows - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Post%20Exploitation%20Koadic.md"&gt;Windows - Post Exploitation Koadic.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Privilege%20Escalation.md"&gt;Windows - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Using%20credentials.md"&gt;Windows - Using credentials.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CVE%20Exploits"&gt;CVE Exploits&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Struts 2 CVE-2013-2251 CVE-2017-5638 CVE-2018-11776_.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2017-9805.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2018-11776.py&lt;/li&gt;
&lt;li&gt;Docker API RCE.py&lt;/li&gt;
&lt;li&gt;Drupalgeddon2 CVE-2018-7600.rb&lt;/li&gt;
&lt;li&gt;Heartbleed CVE-2014-0160.py&lt;/li&gt;
&lt;li&gt;JBoss CVE-2015-7501.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2015-8103.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2016-0792.py&lt;/li&gt;
&lt;li&gt;Rails CVE-2019-5420.rb&lt;/li&gt;
&lt;li&gt;Shellshock CVE-2014-6271.py&lt;/li&gt;
&lt;li&gt;Tomcat CVE-2017-12617.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2016-3510.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2017-10271.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2018-2894.py&lt;/li&gt;
&lt;li&gt;WebSphere CVE-2015-7450.py&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/YOUTUBE.md"&gt;Youtube videos&lt;/a&gt; selections.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>swisskyrepo</author><guid isPermaLink="false">https://github.com/swisskyrepo/PayloadsAllTheThings</guid><pubDate>Tue, 05 Nov 2019 00:15:00 GMT</pubDate></item><item><title>zhaoolee/ChromeAppHeroes #16 in Python, This week</title><link>https://github.com/zhaoolee/ChromeAppHeroes</link><description>&lt;p&gt;&lt;i&gt;🌈谷粒-Chrome插件英雄榜, 为优秀的Chrome插件写一本中文说明书, 让Chrome插件英雄们造福人类~  ChromePluginHeroes, Write a Chinese manual for the excellent Chrome plugin, let the Chrome plugin heroes benefit the human~&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/readme-en.html" rel="nofollow"&gt;English&lt;/a&gt; | &lt;a href="https://zhaoolee.gitbooks.io/chrome/content/" rel="nofollow"&gt;中文简体&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/996icu/996.ICU/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/41215df7ff78cefe41536bf897fe1c7e55b10bd2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d416e74692532303939362d626c75652e737667" alt="LICENSE" data-canonical-src="https://img.shields.io/badge/license-Anti%20996-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://996.icu" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13ac320a9a774e316fe72ffb1eaacf09b01b59a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e6b2d3939362e6963752d7265642e737667" alt="996.icu" data-canonical-src="https://img.shields.io/badge/link-996.icu-red.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-谷粒-chrome插件英雄榜" class="anchor" aria-hidden="true" href="#谷粒-chrome插件英雄榜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;谷粒-Chrome插件英雄榜&lt;/h1&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="rainbow" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f308.png"&gt;🌈&lt;/g-emoji&gt;谷粒-Chrome插件英雄榜, 为优秀的Chrome插件写一本中文说明书, 让Chrome插件英雄们造福人类~
ChromeAppHeroes, Write a Chinese manual for the excellent Chrome plugin, let the Chrome plugin heroes benefit the human~&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/5ecd2856f287477c89c20efb7de11a9b.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/5ecd2856f287477c89c20efb7de11a9b.png" alt="谷粒VI设计.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感谢&lt;a href="https://github.com/LuoJiangYong"&gt;老罗巴扎嘿&lt;/a&gt;为本项目设计的新的Logo | &lt;a href="https://zhaoolee.gitbooks.io/chrome/content/gu-li-qu-yi.html" rel="nofollow"&gt;谷粒文化(老罗巴扎嘿语录)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-相关项目推广-用chrome学编程" class="anchor" aria-hidden="true" href="#相关项目推广-用chrome学编程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;相关项目推广: &lt;a href="https://github.com/zhaoolee/ProgrammingWithChrome"&gt;用Chrome学编程&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;《用Chrome学编程(如何用Chrome优雅装B)》, 用Gif图展示Chrome的骚操作, 充分挖掘Chrome的编程潜力! &lt;a href="https://github.com/zhaoolee/ProgrammingWithChrome"&gt;https://github.com/zhaoolee/ProgrammingWithChrome&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://github.com/996icu/996.ICU/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/41215df7ff78cefe41536bf897fe1c7e55b10bd2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d416e74692532303939362d626c75652e737667" alt="LICENSE" data-canonical-src="https://img.shields.io/badge/license-Anti%20996-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://996.icu" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/13ac320a9a774e316fe72ffb1eaacf09b01b59a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e6b2d3939362e6963752d7265642e737667" alt="996.icu" data-canonical-src="https://img.shields.io/badge/link-996.icu-red.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/1f62c412c50e5397395878c4da31205080db55ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/issues/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265"&gt;&lt;img src="https://camo.githubusercontent.com/eae70f04ac75459320f0ec7397f12bded49476bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a68616f6f6c65652f4368726f6d654170704865726f65732e7376673f7374796c653d706f706f75742d737175617265" alt="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" data-canonical-src="https://img.shields.io/github/stars/zhaoolee/ChromeAppHeroes.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-目录点击以下标题-可以进入文章页" class="anchor" aria-hidden="true" href="#目录点击以下标题-可以进入文章页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录(点击以下标题, 可以进入文章页~)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/060_tabagotchi.html" rel="nofollow"&gt;060《Tabagotchi》为减缓全球变暖做出贡献&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/059_page_speed_insight_and_check_list.html" rel="nofollow"&gt;059《PageSpeed Insight and CheckList》为网页优化提供建议和量化指标&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/058_ip_address.html" rel="nofollow"&gt;058《IP-Address》快速查看当前设备IP&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/057_webp_save_as_png.html" rel="nofollow"&gt;057《图片另存为JPG/PNG/WebP》让WebP图片下载为PNG格式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/056_search.html" rel="nofollow"&gt;056《Search》为Chrome设置搜索引擎关键词&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/055_keylines.html" rel="nofollow"&gt;055《Keylines》为网页元素添加随机描边颜色 &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/054_er_xiang_yi_tu_sou_tu.html" rel="nofollow"&gt;054《二箱 以图搜图》让你在搜图方面随心所欲（为所欲为）&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/053_shu_biao_dian_ji_te_xiao.html" rel="nofollow"&gt;053《鼠标点击特效 (๑•́ ∀ •̀๑)》为鼠标点击添加有趣的特效&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/052_site_palette.html" rel="nofollow"&gt;052《Site Palette》自动提取网站配色&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/051_custom_cursor_for_chrome.html" rel="nofollow"&gt;051《Custom Cursor for Chrome™》为Chrome换上可爱初音光标&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/050_google_results_previewer.html" rel="nofollow"&gt;050《Google Results Previewer》无点击查看谷歌搜索结果&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/049_web_server_for_chrome.html" rel="nofollow"&gt;049《Web Server for Chrome》搭建本地Web服务器, 实现局域网共享文件夹&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/048_words_discoverer.html" rel="nofollow"&gt;048《Words Discoverer》高亮标注单词,提升你的词汇量&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/047_go_to_tab.html" rel="nofollow"&gt;047《Go to Tab》快速跳转到打开的网页&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/046_whatfont.html" rel="nofollow"&gt;046《WhatFont》字体爱好者优雅查看网页字体&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/045_restlet_client.html" rel="nofollow"&gt;045《Restlet Client》优秀的Api测试工具&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/044_gu_ge_fang_wen_zhu_shou.html" rel="nofollow"&gt;044《谷歌访问助手》访问Chrome商店 Gmail 谷歌搜索&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/043_dream_afar_new_tab.html" rel="nofollow"&gt;043《Dream Afar New Tab》探索世界的新方式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/042_edge.html" rel="nofollow"&gt;042 在Edge中安装Chrome扩展程序&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/041_copy_all_urls.html" rel="nofollow"&gt;041《Copy All Urls》优雅地保存-开启多个标签页&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/040_gitzip_for_github.html" rel="nofollow"&gt;040《GitZip for github》从Github批量下载表情包&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/039_simplify_gmail.html" rel="nofollow"&gt;039《Simplify Gmail》让网页版Gmail更清爽&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/038_alexa_traffic_rank.html" rel="nofollow"&gt;038《Alexa Traffic Rank》一键查看网站全球排名&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/037_saladict.html" rel="nofollow"&gt;037《Saladict》谷歌!有道!我全都要! 聚合词典, 并行翻译&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/036_screen_shader.html" rel="nofollow"&gt;036《Screen Shader》把网页调成暖色，你的眼睛会感谢你&lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;🙏&lt;/g-emoji&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/035_print_friendly_and_pdf.html" rel="nofollow"&gt;035《Print Friendly &amp;amp; PDF》让你拥有最佳的打印阅读体验&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/034_astro_bot.html" rel="nofollow"&gt;034《Astro Bot》用新标签页刷编程题&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/033_yi_ye.html" rel="nofollow"&gt;033《一叶》在任意网页开启实时弹幕 聊天窗口 留言板&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/032_smallpdf.html" rel="nofollow"&gt;032《Smallpdf》简单好用的线上PDF工具&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/031_onetab.html" rel="nofollow"&gt;031《OneTab》把多个Tab转换为一个列表&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/030_jue_jin.html" rel="nofollow"&gt;030《掘金》相信优质技术内容的力量&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/029_simread.html" rel="nofollow"&gt;029 《SimpRead》为任意网页开启阅读模式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/028_adblock.html" rel="nofollow"&gt;028《AdBlock》Adblock自定义屏蔽简书广告&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/027_text.html" rel="nofollow"&gt;027《Text》来自Chrome实验室的跨平台记事本&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/026_quickey_launcher.html" rel="nofollow"&gt;026《Quickey Launcher》打开网站只需一键&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/025_console.html" rel="nofollow"&gt;025《Console》Chrome自带好用的计算器&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/024_dark_reader.html" rel="nofollow"&gt;024《Dark Reader》为任意网站启用夜间模式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/023_fireshot.html" rel="nofollow"&gt;023《FireShot》一键滚动截屏整个网页&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/022kuo_zhan_guan_li_qi.html" rel="nofollow"&gt;022《扩展管理器》管理你的Chrome扩展&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/021_bi_li_bi_li_zhu_shou.html" rel="nofollow"&gt;021《哔哩哔哩助手》助你快速成为B站老司机&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/020_boxel_rebound.html" rel="nofollow"&gt;020《Boxel Rebound》“嗨到中毒”的弹跳小方块(附自制赛道分享方法)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/019_mega.html" rel="nofollow"&gt;019《MEGA》网盘可以良心到什么程度? 试试MEGA吧!&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/018_enhanced_github.html" rel="nofollow"&gt;018《Enhanced Github》从“冰柜”到“冰棍儿”,下载Github单个文件&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/017_xin_lang_wei_bo_tu_chuang.html" rel="nofollow"&gt;017《新浪微博图床》本地Markdown编写更流畅, 新浪微博图床来帮忙&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/016_jie_chu_b_zhan_qu_yu_xian_zhi.html" rel="nofollow"&gt;016《解除B站区域限制》查看进击的巨人第三季&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/015_xpath_helper.html" rel="nofollow"&gt;015 《XPath Helper》完成Bing每日壁纸的小爬虫&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/014_chao_ji_ma_li_ao_you_xi.html" rel="nofollow"&gt;014《超级马里奥游戏》Chrome变身小霸王&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/013_quick_qr.html" rel="nofollow"&gt;013《Quick QR》用二维码实现云粘贴&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/012_ourstickys.html" rel="nofollow"&gt;012《OurStickys》Chrome特色网页便签纸&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/011_whatruns.html" rel="nofollow"&gt;011 《whatruns》一键分析网站技术栈&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/010_speedtest.html" rel="nofollow"&gt;010《speedtest》网络测速插件speedtest&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/009_vimium.html" rel="nofollow"&gt;009《vimium》Chrome与vim双神器融合&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/008_chrome_cleaner_pro.html" rel="nofollow"&gt;008《Chrome Cleaner Pro》为Chrome加速&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/007_loom.html" rel="nofollow"&gt;007《loom》 Chrome翻录网页视频神器&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/006_similarsites.html" rel="nofollow"&gt;006《SimilarSites》 一键查找姊妹网站 SimilarSites&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/005_video_speed_controller.html" rel="nofollow"&gt;005《Video Speed Controller》 刷课（刷剧）神器！给网页视频加个速(最快可达16倍!)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/004_tampermonkey.html" rel="nofollow"&gt;004《Tampermonkey》 油猴子! 给浏览器开个挂&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/003_secure_shell_app.html" rel="nofollow"&gt;003《Secure Shell App》 Chrome中开启ssh一种什么体验&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/002_chrono.html" rel="nofollow"&gt;002《chrono》 让Chrome下载资源更容易&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/001_markdown_here.html" rel="nofollow"&gt;001《markdown-here》 Markdown一键转换到"富文本格式"&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-开源插件推广作者自荐" class="anchor" aria-hidden="true" href="#开源插件推广作者自荐"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;开源插件推广(作者自荐)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;作者主页&lt;/th&gt;
&lt;th&gt;开源信息&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/the-fucking-github/agajobpbaphiohkbkjigcalebbfmofdo" rel="nofollow"&gt;The Fucking Github&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/lvxianchao"&gt;lvxianchao&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/lvxianchao/the-fucking-github"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;很方便地查看、整理、搜索你已经 Star 过的项目和搜索 Github 上的项目。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/hitup/eiokaohkigpbonodjcbjpecbnccijkjb" rel="nofollow"&gt;HitUP&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wonderbeyond"&gt;wonderbeyond&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wonderbeyond/HitUP"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;利用 New Tab “空白页” 助您保持对流行技术趋势的跟进，附带其它福利。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/gitako-github-file-tree/giljefjcheohhamkjphiebfjnlphnokk" rel="nofollow"&gt;Gitako - Github file tree&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/EnixCoda"&gt;EnixCoda&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/EnixCoda/Gitako"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;功能上类似于大名鼎鼎的 Octotree ，但是用了更现代化的前端工具，性能好很多。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/githuber/janmcneaglgklfljjcpihkkomeghljnf" rel="nofollow"&gt;GITHUBER&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/zhuowenli"&gt;zhuowenli&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/zhuowenli/githuber"&gt;Github仓库地址&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;这是一个帮助 GitHub 开发者每日发现优质内容的 Chrome 主页拓展。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/60c92f0de3d44bb7a612d08e2e1f3d18.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/60c92f0de3d44bb7a612d08e2e1f3d18.png" alt="造福人类.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-咦微信打赏" class="anchor" aria-hidden="true" href="#咦微信打赏"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;咦?(微信打赏)&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c4fdea49e11241e392d6bcaa33855897.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c4fdea49e11241e392d6bcaa33855897.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;赞赏金额&lt;/th&gt;
&lt;th&gt;赞赏者(微信名)&lt;/th&gt;
&lt;th&gt;赞赏时间&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年8月2日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年7月11日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12.34&lt;/td&gt;
&lt;td&gt;张明辉&lt;/td&gt;
&lt;td&gt;2019年8月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;六小登登&lt;/td&gt;
&lt;td&gt;2019年9月5日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;云淡风晴&lt;/td&gt;
&lt;td&gt;2019年7月24日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;金三古月&lt;/td&gt;
&lt;td&gt;2019年6月2日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;Azuno&lt;/td&gt;
&lt;td&gt;2019年6月1日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10.00&lt;/td&gt;
&lt;td&gt;邦妥&lt;/td&gt;
&lt;td&gt;2019年5月22日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;enjoy life&lt;/td&gt;
&lt;td&gt;2019年9月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;L__hoo原&lt;/td&gt;
&lt;td&gt;2019年9月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;梦想旅程(公众号:苏生不惑)&lt;/td&gt;
&lt;td&gt;2019年9月14日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;1111&lt;/td&gt;
&lt;td&gt;2019年7月27日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;那都不重要&lt;/td&gt;
&lt;td&gt;2019年5月19日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;Lismg&lt;/td&gt;
&lt;td&gt;2019年6月5日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.00&lt;/td&gt;
&lt;td&gt;small胖&lt;/td&gt;
&lt;td&gt;2019年7月9日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.00&lt;/td&gt;
&lt;td&gt;良辰美&lt;/td&gt;
&lt;td&gt;2019年7月20日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.00&lt;/td&gt;
&lt;td&gt;@Coolstar&lt;/td&gt;
&lt;td&gt;2019年7月6日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年9月26日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;夏天的小虫子&lt;/td&gt;
&lt;td&gt;2019年9月23日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年7月26日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;2019年7月12日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年6月13日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Walter Wu&lt;/td&gt;
&lt;td&gt;2019年6月1日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Joseph&lt;/td&gt;
&lt;td&gt;2019年4月24日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年4月12日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;于云鹏Edward&lt;/td&gt;
&lt;td&gt;2019年4月12日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;黄金星&lt;/td&gt;
&lt;td&gt;2019年4月11日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;Cloud 9&lt;/td&gt;
&lt;td&gt;2019年4月5日&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.20&lt;/td&gt;
&lt;td&gt;(未留姓名)&lt;/td&gt;
&lt;td&gt;2019年7月25日&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;感谢以上赞赏者对本开源项目的支持[手动滑稽]&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-060tabagotchi为减缓全球变暖做出贡献" class="anchor" aria-hidden="true" href="#060tabagotchi为减缓全球变暖做出贡献"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/060_tabagotchi.html" rel="nofollow"&gt;060《Tabagotchi》为减缓全球变暖做出贡献&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63478935-7b1f7400-c4be-11e9-8679-5f4a6a56c89c.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63478935-7b1f7400-c4be-11e9-8679-5f4a6a56c89c.gif" alt="tabagotchi" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tabagotchi扩展以一种有趣的方式, 提醒我们减少标签页数量, 减少了计算机产生的热量, 为阻止全球变暖做出了贡献~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-059pagespeed-insight-and-checklist为网页优化提供建议和量化指标" class="anchor" aria-hidden="true" href="#059pagespeed-insight-and-checklist为网页优化提供建议和量化指标"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/059_page_speed_insight_and_check_list.html" rel="nofollow"&gt;059《PageSpeed Insight and CheckList》为网页优化提供建议和量化指标&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63309328-f818e500-c328-11e9-8f1a-68fed13a4015.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63309328-f818e500-c328-11e9-8f1a-68fed13a4015.gif" alt="pag_speed" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63309327-f7804e80-c328-11e9-8eab-9055db8a5d2c.png"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63309327-f7804e80-c328-11e9-8eab-9055db8a5d2c.png" alt="001" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PageSpeed Insight and CheckList 和 Google Page Speed 结合使用, 能够为网页质量评分,量化网页优化的效果,也为优化网页指明了方向,对前端工程师而言,是非常重要的工具&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-058ip-address快速查看当前设备ip" class="anchor" aria-hidden="true" href="#058ip-address快速查看当前设备ip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/058_ip_address.html" rel="nofollow"&gt;058《IP-Address》快速查看当前设备IP&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63222725-ee369b00-c1dd-11e9-986e-cbc002168db8.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63222725-ee369b00-c1dd-11e9-986e-cbc002168db8.gif" alt="ip_address" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;获取当前设备的IP地址,对于开发者而言,是一个经常遇到的问题,而《IP-Address》这款简洁小巧的软件, 能满足我们的需求&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-057图片另存为jpgpngwebp让webp图片下载为png格式md" class="anchor" aria-hidden="true" href="#057图片另存为jpgpngwebp让webp图片下载为png格式md"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/057_webp_save_as_png.html" rel="nofollow"&gt;057《图片另存为JPG/PNG/WebP》让WebP图片下载为PNG格式.md&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/63221240-ce48ac80-c1c8-11e9-9860-376fedc0845e.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/63221240-ce48ac80-c1c8-11e9-9860-376fedc0845e.gif" alt="save_as_png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;WebP是非常先进的格式, 但由于Photoshop这类元老级图像编辑软件不支持, 我们只能将图片为png格式,再进行编辑, 先进技术改变世界, 需要一个过程, 而在过程中提供一个折中的方案(把WebP装换为png, 再将png图片装换为WebP), 也是一件有价值的事~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-056search为chrome设置搜索引擎关键词" class="anchor" aria-hidden="true" href="#056search为chrome设置搜索引擎关键词"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/056_search.html" rel="nofollow"&gt;056《Search》为Chrome设置搜索引擎关键词&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/62503773-3c37c000-b828-11e9-9605-4ecce76830ec.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/62503773-3c37c000-b828-11e9-9605-4ecce76830ec.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在早期的网址导航主页上, 可以通过点击选择不同的搜索引擎进行搜索(数量有限, 而且不支持自定义), 而Chrome搜索更极客一些, 通过&lt;strong&gt;自定义关键词加空格&lt;/strong&gt;的方法, 在搜索引擎之间自由切换, 是一种兼具扩展性与易用性的做法&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-055keylines为网页元素添加随机描边颜色-" class="anchor" aria-hidden="true" href="#055keylines为网页元素添加随机描边颜色-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/055_keylines.html" rel="nofollow"&gt;055《Keylines》为网页元素添加随机描边颜色 &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61917657-dbcf9580-af80-11e9-87d3-528609ab85b0.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61917657-dbcf9580-af80-11e9-87d3-528609ab85b0.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Keylines的实现原理非常简单(为网页dom元素添加了outline属性), 但展示的效果却非常惊艳, 这应该归功于Keylines作者优秀的想法, 很多时候, 优秀的软件并不一定使用了很难掌握的技术, 而是包含了作者优秀的想法~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-054二箱以图搜图让你在搜图方面随心所欲为所欲为" class="anchor" aria-hidden="true" href="#054二箱以图搜图让你在搜图方面随心所欲为所欲为"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/054_er_xiang_yi_tu_sou_tu.html" rel="nofollow"&gt;054《二箱+以图搜图》让你在搜图方面随心所欲（为所欲为）&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61757068-93ce3880-adf1-11e9-8903-ebf313fb6098.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61757068-93ce3880-adf1-11e9-8903-ebf313fb6098.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;《二箱 以图搜图》是一款简单实用的搜图小工具，如果你是一名设计师, 可以帮你快速查找他人设计作品中所用的素材来源, 提升你的工作效率~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-053鼠标点击特效-๑́--̀๑为鼠标点击添加有趣的特效" class="anchor" aria-hidden="true" href="#053鼠标点击特效-๑́--̀๑为鼠标点击添加有趣的特效"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/053_shu_biao_dian_ji_te_xiao.htmll" rel="nofollow"&gt;053《鼠标点击特效 (๑•́ ∀ •̀๑)》为鼠标点击添加有趣的特效&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61600040-04921b00-ac61-11e9-8446-533752d71de1.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61600040-04921b00-ac61-11e9-8446-533752d71de1.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;《鼠标点击特效 (๑•́ ∀ •̀๑)》是一款为鼠标点击添加有趣的特效的扩展程序,虽然没啥实际用途,但很好玩, 录制一些有趣的网页小程序时, 会非常出彩~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-052site-palette自动提取网站配色" class="anchor" aria-hidden="true" href="#052site-palette自动提取网站配色"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/052_site_palette.html" rel="nofollow"&gt;052《Site Palette》自动提取网站配色&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61169390-2f101400-a58f-11e9-8769-4d62b7b64f37.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61169390-2f101400-a58f-11e9-8769-4d62b7b64f37.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Site Palette使用简单, 功能实用, 没有广告, 是典型的小而美的扩展程序, 这类扩展程序越多, Chrome的用户体验也就越好~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-051custom-cursor-for-chrome为chrome换上可爱初音光标" class="anchor" aria-hidden="true" href="#051custom-cursor-for-chrome为chrome换上可爱初音光标"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/051_custom_cursor_for_chrome.html" rel="nofollow"&gt;051《Custom Cursor for Chrome™》为Chrome换上可爱初音光标&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/61166967-d0846f00-a569-11e9-9141-15cef4983098.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/61166967-d0846f00-a569-11e9-9141-15cef4983098.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;早期的QQ空间和个人博客, 我们会给页面加各种各样的装饰, 连鼠标指针也要定制一下, 当时感觉乐趣无穷, 后面就失去了兴趣, 对于个人博客, 感觉越简洁越好, 于是就有了Next这些大量留白的博客主题,但我感觉在Next这类主题中加一些定制化的小物件也是不错的, 在简洁与花哨之间找到平衡, 不正是生活的乐趣之源么~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-050google-results-previewer无点击查看谷歌搜索结果" class="anchor" aria-hidden="true" href="#050google-results-previewer无点击查看谷歌搜索结果"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/050_google_results_previewer.html" rel="nofollow"&gt;050《Google Results Previewer》无点击查看谷歌搜索结果&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/9219a092f0f4eb1c6f614c1667b316d1.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/9219a092f0f4eb1c6f614c1667b316d1.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Google Results Previewer的功能简单实用, 也没有多余的设置, 属于新手友好型工具&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-049web-server-for-chrome搭建本地web服务器-实现局域网共享文件夹" class="anchor" aria-hidden="true" href="#049web-server-for-chrome搭建本地web服务器-实现局域网共享文件夹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/049_web_server_for_chrome.html" rel="nofollow"&gt;049《Web Server for Chrome》搭建本地Web服务器, 实现局域网共享文件夹&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/74d3eb882b103e0fb1e5e5dd651c052f.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/74d3eb882b103e0fb1e5e5dd651c052f.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Web Server for Chrome可以帮我们在本地快速开启http服务,让开发和测试变得更加简单, 如果你想和同处某个局域网的小伙伴, 建立一个共享文件夹, Web Server for Chrome或许是你最简单的实现方法~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-048words-discoverer背单词新姿势提升你的词汇量" class="anchor" aria-hidden="true" href="#048words-discoverer背单词新姿势提升你的词汇量"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/048_words_discoverer.html" rel="nofollow"&gt;048《Words Discoverer》背单词新姿势,提升你的词汇量&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/305439fdd84017da654e00f16aaee752.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/305439fdd84017da654e00f16aaee752.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Words Discoverer(中文译名: 单词发现者),&lt;strong&gt;可以突出显示网页上罕见的英语字典词汇和惯用语。促进英语语言学习并扩大词汇量&lt;/strong&gt;,通过自动高亮网页单词, 辅助单词记忆是一个很好的路子, 建议过一段时间,就稍微调高&lt;strong&gt;不突出显示 最常用的英语单词&lt;/strong&gt;的数量, 比如从默认的15%调整到16%,  单词发现者与沙拉查词结合使用, 真的是体验极佳~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-047go-to-tab快速跳转到打开的网页" class="anchor" aria-hidden="true" href="#047go-to-tab快速跳转到打开的网页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/047_go_to_tab.html" rel="nofollow"&gt;047《Go to Tab》快速跳转到打开的网页&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/59550928-2a623b00-8fa4-11e9-8525-8e830907463b.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/59550928-2a623b00-8fa4-11e9-8525-8e830907463b.gif" alt="2019-06-15-18 54 23" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Go to Tab对于工作期间大量打开页面, 又长时间不关机的程序员们, 是非常有帮助的&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-046whatfont字体爱好者优雅查看网页字体" class="anchor" aria-hidden="true" href="#046whatfont字体爱好者优雅查看网页字体"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/046_whatfont.html" rel="nofollow"&gt;046《WhatFont》字体爱好者优雅查看网页字体&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/15868458/59549312-4529b500-8f8e-11e9-8107-004486a02258.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/15868458/59549312-4529b500-8f8e-11e9-8107-004486a02258.gif" alt="font 2019-06-15 16_04_10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;WhatFont属于功能非常单一的小工具, 让字体爱好者优雅查看网页字体属性, 如果你对漂亮字体有一份执念, 推荐到&lt;a href="https://fonts.google.com/" rel="nofollow"&gt;https://fonts.google.com/&lt;/a&gt;, &lt;a href="https://www.myfonts.com/" rel="nofollow"&gt;https://www.myfonts.com/&lt;/a&gt;
等字体网站,找寻更多可爱的字体~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-045restlet-client优秀的api测试工具" class="anchor" aria-hidden="true" href="#045restlet-client优秀的api测试工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/045_restlet_client.html" rel="nofollow"&gt;045《Restlet Client》优秀的Api测试工具&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/89ea1e51dab48d5a84f089adf33eb274.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/zhaoolee_images000000/89ea1e51dab48d5a84f089adf33eb274.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Restlet Client是一款开发实用工具, 支持一键导入Postman等api测试工具的测试用例&lt;/li&gt;
&lt;li&gt;近来, Postman开始主推自己的70M左右的客户端安装包, 功能没什么改进, 体积却变得超大,而且Postman的Chrome扩展程序, 对macOS的支持不太好(每次打开, 都会弹窗报一个错)&lt;/li&gt;
&lt;li&gt;Restlet Client依然只是一个开箱即用的Chrome扩展程序, 非常适合硬盘空间有限的小伙伴使用(软件功能够用就可以了~)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-044谷歌访问助手访问chrome商店-gmail-谷歌搜索" class="anchor" aria-hidden="true" href="#044谷歌访问助手访问chrome商店-gmail-谷歌搜索"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/044_gu_ge_fang_wen_zhu_shou.html" rel="nofollow"&gt;044《谷歌访问助手》访问Chrome商店 Gmail 谷歌搜索&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/deff71a536ba4027a01fe3c7a558c277.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/deff71a536ba4027a01fe3c7a558c277.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;《谷歌访问助手》可以让我们访问Chrome商店, 以及谷歌搜索, 谷歌Gmail等服务
&lt;code&gt;仅为香港地区用户提，供方便科研,外贸提供帮助,不良用户,将封锁访问IP,后果自负&lt;/code&gt;, 谷歌访问助手需要你设置主页为&lt;code&gt;https://2018.hao245.com/&lt;/code&gt;才能使用, 有百度全家桶, 360全家桶的流氓内涵~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-043dream-afar-new-tab探索世界的新方式" class="anchor" aria-hidden="true" href="#043dream-afar-new-tab探索世界的新方式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/043_dream_afar_new_tab.html" rel="nofollow"&gt;043《Dream Afar New Tab》探索世界的新方式&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e40b7bec41ce4ac892578bc88a03d25c.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e40b7bec41ce4ac892578bc88a03d25c.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;《Dream Afar New Tab》的设计非常漂亮, 功能调节也非常简单, 只有两级菜单, 壁纸也非常精美, 对浏览器颜值有要求的小伙伴, 可以试一试~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-042-在edge中安装chrome扩展程序" class="anchor" aria-hidden="true" href="#042-在edge中安装chrome扩展程序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/042_edge.html" rel="nofollow"&gt;042 在Edge中安装Chrome扩展程序&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a131b9833d20424ab93cb258ab8542e8.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a131b9833d20424ab93cb258ab8542e8.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Edge可以安装绝大多数Chrome商店中的扩展, 但Chrome中的谷歌开发App程序, 类似&lt;a href="https://chrome.google.com/webstore/detail/secure-shell-app/pnhechapfaindjhompbnflcldabbghjo" rel="nofollow"&gt;Secure Shell App&lt;/a&gt;, 目前是无法安装的, 新版Edge使用了Chrome的Chromium内核, 可以兼容安装Chrome生态中的各种应用程序,为Edge未来的发展带来了无限可能~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-041copy-all-urls优雅地保存-开启多个标签页" class="anchor" aria-hidden="true" href="#041copy-all-urls优雅地保存-开启多个标签页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/041_copy_all_urls.html" rel="nofollow"&gt;041《Copy All Urls》优雅地保存-开启多个标签页&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/eac219ff189a4295bbf88974b045ba5b.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/eac219ff189a4295bbf88974b045ba5b.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Copy All Urls属于小而美地工具，如果你每天都需要查看几个固定的网页, Copy All Urls能帮你省很多时间~&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-040gitzip-for-github从github批量下载表情包" class="anchor" aria-hidden="true" href="#040gitzip-for-github从github批量下载表情包"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/040_gitzip_for_github.html" rel="nofollow"&gt;040《GitZip for github》从Github批量下载表情包&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/f5b923dc4a21437484e90859342ed366.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/f5b923dc4a21437484e90859342ed366.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;以前介绍过Github快速下载单个文件的扩展工具&lt;a href="https://zhaoolee.gitbooks.io/chrome/content/018enhanced-github300b-cong-201c-bing-gui-201d-dao-201c-bing-gun-er-201d2c-xia-zai-github-dan-ge-wen-jian.html" rel="nofollow"&gt;《Enhanced Github》&lt;/a&gt; , 《Enhanced Github》 和 《GitZip for github》 结合到一起, 就可以让我们快速下载, github任意仓库任意文件夹的优质资源了~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-039simplify-gmail让网页版gmail更清爽" class="anchor" aria-hidden="true" href="#039simplify-gmail让网页版gmail更清爽"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/039_simplify_gmail.html" rel="nofollow"&gt;039《Simplify Gmail》让网页版Gmail更清爽&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c9b1aa8201c24208b0e0aedfcdbdc992.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c9b1aa8201c24208b0e0aedfcdbdc992.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;好的扩展程序就应该这样, 让人见到后耳目一新, 使用的方法却非常简单。
如果你并没有注册过Gmail邮箱, 可以尝试注册一个, Gmail是非常好用的, 拥有规范的接口, 不会随便拦截邮件, 也不会在页面铺满广告&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-038alexa-traffic-rank一键查看网站全球排名" class="anchor" aria-hidden="true" href="#038alexa-traffic-rank一键查看网站全球排名"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/038_alexa_traffic_rank.html" rel="nofollow"&gt;038《Alexa Traffic Rank》一键查看网站全球排名&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcefd45a5cc74e4c824f567535f79c5c.webp"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcefd45a5cc74e4c824f567535f79c5c.webp" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Alexa给出的网站排名, 是目前公认最具参考价值的排名, 打开一个新站点, 查一下新站点的Alexa排名, 以及与它类似的站点, 让我们很快对新站点的定位, 有一个大致的认知~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-037saladict谷歌有道我全都要-聚合词典-并行翻译" class="anchor" aria-hidden="true" href="#037saladict谷歌有道我全都要-聚合词典-并行翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/037_saladict.html" rel="nofollow"&gt;037《Saladict》谷歌!有道!我全都要! 聚合词典, 并行翻译&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/07322f3c4b13484a8a048194558cec5c.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/07322f3c4b13484a8a048194558cec5c.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;沙拉查词(Saladict)是一款非常优秀的查词扩展, 上文只是提及了它最常用的一些功能, 沙拉查词的后台管理选项非常丰富, 感兴趣的小伙伴可以慢慢探索&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-036screen-shader把屏幕调成暖色你的眼睛会感谢你" class="anchor" aria-hidden="true" href="#036screen-shader把屏幕调成暖色你的眼睛会感谢你"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/036_screen_shader.html" rel="nofollow"&gt;036《Screen Shader》把屏幕调成暖色，你的眼睛会感谢你&lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;🙏&lt;/g-emoji&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/3a94a283267047c39114694706de7293.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/3a94a283267047c39114694706de7293.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;对于长时间看电脑的办公人员, 可以尝试吧屏幕调成暖色, 开始可能会不习惯, 但后面会感觉眼睛会舒服很多, 你的眼睛也会感谢你的~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-035print-friendly--pdf让你拥有最佳的打印阅读体验" class="anchor" aria-hidden="true" href="#035print-friendly--pdf让你拥有最佳的打印阅读体验"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/035_print_friendly_and_pdf.html" rel="nofollow"&gt;035《Print Friendly &amp;amp; PDF》让你拥有最佳的打印阅读体验&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a71d2b280298482ba2408482c1537bf9.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/a71d2b280298482ba2408482c1537bf9.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;《Print Friendly &amp;amp; PDF》是一款文件打印chrome插件，会在打印之前删除垃圾广告，导航和无用浮窗从而实现页面优化，让你拥有最佳的打印阅读体验, 如果你经常需要打印网页, 可以通过《Print Friendly &amp;amp; PDF》让你的打印工作变得省时省力~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-034astro-bot用新标签页刷编程题" class="anchor" aria-hidden="true" href="#034astro-bot用新标签页刷编程题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/034_astro_bot.html" rel="nofollow"&gt;034《Astro Bot》用新标签页刷编程题&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/671d39ca714f437fa1d287bfb988724e.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/671d39ca714f437fa1d287bfb988724e.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Astro Bot可以在新标签页,展示一道与程序相关的问题或相关新闻&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-033一叶在任意网页开启实时弹幕-聊天窗口-留言板" class="anchor" aria-hidden="true" href="#033一叶在任意网页开启实时弹幕-聊天窗口-留言板"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/033_yi_ye.html" rel="nofollow"&gt;033《一叶》在任意网页开启实时弹幕 聊天窗口 留言板&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6a328e8eb9984f5abea5816c681b8e4e.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6a328e8eb9984f5abea5816c681b8e4e.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;一叶是一款很有想法的产品,但目前用户量还是很少, 对此,我个人也有一些想法,如果官方可以效仿pokemongo这类寻宝游戏,在各大网站的主页对应的留言板内,埋下一些有意思的彩蛋,让用户去寻宝,或许会有利于产品的推广~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-032smallpdf简单好用的线上pdf工具" class="anchor" aria-hidden="true" href="#032smallpdf简单好用的线上pdf工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/032_smallpdf.html" rel="nofollow"&gt;032《Smallpdf》简单好用的线上PDF工具&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/2c00d25291db4750963c60e78344d4cc.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/2c00d25291db4750963c60e78344d4cc.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Smallpdf是一个非常好用的PDF工具,可以收藏起来,作为日常办公的工具, Smallpdf可以进行多份pdf在线合并, pdf在线编辑, 如果你是一个经常和PDF打交道的人, 可不要错过它~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-031onetab把多个tab转换为一个列表" class="anchor" aria-hidden="true" href="#031onetab把多个tab转换为一个列表"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/031_onetab.htmll" rel="nofollow"&gt;031《OneTab》把多个Tab转换为一个列表&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/93781d48870742e08dc68fa17e79169e.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/93781d48870742e08dc68fa17e79169e.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当你发现自己有太多的标签页时,单击OneTab图标,所有标签页会转换成一个列表,当你需要再次访问这些标签页时,点击OneTab图标唤出列表,点击列表恢复标签页&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-030掘金相信优质技术内容的力量" class="anchor" aria-hidden="true" href="#030掘金相信优质技术内容的力量"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/030_jue_jin.html" rel="nofollow"&gt;030《掘金》相信优质技术内容的力量&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcca47d65f2542808281c17ec379d7d9.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fcca47d65f2542808281c17ec379d7d9.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;如果你想对 程序员, 产品经理, 设计师的行业知识有所了解, 可以没事儿打开掘金插件看一看, 如果你感觉很喜欢里面的内容, 可以到掘金官网 &lt;a href="https://juejin.im/" rel="nofollow"&gt;https://juejin.im/&lt;/a&gt; 逛一逛&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-029-simpread为任意网页开启阅读模式" class="anchor" aria-hidden="true" href="#029-simpread为任意网页开启阅读模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/029_simread.html" rel="nofollow"&gt;029 《SimpRead》为任意网页开启阅读模式&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0f9aa9ca332c4325806f92784af9f9ac.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0f9aa9ca332c4325806f92784af9f9ac.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
为网页开启阅读模式, 能让我们更专注于内容, 不会被花花绿绿的广告推广分散精力, 而SimpRead就是一歀为网页开启&lt;strong&gt;阅读模式&lt;/strong&gt;的插件&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-028adblockadblock屏蔽简书广告" class="anchor" aria-hidden="true" href="#028adblockadblock屏蔽简书广告"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/028_adblock.html" rel="nofollow"&gt;028《AdBlock》Adblock屏蔽简书广告&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e149c42ac1f343b88f50e522cba9ad64.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e149c42ac1f343b88f50e522cba9ad64.gif" alt="屏蔽简书广告" style="max-width:100%;"&gt;&lt;/a&gt;
Adblock的功能非常丰富, 但很多功能基本用不到, 普通用户只需要开启Adblock, 能使用右键工具屏蔽不喜欢的广告, 也就够用了~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-027text来自chrome实验室的跨平台记事本" class="anchor" aria-hidden="true" href="#027text来自chrome实验室的跨平台记事本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/027_text.html" rel="nofollow"&gt;027《Text》来自Chrome实验室的跨平台记事本&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6e287798ca1d4b939705447d4b8b2b3b.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6e287798ca1d4b939705447d4b8b2b3b.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Text由谷歌Chrome实验室研发并开源, 开源地址&lt;a href="https://github.com/GoogleChromeLabs/text-app"&gt;https://github.com/GoogleChromeLabs/text-app&lt;/a&gt; , Text属于小而美的产品, 功能不算强大, 但是够用, 而且借助Chrome完成了跨平台(在Linux也可以使用哦~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-026quickey-launcher打开网站只需一键" class="anchor" aria-hidden="true" href="#026quickey-launcher打开网站只需一键"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/026_quickey_launcher.html" rel="nofollow"&gt;026《Quickey Launcher》打开网站只需一键&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/322a82d214b34ff2ba70d9c1cd71d276.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/322a82d214b34ff2ba70d9c1cd71d276.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Quickey Launcher以优雅的方式, 为任意网页绑定一个快捷键, 绑定完成后, 即可通过快捷键,打开网页&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-025consolechrome自带好用的计算器" class="anchor" aria-hidden="true" href="#025consolechrome自带好用的计算器"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/025_console.html" rel="nofollow"&gt;025《Console》Chrome自带好用的计算器&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c7bc7cabd06a453dbed2bae0a2bf08d5.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/c7bc7cabd06a453dbed2bae0a2bf08d5.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome计算机的好用之处: 既可以看到加数字的记录,也可以实时预览运算的结果, 输入完成后还可以很方便的核查一遍, 还有一点: Chrome计算器观赏性强(逼格很高)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-024dark-reader为任意网站启用夜间模式" class="anchor" aria-hidden="true" href="#024dark-reader为任意网站启用夜间模式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/024_dark_reader.html" rel="nofollow"&gt;024《Dark Reader》为任意网站启用夜间模式&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/35e84f58945d4775a31154ea4dc51cac.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/35e84f58945d4775a31154ea4dc51cac.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;喜欢夜间模式的小伙伴, Dark Reader应该可以满足你了~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;&lt;a id="user-content-023fireshot一键滚动截屏整个网页" class="anchor" aria-hidden="true" href="#023fireshot一键滚动截屏整个网页"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/023_fireshot.html" rel="nofollow"&gt;023《FireShot》一键滚动截屏整个网页&lt;/a&gt;&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/81ac43fe1d6e454b93dc7f3ae57d96cd.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/81ac43fe1d6e454b93dc7f3ae57d96cd.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
总体来讲, FireShot是一款不错的软件, 免费且功能够用, 滚动截图的功能比同类软件做的都要好&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-022扩展管理器管理你的chrome扩展" class="anchor" aria-hidden="true" href="#022扩展管理器管理你的chrome扩展"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/022kuo_zhan_guan_li_qi.html" rel="nofollow"&gt;022《扩展管理器》管理你的Chrome扩展&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0480fffebb10437c8d5555f085de9006.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/0480fffebb10437c8d5555f085de9006.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
如果Chrome安装的插件很多, 我们可以对插件进行分组, 按照场景,启用不同组的插件&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-021哔哩哔哩助手助你快速成为b站老司机" class="anchor" aria-hidden="true" href="#021哔哩哔哩助手助你快速成为b站老司机"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/021_bi_li_bi_li_zhu_shou.html" rel="nofollow"&gt;021《哔哩哔哩助手》助你快速成为B站老司机&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6ccb9837b60d4d79814a8add20723d97.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/6ccb9837b60d4d79814a8add20723d97.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;哔哩哔哩助手, 功能实用,开发者也一直保持着较高频率的更新,可以放心食用~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-020boxel-rebound嗨到中毒的弹跳小方块附自制赛道分享方法" class="anchor" aria-hidden="true" href="#020boxel-rebound嗨到中毒的弹跳小方块附自制赛道分享方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/020_boxel_rebound.html" rel="nofollow"&gt;020《Boxel Rebound》“嗨到中毒”的弹跳小方块(附自制赛道分享方法)&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dbc83cc53c26492db8843ff3e35fc75d.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dbc83cc53c26492db8843ff3e35fc75d.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
Boxel Rebound是一个偏极客的小游戏, 玩法简单, 可以自由创建赛道, 分享赛道, 获取别人的赛道进行二次开发; 无论你是Mac用户,Windows用户,Linux用户, 只要安装了Chrome浏览器, 就可以玩耍Boxel Rebound&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-019mega网盘可以良心到什么程度-试试mega吧" class="anchor" aria-hidden="true" href="#019mega网盘可以良心到什么程度-试试mega吧"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/019_mega.html" rel="nofollow"&gt;019《MEGA》网盘可以良心到什么程度? 试试MEGA吧!&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b5aea0b5e3c54f0a9a050a754a67093d.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b5aea0b5e3c54f0a9a050a754a67093d.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;没有限速的概念(真的被百度盘的限速策略恶心到了)&lt;/li&gt;
&lt;li&gt;在国内可用(google虽好, 但国内用不了, MEGAsync亲测国内可用)&lt;/li&gt;
&lt;li&gt;云端加密, 资源不会被封杀&lt;/li&gt;
&lt;li&gt;官方提供了Linux客户端&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-018enhanced-github从冰柜到冰棍儿下载github单个文件" class="anchor" aria-hidden="true" href="#018enhanced-github从冰柜到冰棍儿下载github单个文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/018_enhanced_github.html" rel="nofollow"&gt;018《Enhanced Github》从“冰柜”到“冰棍儿”,下载Github单个文件&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/769a22f995d74226ba4104aba7e8ab59.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/769a22f995d74226ba4104aba7e8ab59.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/00541b7bd6954f8ea2a6a1beaebbb79b.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/00541b7bd6954f8ea2a6a1beaebbb79b.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
我需要Github给我一根冰棍解暑,Github却坚持把装有冰棍的冰柜也送给我（哥们儿真够意思）... 有了Enhanced Github这款插件, 我们可以下载Github优秀项目中最核心的代码文件进行学习, 而不是 下载 整个仓库作为藏品&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-017新浪微博图床本地markdown编写更流畅-新浪微博图床来帮忙" class="anchor" aria-hidden="true" href="#017新浪微博图床本地markdown编写更流畅-新浪微博图床来帮忙"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/017_xin_lang_wei_bo_tu_chuang.html" rel="nofollow"&gt;017《新浪微博图床》本地Markdown编写更流畅, 新浪微博图床来帮忙&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/48c12b3864f84e988e073209fd7cf8e4.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/48c12b3864f84e988e073209fd7cf8e4.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
用Markdown写文章, 如果文章中使用了本地配图, 那本地配图就要和文章一起打包,否则别人是看不到图片的,如果把本地图片放到网络服务器, 然后直接把图片的url粘贴到文章里面, 就可以免除图片打包的步骤&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-016解除b站区域限制查看进击的巨人第三季" class="anchor" aria-hidden="true" href="#016解除b站区域限制查看进击的巨人第三季"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/016_jie_chu_b_zhan_qu_yu_xian_zhi.html" rel="nofollow"&gt;016《解除B站区域限制》查看进击的巨人第三季&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/34d50d4d15094ca08e1bbd76c477122a.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/34d50d4d15094ca08e1bbd76c477122a.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/99fd518796894945aa87225a5022c453.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/99fd518796894945aa87225a5022c453.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;
解除B站区域限制,B站老司机必备技能&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-015xpath-helper完成bing每日壁纸的小爬虫" class="anchor" aria-hidden="true" href="#015xpath-helper完成bing每日壁纸的小爬虫"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/015_xpath_helper.html" rel="nofollow"&gt;015《XPath Helper》完成Bing每日壁纸的小爬虫&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/308bec78f4674130b85a5852f0b25a88.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/308bec78f4674130b85a5852f0b25a88.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;XPath是一个辅助我们写爬虫的小插件, 我们可以用XPath辅助我们完成一个Bing壁纸的小爬虫~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-014超级马里奥游戏chrome变身小霸王" class="anchor" aria-hidden="true" href="#014超级马里奥游戏chrome变身小霸王"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/014_chao_ji_ma_li_ao_you_xi.html" rel="nofollow"&gt;014《超级马里奥游戏》Chrome变身小霸王&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/008f3bd3c8b8483b9d70be5d5ed4f9ee.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/008f3bd3c8b8483b9d70be5d5ed4f9ee.gif" alt="超级玛丽.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;用Chrome玩超级马里奥是一种什么体验? 哈哈, 好玩! 《超级马里奥游戏》这款插件,可以让你打开Chrome, 随时玩一局超级玛丽, 嘿嘿&lt;g-emoji class="g-emoji" alias="yum" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60b.png"&gt;😋&lt;/g-emoji&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-013quick-qr用二维码实现云粘贴" class="anchor" aria-hidden="true" href="#013quick-qr用二维码实现云粘贴"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/013_quick_qr.html" rel="nofollow"&gt;013《Quick QR》用二维码实现云粘贴&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b59f299316624e86aa7cdd379a02aac4.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b59f299316624e86aa7cdd379a02aac4.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;通过Quick QR, 我们可以不借助任何通讯软件,通过手机扫码,获取PC浏览器上任意一段文字信息(云粘贴板哦~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-012ourstickyschrome特色网页便签纸" class="anchor" aria-hidden="true" href="#012ourstickyschrome特色网页便签纸"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/012_ourstickys.html" rel="nofollow"&gt;012《OurStickys》Chrome特色网页便签纸&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/62597d60ffd6443396725c9677951221.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/62597d60ffd6443396725c9677951221.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;向众人介绍喜欢的网页功能时,可以边讲,边向网页打便签,这样既能让人眼前一亮,也让听众容易抓住重点~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-011-whatruns一键分析网站技术栈" class="anchor" aria-hidden="true" href="#011-whatruns一键分析网站技术栈"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/011_whatruns.html" rel="nofollow"&gt;011 《whatruns》一键分析网站技术栈&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/28cc002358c647878b54f9bcaaf67a0a.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/28cc002358c647878b54f9bcaaf67a0a.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;如果你对当前浏览的网站非常感兴趣, 可以通过whatruns了解软件的技术栈, 比如看看这个名为facebook用了什么技术&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-010speedtest网络测速插件speedtest" class="anchor" aria-hidden="true" href="#010speedtest网络测速插件speedtest"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/010_speedtest.html" rel="nofollow"&gt;010《speedtest》网络测速插件speedtest&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9aa1e5323a6a4cbcb96304b33a5261c8.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9aa1e5323a6a4cbcb96304b33a5261c8.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当上网速度很慢的时候, 人们想到的第一件事就进行网络测速,在window上, 只要你安装了360全家桶, 测速功能就是默认安装的, 但测速这种功能根本不需要安装到本地, 交给浏览器就好了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-009vimiumchrome与vim双神器融合" class="anchor" aria-hidden="true" href="#009vimiumchrome与vim双神器融合"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/009_vimium.html" rel="nofollow"&gt;009《vimium》Chrome与vim双神器融合&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/7d6e9fadef3f48409c81a8c76d24e0cc.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/7d6e9fadef3f48409c81a8c76d24e0cc.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;vimium可以让我们只使用键盘就可以浏览网页, 如果你第一次看到有人使用vimium, 它的操作方式绝对能让你感到惊艳~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-008chrome-cleaner-pro为chrome加速" class="anchor" aria-hidden="true" href="#008chrome-cleaner-pro为chrome加速"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/008_chrome_cleaner_pro.html" rel="nofollow"&gt;008《Chrome Cleaner Pro》为Chrome加速&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/30899ae22f644a9bb62eb8b24d75c884.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/30899ae22f644a9bb62eb8b24d75c884.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Chrome经过最近几年的发展, 强力的扩展越来越多, 离Chrome OS的目标也越来越近, 软件做大了就会有类似Windows的通病, 软件会变慢, 让Chrome变快的最简单方式就是清理垃圾, 而Chrome Cleaner Pro走的是一键清理的路子~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-007loom-chrome翻录网页视频神器" class="anchor" aria-hidden="true" href="#007loom-chrome翻录网页视频神器"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/007_loom.html" rel="nofollow"&gt;007《loom》 Chrome翻录网页视频神器&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/4058cf0008074c5f86b8eb1684e7a1a0.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/4058cf0008074c5f86b8eb1684e7a1a0.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Loom可以一键录制浏览器的单个标签页(盗版翻录视频的神器), 录制完成后自动生成在线网页,进行视频播放, 可以下载刚刚录制的视频, 也可以为刚刚生成的在线视频设置密码(盗版录屏加发布一条龙服务~)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-006similarsites-一键查找姊妹网站-similarsites" class="anchor" aria-hidden="true" href="#006similarsites-一键查找姊妹网站-similarsites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/006_similarsites.html" rel="nofollow"&gt;006《SimilarSites》 一键查找姊妹网站 SimilarSites&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/26c6c604be1c41e88ebfe79c733173b0.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/26c6c604be1c41e88ebfe79c733173b0.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;当你浏览一个很棒的站点的时候, 或许你会想到, 和它"差不多"的站点有哪些, 尤其是针对一些资源站点, 这个站点没有, 而它同类的站点"往往有"! SimilarSites, 它的作用只有一个, 发现同类站点!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-005video-speed-controller-刷课刷剧神器给网页视频加个速最快可达16倍" class="anchor" aria-hidden="true" href="#005video-speed-controller-刷课刷剧神器给网页视频加个速最快可达16倍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/005_video_speed_controller.html" rel="nofollow"&gt;005《Video Speed Controller》 刷课（刷剧）神器！给网页视频加个速(最快可达16倍!)&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/083c51a1c32a4ad6931646bb005fd5a3.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/083c51a1c32a4ad6931646bb005fd5a3.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;刷一些没营养视频的时候, 我们会有倍速播放视频的需求, 而网站的在线播放器一般只提供不高于4倍的播放速度, 而Video Speed Controller可以将视频播放速度提高到16倍速~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-004tampermonkey-油猴子-给浏览器开个挂" class="anchor" aria-hidden="true" href="#004tampermonkey-油猴子-给浏览器开个挂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/004_tampermonkey.html" rel="nofollow"&gt;004《Tampermonkey》 油猴子! 给浏览器开个挂&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e87601eb459549b3b8e33994fc3fdfb4.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/e87601eb459549b3b8e33994fc3fdfb4.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;油猴子必备成为Chrome的第二应用商店, 有了油猴子, 你可以免费查看VIP视频, 清除各种网页广告, 在豆瓣影评页面显示电影资源的下载地址~&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-003secure-shell-app-chrome中开启ssh一种什么体验" class="anchor" aria-hidden="true" href="#003secure-shell-app-chrome中开启ssh一种什么体验"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/003_secure_shell_app.html" rel="nofollow"&gt;003《Secure Shell App》 Chrome中开启ssh一种什么体验&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/87b66b4cbd12426bbab65a3443f1f1ec.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/87b66b4cbd12426bbab65a3443f1f1ec.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;很多小白想要通过购买服务器搭建自己的VPN, 购买服务器后, 第一步就是要通过ssh登录服务器, 而Windows并没有自带ssh软件,现在你无需下载putty或xshell ,可以通过这款Secure Shell App在chrome直接实现ssh登录服务器了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-002-chrono-让chrome下载资源更容易" class="anchor" aria-hidden="true" href="#002-chrono-让chrome下载资源更容易"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/002_chrono.html" rel="nofollow"&gt;002 《chrono》 让Chrome下载资源更容易&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b574ee1798984ff49396837b620f51ef.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/b574ee1798984ff49396837b620f51ef.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;chrono可以非常方便的嗅探识别网页中的资源, 然后一键下载所有资源(收图喽!)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-001markdown-here-markdown一键转换到富文本格式" class="anchor" aria-hidden="true" href="#001markdown-here-markdown一键转换到富文本格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/page/001_markdown_here.html" rel="nofollow"&gt;001《markdown-here》 Markdown一键转换到"富文本格式"&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fc5de2eb22184a138c618728cfb40ede.gif"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/fc5de2eb22184a138c618728cfb40ede.gif" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;有了markdown-here这个插件, 可以在网页版 QQ邮箱, Gmail, 新浪头条文章, 里面使用mardown格式进行书写,然后一键转换为富文本&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-他人眼中的-chrome插件英雄榜商业互吹模块" class="anchor" aria-hidden="true" href="#他人眼中的-chrome插件英雄榜商业互吹模块"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;他人眼中的 Chrome插件英雄榜(商业互吹模块)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/88386634" rel="nofollow"&gt;《这份“插件英雄榜Top20”才是Chrome的正确打开方式！》&lt;/a&gt; 作者: &lt;a href="https://me.csdn.net/dQCFKyQDXYm3F8rB0" rel="nofollow"&gt;AI科技大本营&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58636515" rel="nofollow"&gt;《Chrome 插件英雄榜》&lt;/a&gt; 作者: &lt;a href="https://www.zhihu.com/people/loonggg/activities" rel="nofollow"&gt;非著名程序员&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openingsource.org/6190/zh-tw/" rel="nofollow"&gt;《開源日報第363期》&lt;/a&gt; 作者: &lt;a href="https://openingsource.org/" rel="nofollow"&gt;开源工厂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/Y-9ht-E7-OdJOEDDb3yyWw" rel="nofollow"&gt;《一根火柴的N种打开方式》&lt;/a&gt; 作者: &lt;a href="https://github.com/LuoJiangYong"&gt;老罗巴扎嘿&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-名字起啥好" class="anchor" aria-hidden="true" href="#名字起啥好"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;名字起啥好?&lt;/h2&gt;
&lt;p&gt;将这个仓库命名为&lt;strong&gt;Chrome扩展英雄榜&lt;/strong&gt;可能更准确些,但&lt;strong&gt;插件&lt;/strong&gt;这个名词, 更通俗易懂, 所以就使用了&lt;strong&gt;Chrome插件英雄榜&lt;/strong&gt;这个命名 ,感谢@&lt;a href="https://github.com/hjthjthjt"&gt;hjthjthjt&lt;/a&gt; 给出的&lt;a href="https://github.com/zhaoolee/ChromeAppHeroes/issues/14"&gt;issue&lt;/a&gt;纠正&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-推荐姊妹仓库" class="anchor" aria-hidden="true" href="#推荐姊妹仓库"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;推荐姊妹仓库&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本仓库的姊妹篇:**&lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;《Github星聚弃疗榜》&lt;/a&gt;**为Github创意项目写一本推荐书，让Github优秀项目造福人类~ 已开源到Github: &lt;a href="https://github.com/zhaoolee/StarsAndClown"&gt;https://github.com/zhaoolee/StarsAndClown&lt;/a&gt; 同样有趣有料哦~&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-感谢" class="anchor" aria-hidden="true" href="#感谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;感谢 掘金沸点运营 &lt;a href="https://juejin.im/user/5b39bd7de51d4558d43ff06d" rel="nofollow"&gt;@清蒸不是水煮&lt;/a&gt; 给出的 &lt;strong&gt;正面最开始放个索引目录比较好&lt;/strong&gt; 的小建议&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;感谢&lt;a href="https://www.jianshu.com/" rel="nofollow"&gt;简书&lt;/a&gt;社区提供超棒的Markdown编辑器,&lt;strong&gt;Chrome插件英雄榜&lt;/strong&gt;的编辑工作,几乎全部由通过简书编辑器完成&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;** emm... &lt;a href="https://zhaoolee.com/ChromeAppHeroes/download_the_chrome_extension_from_the_store.html" rel="nofollow"&gt;从官方商店下载Chrome插件的方法&lt;/a&gt;**&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chrome插件英雄榜&lt;/strong&gt; Github地址: &lt;a href="https://github.com/zhaoolee/ChromeAppHeroes"&gt;https://github.com/zhaoolee/ChromeAppHeroes&lt;/a&gt;
我需要你的支持, 希望你能为本项目填加一个 &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;🌟&lt;/g-emoji&gt;星.
I need your support, I hope you can add a star &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;🌟&lt;/g-emoji&gt; to this project.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-一根火柴的n种打开方式谷粒文化" class="anchor" aria-hidden="true" href="#一根火柴的n种打开方式谷粒文化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/meaning_of_gu_li.html" rel="nofollow"&gt;一根火柴的N种打开方式(谷粒文化)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/9ac21b8aea054eb48fc404fd429638bf.jpeg" alt="smartmockups_juunlhbe.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dc9ab48d958843c98f2a4c9336cff748.png"&gt;&lt;img src="https://raw.githubusercontent.com/zhaoolee/GraphBed/master/ChromeAppHeroes/dc9ab48d958843c98f2a4c9336cff748.png" alt="2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-项目相关阅读" class="anchor" aria-hidden="true" href="#项目相关阅读"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目相关阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://zhaoolee.com/ChromeAppHeroes/chrome_extended_resources_site.html" rel="nofollow"&gt;Chrome扩展资源站点推荐&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zhaoolee</author><guid isPermaLink="false">https://github.com/zhaoolee/ChromeAppHeroes</guid><pubDate>Tue, 05 Nov 2019 00:16:00 GMT</pubDate></item><item><title>hindupuravinash/the-gan-zoo #17 in Python, This week</title><link>https://github.com/hindupuravinash/the-gan-zoo</link><description>&lt;p&gt;&lt;i&gt;A list of all named GANs!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-the-gan-zoo" class="anchor" aria-hidden="true" href="#the-gan-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The GAN Zoo&lt;/h1&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="The_GAN_Zoo.jpg"&gt;&lt;img width="40%" src="The_GAN_Zoo.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Every week, new GAN papers are coming out and it's hard to keep track of them all, not to mention the incredibly creative ways in which researchers are naming these GANs! So, here's a list of what started as a fun activity compiling all named GANs!&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="cumulative_gans.jpg"&gt;&lt;img width="50%" src="cumulative_gans.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also check out the same data in a tabular format with functionality to filter by year or do a quick search by title &lt;a href="https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contributions are welcome. Add links through pull requests in gans.tsv file in the same format or create an issue to lemme know something I missed or to start a discussion.&lt;/p&gt;
&lt;p&gt;Check out &lt;a href="https://deephunt.in" rel="nofollow"&gt;Deep Hunt&lt;/a&gt; - my weekly AI newsletter for this repo as &lt;a href="https://medium.com/deep-hunt/the-gan-zoo-79597dc8c347" rel="nofollow"&gt;blogpost&lt;/a&gt; and follow me on &lt;a href="https://www.twitter.com/hindupuravinash" rel="nofollow"&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3D-ED-GAN - &lt;a href="https://arxiv.org/abs/1711.06375" rel="nofollow"&gt;Shape Inpainting using 3D Generative Adversarial Network and Recurrent Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D-GAN - &lt;a href="https://arxiv.org/abs/1610.07584" rel="nofollow"&gt;Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling&lt;/a&gt; (&lt;a href="https://github.com/zck119/3dgan-release"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;3D-IWGAN - &lt;a href="https://arxiv.org/abs/1707.09557" rel="nofollow"&gt;Improved Adversarial Systems for 3D Object Generation and Reconstruction&lt;/a&gt; (&lt;a href="https://github.com/EdwardSmith1884/3D-IWGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;3D-PhysNet - &lt;a href="https://arxiv.org/abs/1805.00328" rel="nofollow"&gt;3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object Deformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D-RecGAN - &lt;a href="https://arxiv.org/abs/1708.07969" rel="nofollow"&gt;3D Object Reconstruction from a Single Depth View with Adversarial Learning&lt;/a&gt; (&lt;a href="https://github.com/Yang7879/3D-RecGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ABC-GAN - &lt;a href="https://drive.google.com/file/d/0B3wEP_lEl0laVTdGcHE2VnRiMlE/view" rel="nofollow"&gt;ABC-GAN: Adaptive Blur and Control for improved training stability of Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/IgorSusmelj/ABC-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ABC-GAN - &lt;a href="https://arxiv.org/abs/1711.11139" rel="nofollow"&gt;GANs for LIFE: Generative Adversarial Networks for Likelihood Free Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AC-GAN - &lt;a href="https://arxiv.org/abs/1610.09585" rel="nofollow"&gt;Conditional Image Synthesis With Auxiliary Classifier GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;acGAN - &lt;a href="https://arxiv.org/abs/1702.01983" rel="nofollow"&gt;Face Aging With Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ACGAN - &lt;a href="https://arxiv.org/abs/1712.06951" rel="nofollow"&gt;Coverless Information Hiding Based on Generative adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;acGAN - &lt;a href="https://arxiv.org/abs/1808.00020" rel="nofollow"&gt;On-line Adaptative Curriculum Learning for GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ACtuAL - &lt;a href="https://arxiv.org/abs/1711.04755" rel="nofollow"&gt;ACtuAL: Actor-Critic Under Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdaGAN - &lt;a href="https://arxiv.org/abs/1701.02386v1" rel="nofollow"&gt;AdaGAN: Boosting Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive GAN - &lt;a href="https://arxiv.org/abs/1806.10496" rel="nofollow"&gt;Customizing an Adversarial Example Generator with Class-Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdvEntuRe - &lt;a href="https://arxiv.org/abs/1805.04680" rel="nofollow"&gt;AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdvGAN - &lt;a href="https://arxiv.org/abs/1801.02610" rel="nofollow"&gt;Generating adversarial examples with adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AE-GAN - &lt;a href="https://arxiv.org/abs/1707.05474" rel="nofollow"&gt;AE-GAN: adversarial eliminating with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AE-OT - &lt;a href="https://arxiv.org/abs/1809.05964" rel="nofollow"&gt;Latent Space Optimal Transport for Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AEGAN - &lt;a href="https://arxiv.org/abs/1703.10094" rel="nofollow"&gt;Learning Inverse Mapping by Autoencoder based Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AF-DCGAN - &lt;a href="https://arxiv.org/abs/1804.05347" rel="nofollow"&gt;AF-DCGAN: Amplitude Feature Deep Convolutional GAN for Fingerprint Construction in Indoor Localization System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AffGAN - &lt;a href="https://arxiv.org/abs/1610.04490" rel="nofollow"&gt;Amortised MAP Inference for Image Super-resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AIM - &lt;a href="https://arxiv.org/abs/1809.05972" rel="nofollow"&gt;Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AL-CGAN - &lt;a href="https://arxiv.org/abs/1612.00215" rel="nofollow"&gt;Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ALI - &lt;a href="https://arxiv.org/abs/1606.00704" rel="nofollow"&gt;Adversarially Learned Inference&lt;/a&gt; (&lt;a href="https://github.com/IshmaelBelghazi/ALI"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AlignGAN - &lt;a href="https://arxiv.org/abs/1707.01400" rel="nofollow"&gt;AlignGAN: Learning to Align Cross-Domain Images with Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AlphaGAN - &lt;a href="https://arxiv.org/abs/1807.10088" rel="nofollow"&gt;AlphaGAN: Generative adversarial networks for natural image matting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AM-GAN - &lt;a href="https://arxiv.org/abs/1703.02000" rel="nofollow"&gt;Activation Maximization Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AmbientGAN - &lt;a href="https://openreview.net/forum?id=Hy7fDog0b" rel="nofollow"&gt;AmbientGAN: Generative models from lossy measurements&lt;/a&gt; (&lt;a href="https://github.com/AshishBora/ambient-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AMC-GAN - &lt;a href="https://arxiv.org/abs/1807.02635" rel="nofollow"&gt;Video Prediction with Appearance and Motion Conditions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AnoGAN - &lt;a href="https://arxiv.org/abs/1703.05921v1" rel="nofollow"&gt;Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;APD - &lt;a href="https://arxiv.org/abs/1806.10317" rel="nofollow"&gt;Adversarial Distillation of Bayesian Neural Network Posteriors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;APE-GAN - &lt;a href="https://arxiv.org/abs/1707.05474" rel="nofollow"&gt;APE-GAN: Adversarial Perturbation Elimination with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ARAE - &lt;a href="https://arxiv.org/abs/1706.04223" rel="nofollow"&gt;Adversarially Regularized Autoencoders for Generating Discrete Structures&lt;/a&gt; (&lt;a href="https://github.com/jakezhaojb/ARAE"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ARDA - &lt;a href="https://arxiv.org/abs/1707.01217" rel="nofollow"&gt;Adversarial Representation Learning for Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ARIGAN - &lt;a href="https://arxiv.org/abs/1709.00938" rel="nofollow"&gt;ARIGAN: Synthetic Arabidopsis Plants using Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ArtGAN - &lt;a href="https://arxiv.org/abs/1702.03410" rel="nofollow"&gt;ArtGAN: Artwork Synthesis with Conditional Categorial GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ASDL-GAN - &lt;a href="https://ieeexplore.ieee.org/document/8017430/" rel="nofollow"&gt;Automatic Steganographic Distortion Learning Using a Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ATA-GAN - &lt;a href="https://arxiv.org/abs/1802.09070" rel="nofollow"&gt;Attention-Aware Generative Adversarial Networks (ATA-GANs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Attention-GAN - &lt;a href="https://arxiv.org/abs/1803.06798" rel="nofollow"&gt;Attention-GAN for Object Transfiguration in Wild Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AttGAN - &lt;a href="https://arxiv.org/abs/1711.10678" rel="nofollow"&gt;Arbitrary Facial Attribute Editing: Only Change What You Want&lt;/a&gt; (&lt;a href="https://github.com/LynnHo/AttGAN-Tensorflow"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AttnGAN - &lt;a href="https://arxiv.org/abs/1711.10485" rel="nofollow"&gt;AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/taoxugit/AttnGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AVID - &lt;a href="https://arxiv.org/abs/1805.09521" rel="nofollow"&gt;AVID: Adversarial Visual Irregularity Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;B-DCGAN - &lt;a href="https://arxiv.org/abs/1803.10930" rel="nofollow"&gt;B-DCGAN:Evaluation of Binarized DCGAN for FPGA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;b-GAN - &lt;a href="https://arxiv.org/abs/1610.02920" rel="nofollow"&gt;Generative Adversarial Nets from a Density Ratio Estimation Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BAGAN - &lt;a href="https://arxiv.org/abs/1803.09655" rel="nofollow"&gt;BAGAN: Data Augmentation with Balancing GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bayesian GAN - &lt;a href="https://arxiv.org/abs/1702.08896" rel="nofollow"&gt;Deep and Hierarchical Implicit Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bayesian GAN - &lt;a href="https://arxiv.org/abs/1705.09558" rel="nofollow"&gt;Bayesian GAN&lt;/a&gt; (&lt;a href="https://github.com/andrewgordonwilson/bayesgan/"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;BCGAN - &lt;a href="https://arxiv.org/abs/1706.05477" rel="nofollow"&gt;Bayesian Conditional Generative Adverserial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BCGAN - &lt;a href="https://arxiv.org/abs/1711.07461" rel="nofollow"&gt;Bidirectional Conditional Generative Adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEAM - &lt;a href="https://arxiv.org/abs/1804.08682" rel="nofollow"&gt;Boltzmann Encoded Adversarial Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEGAN - &lt;a href="https://arxiv.org/abs/1703.10717" rel="nofollow"&gt;BEGAN: Boundary Equilibrium Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BEGAN-CS - &lt;a href="https://arxiv.org/abs/1808.07258" rel="nofollow"&gt;Escaping from Collapsing Modes in a Constrained Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bellman GAN - &lt;a href="https://arxiv.org/abs/1808.01960" rel="nofollow"&gt;Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BGAN - &lt;a href="https://arxiv.org/abs/1708.04150" rel="nofollow"&gt;Binary Generative Adversarial Networks for Image Retrieval&lt;/a&gt; (&lt;a href="https://github.com/htconquer/BGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Bi-GAN - &lt;a href="https://arxiv.org/abs/1809.10244" rel="nofollow"&gt;Autonomously and Simultaneously Refining Deep Neural Network Parameters by a Bi-Generative Adversarial Network Aided Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BicycleGAN - &lt;a href="https://arxiv.org/abs/1711.11586" rel="nofollow"&gt;Toward Multimodal Image-to-Image Translation&lt;/a&gt; (&lt;a href="https://github.com/junyanz/BicycleGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;BiGAN - &lt;a href="https://arxiv.org/abs/1605.09782v7" rel="nofollow"&gt;Adversarial Feature Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BinGAN - &lt;a href="https://arxiv.org/abs/1806.06778" rel="nofollow"&gt;BinGAN: Learning Compact Binary Descriptors with a Regularized GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BourGAN - &lt;a href="https://arxiv.org/abs/1805.07674" rel="nofollow"&gt;BourGAN: Generative Networks with Metric Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BranchGAN - &lt;a href="https://arxiv.org/abs/1803.08467" rel="nofollow"&gt;Branched Generative Adversarial Networks for Multi-Scale Image Manifold Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BRE - &lt;a href="https://arxiv.org/abs/1805.03644" rel="nofollow"&gt;Improving GAN Training via Binarized Representation Entropy (BRE) Regularization&lt;/a&gt; (&lt;a href="https://github.com/BorealisAI/bre-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;BridgeGAN - &lt;a href="https://arxiv.org/abs/1808.00327" rel="nofollow"&gt;Generative Adversarial Frontal View to Bird View Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BS-GAN - &lt;a href="https://arxiv.org/abs/1702.08431v1" rel="nofollow"&gt;Boundary-Seeking Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BubGAN - &lt;a href="https://arxiv.org/abs/1809.02266" rel="nofollow"&gt;BubGAN: Bubble Generative Adversarial Networks for Synthesizing Realistic Bubbly Flow Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;BWGAN - &lt;a href="https://arxiv.org/abs/1806.06621" rel="nofollow"&gt;Banach Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;C-GAN  - &lt;a href="https://arxiv.org/abs/1802.00237" rel="nofollow"&gt;Face Aging with Contextual Generative Adversarial Nets &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;C-RNN-GAN - &lt;a href="https://arxiv.org/abs/1611.09904" rel="nofollow"&gt;C-RNN-GAN: Continuous recurrent neural networks with adversarial training&lt;/a&gt; (&lt;a href="https://github.com/olofmogren/c-rnn-gan/"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;CA-GAN - &lt;a href="https://arxiv.org/abs/1712.00899" rel="nofollow"&gt;Composition-aided Sketch-realistic Portrait Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CaloGAN - &lt;a href="https://arxiv.org/abs/1705.02355" rel="nofollow"&gt;CaloGAN: Simulating 3D High Energy Particle Showers in Multi-Layer Electromagnetic Calorimeters with Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/hep-lbdl/CaloGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;CAN - &lt;a href="https://arxiv.org/abs/1706.07068" rel="nofollow"&gt;CAN: Creative Adversarial Networks, Generating Art by Learning About Styles and Deviating from Style Norms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CapsGAN - &lt;a href="https://arxiv.org/abs/1806.03968" rel="nofollow"&gt;CapsGAN: Using Dynamic Routing for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CapsuleGAN - &lt;a href="http://arxiv.org/abs/1802.06167" rel="nofollow"&gt;CapsuleGAN: Generative Adversarial Capsule Network &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CatGAN - &lt;a href="https://arxiv.org/abs/1511.06390v2" rel="nofollow"&gt;Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CatGAN - &lt;a href="https://arxiv.org/abs/1711.08904" rel="nofollow"&gt;CatGAN: Coupled Adversarial Transfer for Domain Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CausalGAN - &lt;a href="https://arxiv.org/abs/1709.02023" rel="nofollow"&gt;CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CC-GAN - &lt;a href="https://arxiv.org/abs/1611.06430" rel="nofollow"&gt;Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/edenton/cc-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;cd-GAN - &lt;a href="https://arxiv.org/abs/1805.00251" rel="nofollow"&gt;Conditional Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CDcGAN - &lt;a href="https://arxiv.org/abs/1708.09105" rel="nofollow"&gt;Simultaneously Color-Depth Super-Resolution with Conditional Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CE-GAN - &lt;a href="https://arxiv.org/abs/1807.04585" rel="nofollow"&gt;Deep Learning for Imbalance Data Classification using Class Expert Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CFG-GAN - &lt;a href="https://arxiv.org/abs/1801.06309" rel="nofollow"&gt;Composite Functional Gradient Learning of Generative Adversarial Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CGAN - &lt;a href="https://arxiv.org/abs/1411.1784" rel="nofollow"&gt;Conditional Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CGAN - &lt;a href="https://arxiv.org/abs/1708.00598" rel="nofollow"&gt;Controllable Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chekhov GAN - &lt;a href="https://arxiv.org/abs/1706.03269" rel="nofollow"&gt;An Online Learning Approach to Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ciGAN - &lt;a href="https://arxiv.org/abs/1807.08093" rel="nofollow"&gt;Conditional Infilling GANs for Data Augmentation in Mammogram Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CinCGAN - &lt;a href="https://arxiv.org/abs/1809.00437" rel="nofollow"&gt;Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CipherGAN - &lt;a href="https://arxiv.org/abs/1801.04883" rel="nofollow"&gt;Unsupervised Cipher Cracking Using Discrete GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ClusterGAN - &lt;a href="https://arxiv.org/abs/1809.03627" rel="nofollow"&gt;ClusterGAN : Latent Space Clustering in Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CM-GAN - &lt;a href="https://arxiv.org/abs/1710.05106" rel="nofollow"&gt;CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoAtt-GAN - &lt;a href="https://arxiv.org/abs/1711.07613" rel="nofollow"&gt;Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoGAN - &lt;a href="https://arxiv.org/abs/1606.07536v2" rel="nofollow"&gt;Coupled Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ComboGAN - &lt;a href="https://arxiv.org/abs/1712.06909" rel="nofollow"&gt;ComboGAN: Unrestrained Scalability for Image Domain Translation&lt;/a&gt; (&lt;a href="https://github.com/AAnoosheh/ComboGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ConceptGAN - &lt;a href="https://arxiv.org/abs/1711.06148" rel="nofollow"&gt;Learning Compositional Visual Concepts with Mutual Consistency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional cycleGAN - &lt;a href="https://arxiv.org/abs/1705.09966" rel="nofollow"&gt;Conditional CycleGAN for Attribute Guided Face Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;constrast-GAN - &lt;a href="https://arxiv.org/abs/1708.00315" rel="nofollow"&gt;Generative Semantic Manipulation with Contrasting GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Context-RNN-GAN - &lt;a href="https://arxiv.org/abs/1609.09444" rel="nofollow"&gt;Contextual RNN-GANs for Abstract Reasoning Diagram Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CorrGAN - &lt;a href="https://arxiv.org/abs/1804.00925" rel="nofollow"&gt;Correlated discrete data generation using adversarial training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Coulomb GAN - &lt;a href="https://arxiv.org/abs/1708.08819" rel="nofollow"&gt;Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cover-GAN - &lt;a href="https://arxiv.org/abs/1711.04916" rel="nofollow"&gt;Generative Steganography with Kerckhoffs' Principle based on Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;cowboy - &lt;a href="https://arxiv.org/abs/1805.10652" rel="nofollow"&gt;Defending Against Adversarial Attacks by Leveraging an Entire GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CR-GAN - &lt;a href="https://arxiv.org/abs/1806.11191" rel="nofollow"&gt;CR-GAN: Learning Complete Representations for Multi-view Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cramèr GAN  - &lt;a href="https://arxiv.org/abs/1705.10743" rel="nofollow"&gt;The Cramer Distance as a Solution to Biased Wasserstein Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cross-GAN - &lt;a href="https://arxiv.org/abs/1801.01760" rel="nofollow"&gt;Crossing Generative Adversarial Networks for Cross-View Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;crVAE-GAN - &lt;a href="https://arxiv.org/abs/1706.03729" rel="nofollow"&gt;Channel-Recurrent Variational Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CS-GAN - &lt;a href="https://arxiv.org/abs/1703.04887" rel="nofollow"&gt;Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CSG - &lt;a href="https://arxiv.org/abs/1806.00154" rel="nofollow"&gt;Speech-Driven Expressive Talking Lips with Conditional Sequential Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CT-GAN - &lt;a href="https://arxiv.org/abs/1807.04812" rel="nofollow"&gt;CT-GAN: Conditional Transformation Generative Adversarial Network for Image Attribute Modification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CVAE-GAN - &lt;a href="https://arxiv.org/abs/1703.10155" rel="nofollow"&gt;CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CycleGAN - &lt;a href="https://arxiv.org/abs/1703.10593" rel="nofollow"&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/junyanz/CycleGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;D-GAN - &lt;a href="https://arxiv.org/abs/1711.10267" rel="nofollow"&gt;Differential Generative Adversarial Networks: Synthesizing Non-linear Facial Variations with Limited Number of Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D-WCGAN - &lt;a href="https://arxiv.org/abs/1804.00290" rel="nofollow"&gt;I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D2GAN - &lt;a href="http://arxiv.org/abs/1709.03831" rel="nofollow"&gt;Dual Discriminator Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D2IA-GAN - &lt;a href="https://arxiv.org/abs/1804.00113" rel="nofollow"&gt;Tagging like Humans: Diverse and Distinct Image Annotation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DA-GAN  - &lt;a href="http://arxiv.org/abs/1802.06454" rel="nofollow"&gt;DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks (with Supplementary Materials)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DADA - &lt;a href="https://arxiv.org/abs/1809.00981" rel="nofollow"&gt;DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DAGAN - &lt;a href="https://arxiv.org/abs/1711.04340" rel="nofollow"&gt;Data Augmentation Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DAN - &lt;a href="https://arxiv.org/abs/1706.09549" rel="nofollow"&gt;Distributional Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DBLRGAN - &lt;a href="https://arxiv.org/abs/1804.00533" rel="nofollow"&gt;Adversarial Spatio-Temporal Learning for Video Deblurring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DCGAN - &lt;a href="https://arxiv.org/abs/1511.06434" rel="nofollow"&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/Newmu/dcgan_code"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;DE-GAN - &lt;a href="https://arxiv.org/abs/1807.03923" rel="nofollow"&gt;Generative Adversarial Networks with Decoder-Encoder Output Noise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DeblurGAN - &lt;a href="https://arxiv.org/abs/1711.07064" rel="nofollow"&gt;DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/KupynOrest/DeblurGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;DeepFD - &lt;a href="https://arxiv.org/abs/1809.08754" rel="nofollow"&gt;Learning to Detect Fake Face Images in the Wild&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Defense-GAN - &lt;a href="https://arxiv.org/abs/1805.06605" rel="nofollow"&gt;Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models &lt;/a&gt; (&lt;a href="https://github.com/kabkabm/defensegan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Defo-Net - &lt;a href="https://arxiv.org/abs/1804.05928" rel="nofollow"&gt;Defo-Net: Learning Body Deformation using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DeliGAN - &lt;a href="https://arxiv.org/abs/1706.02071" rel="nofollow"&gt;DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data&lt;/a&gt; (&lt;a href="https://github.com/val-iisc/deligan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;DF-GAN - &lt;a href="https://arxiv.org/abs/1712.04646" rel="nofollow"&gt;Learning Disentangling and Fusing Networks for Face Completion Under Structured Occlusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DialogWAE - &lt;a href="https://arxiv.org/abs/1805.12352" rel="nofollow"&gt;DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DiscoGAN - &lt;a href="https://arxiv.org/abs/1703.05192v1" rel="nofollow"&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DistanceGAN - &lt;a href="https://arxiv.org/abs/1706.00826" rel="nofollow"&gt;One-Sided Unsupervised Domain Mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DM-GAN - &lt;a href="https://arxiv.org/abs/1708.00284" rel="nofollow"&gt;Dual Motion GAN for Future-Flow Embedded Video Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DMGAN - &lt;a href="https://arxiv.org/abs/1806.00880" rel="nofollow"&gt;Disconnected Manifold Learning for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DNA-GAN - &lt;a href="https://arxiv.org/abs/1711.05415" rel="nofollow"&gt;DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DOPING - &lt;a href="https://arxiv.org/abs/1808.07632" rel="nofollow"&gt;DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;dp-GAN - &lt;a href="https://arxiv.org/abs/1801.01594" rel="nofollow"&gt;Differentially Private Releasing via Deep Generative Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DP-GAN - &lt;a href="https://arxiv.org/abs/1802.01345" rel="nofollow"&gt;DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DPGAN  - &lt;a href="http://arxiv.org/abs/1802.06739" rel="nofollow"&gt;Differentially Private Generative Adversarial Network &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DR-GAN - &lt;a href="https://arxiv.org/abs/1705.11136" rel="nofollow"&gt;Representation Learning by Rotating Your Faces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DRAGAN - &lt;a href="https://arxiv.org/abs/1705.07215" rel="nofollow"&gt;How to Train Your DRAGAN&lt;/a&gt; (&lt;a href="https://github.com/kodalinaveen3/DRAGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Dropout-GAN - &lt;a href="https://arxiv.org/abs/1807.11346" rel="nofollow"&gt;Dropout-GAN: Learning from a Dynamic Ensemble of Discriminators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DRPAN - &lt;a href="https://arxiv.org/abs/1711.09554" rel="nofollow"&gt;Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DSH-GAN - &lt;a href="https://arxiv.org/abs/1804.08275" rel="nofollow"&gt;Deep Semantic Hashing with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DSP-GAN - &lt;a href="https://arxiv.org/abs/1706.00212" rel="nofollow"&gt;Depth Structure Preserving Scene Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DTLC-GAN - &lt;a href="https://arxiv.org/abs/1805.10603" rel="nofollow"&gt;Generative Adversarial Image Synthesis with Decision Tree Latent Controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DTN - &lt;a href="https://arxiv.org/abs/1611.02200" rel="nofollow"&gt;Unsupervised Cross-Domain Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DTR-GAN - &lt;a href="https://arxiv.org/abs/1804.11228" rel="nofollow"&gt;DTR-GAN: Dilated Temporal Relational Adversarial Network for Video Summarization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DualGAN - &lt;a href="https://arxiv.org/abs/1704.02510v1" rel="nofollow"&gt;DualGAN: Unsupervised Dual Learning for Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dualing GAN - &lt;a href="https://arxiv.org/abs/1706.06216" rel="nofollow"&gt;Dualing GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DVGAN - &lt;a href="https://arxiv.org/abs/1804.10652" rel="nofollow"&gt;Human Motion Modeling using DVGANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dynamics Transfer GAN - &lt;a href="https://arxiv.org/abs/1712.03534" rel="nofollow"&gt;Dynamics Transfer GAN: Generating Video by Transferring Arbitrary Temporal Dynamics from a Source Video to a Single Target Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;E-GAN - &lt;a href="https://arxiv.org/abs/1803.00657" rel="nofollow"&gt;Evolutionary Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EAR - &lt;a href="https://arxiv.org/abs/1804.09858" rel="nofollow"&gt;Generative Model for Heterogeneous Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EBGAN - &lt;a href="https://arxiv.org/abs/1609.03126v4" rel="nofollow"&gt;Energy-based Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ecGAN - &lt;a href="https://arxiv.org/abs/1801.03244" rel="nofollow"&gt;eCommerceGAN : A Generative Adversarial Network for E-commerce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ED//GAN - &lt;a href="https://arxiv.org/abs/1705.09367" rel="nofollow"&gt;Stabilizing Training of Generative Adversarial Networks through Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Editable GAN - &lt;a href="https://arxiv.org/abs/1807.07700" rel="nofollow"&gt;Editable Generative Adversarial Networks: Generating and Editing Faces Simultaneously&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EGAN - &lt;a href="https://arxiv.org/abs/1705.08245" rel="nofollow"&gt;Enhanced Experience Replay Generation for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EL-GAN - &lt;a href="https://arxiv.org/abs/1806.05525" rel="nofollow"&gt;EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ELEGANT - &lt;a href="https://arxiv.org/abs/1803.10562" rel="nofollow"&gt;ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EnergyWGAN - &lt;a href="https://arxiv.org/abs/1712.01026" rel="nofollow"&gt;Energy-relaxed Wassertein GANs (EnergyWGAN): Towards More Stable and High Resolution Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ESRGAN - &lt;a href="https://arxiv.org/abs/1809.00219" rel="nofollow"&gt;ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ExGAN - &lt;a href="https://arxiv.org/abs/1712.03999" rel="nofollow"&gt;Eye In-Painting with Exemplar Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ExposureGAN - &lt;a href="https://arxiv.org/abs/1709.09602" rel="nofollow"&gt;Exposure: A White-Box Photo Post-Processing Framework&lt;/a&gt; (&lt;a href="https://github.com/yuanming-hu/exposure"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ExprGAN - &lt;a href="https://arxiv.org/abs/1709.03842" rel="nofollow"&gt;ExprGAN: Facial Expression Editing with Controllable Expression Intensity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;f-CLSWGAN - &lt;a href="https://arxiv.org/abs/1712.00981" rel="nofollow"&gt;Feature Generating Networks for Zero-Shot Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;f-GAN - &lt;a href="https://arxiv.org/abs/1606.00709" rel="nofollow"&gt;f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FairGAN - &lt;a href="https://arxiv.org/abs/1805.11202" rel="nofollow"&gt;FairGAN: Fairness-aware Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fairness GAN - &lt;a href="https://arxiv.org/abs/1805.09910" rel="nofollow"&gt;Fairness GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FakeGAN - &lt;a href="https://arxiv.org/abs/1805.10364" rel="nofollow"&gt;Detecting Deceptive Reviews using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FBGAN - &lt;a href="https://arxiv.org/abs/1804.01694" rel="nofollow"&gt;Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for Optimizing Protein Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FBGAN - &lt;a href="https://arxiv.org/abs/1805.07862" rel="nofollow"&gt;Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FC-GAN - &lt;a href="https://arxiv.org/abs/1805.01972" rel="nofollow"&gt;Fast-converging Conditional Generative Adversarial Networks for Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FF-GAN - &lt;a href="https://arxiv.org/abs/1704.06244" rel="nofollow"&gt;Towards Large-Pose Face Frontalization in the Wild&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FGGAN - &lt;a href="https://arxiv.org/abs/1807.02247" rel="nofollow"&gt;Adversarial Learning for Fine-grained Image Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fictitious GAN - &lt;a href="https://arxiv.org/abs/1803.08647" rel="nofollow"&gt;Fictitious GAN: Training GANs with Historical Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FIGAN - &lt;a href="https://arxiv.org/abs/1711.06045" rel="nofollow"&gt;Frame Interpolation with Multi-Scale Deep Loss Functions and Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fila-GAN - &lt;a href="https://arxiv.org/abs/1706.02185" rel="nofollow"&gt;Synthesizing Filamentary Structured Images with GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;First Order GAN  - &lt;a href="https://arxiv.org/abs/1802.04591" rel="nofollow"&gt;First Order Generative Adversarial Networks &lt;/a&gt; (&lt;a href="https://github.com/zalandoresearch/first_order_gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Fisher GAN - &lt;a href="https://arxiv.org/abs/1705.09675" rel="nofollow"&gt;Fisher GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Flow-GAN - &lt;a href="https://arxiv.org/abs/1705.08868" rel="nofollow"&gt;Flow-GAN: Bridging implicit and prescribed learning in generative models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FrankenGAN - &lt;a href="https://arxiv.org/abs/1806.07179" rel="nofollow"&gt;rankenGAN: Guided Detail Synthesis for Building Mass-Models Using Style-Synchonized GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FSEGAN - &lt;a href="https://arxiv.org/abs/1711.05747" rel="nofollow"&gt;Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FTGAN - &lt;a href="https://arxiv.org/abs/1711.09618" rel="nofollow"&gt;Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FusedGAN - &lt;a href="https://arxiv.org/abs/1801.05551" rel="nofollow"&gt;Semi-supervised FusedGAN for Conditional Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FusionGAN - &lt;a href="https://arxiv.org/abs/1712.01456" rel="nofollow"&gt;Learning to Fuse Music Genres with Generative Adversarial Dual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FusionGAN - &lt;a href="https://arxiv.org/abs/1804.07455" rel="nofollow"&gt;Generating a Fusion Image: One's Identity and Another's Shape&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;G2-GAN - &lt;a href="https://arxiv.org/abs/1712.03474" rel="nofollow"&gt;Geometry Guided Adversarial Facial Expression Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAAN - &lt;a href="https://arxiv.org/abs/1803.08887" rel="nofollow"&gt;Generative Adversarial Autoencoder Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAF - &lt;a href="https://arxiv.org/abs/1805.05185" rel="nofollow"&gt;Generative Adversarial Forests for Better Conditioned Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAGAN - &lt;a href="https://arxiv.org/abs/1712.00684" rel="nofollow"&gt;GAGAN: Geometry-Aware Generative Adverserial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAIA - &lt;a href="https://arxiv.org/abs/1807.06650" rel="nofollow"&gt;Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAIN  - &lt;a href="https://arxiv.org/abs/1806.02920" rel="nofollow"&gt;GAIN: Missing Data Imputation using Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAMN - &lt;a href="https://arxiv.org/abs/1709.09820" rel="nofollow"&gt;Generative Adversarial Mapping Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN - &lt;a href="https://arxiv.org/abs/1406.2661" rel="nofollow"&gt;Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/goodfeli/adversarial"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GAN Lab - &lt;a href="https://arxiv.org/abs/1809.01587" rel="nofollow"&gt;GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN Q-learning - &lt;a href="https://arxiv.org/abs/1805.04874" rel="nofollow"&gt;GAN Q-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-AD - &lt;a href="https://arxiv.org/abs/1809.04758" rel="nofollow"&gt;Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-ATV - &lt;a href="https://arxiv.org/abs/1710.10553" rel="nofollow"&gt;A Novel Approach to Artistic Textual Visualization via GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-CLS - &lt;a href="https://arxiv.org/abs/1605.05396" rel="nofollow"&gt;Generative Adversarial Text to Image Synthesis&lt;/a&gt; (&lt;a href="https://github.com/reedscot/icml2016"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GAN-RS - &lt;a href="https://arxiv.org/abs/1712.00736" rel="nofollow"&gt;Towards Qualitative Advancement of Underwater Machine Vision with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-SD - &lt;a href="https://arxiv.org/abs/1805.10000" rel="nofollow"&gt;Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-sep - &lt;a href="https://arxiv.org/abs/1708.04692" rel="nofollow"&gt;GANs for Biological Image Synthesis&lt;/a&gt; (&lt;a href="https://github.com/aosokin/biogans"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GAN-VFS - &lt;a href="https://arxiv.org/abs/1708.02681" rel="nofollow"&gt;Generative Adversarial Network-based Synthesis of Visible Faces from Polarimetric Thermal Faces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAN-Word2Vec - &lt;a href="https://arxiv.org/abs/1805.08720" rel="nofollow"&gt;Adversarial Training of Word2Vec for Basket Completion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANAX - &lt;a href="https://arxiv.org/abs/1806.01107" rel="nofollow"&gt;GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANCS - &lt;a href="https://arxiv.org/abs/1706.00051" rel="nofollow"&gt;Deep Generative Adversarial Networks for Compressed Sensing Automates MRI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANDI - &lt;a href="https://arxiv.org/abs/1711.01391" rel="nofollow"&gt;Guiding the search in continuous state-action spaces by learning an action sampling distribution from off-target samples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANG - &lt;a href="https://arxiv.org/abs/1712.00679" rel="nofollow"&gt;GANGs: Generative Adversarial Network Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANG - &lt;a href="https://arxiv.org/abs/1806.07268" rel="nofollow"&gt;Beyond Local Nash Equilibria for Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANosaic - &lt;a href="https://arxiv.org/abs/1712.00269" rel="nofollow"&gt;GANosaic: Mosaic Creation with Generative Texture Manifolds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GANVO - &lt;a href="https://arxiv.org/abs/1809.05786" rel="nofollow"&gt;GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAP - &lt;a href="https://arxiv.org/abs/1710.09549" rel="nofollow"&gt;Context-Aware Generative Adversarial Privacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAP - &lt;a href="https://arxiv.org/abs/1807.05306" rel="nofollow"&gt;Generative Adversarial Privacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GATS - &lt;a href="https://arxiv.org/abs/1806.05780" rel="nofollow"&gt;Sample-Efficient Deep RL with Generative Adversarial Tree Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GAWWN - &lt;a href="https://arxiv.org/abs/1610.02454" rel="nofollow"&gt;Learning What and Where to Draw&lt;/a&gt; (&lt;a href="https://github.com/reedscot/nips2016"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GC-GAN - &lt;a href="https://arxiv.org/abs/1802.01822" rel="nofollow"&gt;Geometry-Contrastive Generative Adversarial Network for Facial Expression Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GcGAN - &lt;a href="https://arxiv.org/abs/1809.05852" rel="nofollow"&gt;Geometry-Consistent Adversarial Networks for One-Sided Unsupervised Domain Mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GeneGAN - &lt;a href="https://arxiv.org/abs/1705.04932" rel="nofollow"&gt;GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data&lt;/a&gt; (&lt;a href="https://github.com/Prinsphield/GeneGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GeoGAN - &lt;a href="https://arxiv.org/abs/1801.08839" rel="nofollow"&gt;Generating Instance Segmentation Annotation by Geometry-guided GAN &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Geometric GAN - &lt;a href="https://arxiv.org/abs/1705.02894" rel="nofollow"&gt;Geometric GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GIN - &lt;a href="https://arxiv.org/abs/1808.04495" rel="nofollow"&gt;Generative Invertible Networks (GIN): Pathophysiology-Interpretable Feature Mapping and Virtual Patient Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GLCA-GAN - &lt;a href="https://arxiv.org/abs/1801.08390" rel="nofollow"&gt;Global and Local Consistent Age Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GM-GAN - &lt;a href="https://arxiv.org/abs/1808.10356" rel="nofollow"&gt;Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GMAN - &lt;a href="http://arxiv.org/abs/1611.01673" rel="nofollow"&gt;Generative Multi-Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GMM-GAN - &lt;a href="https://arxiv.org/abs/1706.09884" rel="nofollow"&gt;Towards Understanding the Dynamics of Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GoGAN - &lt;a href="https://arxiv.org/abs/1704.04865" rel="nofollow"&gt;Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GONet - &lt;a href="https://arxiv.org/abs/1803.03254" rel="nofollow"&gt;GONet: A Semi-Supervised Deep Learning Approach For Traversability Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GP-GAN - &lt;a href="https://arxiv.org/abs/1703.07195" rel="nofollow"&gt;GP-GAN: Towards Realistic High-Resolution Image Blending&lt;/a&gt; (&lt;a href="https://github.com/wuhuikai/GP-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GP-GAN - &lt;a href="https://arxiv.org/abs/1710.00962" rel="nofollow"&gt;GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GPU - &lt;a href="https://arxiv.org/abs/1711.08054" rel="nofollow"&gt;A generative adversarial framework for positive-unlabeled classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GRAN - &lt;a href="https://arxiv.org/abs/1602.05110" rel="nofollow"&gt;Generating images with recurrent adversarial networks&lt;/a&gt; (&lt;a href="https://github.com/jiwoongim/GRAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Graphical-GAN - &lt;a href="https://arxiv.org/abs/1804.03429" rel="nofollow"&gt;Graphical Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GraphSGAN - &lt;a href="https://arxiv.org/abs/1809.00130" rel="nofollow"&gt;Semi-supervised Learning on Graphs with Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GraspGAN - &lt;a href="https://arxiv.org/abs/1709.07857" rel="nofollow"&gt;Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GT-GAN - &lt;a href="https://arxiv.org/abs/1805.09980" rel="nofollow"&gt;Deep Graph Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HAN - &lt;a href="https://arxiv.org/abs/1711.06448" rel="nofollow"&gt;Chinese Typeface Transformation with Hierarchical Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HAN - &lt;a href="https://arxiv.org/abs/1805.08006" rel="nofollow"&gt;Bidirectional Learning for Robust Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HiGAN - &lt;a href="https://arxiv.org/abs/1805.04384" rel="nofollow"&gt;Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HP-GAN - &lt;a href="https://arxiv.org/abs/1711.09561" rel="nofollow"&gt;HP-GAN: Probabilistic 3D human motion prediction via GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HR-DCGAN - &lt;a href="https://arxiv.org/abs/1711.06491" rel="nofollow"&gt;High-Resolution Deep Convolutional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hredGAN - &lt;a href="https://arxiv.org/abs/1805.11752" rel="nofollow"&gt;Multi-turn Dialogue Response Generation in an Adversarial Learning framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IAN - &lt;a href="https://arxiv.org/abs/1609.07093" rel="nofollow"&gt;Neural Photo Editing with Introspective Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/ajbrock/Neural-Photo-Editor"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IcGAN - &lt;a href="https://arxiv.org/abs/1611.06355" rel="nofollow"&gt;Invertible Conditional GANs for image editing&lt;/a&gt; (&lt;a href="https://github.com/Guim3/IcGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;ID-CGAN - &lt;a href="https://arxiv.org/abs/1701.05957v3" rel="nofollow"&gt;Image De-raining Using a Conditional Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IdCycleGAN - &lt;a href="https://arxiv.org/abs/1712.00971" rel="nofollow"&gt;Face Translation between Images and Videos using Identity-aware CycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IFcVAEGAN - &lt;a href="https://arxiv.org/abs/1711.05175" rel="nofollow"&gt;Conditional Autoencoders with Adversarial Information Factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;iGAN - &lt;a href="https://arxiv.org/abs/1609.03552v2" rel="nofollow"&gt;Generative Visual Manipulation on the Natural Image Manifold&lt;/a&gt; (&lt;a href="https://github.com/junyanz/iGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IGMM-GAN - &lt;a href="https://arxiv.org/abs/1809.02728" rel="nofollow"&gt;Coupled IGMM-GANs for deep multimodal anomaly detection in human mobility data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved GAN - &lt;a href="https://arxiv.org/abs/1606.03498" rel="nofollow"&gt;Improved Techniques for Training GANs&lt;/a&gt; (&lt;a href="https://github.com/openai/improved-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;In2I - &lt;a href="https://arxiv.org/abs/1711.09334" rel="nofollow"&gt;In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;InfoGAN - &lt;a href="https://arxiv.org/abs/1606.03657v1" rel="nofollow"&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/a&gt; (&lt;a href="https://github.com/openai/InfoGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IntroVAE - &lt;a href="https://arxiv.org/abs/1807.06358" rel="nofollow"&gt;IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IR2VI - &lt;a href="https://arxiv.org/abs/1806.09565" rel="nofollow"&gt;IR2VI: Enhanced Night Environmental Perception by Unsupervised Thermal Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRGAN - &lt;a href="https://arxiv.org/abs/1705.10513v1" rel="nofollow"&gt;IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IRGAN - &lt;a href="https://arxiv.org/abs/1806.03577" rel="nofollow"&gt;Generative Adversarial Nets for Information Retrieval: Fundamentals and Advances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ISGAN - &lt;a href="https://arxiv.org/abs/1807.08571" rel="nofollow"&gt;Invisible Steganography via Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ISP-GPM - &lt;a href="https://arxiv.org/abs/1808.02104" rel="nofollow"&gt;Inner Space Preserving Generative Pose Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Iterative-GAN - &lt;a href="https://arxiv.org/abs/1711.06078" rel="nofollow"&gt;Two Birds with One Stone: Iteratively Learn Facial Attributes with GANs&lt;/a&gt; (&lt;a href="https://github.com/punkcure/Iterative-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IterGAN - &lt;a href="https://arxiv.org/abs/1804.05651" rel="nofollow"&gt;IterGANs: Iterative GANs to Learn and Control 3D Object Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;IVE-GAN - &lt;a href="https://arxiv.org/abs/1711.08646" rel="nofollow"&gt;IVE-GAN: Invariant Encoding Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;iVGAN - &lt;a href="https://arxiv.org/abs/1711.11453" rel="nofollow"&gt;Towards an Understanding of Our World by GANing Videos in the Wild&lt;/a&gt; (&lt;a href="https://github.com/bernhard2202/improved-video-gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IWGAN - &lt;a href="https://arxiv.org/abs/1706.00550" rel="nofollow"&gt;On Unifying Deep Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;JointGAN - &lt;a href="https://arxiv.org/abs/1806.02978" rel="nofollow"&gt;JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;JR-GAN - &lt;a href="https://arxiv.org/abs/1806.09235" rel="nofollow"&gt;JR-GAN: Jacobian Regularization for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KBGAN - &lt;a href="https://arxiv.org/abs/1711.04071" rel="nofollow"&gt;KBGAN: Adversarial Learning for Knowledge Graph Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KGAN - &lt;a href="https://arxiv.org/abs/1711.01744" rel="nofollow"&gt;KGAN: How to Break The Minimax Game in GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;l-GAN - &lt;a href="https://arxiv.org/abs/1707.02392" rel="nofollow"&gt;Representation Learning and Adversarial Generation of 3D Point Clouds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LAC-GAN - &lt;a href="https://arxiv.org/abs/1801.05096" rel="nofollow"&gt;Grounded Language Understanding for Manipulation Instructions Using GAN-Based Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LAGAN - &lt;a href="https://arxiv.org/abs/1701.05927" rel="nofollow"&gt;Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LAPGAN - &lt;a href="https://arxiv.org/abs/1506.05751" rel="nofollow"&gt;Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/facebook/eyescream"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;LB-GAN - &lt;a href="http://arxiv.org/abs/1802.07447" rel="nofollow"&gt;Load Balanced GANs for Multi-view Face Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LBT - &lt;a href="https://arxiv.org/abs/1807.03870" rel="nofollow"&gt;Learning Implicit Generative Models by Teaching Explicit Ones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LCC-GAN - &lt;a href="https://arxiv.org/abs/1806.04895" rel="nofollow"&gt;Adversarial Learning with Local Coordinate Coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LD-GAN - &lt;a href="https://arxiv.org/abs/1707.07831" rel="nofollow"&gt;Linear Discriminant Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LDAN - &lt;a href="https://arxiv.org/abs/1709.01993" rel="nofollow"&gt;Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LeakGAN - &lt;a href="https://arxiv.org/abs/1709.08624" rel="nofollow"&gt;Long Text Generation via Adversarial Training with Leaked Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LeGAN - &lt;a href="https://arxiv.org/abs/1707.07530" rel="nofollow"&gt;Likelihood Estimation for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LGAN - &lt;a href="https://arxiv.org/abs/1711.06020" rel="nofollow"&gt;Global versus Localized Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lipizzaner - &lt;a href="https://arxiv.org/abs/1807.08194" rel="nofollow"&gt;Towards Distributed Coevolutionary GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LR-GAN - &lt;a href="https://arxiv.org/abs/1703.01560v1" rel="nofollow"&gt;LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LS-GAN - &lt;a href="https://arxiv.org/abs/1701.06264" rel="nofollow"&gt;Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSGAN - &lt;a href="https://arxiv.org/abs/1611.04076v3" rel="nofollow"&gt;Least Squares Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;M-AAE - &lt;a href="https://arxiv.org/abs/1804.08882" rel="nofollow"&gt;Mask-aware Photorealistic Face Attribute Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MAD-GAN - &lt;a href="https://arxiv.org/abs/1704.02906" rel="nofollow"&gt;Multi-Agent Diverse Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MAGAN - &lt;a href="https://arxiv.org/abs/1704.03817v1" rel="nofollow"&gt;MAGAN: Margin Adaptation for Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MAGAN - &lt;a href="https://arxiv.org/abs/1803.00385" rel="nofollow"&gt;MAGAN: Aligning Biological Manifolds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MalGAN - &lt;a href="https://arxiv.org/abs/1702.05983v1" rel="nofollow"&gt;Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MaliGAN - &lt;a href="https://arxiv.org/abs/1702.07983" rel="nofollow"&gt;Maximum-Likelihood Augmented Discrete Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;manifold-WGAN - &lt;a href="https://arxiv.org/abs/1712.01551" rel="nofollow"&gt;Manifold-valued Image Generation with Wasserstein Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MARTA-GAN - &lt;a href="https://arxiv.org/abs/1612.08879" rel="nofollow"&gt;Deep Unsupervised Representation Learning for Remote Sensing Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MaskGAN - &lt;a href="https://arxiv.org/abs/1801.07736" rel="nofollow"&gt;MaskGAN: Better Text Generation via Filling in the ______ &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MC-GAN - &lt;a href="https://arxiv.org/abs/1712.00516" rel="nofollow"&gt;Multi-Content GAN for Few-Shot Font Style Transfer&lt;/a&gt; (&lt;a href="https://github.com/azadis/MC-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;MC-GAN - &lt;a href="https://arxiv.org/abs/1805.01123" rel="nofollow"&gt;MC-GAN: Multi-conditional Generative Adversarial Network for Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;McGAN - &lt;a href="https://arxiv.org/abs/1702.08398v1" rel="nofollow"&gt;McGan: Mean and Covariance Feature Matching GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MD-GAN - &lt;a href="https://arxiv.org/abs/1709.07592" rel="nofollow"&gt;Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MDGAN - &lt;a href="https://arxiv.org/abs/1612.02136" rel="nofollow"&gt;Mode Regularized Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MedGAN - &lt;a href="https://arxiv.org/abs/1703.06490v1" rel="nofollow"&gt;Generating Multi-label Discrete Electronic Health Records using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MedGAN - &lt;a href="https://arxiv.org/abs/1806.06397" rel="nofollow"&gt;MedGAN: Medical Image Translation using GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MEGAN - &lt;a href="https://arxiv.org/abs/1805.02481" rel="nofollow"&gt;MEGAN: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MelanoGAN - &lt;a href="https://arxiv.org/abs/1804.04338" rel="nofollow"&gt;MelanoGANs: High Resolution Skin Lesion Synthesis with GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;memoryGAN - &lt;a href="https://arxiv.org/abs/1803.01500" rel="nofollow"&gt;Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MeRGAN - &lt;a href="https://arxiv.org/abs/1809.02058" rel="nofollow"&gt;Memory Replay GANs: learning to generate images from new categories without forgetting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MGAN - &lt;a href="https://arxiv.org/abs/1604.04382" rel="nofollow"&gt;Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/chuanli11/MGANs"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;MGGAN - &lt;a href="https://arxiv.org/abs/1708.02556" rel="nofollow"&gt;Multi-Generator Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MGGAN - &lt;a href="https://arxiv.org/abs/1804.04391" rel="nofollow"&gt;MGGAN: Solving Mode Collapse using Manifold Guided Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MIL-GAN - &lt;a href="https://arxiv.org/abs/1712.01455" rel="nofollow"&gt;Multimodal Storytelling via Generative Adversarial Imitation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MinLGAN - &lt;a href="https://arxiv.org/abs/1808.00200" rel="nofollow"&gt;Anomaly Detection via Minimum Likelihood Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MIX+GAN - &lt;a href="https://arxiv.org/abs/1703.00573v3" rel="nofollow"&gt;Generalization and Equilibrium in Generative Adversarial Nets (GANs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MIXGAN - &lt;a href="https://arxiv.org/abs/1807.01659" rel="nofollow"&gt;MIXGAN: Learning Concepts from Different Domains for Mixture Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MLGAN - &lt;a href="https://arxiv.org/abs/1711.02792" rel="nofollow"&gt;Metric Learning-based Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MMC-GAN - &lt;a href="https://arxiv.org/abs/1806.03847" rel="nofollow"&gt;A Multimodal Classifier Generative Adversarial Network for Carry and Place Tasks from Ambiguous Language Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MMD-GAN - &lt;a href="https://arxiv.org/abs/1705.08584" rel="nofollow"&gt;MMD GAN: Towards Deeper Understanding of Moment Matching Network&lt;/a&gt; (&lt;a href="https://github.com/dougalsutherland/opt-mmd"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;MMGAN - &lt;a href="https://arxiv.org/abs/1707.08273" rel="nofollow"&gt;MMGAN: Manifold Matching Generative Adversarial Network for Generating Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MoCoGAN - &lt;a href="https://arxiv.org/abs/1707.04993" rel="nofollow"&gt;MoCoGAN: Decomposing Motion and Content for Video Generation&lt;/a&gt; (&lt;a href="https://github.com/sergeytulyakov/mocogan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Modified GAN-CLS - &lt;a href="https://arxiv.org/abs/1806.11302" rel="nofollow"&gt;Generate the corresponding Image from Text Description using Modified GAN-CLS Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ModularGAN - &lt;a href="https://arxiv.org/abs/1804.03343" rel="nofollow"&gt;Modular Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MolGAN - &lt;a href="https://arxiv.org/abs/1805.11973" rel="nofollow"&gt;MolGAN: An implicit generative model for small molecular graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MPM-GAN - &lt;a href="https://arxiv.org/abs/1612.01294" rel="nofollow"&gt;Message Passing Multi-Agent GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MS-GAN - &lt;a href="http://papers.nips.cc/paper/7014-temporal-coherency-based-criteria-for-predicting-video-frames-using-deep-multi-stage-generative-adversarial-networks" rel="nofollow"&gt;Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MTGAN - &lt;a href="https://arxiv.org/abs/1803.09059" rel="nofollow"&gt;MTGAN: Speaker Verification through Multitasking Triplet Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MuseGAN - &lt;a href="https://arxiv.org/abs/1709.06298" rel="nofollow"&gt;MuseGAN: Symbolic-domain Music Generation and Accompaniment with Multi-track Sequential Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MV-BiGAN - &lt;a href="https://arxiv.org/abs/1611.02019v1" rel="nofollow"&gt;Multi-view Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;N2RPP - &lt;a href="https://arxiv.org/abs/1805.02825" rel="nofollow"&gt;N2RPP: An Adversarial Network to Rebuild Plantar Pressure for ACLD Patients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NAN - &lt;a href="https://arxiv.org/abs/1804.03287" rel="nofollow"&gt;Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NCE-GAN - &lt;a href="https://arxiv.org/abs/1803.10996" rel="nofollow"&gt;Dihedral angle prediction using generative adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ND-GAN - &lt;a href="https://arxiv.org/abs/1802.10560" rel="nofollow"&gt;Novelty Detection with GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NetGAN - &lt;a href="https://arxiv.org/abs/1803.00816" rel="nofollow"&gt;NetGAN: Generating Graphs via Random Walks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OCAN - &lt;a href="https://arxiv.org/abs/1803.01798" rel="nofollow"&gt;One-Class Adversarial Nets for Fraud Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OptionGAN - &lt;a href="https://arxiv.org/abs/1709.06683" rel="nofollow"&gt;OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ORGAN - &lt;a href="https://arxiv.org/abs/1705.10843" rel="nofollow"&gt;Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ORGAN - &lt;a href="https://arxiv.org/abs/1711.06363" rel="nofollow"&gt;3D Reconstruction of Incomplete Archaeological Objects Using a Generative Adversary Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OT-GAN - &lt;a href="https://arxiv.org/abs/1803.05573" rel="nofollow"&gt;Improving GANs Using Optimal Transport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PacGAN - &lt;a href="https://arxiv.org/abs/1712.04086" rel="nofollow"&gt;PacGAN: The power of two samples in generative adversarial networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PAN - &lt;a href="https://arxiv.org/abs/1706.09138" rel="nofollow"&gt;Perceptual Adversarial Networks for Image-to-Image Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PassGAN - &lt;a href="https://arxiv.org/abs/1709.00440" rel="nofollow"&gt;PassGAN: A Deep Learning Approach for Password Guessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PD-WGAN - &lt;a href="https://arxiv.org/abs/1805.09575" rel="nofollow"&gt;Primal-Dual Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceptual GAN - &lt;a href="https://arxiv.org/abs/1706.05274" rel="nofollow"&gt;Perceptual Generative Adversarial Networks for Small Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PGAN - &lt;a href="https://arxiv.org/abs/1708.01886" rel="nofollow"&gt;Probabilistic Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PGD-GAN - &lt;a href="https://arxiv.org/abs/1802.08406" rel="nofollow"&gt;Solving Linear Inverse Problems Using GAN Priors: An Algorithm with Provable Guarantees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PGGAN - &lt;a href="https://arxiv.org/abs/1803.07422" rel="nofollow"&gt;Patch-Based Image Inpainting with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PIONEER - &lt;a href="https://arxiv.org/abs/1807.03026" rel="nofollow"&gt;Pioneer Networks: Progressively Growing Generative Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pip-GAN - &lt;a href="https://arxiv.org/abs/1711.10742" rel="nofollow"&gt;Pipeline Generative Adversarial Networks for Facial Images Generation with Multiple Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pix2pix - &lt;a href="https://arxiv.org/abs/1611.07004" rel="nofollow"&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/phillipi/pix2pix"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;pix2pixHD - &lt;a href="https://arxiv.org/abs/1711.11585" rel="nofollow"&gt;High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs&lt;/a&gt; (&lt;a href="https://github.com/NVIDIA/pix2pixHD"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;PixelGAN - &lt;a href="https://arxiv.org/abs/1706.00531" rel="nofollow"&gt;PixelGAN Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PM-GAN - &lt;a href="https://arxiv.org/abs/1804.06248" rel="nofollow"&gt;PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-modalities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PN-GAN - &lt;a href="https://arxiv.org/abs/1712.02225" rel="nofollow"&gt;Pose-Normalized Image Generation for Person Re-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;POGAN - &lt;a href="https://arxiv.org/abs/1805.01084" rel="nofollow"&gt;Perceptually Optimized Generative Adversarial Network for Single Image Dehazing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pose-GAN - &lt;a href="https://arxiv.org/abs/1705.00053" rel="nofollow"&gt;The Pose Knows: Video Forecasting by Generating Pose Futures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PP-GAN - &lt;a href="https://arxiv.org/abs/1806.08906" rel="nofollow"&gt;Privacy-Protective-GAN for Face De-identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PPAN - &lt;a href="https://arxiv.org/abs/1712.07008" rel="nofollow"&gt;Privacy-Preserving Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PPGN - &lt;a href="https://arxiv.org/abs/1612.00005" rel="nofollow"&gt;Plug &amp;amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PrGAN - &lt;a href="https://arxiv.org/abs/1612.05872" rel="nofollow"&gt;3D Shape Induction from 2D Views of Multiple Objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ProGanSR - &lt;a href="https://arxiv.org/abs/1804.02900" rel="nofollow"&gt;A Fully Progressive Approach to Single-Image Super-Resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Progressive GAN - &lt;a href="https://arxiv.org/abs/1710.10196" rel="nofollow"&gt;Progressive Growing of GANs for Improved Quality, Stability, and Variation&lt;/a&gt; (&lt;a href="https://github.com/tkarras/progressive_growing_of_gans"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;PS-GAN - &lt;a href="https://arxiv.org/abs/1804.02047" rel="nofollow"&gt;Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PSGAN - &lt;a href="http://arxiv.org/abs/1705.06566" rel="nofollow"&gt;Learning Texture Manifolds with the Periodic Spatial GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PSGAN - &lt;a href="https://arxiv.org/abs/1805.03371" rel="nofollow"&gt;PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PS²-GAN - &lt;a href="https://arxiv.org/abs/1710.10182" rel="nofollow"&gt;High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RadialGAN - &lt;a href="http://arxiv.org/abs/1802.06403" rel="nofollow"&gt;RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RaGAN - &lt;a href="https://arxiv.org/abs/1807.00734" rel="nofollow"&gt;The relativistic discriminator: a key element missing from standard GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RAN - &lt;a href="https://arxiv.org/abs/1712.05444" rel="nofollow"&gt;RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment&lt;/a&gt; (&lt;a href=""&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RankGAN - &lt;a href="https://arxiv.org/abs/1705.11001" rel="nofollow"&gt;Adversarial Ranking for Language Generation &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RCGAN - &lt;a href="https://arxiv.org/abs/1706.02633" rel="nofollow"&gt;Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ReConNN - &lt;a href="https://arxiv.org/abs/1805.00528" rel="nofollow"&gt;Reconstruction of Simulation-Based Physical Field with Limited Samples by Reconstruction Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recycle-GAN - &lt;a href="https://arxiv.org/abs/1808.05174" rel="nofollow"&gt;Recycle-GAN: Unsupervised Video Retargeting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RefineGAN - &lt;a href="https://arxiv.org/abs/1709.00753" rel="nofollow"&gt;Compressed Sensing MRI Reconstruction with Cyclic Loss in Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ReGAN - &lt;a href="https://arxiv.org/abs/1805.02788" rel="nofollow"&gt;ReGAN: RE[LAX|BAR|INFORCE] based Sequence Generation using GANs&lt;/a&gt; (&lt;a href="https://github.com/TalkToTheGAN/REGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RegCGAN - &lt;a href="https://arxiv.org/abs/1805.02456" rel="nofollow"&gt;Unpaired Multi-Domain Image Generation via Regularized Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RenderGAN - &lt;a href="https://arxiv.org/abs/1611.01331" rel="nofollow"&gt;RenderGAN: Generating Realistic Labeled Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Resembled GAN - &lt;a href="https://arxiv.org/abs/1807.00947" rel="nofollow"&gt;Resembled Generative Adversarial Networks: Two Domains with Similar Attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ResGAN - &lt;a href="https://arxiv.org/abs/1707.04881" rel="nofollow"&gt;Generative Adversarial Network based on Resnet for Conditional Image Restoration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RNN-WGAN - &lt;a href="https://arxiv.org/abs/1706.01399" rel="nofollow"&gt;Language Generation with Recurrent Generative Adversarial Networks without Pre-training&lt;/a&gt; (&lt;a href="https://github.com/amirbar/rnn.wgan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RoCGAN - &lt;a href="https://arxiv.org/abs/1805.08657" rel="nofollow"&gt;Robust Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RPGAN - &lt;a href="https://arxiv.org/abs/1705.07831" rel="nofollow"&gt;Stabilizing GAN Training with Multiple Random Projections&lt;/a&gt; (&lt;a href="https://github.com/ayanc/rpgan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;RTT-GAN - &lt;a href="https://arxiv.org/abs/1703.07022v2" rel="nofollow"&gt;Recurrent Topic-Transition GAN for Visual Paragraph Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RWGAN - &lt;a href="https://arxiv.org/abs/1705.07164" rel="nofollow"&gt;Relaxed Wasserstein with Applications to GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAD-GAN - &lt;a href="https://arxiv.org/abs/1611.08788v1" rel="nofollow"&gt;SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAGA - &lt;a href="https://arxiv.org/abs/1804.00709" rel="nofollow"&gt;Generative Adversarial Learning for Spectrum Sensing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAGAN - &lt;a href="https://arxiv.org/abs/1805.08318" rel="nofollow"&gt;Self-Attention Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SalGAN - &lt;a href="https://arxiv.org/abs/1701.01081" rel="nofollow"&gt;SalGAN: Visual Saliency Prediction with Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/imatge-upc/saliency-salgan-2017"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SAM - &lt;a href="https://arxiv.org/abs/1809.02064" rel="nofollow"&gt;Sample-Efficient Imitation Learning via Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sAOG - &lt;a href="https://arxiv.org/abs/1807.03877" rel="nofollow"&gt;Deep Structured Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SAR-GAN - &lt;a href="https://arxiv.org/abs/1802.10036" rel="nofollow"&gt;Generating High Quality Visible Images from SAR Images Using CNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SBADA-GAN - &lt;a href="https://arxiv.org/abs/1705.08824" rel="nofollow"&gt;From source to target and back: symmetric bi-directional adaptive GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ScarGAN - &lt;a href="https://arxiv.org/abs/1808.04500" rel="nofollow"&gt;ScarGAN: Chained Generative Adversarial Networks to Simulate Pathological Tissue on Cardiovascular MR Scans&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SCH-GAN - &lt;a href="https://arxiv.org/abs/1802.02488" rel="nofollow"&gt;SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SD-GAN - &lt;a href="https://arxiv.org/abs/1705.07904" rel="nofollow"&gt;Semantically Decomposing the Latent Spaces of Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sdf-GAN - &lt;a href="https://arxiv.org/abs/1803.06657" rel="nofollow"&gt;Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SEGAN - &lt;a href="https://arxiv.org/abs/1703.09452v1" rel="nofollow"&gt;SEGAN: Speech Enhancement Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeGAN - &lt;a href="https://arxiv.org/abs/1703.10239" rel="nofollow"&gt;SeGAN: Segmenting and Generating the Invisible&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SegAN - &lt;a href="https://arxiv.org/abs/1706.01805" rel="nofollow"&gt;SegAN: Adversarial Network with Multi-scale L1 Loss for Medical Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sem-GAN - &lt;a href="https://arxiv.org/abs/1807.04409" rel="nofollow"&gt;Sem-GAN: Semantically-Consistent Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SeqGAN - &lt;a href="https://arxiv.org/abs/1609.05473v5" rel="nofollow"&gt;SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient&lt;/a&gt; (&lt;a href="https://github.com/LantaoYu/SeqGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SeUDA - &lt;a href="https://arxiv.org/abs/1806.00600" rel="nofollow"&gt;Semantic-Aware Generative Adversarial Nets for Unsupervised Domain Adaptation in Chest X-ray Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SG-GAN - &lt;a href="https://arxiv.org/abs/1801.01726" rel="nofollow"&gt;Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption&lt;/a&gt; (&lt;a href="https://github.com/Peilun-Li/SG-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SG-GAN - &lt;a href="https://arxiv.org/abs/1805.07509" rel="nofollow"&gt;Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1611.08207" rel="nofollow"&gt;Texture Synthesis with Spatial Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1612.04357v4" rel="nofollow"&gt;Stacked Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/xunhuang1995/SGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1703.05502" rel="nofollow"&gt;Steganographic Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1712.02330" rel="nofollow"&gt;SGAN: An Alternative Training of Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SGAN - &lt;a href="https://arxiv.org/abs/1807.07144" rel="nofollow"&gt;CT Image Enhancement Using Stacked Generative Adversarial Networks and Transfer Learning for Lesion Segmentation Improvement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sGAN  - &lt;a href="https://arxiv.org/abs/1804.04366" rel="nofollow"&gt;Generative Adversarial Training for MRA Image Synthesis Using Multi-Contrast MRI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SiftingGAN - &lt;a href="https://arxiv.org/abs/1809.04985" rel="nofollow"&gt;SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline in vitro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SiGAN - &lt;a href="https://arxiv.org/abs/1807.08370" rel="nofollow"&gt;SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SimGAN - &lt;a href="https://arxiv.org/abs/1612.07828" rel="nofollow"&gt;Learning from Simulated and Unsupervised Images through Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SisGAN - &lt;a href="https://arxiv.org/abs/1707.06873" rel="nofollow"&gt;Semantic Image Synthesis via Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sketcher-Refiner GAN - &lt;a href="https://arxiv.org/abs/1804.08039" rel="nofollow"&gt;Learning Myelin Content in Multiple Sclerosis from Multimodal MRI through Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SketchGAN - &lt;a href="https://arxiv.org/abs/1607.02748" rel="nofollow"&gt;Adversarial Training For Sketch Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SketchyGAN - &lt;a href="https://arxiv.org/abs/1801.02753" rel="nofollow"&gt;SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Skip-Thought GAN - &lt;a href="https://arxiv.org/abs/1808.08703" rel="nofollow"&gt;Generating Text through Adversarial Training using Skip-Thought Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SL-GAN - &lt;a href="https://arxiv.org/abs/1704.02166" rel="nofollow"&gt;Semi-Latent GAN: Learning to generate and modify facial images from attributes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SLSR - &lt;a href="https://arxiv.org/abs/1809.04976" rel="nofollow"&gt;Sparse Label Smoothing for Semi-supervised Person Re-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SN-DCGAN - &lt;a href="https://arxiv.org/abs/1806.00236" rel="nofollow"&gt;Generative Adversarial Networks for Unsupervised Object Co-localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SN-GAN - &lt;a href="https://drive.google.com/file/d/0B8HZ50DPgR3eSVV6YlF3XzQxSjQ/view" rel="nofollow"&gt;Spectral Normalization for Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/pfnet-research/chainer-gan-lib"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SN-PatchGAN - &lt;a href="https://arxiv.org/abs/1806.03589" rel="nofollow"&gt;Free-Form Image Inpainting with Gated Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sobolev GAN - &lt;a href="https://arxiv.org/abs/1711.04894" rel="nofollow"&gt;Sobolev GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Social GAN - &lt;a href="https://arxiv.org/abs/1803.10892" rel="nofollow"&gt;Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Softmax GAN - &lt;a href="https://arxiv.org/abs/1704.06191" rel="nofollow"&gt;Softmax GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SoPhie - &lt;a href="https://arxiv.org/abs/1806.01482" rel="nofollow"&gt;SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;speech-driven animation GAN - &lt;a href="https://arxiv.org/abs/1805.09313" rel="nofollow"&gt;End-to-End Speech-Driven Facial Animation with Temporal GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spike-GAN - &lt;a href="https://arxiv.org/abs/1803.00338" rel="nofollow"&gt;Synthesizing realistic neural population activity patterns using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Splitting GAN - &lt;a href="https://arxiv.org/abs/1709.07359" rel="nofollow"&gt;Class-Splitting Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SR-CNN-VAE-GAN - &lt;a href="https://arxiv.org/abs/1806.00509" rel="nofollow"&gt;Semi-Recurrent CNN-based VAE-GAN for Sequential Data Generation&lt;/a&gt; (&lt;a href="https://github.com/makbari7/SR-CNN-VAE-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;SRGAN - &lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SRPGAN - &lt;a href="https://arxiv.org/abs/1712.05927" rel="nofollow"&gt;SRPGAN: Perceptual Generative Adversarial Network for Single Image Super Resolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SS-GAN - &lt;a href="https://arxiv.org/abs/1708.05789" rel="nofollow"&gt;Semi-supervised Conditional GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ss-InfoGAN - &lt;a href="https://arxiv.org/abs/1707.04487" rel="nofollow"&gt;Guiding InfoGAN with Semi-Supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SSGAN - &lt;a href="https://arxiv.org/abs/1707.01613" rel="nofollow"&gt;SSGAN: Secure Steganography Based on Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SSL-GAN - &lt;a href="https://arxiv.org/abs/1611.06430v1" rel="nofollow"&gt;Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ST-CGAN - &lt;a href="https://arxiv.org/abs/1712.02478" rel="nofollow"&gt;Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ST-GAN - &lt;a href="https://arxiv.org/abs/1702.06762" rel="nofollow"&gt;Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ST-GAN - &lt;a href="https://arxiv.org/abs/1803.01837" rel="nofollow"&gt;ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;StackGAN - &lt;a href="https://arxiv.org/abs/1612.03242v1" rel="nofollow"&gt;StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/hanzhanggit/StackGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;StainGAN - &lt;a href="https://arxiv.org/abs/1804.01601" rel="nofollow"&gt;StainGAN: Stain Style Transfer for Digital Histological Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;StarGAN - &lt;a href="https://arxiv.org/abs/1711.09020" rel="nofollow"&gt;StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/a&gt; (&lt;a href="https://github.com/yunjey/StarGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;StarGAN-VC - &lt;a href="https://arxiv.org/abs/1806.02169" rel="nofollow"&gt;StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SteinGAN - &lt;a href="https://arxiv.org/abs/1707.00797" rel="nofollow"&gt;Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;StepGAN - &lt;a href="https://arxiv.org/abs/1808.05599" rel="nofollow"&gt;Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Super-FAN - &lt;a href="https://arxiv.org/abs/1712.02765" rel="nofollow"&gt;Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SVSGAN - &lt;a href="https://arxiv.org/abs/1710.11428" rel="nofollow"&gt;SVSGAN: Singing Voice Separation via Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SWGAN - &lt;a href="https://arxiv.org/abs/1802.08249" rel="nofollow"&gt;Solving Approximate Wasserstein GANs to Stationarity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SyncGAN - &lt;a href="https://arxiv.org/abs/1804.00410" rel="nofollow"&gt;SyncGAN: Synchronize the Latent Space of Cross-modal Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;S^2GAN - &lt;a href="https://arxiv.org/abs/1603.05631v2" rel="nofollow"&gt;Generative Image Modeling using Style and Structure Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;T2Net - &lt;a href="https://arxiv.org/abs/1808.01454" rel="nofollow"&gt;T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;table-GAN - &lt;a href="https://arxiv.org/abs/1806.03384" rel="nofollow"&gt;Data Synthesis based on Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TAC-GAN - &lt;a href="https://arxiv.org/abs/1703.06412v2" rel="nofollow"&gt;TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network&lt;/a&gt; (&lt;a href="https://github.com/dashayushman/TAC-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;TAN - &lt;a href="https://arxiv.org/abs/1704.08834" rel="nofollow"&gt;Outline Colorization through Tandem Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tcGAN - &lt;a href="https://arxiv.org/abs/1806.05147" rel="nofollow"&gt;Cross-modal Hallucination for Few-shot Fine-grained Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TD-GAN - &lt;a href="https://arxiv.org/abs/1806.07201" rel="nofollow"&gt;Task Driven Generative Modeling for Unsupervised Domain Adaptation: Application to X-ray Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tempCycleGAN - &lt;a href="https://arxiv.org/abs/1806.03627" rel="nofollow"&gt;Improving Surgical Training Phantoms by Hyperrealism: Deep Unpaired Image-to-Image Translation from Real Surgeries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tempoGAN - &lt;a href="https://arxiv.org/abs/1801.09710" rel="nofollow"&gt;tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TequilaGAN - &lt;a href="https://arxiv.org/abs/1807.04919" rel="nofollow"&gt;TequilaGAN: How to easily identify GAN samples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Text2Shape - &lt;a href="https://arxiv.org/abs/1803.08495" rel="nofollow"&gt;Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;textGAN - &lt;a href="https://zhegan27.github.io/Papers/textGAN_nips2016_workshop.pdf" rel="nofollow"&gt;Generating Text via Adversarial Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TextureGAN - &lt;a href="https://arxiv.org/abs/1706.02823" rel="nofollow"&gt;TextureGAN: Controlling Deep Image Synthesis with Texture Patches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGAN - &lt;a href="https://arxiv.org/abs/1611.06624v1" rel="nofollow"&gt;Temporal Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGAN - &lt;a href="https://arxiv.org/abs/1710.10772" rel="nofollow"&gt;Tensorizing Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGAN - &lt;a href="https://arxiv.org/abs/1711.02666" rel="nofollow"&gt;Tensor-Generative Adversarial Network with Two-dimensional Sparse Coding: Application to Real-time Indoor Localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TGANs-C - &lt;a href="https://arxiv.org/abs/1804.08264" rel="nofollow"&gt;To Create What You Tell: Generating Videos from Captions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tiny-GAN - &lt;a href="https://arxiv.org/abs/1803.05045" rel="nofollow"&gt;Analysis of Nonautonomous Adversarial Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TP-GAN - &lt;a href="https://arxiv.org/abs/1704.04086" rel="nofollow"&gt;Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TreeGAN - &lt;a href="https://arxiv.org/abs/1808.07582" rel="nofollow"&gt;TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Triple-GAN - &lt;a href="https://arxiv.org/abs/1703.02291v2" rel="nofollow"&gt;Triple Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tripletGAN - &lt;a href="https://arxiv.org/abs/1711.05084" rel="nofollow"&gt;TripletGAN: Training Generative Model with Triplet Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TV-GAN - &lt;a href="https://arxiv.org/abs/1712.02514" rel="nofollow"&gt;TV-GAN: Generative Adversarial Network Based Thermal to Visible Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Twin-GAN - &lt;a href="https://arxiv.org/abs/1809.00946" rel="nofollow"&gt;Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UGACH - &lt;a href="https://arxiv.org/abs/1712.00358" rel="nofollow"&gt;Unsupervised Generative Adversarial Cross-modal Hashing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UGAN - &lt;a href="https://arxiv.org/abs/1801.04011" rel="nofollow"&gt;Enhancing Underwater Imagery using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unim2im - &lt;a href="https://arxiv.org/abs/1701.02676" rel="nofollow"&gt;Unsupervised Image-to-Image Translation with Generative Adversarial Networks &lt;/a&gt; (&lt;a href="http://github.com/zsdonghao/Unsup-Im2Im"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;UNIT - &lt;a href="https://arxiv.org/abs/1703.00848" rel="nofollow"&gt;Unsupervised Image-to-image Translation Networks&lt;/a&gt; (&lt;a href="https://github.com/mingyuliutw/UNIT"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Unrolled GAN - &lt;a href="https://arxiv.org/abs/1611.02163" rel="nofollow"&gt;Unrolled Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/poolio/unrolled_gan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;UT-SCA-GAN - &lt;a href="https://arxiv.org/abs/1804.07939" rel="nofollow"&gt;Spatial Image Steganography Based on Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;UV-GAN - &lt;a href="https://arxiv.org/abs/1712.04695" rel="nofollow"&gt;UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VA-GAN - &lt;a href="https://arxiv.org/abs/1711.08998" rel="nofollow"&gt;Visual Feature Attribution using Wasserstein GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VAC+GAN  - &lt;a href="https://arxiv.org/abs/1806.07751" rel="nofollow"&gt;Versatile Auxiliary Classifier with Generative Adversarial Network (VAC+GAN), Multi Class Scenarios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VAE-GAN - &lt;a href="https://arxiv.org/abs/1512.09300" rel="nofollow"&gt;Autoencoding beyond pixels using a learned similarity metric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VariGAN - &lt;a href="https://arxiv.org/abs/1704.04886" rel="nofollow"&gt;Multi-View Image Generation from a Single-View&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VAW-GAN - &lt;a href="https://arxiv.org/abs/1704.00849" rel="nofollow"&gt;Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VEEGAN - &lt;a href="https://arxiv.org/abs/1705.07761" rel="nofollow"&gt;VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning&lt;/a&gt; (&lt;a href="https://github.com/akashgit/VEEGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;VGAN - &lt;a href="https://arxiv.org/abs/1609.02612" rel="nofollow"&gt;Generating Videos with Scene Dynamics&lt;/a&gt; (&lt;a href="https://github.com/cvondrick/videogan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;VGAN - &lt;a href="https://arxiv.org/abs/1611.01799" rel="nofollow"&gt;Generative Adversarial Networks as Variational Training of Energy Based Models&lt;/a&gt; (&lt;a href="https://github.com/Shuangfei/vgan"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;VGAN - &lt;a href="https://arxiv.org/abs/1712.00170" rel="nofollow"&gt;Text Generation Based on Generative Adversarial Nets with Latent Variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ViGAN - &lt;a href="https://arxiv.org/abs/1701.04568v1" rel="nofollow"&gt;Image Generation and Editing with Variational Info Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VIGAN - &lt;a href="https://arxiv.org/abs/1708.06724" rel="nofollow"&gt;VIGAN: Missing View Imputation with Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VoiceGAN - &lt;a href="http://arxiv.org/abs/1802.06840" rel="nofollow"&gt;Voice Impersonation using Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VOS-GAN - &lt;a href="https://arxiv.org/abs/1803.09092" rel="nofollow"&gt;VOS-GAN: Adversarial Learning of Visual-Temporal Dynamics for Unsupervised Dense Prediction in Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VRAL - &lt;a href="https://arxiv.org/abs/1707.00309" rel="nofollow"&gt;Variance Regularizing Adversarial Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaterGAN - &lt;a href="https://arxiv.org/abs/1702.07392v1" rel="nofollow"&gt;WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaveGAN - &lt;a href="https://arxiv.org/abs/1802.04208" rel="nofollow"&gt;Synthesizing Audio with Generative Adversarial Networks &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WaveletGLCA-GAN - &lt;a href="https://arxiv.org/abs/1809.07764" rel="nofollow"&gt;Global and Local Consistent Wavelet-domain Age Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;weGAN - &lt;a href="https://arxiv.org/abs/1712.09127" rel="nofollow"&gt;Generative Adversarial Nets for Multiple Text Corpora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WGAN - &lt;a href="https://arxiv.org/abs/1701.07875v2" rel="nofollow"&gt;Wasserstein GAN&lt;/a&gt; (&lt;a href="https://github.com/martinarjovsky/WassersteinGAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;WGAN-CLS - &lt;a href="https://arxiv.org/abs/1805.00676" rel="nofollow"&gt;Text to Image Synthesis Using Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WGAN-GP - &lt;a href="https://arxiv.org/abs/1704.00028" rel="nofollow"&gt;Improved Training of Wasserstein GANs&lt;/a&gt; (&lt;a href="https://github.com/igul222/improved_wgan_training"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;WGAN-L1 - &lt;a href="https://arxiv.org/abs/1807.04418" rel="nofollow"&gt;Subsampled Turbulence Removal Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WS-GAN - &lt;a href="https://arxiv.org/abs/1705.10904" rel="nofollow"&gt;Weakly Supervised Generative Adversarial Networks for 3D Reconstruction &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;X-GANs - &lt;a href="https://arxiv.org/abs/1808.04432" rel="nofollow"&gt;X-GANs: Image Reconstruction Made Easy for Extreme Cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;XGAN - &lt;a href="https://arxiv.org/abs/1711.05139" rel="nofollow"&gt;XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ZipNet-GAN - &lt;a href="https://arxiv.org/abs/1711.02413" rel="nofollow"&gt;ZipNet-GAN: Inferring Fine-grained Mobile Traffic Patterns via a Generative Adversarial Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;α-GAN - &lt;a href="https://arxiv.org/abs/1706.04987" rel="nofollow"&gt;Variational Approaches for Auto-Encoding Generative Adversarial Networks&lt;/a&gt; (&lt;a href="https://github.com/victor-shepardson/alpha-GAN"&gt;github&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;β-GAN - &lt;a href="https://arxiv.org/abs/1705.07505" rel="nofollow"&gt;Annealed Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Δ-GAN - &lt;a href="https://arxiv.org/abs/1709.06548" rel="nofollow"&gt;Triangle Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hindupuravinash</author><guid isPermaLink="false">https://github.com/hindupuravinash/the-gan-zoo</guid><pubDate>Tue, 05 Nov 2019 00:17:00 GMT</pubDate></item><item><title>hanxiao/bert-as-service #18 in Python, This week</title><link>https://github.com/hanxiao/bert-as-service</link><description>&lt;p&gt;&lt;i&gt;Mapping a variable-length sentence to a fixed-length vector using BERT model&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-bert-as-service" class="anchor" aria-hidden="true" href="#bert-as-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bert-as-service&lt;/h1&gt;
&lt;p align="center"&gt;Using BERT model as a sentence encoding service, i.e. mapping a variable-length sentence to a fixed-length vector.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/stargazers"&gt;
    &lt;img src="https://camo.githubusercontent.com/827fc64cf3b84a82a3057b15bd67bd110c3f094f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616e7869616f2f626572742d61732d736572766963652e7376673f636f6c6f72413d6f72616e676526636f6c6f72423d6f72616e6765266c6f676f3d676974687562" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/hanxiao/bert-as-service.svg?colorA=orange&amp;amp;colorB=orange&amp;amp;logo=github" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://pypi.org/search/?q=bert-serving" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/17c79028fcd99fcb6ffd09d9078a9e90ca72dabe/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626572742d73657276696e672d7365727665722e7376673f636f6c6f72423d627269676874677265656e" alt="Pypi package" data-canonical-src="https://img.shields.io/pypi/v/bert-serving-server.svg?colorB=brightgreen" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;a href="https://bert-as-service.readthedocs.io/" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/97f0af3aadd65722bd4510764170eb12e26c20a2/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;a href="https://pypi.org/search/?q=bert-serving" rel="nofollow"&gt;
      &lt;img alt="PyPI - Downloads" src="https://camo.githubusercontent.com/f0c139454d4564bf4abdcf294e009d886e53e1f5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626572742d73657276696e672d736572766572" data-canonical-src="https://img.shields.io/pypi/dm/bert-serving-server" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/issues"&gt;
        &lt;img src="https://camo.githubusercontent.com/37c917311b11a54f44462c0271a9e62fbd82dc03/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f68616e7869616f2f626572742d61732d736572766963652e737667" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/hanxiao/bert-as-service.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/hanxiao/bert-as-service/blob/master/LICENSE"&gt;
        &lt;img src="https://camo.githubusercontent.com/82e75359cfc65c373073242222565e1d21cd5979/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68616e7869616f2f626572742d61732d736572766963652e737667" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/hanxiao/bert-as-service.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://twitter.com/intent/tweet?text=Wow:&amp;amp;url=https%3A%2F%2Fgithub.com%2Fhanxiao%2Fbert-as-service" rel="nofollow"&gt;
  &lt;img src="https://camo.githubusercontent.com/c03e98ee22b873659d1c89f929e35fc8eafbeada/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f6769746875622e636f6d2f68616e7869616f2f626572742d61732d736572766963652e7376673f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/https/github.com/hanxiao/bert-as-service.svg?style=social" style="max-width:100%;"&gt;
  &lt;/a&gt;      
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="#highlights"&gt;Highlights&lt;/a&gt; •
  &lt;a href="#what-is-it"&gt;What is it&lt;/a&gt; •
  &lt;a href="#install"&gt;Install&lt;/a&gt; •
  &lt;a href="#getting-started"&gt;Getting Started&lt;/a&gt; •
  &lt;a href="#server-and-client-api"&gt;API&lt;/a&gt; •
  &lt;a href="#book-tutorial"&gt;Tutorials&lt;/a&gt; •
  &lt;a href="#speech_balloon-faq"&gt;FAQ&lt;/a&gt; •
  &lt;a href="#zap-benchmark"&gt;Benchmark&lt;/a&gt; •
  &lt;a href="https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" rel="nofollow"&gt;Blog&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href=".github/demo.gif?raw=true"&gt;&lt;img src=".github/demo.gif?raw=true" width="700" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-made-by-han-xiao--globe_with_meridians-httpshanxiaogithubio" class="anchor" aria-hidden="true" href="#made-by-han-xiao--globe_with_meridians-httpshanxiaogithubio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Made by Han Xiao • &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;🌐&lt;/g-emoji&gt; &lt;a href="https://hanxiao.github.io" rel="nofollow"&gt;https://hanxiao.github.io&lt;/a&gt;&lt;/h6&gt;

&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
  &lt;td width="25%"&gt;&lt;a href="https://github.com/gnes-ai/gnes"&gt;
      &lt;img src=".github/gnes-logo-tight.svg" alt="GNES is Generic Neural Elastic Search (logo made by Han Xiao)" style="max-width:100%;"&gt;
      &lt;/a&gt;&lt;/td&gt;
  &lt;td&gt;
  &lt;b&gt;&lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt;Looking for X-as-service? Or more generic and cloud-native solution?&lt;/b&gt;
  &lt;p&gt;&lt;br&gt;Checkout my new project &lt;a href="https://github.com/gnes-ai/gnes"&gt;GNES&lt;/a&gt;! GNES is Generic Neural Elastic Search, a cloud-native semantic search system based on deep neural network. GNES enables large-scale index and semantic search for text-to-text, image-to-image, video-to-video and any-to-any content form.&lt;/p&gt;
&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;h2 align="center"&gt;&lt;a id="user-content-what-is-it" class="anchor" aria-hidden="true" href="#what-is-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is it&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt; is a NLP model &lt;a href="https://github.com/google-research/bert"&gt;developed by Google&lt;/a&gt; for pre-training language representations. It leverages an enormous amount of plain text data publicly available on the web and is trained in an unsupervised manner. Pre-training a BERT model is a fairly expensive yet one-time procedure for each language. Fortunately, Google released several pre-trained models where &lt;a href="https://github.com/google-research/bert#pre-trained-models"&gt;you can download from here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sentence Encoding/Embedding&lt;/strong&gt; is a upstream task required in many NLP applications, e.g. sentiment analysis, text classification. The goal is to represent a variable length sentence into a fixed length vector, e.g. &lt;code&gt;hello world&lt;/code&gt; to &lt;code&gt;[0.1, 0.3, 0.9]&lt;/code&gt;. Each element of the vector should "encode" some semantics of the original sentence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finally, &lt;code&gt;bert-as-service&lt;/code&gt;&lt;/strong&gt; uses BERT as a sentence encoder and hosts it as a service via ZeroMQ, allowing you to map sentences into fixed-length representations in just two lines of code.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-highlights" class="anchor" aria-hidden="true" href="#highlights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="telescope" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52d.png"&gt;🔭&lt;/g-emoji&gt; &lt;strong&gt;State-of-the-art&lt;/strong&gt;: build on pretrained 12/24-layer BERT models released by Google AI, which is considered as a milestone in the NLP community.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="hatching_chick" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f423.png"&gt;🐣&lt;/g-emoji&gt; &lt;strong&gt;Easy-to-use&lt;/strong&gt;: require only two lines of code to get sentence/token-level encodes.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="zap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png"&gt;⚡️&lt;/g-emoji&gt; &lt;strong&gt;Fast&lt;/strong&gt;: 900 sentences/s on a single Tesla M40 24GB. Low latency, optimized for speed. See &lt;a href="#zap-benchmark"&gt;benchmark&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="octopus" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f419.png"&gt;🐙&lt;/g-emoji&gt; &lt;strong&gt;Scalable&lt;/strong&gt;: scale nicely and smoothly on multiple GPUs and multiple clients without worrying about concurrency. See &lt;a href="#speed-wrt-num_client"&gt;benchmark&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="gem" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f48e.png"&gt;💎&lt;/g-emoji&gt; &lt;strong&gt;Reliable&lt;/strong&gt;: tested on multi-billion sentences; days of running without a break or OOM or any nasty exceptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More features: &lt;a href="#speed-wrt--fp16-and--xla"&gt;XLA &amp;amp; FP16 support&lt;/a&gt;; mix GPU-CPU workloads; optimized graph; &lt;code&gt;tf.data&lt;/code&gt; friendly; customized tokenizer; flexible pooling strategy; &lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;build-in HTTP server&lt;/a&gt; and dashboard; &lt;a href="#asynchronous-encoding"&gt;async encoding&lt;/a&gt;; &lt;a href="#broadcasting-to-multiple-clients"&gt;multicasting&lt;/a&gt;; etc.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;Install the server and client via &lt;code&gt;pip&lt;/code&gt;. They can be installed separately or even on &lt;em&gt;different&lt;/em&gt; machines:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install bert-serving-server  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; server&lt;/span&gt;
pip install bert-serving-client  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; client, independent of `bert-serving-server`&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the server MUST be running on &lt;strong&gt;Python &amp;gt;= 3.5&lt;/strong&gt; with &lt;strong&gt;Tensorflow &amp;gt;= 1.10&lt;/strong&gt; (&lt;em&gt;one-point-ten&lt;/em&gt;). Again, the server does not support Python 2!&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png"&gt;☝️&lt;/g-emoji&gt; The client can be running on both Python 2 and 3 &lt;a href="#q-can-i-run-it-in-python-2"&gt;for the following consideration&lt;/a&gt;.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-1-download-a-pre-trained-bert-model" class="anchor" aria-hidden="true" href="#1-download-a-pre-trained-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Download a Pre-trained BERT Model&lt;/h4&gt;
&lt;p&gt;Download a model listed below, then uncompress the zip file into some folder, say &lt;code&gt;/tmp/english_L-12_H-768_A-12/&lt;/code&gt;&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;List of released pretrained BERT models (click to expand...)&lt;/summary&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Uncased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;BERT-Large, Uncased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Cased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;12-layer, 768-hidden, 12-heads , 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;BERT-Large, Cased&lt;/a&gt;&lt;/td&gt;&lt;td&gt;24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Multilingual Cased (New)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Multilingual Cased (Old)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;BERT-Base, Chinese&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/details&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt; fine-tuning the model on your downstream task. &lt;a href="#q-are-you-suggesting-using-bert-without-fine-tuning"&gt;Why is it optional?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-2-start-the-bert-service" class="anchor" aria-hidden="true" href="#2-start-the-bert-service"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Start the BERT service&lt;/h4&gt;
&lt;p&gt;After installing the server, you should be able to use &lt;code&gt;bert-serving-start&lt;/code&gt; CLI as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will start a service with four workers, meaning that it can handle up to four &lt;strong&gt;concurrent&lt;/strong&gt; requests. More concurrent requests will be queued in a load balancer. Details can be found in our &lt;a href="#q-what-is-the-parallel-processing-model-behind-the-scene"&gt;FAQ&lt;/a&gt; and &lt;a href="#speed-wrt-num_client"&gt;the benchmark on number of clients&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below shows what the server looks like when starting correctly:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/server-demo.gif?raw=true"&gt;&lt;img src=".github/server-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Alternatively, one can start the BERT Service in a Docker Container (click to expand...)&lt;/summary&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker build -t bert-as-service -f ./docker/Dockerfile &lt;span class="pl-c1"&gt;.&lt;/span&gt;
NUM_WORKER=1
PATH_MODEL=/PATH_TO/_YOUR_MODEL/
docker run --runtime nvidia -dit -p 5555:5555 -p 5556:5556 -v &lt;span class="pl-smi"&gt;$PATH_MODEL&lt;/span&gt;:/model -t bert-as-service &lt;span class="pl-smi"&gt;$NUM_WORKER&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h4&gt;&lt;a id="user-content-3-use-client-to-get-sentence-encodes" class="anchor" aria-hidden="true" href="#3-use-client-to-get-sentence-encodes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Use Client to Get Sentence Encodes&lt;/h4&gt;
&lt;p&gt;Now you can encode sentences simply as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.client &lt;span class="pl-k"&gt;import&lt;/span&gt; BertClient
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it better&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It will return a &lt;code&gt;ndarray&lt;/code&gt; (or &lt;code&gt;List[List[float]]&lt;/code&gt; if you wish), in which each row is a fixed-length vector representing a sentence. Having thousands of sentences? Just &lt;code&gt;encode&lt;/code&gt;! &lt;em&gt;Don't even bother to batch&lt;/em&gt;, the server will take care of it.&lt;/p&gt;
&lt;p&gt;As a feature of BERT, you may get encodes of a pair of sentences by concatenating them with &lt;code&gt;|||&lt;/code&gt; (with whitespace before and after), e.g.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it ||| then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Below shows what the server looks like while encoding:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/server-run-demo.gif?raw=true"&gt;&lt;img src=".github/server-run-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-use-bert-service-remotely" class="anchor" aria-hidden="true" href="#use-bert-service-remotely"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use BERT Service Remotely&lt;/h4&gt;
&lt;p&gt;One may also start the service on one (GPU) machine and call it from another (CPU) machine as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; on another CPU machine&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.client &lt;span class="pl-k"&gt;import&lt;/span&gt; BertClient
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;ip&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xx.xx.xx.xx&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ip address of the GPU machine&lt;/span&gt;
bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;First do it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;then do it better&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that you only need &lt;code&gt;pip install -U bert-serving-client&lt;/code&gt; in this case, the server side is not required. You may also &lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;call the service via HTTP requests.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png"&gt;💡&lt;/g-emoji&gt; &lt;strong&gt;Want to learn more? Checkout our tutorials:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;Building a QA semantic search engine in 3 min.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serving-a-fine-tuned-bert-model"&gt;Serving a fine-tuned BERT model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-elmo-like-contextual-word-embedding"&gt;Getting ELMo-like contextual word embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-your-own-tokenizer"&gt;Using your own tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bertclient-with-tfdata-api"&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;Training a text classifier using BERT features and tf.estimator API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#saving-and-loading-with-tfrecord-data"&gt;Saving and loading with TFRecord data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asynchronous-encoding"&gt;Asynchronous encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#broadcasting-to-multiple-clients"&gt;Broadcasting to multiple clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#monitoring-the-service-status-in-a-dashboard"&gt;Monitoring the service status in a dashboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#starting-bertserver-from-python"&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-server-and-client-api" class="anchor" aria-hidden="true" href="#server-and-client-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server and Client API&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bert-as-service.readthedocs.io" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The best way to learn &lt;code&gt;bert-as-service&lt;/code&gt; &lt;strong&gt;latest API&lt;/strong&gt; is &lt;a href="http://bert-as-service.readthedocs.io" rel="nofollow"&gt;reading the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-server-api" class="anchor" aria-hidden="true" href="#server-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Server API&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/server.html#server-side-api" rel="nofollow"&gt;Please always refer to the latest server-side API documented here.&lt;/a&gt;, you may get the latest usage via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start --help
bert-serving-terminate --help
bert-serving-benchmark --help&lt;/pre&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;model_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Required&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;folder path of the pre-trained BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tuned_model_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;(Optional)&lt;/td&gt;
&lt;td&gt;folder path of a fine-tuned BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ckpt_name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_model.ckpt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;filename of the checkpoint file.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;config_name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bert_config.json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;filename of the JSON config file for BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;graph_tmp_dir&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;path to graph temp file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_seq_len&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;maximum length of sequence, longer sequence will be trimmed on the right side. Set it to NONE for dynamically using the longest sequence in a (mini)batch.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cased_tokenization&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;Whether tokenizer should skip the default lowercasing and accent removal. Should be used for e.g. the multilingual cased pretrained BERT model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mask_cls_sep&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;masking the embedding on [CLS] and [SEP] with zero.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;num_worker&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;number of (GPU/CPU) worker runs BERT model, each works in a separate process.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;max_batch_size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;maximum number of sequences handled by each worker, larger batch will be partitioned into small batches.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;priority_batch_size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;batch smaller than this size will be labeled as high priority, and jumps forward in the job queue to get result faster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5555&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for pushing data from client to server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port_out&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5556&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for publishing results from server to client&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;http_port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;server port for receiving HTTP requests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cors&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;*&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;setting "Access-Control-Allow-Origin" for HTTP requests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pooling_strategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the pooling strategy for generating encoding vectors, valid values are &lt;code&gt;NONE&lt;/code&gt;, &lt;code&gt;REDUCE_MEAN&lt;/code&gt;, &lt;code&gt;REDUCE_MAX&lt;/code&gt;, &lt;code&gt;REDUCE_MEAN_MAX&lt;/code&gt;, &lt;code&gt;CLS_TOKEN&lt;/code&gt;, &lt;code&gt;FIRST_TOKEN&lt;/code&gt;, &lt;code&gt;SEP_TOKEN&lt;/code&gt;, &lt;code&gt;LAST_TOKEN&lt;/code&gt;. Explanation of these strategies &lt;a href="#q-what-are-the-available-pooling-strategies"&gt;can be found here&lt;/a&gt;. To get encoding for each token in the sequence, please set this to &lt;code&gt;NONE&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pooling_layer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[-2]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the encoding layer that pooling operates on, where &lt;code&gt;-1&lt;/code&gt; means the last layer, &lt;code&gt;-2&lt;/code&gt; means the second-to-last, &lt;code&gt;[-1, -2]&lt;/code&gt; means concatenating the result of last two layers, etc.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;gpu_memory_fraction&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the fraction of the overall amount of memory that each GPU should be allocated per worker&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cpu&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;run on CPU instead of GPU&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;xla&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;enable &lt;a href="https://www.tensorflow.org/xla/jit" rel="nofollow"&gt;XLA compiler&lt;/a&gt; for graph optimization (&lt;em&gt;experimental!&lt;/em&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fp16&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;use float16 precision (experimental)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;device_map&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;specify the list of GPU device ids that will be used (id starts from 0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;show_tokens_to_client&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;sending tokenization results to client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-client-api" class="anchor" aria-hidden="true" href="#client-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client API&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/client.html#module-client" rel="nofollow"&gt;Please always refer to the latest client-side API documented here.&lt;/a&gt; Client-side provides a Python class called &lt;code&gt;BertClient&lt;/code&gt;, which accepts arguments as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Argument&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ip&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address of the server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5555&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for pushing data from client to server, &lt;em&gt;must be consistent with the server side config&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;port_out&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5556&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;port for publishing results from server to client, &lt;em&gt;must be consistent with the server side config&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;output_fmt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ndarray&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;the output format of the sentence encodes, either in numpy array or python List[List[float]] (&lt;code&gt;ndarray&lt;/code&gt;/&lt;code&gt;list&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;show_server_config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;whether to show server configs when first connected&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;check_version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;whether to force client and server to have the same version&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;identity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;a UUID that identifies the client, useful in multi-casting&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;timeout&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;set the timeout (milliseconds) for receive operation on the client&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A &lt;code&gt;BertClient&lt;/code&gt; implements the following methods and properties:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.encode()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Encode a list of strings to a list of vectors&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.encode_async()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Asynchronous encode batches from a generator&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.fetch()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch all encoded vectors from server and return them in a generator, use it with &lt;code&gt;.encode_async()&lt;/code&gt; or &lt;code&gt;.encode(blocking=False)&lt;/code&gt;. Sending order is NOT preserved.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.fetch_all()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fetch all encoded vectors from server and return them in a list, use it with &lt;code&gt;.encode_async()&lt;/code&gt; or &lt;code&gt;.encode(blocking=False)&lt;/code&gt;. Sending order is preserved.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.close()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Gracefully close the connection between the client and the server&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Get the client status in JSON format&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.server_status&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Get the server status in JSON format&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-book-tutorial" class="anchor" aria-hidden="true" href="#book-tutorial"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; Tutorial&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/faq.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The full list of examples can be found in &lt;a href="example"&gt;&lt;code&gt;example/&lt;/code&gt;&lt;/a&gt;. You can run each via &lt;code&gt;python example/example-k.py&lt;/code&gt;. Most of examples require you to start a BertServer first, please follow &lt;a href="#2-start-the-bert-service"&gt;the instruction here&lt;/a&gt;. Note that although &lt;code&gt;BertClient&lt;/code&gt; works universally on both Python 2.x and 3.x, examples are only tested on Python 3.6.&lt;/p&gt;
&lt;details&gt;
 &lt;summary&gt;Table of contents (click to expand...)&lt;/summary&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;Building a QA semantic search engine in 3 min.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#serving-a-fine-tuned-bert-model"&gt;Serving a fine-tuned BERT model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-elmo-like-contextual-word-embedding"&gt;Getting ELMo-like contextual word embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-your-own-tokenizer"&gt;Using your own tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bertclient-with-tfdata-api"&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;Training a text classifier using BERT features and tf.estimator API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#saving-and-loading-with-tfrecord-data"&gt;Saving and loading with TFRecord data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#asynchronous-encoding"&gt;Asynchronous encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#broadcasting-to-multiple-clients"&gt;Broadcasting to multiple clients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#monitoring-the-service-status-in-a-dashboard"&gt;Monitoring the service status in a dashboard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#starting-bertserver-from-python"&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-building-a-qa-semantic-search-engine-in-3-minutes" class="anchor" aria-hidden="true" href="#building-a-qa-semantic-search-engine-in-3-minutes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building a QA semantic search engine in 3 minutes&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example8.py"&gt;be found example8.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As the first example, we will implement a simple QA search engine using &lt;code&gt;bert-as-service&lt;/code&gt; in just three minutes. No kidding! The goal is to find similar questions to user's input and return the corresponding answer. To start, we need a list of question-answer pairs. Fortunately, this README file already contains &lt;a href="#speech_balloon-faq"&gt;a list of FAQ&lt;/a&gt;, so I will just use that to make this example perfectly self-contained. Let's first load all questions and show some statistics.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;prefix_q &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;##### **Q:** &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;README.md&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; fp:
    questions &lt;span class="pl-k"&gt;=&lt;/span&gt; [v.replace(prefix_q, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).strip() &lt;span class="pl-k"&gt;for&lt;/span&gt; v &lt;span class="pl-k"&gt;in&lt;/span&gt; fp &lt;span class="pl-k"&gt;if&lt;/span&gt; v.strip() &lt;span class="pl-k"&gt;and&lt;/span&gt; v.startswith(prefix_q)]
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-c1"&gt;%d&lt;/span&gt; questions loaded, avg. len of &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (&lt;span class="pl-c1"&gt;len&lt;/span&gt;(questions), np.mean([&lt;span class="pl-c1"&gt;len&lt;/span&gt;(d.split()) &lt;span class="pl-k"&gt;for&lt;/span&gt; d &lt;span class="pl-k"&gt;in&lt;/span&gt; questions])))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives &lt;code&gt;33 questions loaded, avg. len of 9&lt;/code&gt;. So looks like we have enough questions. Now start a BertServer with &lt;code&gt;uncased_L-12_H-768_A-12&lt;/code&gt; pretrained BERT model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -num_worker=1 -model_dir=/data/cips/data/lab/data/model/uncased_L-12_H-768_A-12&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we need to encode our questions into vectors:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;port&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4000&lt;/span&gt;, &lt;span class="pl-v"&gt;port_out&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4001&lt;/span&gt;)
doc_vecs &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(questions)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we are ready to receive new query and perform a simple "fuzzy" search against the existing questions. To do that, every time a new query is coming, we encode it as a vector and compute its dot product with &lt;code&gt;doc_vecs&lt;/code&gt;; sort the result descendingly; and return the top-k similar questions as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
    query &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;input&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;your question: &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    query_vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode([query])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; compute normalized dot product as score&lt;/span&gt;
    score &lt;span class="pl-k"&gt;=&lt;/span&gt; np.sum(query_vec &lt;span class="pl-k"&gt;*&lt;/span&gt; doc_vecs, &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;) &lt;span class="pl-k"&gt;/&lt;/span&gt; np.linalg.norm(doc_vecs, &lt;span class="pl-v"&gt;axis&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)
    topk_idx &lt;span class="pl-k"&gt;=&lt;/span&gt; np.argsort(score)[::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;][:topk]
    &lt;span class="pl-k"&gt;for&lt;/span&gt; idx &lt;span class="pl-k"&gt;in&lt;/span&gt; topk_idx:
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&amp;gt; &lt;span class="pl-c1"&gt;%s&lt;/span&gt;&lt;span class="pl-cce"&gt;\t&lt;/span&gt;&lt;span class="pl-c1"&gt;%s&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (score[idx], questions[idx]))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! Now run the code and type your query, see how this search engine handles fuzzy match:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/qasearch-demo.gif?raw=true"&gt;&lt;img src=".github/qasearch-demo.gif?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serving-a-fine-tuned-bert-model" class="anchor" aria-hidden="true" href="#serving-a-fine-tuned-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serving a fine-tuned BERT model&lt;/h3&gt;
&lt;p&gt;Pretrained BERT models often show quite "okayish" performance on many tasks. However, to release the true power of BERT a fine-tuning on the downstream task (or on domain-specific data) is necessary. In this example, I will show you how to serve a fine-tuned BERT model.&lt;/p&gt;
&lt;p&gt;We follow the instruction in &lt;a href="https://github.com/google-research/bert#sentence-and-sentence-pair-classification-tasks"&gt;"Sentence (and sentence-pair) classification tasks"&lt;/a&gt; and use &lt;code&gt;run_classifier.py&lt;/code&gt; to fine tune &lt;code&gt;uncased_L-12_H-768_A-12&lt;/code&gt; model on MRPC task. The fine-tuned model is stored at &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;, which can be changed by specifying &lt;code&gt;--output_dir&lt;/code&gt; of &lt;code&gt;run_classifier.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you look into &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;, it contains something like:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;checkpoint                                        128
&lt;span class="pl-c1"&gt;eval&lt;/span&gt;                                              4.0K
eval_results.txt                                  86
eval.tf_record                                    219K
events.out.tfevents.1545202214.TENCENT64.site     6.1M
events.out.tfevents.1545203242.TENCENT64.site     14M
graph.pbtxt                                       9.0M
model.ckpt-0.data-00000-of-00001                  1.3G
model.ckpt-0.index                                23K
model.ckpt-0.meta                                 3.9M
model.ckpt-343.data-00000-of-00001                1.3G
model.ckpt-343.index                              23K
model.ckpt-343.meta                               3.9M
train.tf_record                                   2.0M&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Don't be afraid of those mysterious files, as the only important one to us is &lt;code&gt;model.ckpt-343.data-00000-of-00001&lt;/code&gt; (looks like my training stops at the 343 step. One may get &lt;code&gt;model.ckpt-123.data-00000-of-00001&lt;/code&gt; or &lt;code&gt;model.ckpt-9876.data-00000-of-00001&lt;/code&gt; depending on the total training steps). Now we have collected all three pieces of information that are needed for serving this fine-tuned model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The pretrained model is downloaded to &lt;code&gt;/path/to/bert/uncased_L-12_H-768_A-12&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Our fine-tuned model is stored at &lt;code&gt;/tmp/mrpc_output/&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Our fine-tuned model checkpoint is named as &lt;code&gt;model.ckpt-343&lt;/code&gt; something something.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now start a BertServer by putting three pieces together:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir=/pretrained/uncased_L-12_H-768_A-12 -tuned_model_dir=/tmp/mrpc_output/ -ckpt_name=model.ckpt-343&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the server started, you should find this line in the log:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;I:GRAPHOPT:[gra:opt: 50]:checkpoint (override by fine-tuned model): /tmp/mrpc_output/model.ckpt-343
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which means the BERT parameters is overrode and successfully loaded from our fine-tuned &lt;code&gt;/tmp/mrpc_output/model.ckpt-343&lt;/code&gt;. Done!&lt;/p&gt;
&lt;p&gt;In short, find your fine-tuned model path and checkpoint name, then feed them to &lt;code&gt;-tuned_model_dir&lt;/code&gt; and &lt;code&gt;-ckpt_name&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-getting-elmo-like-contextual-word-embedding" class="anchor" aria-hidden="true" href="#getting-elmo-like-contextual-word-embedding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting ELMo-like contextual word embedding&lt;/h3&gt;
&lt;p&gt;Start the server with &lt;code&gt;pooling_strategy&lt;/code&gt; set to NONE.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -pooling_strategy NONE -model_dir /tmp/english_L-12_H-768_A-12/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To get the word embedding corresponds to every token, you can simply use slice index as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; max_seq_len = 25&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pooling_strategy = NONE&lt;/span&gt;

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hey you&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;whats up?&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])

vec  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [2, 25, 768]&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 25, 768], sentence embeddings for `hey you`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `[CLS]`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;1&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `hey`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;2&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `you`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;3&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for `[SEP]`&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;4&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; [1, 1, 768], word embedding for padding symbol&lt;/span&gt;
vec[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;25&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; error, out of index!&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that no matter how long your original sequence is, the service will always return a &lt;code&gt;[max_seq_len, 768]&lt;/code&gt; matrix for every sequence. When using slice index to get the word embedding, beware of the special tokens padded to the sequence, i.e. &lt;code&gt;[CLS]&lt;/code&gt;, &lt;code&gt;[SEP]&lt;/code&gt;, &lt;code&gt;0_PAD&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-your-own-tokenizer" class="anchor" aria-hidden="true" href="#using-your-own-tokenizer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using your own tokenizer&lt;/h3&gt;
&lt;p&gt;Often you want to use your own tokenizer to segment sentences instead of the default one from BERT. Simply call &lt;code&gt;encode(is_tokenized=True)&lt;/code&gt; on the client slide as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;texts &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;good day&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; a naive whitespace tokenizer&lt;/span&gt;
texts2 &lt;span class="pl-k"&gt;=&lt;/span&gt; [s.split() &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; texts]

vecs &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(texts2, &lt;span class="pl-v"&gt;is_tokenized&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives &lt;code&gt;[2, 25, 768]&lt;/code&gt; tensor where the first &lt;code&gt;[1, 25, 768]&lt;/code&gt; corresponds to the token-level encoding of "hello world!". If you look into its values, you will find that only the first four elements, i.e. &lt;code&gt;[1, 0:3, 768]&lt;/code&gt; have values, all the others are zeros. This is due to the fact that BERT considers "hello world!" as four tokens: &lt;code&gt;[CLS]&lt;/code&gt; &lt;code&gt;hello&lt;/code&gt; &lt;code&gt;world!&lt;/code&gt; &lt;code&gt;[SEP]&lt;/code&gt;, the rest are padding symbols and are masked out before output.&lt;/p&gt;
&lt;p&gt;Note that there is no need to start a separate server for handling tokenized/untokenized sentences. The server can tell and handle both cases automatically.&lt;/p&gt;
&lt;p&gt;Sometimes you want to know explicitly the tokenization performed on the server side to have better understanding of the embedding result. One such case is asking word embedding from the server (with &lt;code&gt;-pooling_strategy NONE&lt;/code&gt;), one wants to tell which word is tokenized and which is unrecognized. You can get such information with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;enabling &lt;code&gt;-show_tokens_to_client&lt;/code&gt; on the server side;&lt;/li&gt;
&lt;li&gt;calling the server via &lt;code&gt;encode(..., show_tokens=True)&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, a basic usage like&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thisis it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;show_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;returns a tuple, where the first element is the embedding and the second is the tokenization result from the server:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;(array([[[ 0.        , -0.        ,  0.        , ...,  0.        , -0.        , -0.        ],
        [ 1.1100919 , -0.20474958,  0.9895898 , ...,  0.3873255  , -1.4093989 , -0.47620595],
        ..., -0.        , -0.        ]],

       [[ 0.        , -0.        ,  0.        , ...,  0.        , 0.        ,  0.        ],
        [ 0.6293478 , -0.4088499 ,  0.6022662 , ...,  0.41740108, 1.214456  ,  1.2532915 ],
        ..., 0.        ,  0.        ]]], dtype=float32),
         
          [['[CLS]', 'hello', 'world', '!', '[SEP]'], ['[CLS]', 'this', '##is', 'it', '[SEP]']])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When using your own tokenization, you may still want to check if the server respects your tokens. For example,&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hello&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;world!&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;thisis&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;it&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]], &lt;span class="pl-v"&gt;show_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;is_tokenized&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;returns:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;(array([[[ 0.        , -0.        ,  0.        , ...,  0.       ,  -0.        ,  0.        ],
        [ 1.1111546 , -0.56572634,  0.37183186, ...,  0.02397121,  -0.5445367 ,  1.1009651 ],
        ..., -0.        ,  0.        ]],

       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,  -0.        ,  0.        ],
        [ 0.39262453,  0.3782491 ,  0.27096173, ...,  0.7122045 ,  -0.9874849 ,  0.9318679 ],
        ..., -0.        ,  0.        ]]], dtype=float32),
         
         [['[CLS]', 'hello', '[UNK]', '[SEP]'], ['[CLS]', '[UNK]', 'it', '[SEP]']])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One can observe that &lt;code&gt;world!&lt;/code&gt; and &lt;code&gt;thisis&lt;/code&gt; are not recognized on the server, hence they are set to &lt;code&gt;[UNK]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, beware that the pretrained BERT Chinese from Google is character-based, i.e. its vocabulary is made of single Chinese characters. Therefore it makes no sense if you use word-level segmentation algorithm to pre-process the data and feed to such model.&lt;/p&gt;
&lt;p&gt;Extremely curious readers may notice that the first row in the above example is all-zero even though the tokenization result includes &lt;code&gt;[CLS]&lt;/code&gt; (well done, detective!). The reason is that the tokenization result will &lt;strong&gt;always&lt;/strong&gt; includes &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[UNK]&lt;/code&gt; regardless the setting of &lt;code&gt;-mask_cls_sep&lt;/code&gt;. This could be useful when you want to align the tokens afterwards. Remember, &lt;code&gt;-mask_cls_sep&lt;/code&gt; only masks &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; out of the computation. It doesn't affect the tokenization algorithm.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-bertclient-with-tfdata-api" class="anchor" aria-hidden="true" href="#using-bertclient-with-tfdata-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;BertClient&lt;/code&gt; with &lt;code&gt;tf.data&lt;/code&gt; API&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example4.py"&gt;be found example4.py&lt;/a&gt;. There is also &lt;a href="https://github.com/hanxiao/bert-as-service/issues/29#issuecomment-442362241"&gt;an example in Keras&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;a href="https://www.tensorflow.org/guide/datasets" rel="nofollow"&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt; API enables you to build complex input pipelines from simple, reusable pieces. One can also use &lt;code&gt;BertClient&lt;/code&gt; to encode sentences on-the-fly and use the vectors in a downstream model. Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;batch_size &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;256&lt;/span&gt;
num_parallel_calls &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;4&lt;/span&gt;
num_clients &lt;span class="pl-k"&gt;=&lt;/span&gt; num_parallel_calls &lt;span class="pl-k"&gt;*&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; should be at least greater than `num_parallel_calls`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; start a pool of clients&lt;/span&gt;
bc_clients &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertClient(&lt;span class="pl-v"&gt;show_server_config&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;) &lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(num_clients)]


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;get_encodes&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;):
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; x is `batch_size` of lines, each of which is a json object&lt;/span&gt;
    samples &lt;span class="pl-k"&gt;=&lt;/span&gt; [json.loads(l) &lt;span class="pl-k"&gt;for&lt;/span&gt; l &lt;span class="pl-k"&gt;in&lt;/span&gt; x]
    text &lt;span class="pl-k"&gt;=&lt;/span&gt; [s[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;raw_text&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; samples]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; List[List[str]]&lt;/span&gt;
    labels &lt;span class="pl-k"&gt;=&lt;/span&gt; [s[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;label&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; samples]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; List[str]&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get a client from available clients&lt;/span&gt;
    bc_client &lt;span class="pl-k"&gt;=&lt;/span&gt; bc_clients.pop()
    features &lt;span class="pl-k"&gt;=&lt;/span&gt; bc_client.encode(text)
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; after use, put it back&lt;/span&gt;
    bc_clients.append(bc_client)
    &lt;span class="pl-k"&gt;return&lt;/span&gt; features, labels


ds &lt;span class="pl-k"&gt;=&lt;/span&gt; (tf.data.TextLineDataset(train_fp).batch(batch_size)
        .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: tf.py_func(get_encodes, [x], [tf.float32, tf.string]),  &lt;span class="pl-v"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_parallel_calls)
        .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: x, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;label&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: y})
        .make_one_shot_iterator().get_next())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The trick here is to start a pool of &lt;code&gt;BertClient&lt;/code&gt; and reuse them one by one. In this way, we can fully harness the power of &lt;code&gt;num_parallel_calls&lt;/code&gt; of &lt;code&gt;Dataset.map()&lt;/code&gt; API.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training-a-text-classifier-using-bert-features-and-tfestimator-api" class="anchor" aria-hidden="true" href="#training-a-text-classifier-using-bert-features-and-tfestimator-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training a text classifier using BERT features and &lt;code&gt;tf.estimator&lt;/code&gt; API&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example5.py"&gt;be found example5.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Following the last example, we can easily extend it to a full classifier using &lt;code&gt;tf.estimator&lt;/code&gt; API. One only need minor change on the input function as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;estimator &lt;span class="pl-k"&gt;=&lt;/span&gt; DNNClassifier(
    &lt;span class="pl-v"&gt;hidden_units&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[&lt;span class="pl-c1"&gt;512&lt;/span&gt;],
    &lt;span class="pl-v"&gt;feature_columns&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[tf.feature_column.numeric_column(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;shape&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(&lt;span class="pl-c1"&gt;768&lt;/span&gt;,))],
    &lt;span class="pl-v"&gt;n_classes&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;len&lt;/span&gt;(laws),
    &lt;span class="pl-v"&gt;config&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;run_config,
    &lt;span class="pl-v"&gt;label_vocabulary&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;laws_str,
    &lt;span class="pl-v"&gt;dropout&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.1&lt;/span&gt;)

input_fn &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;fp&lt;/span&gt;: (tf.data.TextLineDataset(fp)
                       .apply(tf.contrib.data.shuffle_and_repeat(&lt;span class="pl-v"&gt;buffer_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10000&lt;/span&gt;))
                       .batch(batch_size)
                       .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: tf.py_func(get_encodes, [x], [tf.float32, tf.string]), &lt;span class="pl-v"&gt;num_parallel_calls&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_parallel_calls)
                       .map(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: ({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: x}, y))
                       .prefetch(&lt;span class="pl-c1"&gt;20&lt;/span&gt;))

train_spec &lt;span class="pl-k"&gt;=&lt;/span&gt; TrainSpec(&lt;span class="pl-v"&gt;input_fn&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;lambda&lt;/span&gt;: input_fn(train_fp))
eval_spec &lt;span class="pl-k"&gt;=&lt;/span&gt; EvalSpec(&lt;span class="pl-v"&gt;input_fn&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-k"&gt;lambda&lt;/span&gt;: input_fn(eval_fp), &lt;span class="pl-v"&gt;throttle_secs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
train_and_evaluate(estimator, train_spec, eval_spec)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complete example can &lt;a href="example/example5.py"&gt;be found example5.py&lt;/a&gt;, in which a simple MLP is built on BERT features for predicting the relevant articles according to the fact description in the law documents. The problem is a part of the &lt;a href="https://github.com/thunlp/CAIL/blob/master/README_en.md"&gt;Chinese AI and Law Challenge Competition&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-saving-and-loading-with-tfrecord-data" class="anchor" aria-hidden="true" href="#saving-and-loading-with-tfrecord-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Saving and loading with TFRecord data&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example6.py"&gt;be found example6.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. You can also pre-encode all your sequences and store their encodings to a TFRecord file, then later load it to build a &lt;code&gt;tf.Dataset&lt;/code&gt;. For example, to write encoding into a TFRecord file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
list_vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(lst_str)
list_label &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; lst_str]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; a dummy list of all-zero labels&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; write to tfrecord&lt;/span&gt;
&lt;span class="pl-k"&gt;with&lt;/span&gt; tf.python_io.TFRecordWriter(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tmp.tfrecord&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; writer:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_float_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;float_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.FloatList(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;values))

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_int_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;int64_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.Int64List(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;list&lt;/span&gt;(values)))

    &lt;span class="pl-k"&gt;for&lt;/span&gt; (vec, label) &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;zip&lt;/span&gt;(list_vec, list_label):
        features &lt;span class="pl-k"&gt;=&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;features&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: create_float_feature(vec), &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;labels&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: create_int_feature([label])}
        tf_example &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.train.Example(&lt;span class="pl-v"&gt;features&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.Features(&lt;span class="pl-v"&gt;feature&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;features))
        writer.write(tf_example.SerializeToString())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can load from it and build a &lt;code&gt;tf.Dataset&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_decode_record&lt;/span&gt;(&lt;span class="pl-smi"&gt;record&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Decodes a record to a TensorFlow example.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.parse_single_example(record, {
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;features&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([&lt;span class="pl-c1"&gt;768&lt;/span&gt;], tf.float32),
        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;labels&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([], tf.int64),
    })

ds &lt;span class="pl-k"&gt;=&lt;/span&gt; (tf.data.TFRecordDataset(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;tmp.tfrecord&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;).repeat().shuffle(&lt;span class="pl-v"&gt;buffer_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;).apply(
    tf.contrib.data.map_and_batch(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;record&lt;/span&gt;: _decode_record(record), &lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;))
      .make_one_shot_iterator().get_next())&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To save word/token-level embedding to TFRecord, one needs to first flatten &lt;code&gt;[max_seq_len, num_hidden]&lt;/code&gt; tensor into an 1D array as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;create_float_feature&lt;/span&gt;(&lt;span class="pl-smi"&gt;values&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; tf.train.Feature(&lt;span class="pl-v"&gt;float_list&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;tf.train.FloatList(&lt;span class="pl-v"&gt;value&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;values.reshape(&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And later reconstruct the shape when loading it:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;name_to_features &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;feature&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([max_seq_length &lt;span class="pl-k"&gt;*&lt;/span&gt; num_hidden], tf.float32),
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;label_ids&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: tf.FixedLenFeature([], tf.int64),
}
    
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;_decode_record&lt;/span&gt;(&lt;span class="pl-smi"&gt;record&lt;/span&gt;, &lt;span class="pl-smi"&gt;name_to_features&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Decodes a record to a TensorFlow example.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    example &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.parse_single_example(record, name_to_features)
    example[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.reshape(example[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;feature&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], [max_seq_length, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;])
    &lt;span class="pl-k"&gt;return&lt;/span&gt; example&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be careful, this will generate a huge TFRecord file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-asynchronous-encoding" class="anchor" aria-hidden="true" href="#asynchronous-encoding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Asynchronous encoding&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example2.py"&gt;be found example2.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;BertClient.encode()&lt;/code&gt; offers a nice synchronous way to get sentence encodes. However,   sometimes we want to do it in an asynchronous manner by feeding all textual data to the server first, fetching the encoded results later. This can be easily done by:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; an endless data stream, generating data in an extremely fast speed&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;text_gen&lt;/span&gt;():
    &lt;span class="pl-k"&gt;while&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;:
        &lt;span class="pl-k"&gt;yield&lt;/span&gt; lst_str  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; yield a batch of text lines&lt;/span&gt;

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get encoded vectors&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; bc.encode_async(text_gen(), &lt;span class="pl-v"&gt;max_num_batch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;10&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;received &lt;span class="pl-c1"&gt;%d&lt;/span&gt; x &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (j.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], j.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]))&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-broadcasting-to-multiple-clients" class="anchor" aria-hidden="true" href="#broadcasting-to-multiple-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Broadcasting to multiple clients&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="example/example3.py"&gt;be found in example3.py&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The encoded result is routed to the client according to its identity. If you have multiple clients with same identity, then they all receive the results! You can use this &lt;em&gt;multicast&lt;/em&gt; feature to do some cool things, e.g. training multiple different models (some using &lt;code&gt;scikit-learn&lt;/code&gt; some using &lt;code&gt;tensorflow&lt;/code&gt;) in multiple separated processes while only call &lt;code&gt;BertServer&lt;/code&gt; once. In the example below, &lt;code&gt;bc&lt;/code&gt; and its two clones will all receive encoded vector.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; clone a client by reusing the identity &lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;client_clone&lt;/span&gt;(&lt;span class="pl-smi"&gt;id&lt;/span&gt;, &lt;span class="pl-smi"&gt;idx&lt;/span&gt;):
    bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;identity&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;id&lt;/span&gt;)
    &lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; bc.listen():
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;clone-client-&lt;span class="pl-c1"&gt;%d&lt;/span&gt;: received &lt;span class="pl-c1"&gt;%d&lt;/span&gt; x &lt;span class="pl-c1"&gt;%d&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;%&lt;/span&gt; (idx, j.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], j.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;]))

bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; start two cloned clients sharing the same identity as bc&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; j &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;2&lt;/span&gt;):
    threading.Thread(&lt;span class="pl-v"&gt;target&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;client_clone, &lt;span class="pl-v"&gt;args&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(bc.identity, j)).start()

&lt;span class="pl-k"&gt;for&lt;/span&gt; _ &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;3&lt;/span&gt;):
    bc.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-monitoring-the-service-status-in-a-dashboard" class="anchor" aria-hidden="true" href="#monitoring-the-service-status-in-a-dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monitoring the service status in a dashboard&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The complete example can &lt;a href="plugin/dashboard"&gt;be found in plugin/dashboard/&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a part of the infrastructure, one may also want to monitor the service status and show it in a dashboard. To do that, we can use:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient(&lt;span class="pl-v"&gt;ip&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;server_ip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

json.dumps(bc.server_status, &lt;span class="pl-v"&gt;ensure_ascii&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives the current status of the server including number of requests, number of clients etc. in JSON format. The only thing remained is to start a HTTP server for returning this JSON to the frontend that renders it.&lt;/p&gt;
&lt;p&gt;Alternatively, one may simply expose an HTTP port when starting a server via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -http_port 8001 -model_dir ...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will allow one to use javascript or &lt;code&gt;curl&lt;/code&gt; to fetch the server status at port 8001.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plugin/dashboard/index.html&lt;/code&gt; shows a simple dashboard based on Bootstrap and Vue.js.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/dashboard.png?raw=true"&gt;&lt;img src=".github/dashboard.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-bert-as-service-to-serve-http-requests-in-json" class="anchor" aria-hidden="true" href="#using-bert-as-service-to-serve-http-requests-in-json"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;bert-as-service&lt;/code&gt; to serve HTTP requests in JSON&lt;/h3&gt;
&lt;p&gt;Besides calling &lt;code&gt;bert-as-service&lt;/code&gt; from Python, one can also call it via HTTP request in JSON. It is quite useful especially when low transport layer is prohibited. Behind the scene, &lt;code&gt;bert-as-service&lt;/code&gt; spawns a Flask server in a separate process and then reuse a &lt;code&gt;BertClient&lt;/code&gt; instance as a proxy to communicate with the ventilator.&lt;/p&gt;
&lt;p&gt;To enable the build-in HTTP server, we need to first (re)install the server with some extra Python dependencies:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -U bert-serving-server[http]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then simply start the server with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -model_dir=/YOUR_MODEL -http_port 8125&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Done! Your server is now listening HTTP and TCP requests at port &lt;code&gt;8125&lt;/code&gt; simultaneously!&lt;/p&gt;
&lt;p&gt;To send a HTTP request, first prepare the payload in JSON as following:&lt;/p&gt;
&lt;div class="highlight highlight-source-json"&gt;&lt;pre&gt;{
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;123&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;texts&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;hello world&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;good day!&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;is_tokenized&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;false&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;, where &lt;code&gt;id&lt;/code&gt; is a unique identifier helping you to synchronize the results; &lt;code&gt;is_tokenized&lt;/code&gt; follows the meaning in &lt;a href="https://bert-as-service.readthedocs.io/en/latest/source/client.html#client.BertClient.encode_async" rel="nofollow"&gt;&lt;code&gt;BertClient&lt;/code&gt; API&lt;/a&gt; and &lt;code&gt;false&lt;/code&gt; by default.&lt;/p&gt;
&lt;p&gt;Then simply call the server at &lt;code&gt;/encode&lt;/code&gt; via HTTP POST request. You can use javascript or whatever, here is an example using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;curl -X POST http://xx.xx.xx.xx:8125/encode \
  -H &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;content-type: application/json&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; \
  -d &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;{"id": 123,"texts": ["hello world"], "is_tokenized": false}&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;, which returns a JSON:&lt;/p&gt;
&lt;div class="highlight highlight-source-json"&gt;&lt;pre&gt;{
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;id&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;123&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;results&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: [[&lt;span class="pl-c1"&gt;768&lt;/span&gt; &lt;span class="pl-ii"&gt;float-list&lt;/span&gt;], [&lt;span class="pl-c1"&gt;768&lt;/span&gt; &lt;span class="pl-ii"&gt;float-list&lt;/span&gt;]],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;status&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;200&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To get the server's status and client's status, you can send GET requests at &lt;code&gt;/status/server&lt;/code&gt; and &lt;code&gt;/status/client&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Finally, one may also config CORS to restrict the public access of the server by specifying &lt;code&gt;-cors&lt;/code&gt; when starting &lt;code&gt;bert-serving-start&lt;/code&gt;. By default &lt;code&gt;-cors=*&lt;/code&gt;, meaning the server is public accessible.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-starting-bertserver-from-python" class="anchor" aria-hidden="true" href="#starting-bertserver-from-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting &lt;code&gt;BertServer&lt;/code&gt; from Python&lt;/h3&gt;
&lt;p&gt;Besides shell, one can also start a &lt;code&gt;BertServer&lt;/code&gt; from python. Simply do&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.server.helper &lt;span class="pl-k"&gt;import&lt;/span&gt; get_args_parser
&lt;span class="pl-k"&gt;from&lt;/span&gt; bert_serving.server &lt;span class="pl-k"&gt;import&lt;/span&gt; BertServer
args &lt;span class="pl-k"&gt;=&lt;/span&gt; get_args_parser().parse_args([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-model_dir&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;YOUR_MODEL_PATH_HERE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-port&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;5555&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-port_out&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;5556&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-max_seq_len&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NONE&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-mask_cls_sep&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
                                     &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;-cpu&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
server &lt;span class="pl-k"&gt;=&lt;/span&gt; BertServer(args)
server.start()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that it's basically mirroring the arg-parsing behavior in CLI, so everything in that &lt;code&gt;.parse_args([])&lt;/code&gt; list should be string, e.g. &lt;code&gt;['-port', '5555']&lt;/code&gt; not &lt;code&gt;['-port', 5555]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To shutdown the server, you may call the static method in &lt;code&gt;BertServer&lt;/code&gt; class via:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;BertServer.shutdown(&lt;span class="pl-v"&gt;port&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;5555&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or via shell CLI:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-terminate -port 5555&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will terminate the server running on localhost at port 5555. You may also use it to terminate a remote server, see &lt;code&gt;bert-serving-terminate --help&lt;/code&gt; for details.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-speech_balloon-faq" class="anchor" aria-hidden="true" href="#speech_balloon-faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;💬&lt;/g-emoji&gt; FAQ&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/faq.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-you-have-a-paper-or-other-written-explanation-to-introduce-your-models-details" class="anchor" aria-hidden="true" href="#q-do-you-have-a-paper-or-other-written-explanation-to-introduce-your-models-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do you have a paper or other written explanation to introduce your model's details?&lt;/h5&gt;
&lt;p&gt;The design philosophy and technical details can be found &lt;a href="https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/" rel="nofollow"&gt;in my blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-where-is-the-bert-code-come-from" class="anchor" aria-hidden="true" href="#q-where-is-the-bert-code-come-from"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Where is the BERT code come from?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; &lt;a href="server/bert_serving/server/bert/"&gt;BERT code of this repo&lt;/a&gt; is forked from the &lt;a href="https://github.com/google-research/bert"&gt;original BERT repo&lt;/a&gt; with necessary modification, &lt;a href="server/bert_serving/server/bert/extract_features.py"&gt;especially in extract_features.py&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-large-is-a-sentence-vector" class="anchor" aria-hidden="true" href="#q-how-large-is-a-sentence-vector"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How large is a sentence vector?&lt;/h5&gt;
&lt;p&gt;In general, each sentence is translated to a 768-dimensional vector. Depending on the pretrained BERT you are using, &lt;code&gt;pooling_strategy&lt;/code&gt; and &lt;code&gt;pooling_layer&lt;/code&gt; the dimensions of the output vector could be different.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-do-you-get-the-fixed-representation-did-you-do-pooling-or-something" class="anchor" aria-hidden="true" href="#q-how-do-you-get-the-fixed-representation-did-you-do-pooling-or-something"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How do you get the fixed representation? Did you do pooling or something?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, pooling is required to get a fixed representation of a sentence. In the default strategy &lt;code&gt;REDUCE_MEAN&lt;/code&gt;, I take the second-to-last hidden layer of all of the tokens in the sentence and do average pooling.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-are-you-suggesting-using-bert-without-fine-tuning" class="anchor" aria-hidden="true" href="#q-are-you-suggesting-using-bert-without-fine-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Are you suggesting using BERT without fine-tuning?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes and no. On the one hand, Google pretrained BERT on Wikipedia data, thus should encode enough prior knowledge of the language into the model. Having such feature is not a bad idea. On the other hand, these prior knowledge is not specific to any particular domain. It should be totally reasonable if the performance is not ideal if you are using it on, for example, classifying legal cases. Nonetheless, you can always first fine-tune your own BERT on the downstream task and then use &lt;code&gt;bert-as-service&lt;/code&gt; to extract the feature vectors efficiently. Keep in mind that &lt;code&gt;bert-as-service&lt;/code&gt; is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-get-a-concatenation-of-several-layers-instead-of-a-single-layer-" class="anchor" aria-hidden="true" href="#q-can-i-get-a-concatenation-of-several-layers-instead-of-a-single-layer-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I get a concatenation of several layers instead of a single layer ?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Sure! Just use a list of the layer you want to concatenate when calling the server. Example:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -pooling_layer -4 -3 -2 -1 -model_dir /tmp/english_L-12_H-768_A-12/&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-are-the-available-pooling-strategies" class="anchor" aria-hidden="true" href="#q-what-are-the-available-pooling-strategies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What are the available pooling strategies?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Here is a table summarizes all pooling strategies I implemented. Choose your favorite one by specifying &lt;code&gt;bert-serving-start -pooling_strategy&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Strategy&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;NONE&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;no pooling at all, useful when you want to use word embedding instead of sentence embedding. This will results in a &lt;code&gt;[max_seq_len, 768]&lt;/code&gt; encode matrix for a sequence.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;take the average of the hidden state of encoding layer on the time axis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MAX&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;take the maximum of the hidden state of encoding layer on the time axis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;REDUCE_MEAN_MAX&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;do &lt;code&gt;REDUCE_MEAN&lt;/code&gt; and &lt;code&gt;REDUCE_MAX&lt;/code&gt; separately and then concat them together on the last axis, resulting in 1536-dim sentence encodes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;CLS_TOKEN&lt;/code&gt; or &lt;code&gt;FIRST_TOKEN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;get the hidden state corresponding to &lt;code&gt;[CLS]&lt;/code&gt;, i.e. the first token&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;SEP_TOKEN&lt;/code&gt; or &lt;code&gt;LAST_TOKEN&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;get the hidden state corresponding to &lt;code&gt;[SEP]&lt;/code&gt;, i.e. the last token&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls" class="anchor" aria-hidden="true" href="#q-why-not-use-the-hidden-state-of-the-first-token-as-default-strategy-ie-the-cls"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why not use the hidden state of the first token as default strategy, i.e. the &lt;code&gt;[CLS]&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Because a pre-trained model is not fine-tuned on any downstream tasks yet. In this case, the hidden state of &lt;code&gt;[CLS]&lt;/code&gt; is not a good sentence representation. If later you fine-tune the model, you may use &lt;code&gt;[CLS]&lt;/code&gt; as well.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-bert-has-1224-layers-so-which-layer-are-you-talking-about" class="anchor" aria-hidden="true" href="#q-bert-has-1224-layers-so-which-layer-are-you-talking-about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; BERT has 12/24 layers, so which layer are you talking about?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; By default this service works on the second last layer, i.e. &lt;code&gt;pooling_layer=-2&lt;/code&gt;. You can change it by setting &lt;code&gt;pooling_layer&lt;/code&gt; to other negative values, e.g. -1 corresponds to the last layer.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-not-the-last-hidden-layer-why-second-to-last" class="anchor" aria-hidden="true" href="#q-why-not-the-last-hidden-layer-why-second-to-last"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why not the last hidden layer? Why second-to-last?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets. If you question about this argument and want to use the last hidden layer anyway, please feel free to set &lt;code&gt;pooling_layer=-1&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-so-which-layer-and-which-pooling-strategy-is-the-best" class="anchor" aria-hidden="true" href="#q-so-which-layer-and-which-pooling-strategy-is-the-best"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; So which layer and which pooling strategy is the best?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It depends. Keep in mind that different BERT layers capture different information. To see that more clearly, here is a visualization on &lt;a href="https://www.kaggle.com/uciml/news-aggregator-dataset" rel="nofollow"&gt;UCI-News Aggregator Dataset&lt;/a&gt;, where I randomly sample 20K news titles; get sentence encodes from different layers and with different pooling strategies, finally reduce it to 2D via PCA (one can of course do t-SNE as well, but that's not my point). There are only four classes of the data, illustrated in red, blue, yellow and green. To reproduce the result, please run &lt;a href="example/example7.py"&gt;example7.py&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pool_mean.png?raw=true"&gt;&lt;img src=".github/pool_mean.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pool_max.png?raw=true"&gt;&lt;img src=".github/pool_max.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Intuitively, &lt;code&gt;pooling_layer=-1&lt;/code&gt; is close to the training output, so it may be biased to the training targets. If you don't fine tune the model, then this could lead to a bad representation. &lt;code&gt;pooling_layer=-12&lt;/code&gt; is close to the word embedding, may preserve the very original word information (with no fancy self-attention etc.). On the other hand, you may achieve the very same performance by simply using a word-embedding only. That said, anything in-between [-1, -12] is then a trade-off.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-could-i-use-other-pooling-techniques" class="anchor" aria-hidden="true" href="#q-could-i-use-other-pooling-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Could I use other pooling techniques?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; For sure. But if you introduce new &lt;code&gt;tf.variables&lt;/code&gt; to the graph, then you need to train those variables before using the model. You may also want to check &lt;a href="https://hanxiao.github.io/2018/06/24/4-Encoding-Blocks-You-Need-to-Know-Besides-LSTM-RNN-in-Tensorflow/#pooling-block" rel="nofollow"&gt;some pooling techniques I mentioned in my blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-to-batch-the-data-before-encode" class="anchor" aria-hidden="true" href="#q-do-i-need-to-batch-the-data-before-encode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need to batch the data before &lt;code&gt;encode()&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;No, not at all. Just do &lt;code&gt;encode&lt;/code&gt; and let the server handles the rest. If the batch is too large, the server will do batching automatically and it is more efficient than doing it by yourself. No matter how many sentences you have, 10K or 100K, as long as you can hold it in client's memory, just send it to the server. Please also read &lt;a href="https://github.com/hanxiao/bert-as-service#speed-wrt-client_batch_size"&gt;the benchmark on the client batch size&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-start-multiple-clients-and-send-requests-to-one-server-simultaneously" class="anchor" aria-hidden="true" href="#q-can-i-start-multiple-clients-and-send-requests-to-one-server-simultaneously"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I start multiple clients and send requests to one server simultaneously?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes! That's the purpose of this repo. In fact you can start as many clients as you want. One server can handle all of them (given enough time).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-many-requests-can-one-service-handle-concurrently" class="anchor" aria-hidden="true" href="#q-how-many-requests-can-one-service-handle-concurrently"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How many requests can one service handle concurrently?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The maximum number of concurrent requests is determined by &lt;code&gt;num_worker&lt;/code&gt; in &lt;code&gt;bert-serving-start&lt;/code&gt;. If you a sending more than &lt;code&gt;num_worker&lt;/code&gt; requests concurrently, the new requests will be temporally stored in a queue until a free worker becomes available.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-so-one-request-means-one-sentence" class="anchor" aria-hidden="true" href="#q-so-one-request-means-one-sentence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; So one request means one sentence?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. One request means a list of sentences sent from a client. Think the size of a request as the batch size. A request may contain 256, 512 or 1024 sentences. The optimal size of a request is often determined empirically. One large request can certainly improve the GPU utilization, yet it also increases the overhead of transmission. You may run &lt;code&gt;python example/example1.py&lt;/code&gt; for a simple benchmark.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-about-the-speed-is-it-fast-enough-for-production" class="anchor" aria-hidden="true" href="#q-how-about-the-speed-is-it-fast-enough-for-production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How about the speed? Is it fast enough for production?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It highly depends on the &lt;code&gt;max_seq_len&lt;/code&gt; and the size of a request. On a single Tesla M40 24GB with &lt;code&gt;max_seq_len=40&lt;/code&gt;, you should get about 470 samples per second using a 12-layer BERT. In general, I'd suggest smaller &lt;code&gt;max_seq_len&lt;/code&gt; (25) and larger request size (512/1024).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-did-you-benchmark-the-efficiency" class="anchor" aria-hidden="true" href="#q-did-you-benchmark-the-efficiency"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Did you benchmark the efficiency?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. See &lt;a href="#zap-benchmark"&gt;Benchmark&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To reproduce the results, please run &lt;code&gt;bert-serving-benchmark&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-is-backend-based-on" class="anchor" aria-hidden="true" href="#q-what-is-backend-based-on"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is backend based on?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; &lt;a href="http://zeromq.org/" rel="nofollow"&gt;ZeroMQ&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-what-is-the-parallel-processing-model-behind-the-scene" class="anchor" aria-hidden="true" href="#q-what-is-the-parallel-processing-model-behind-the-scene"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is the parallel processing model behind the scene?&lt;/h5&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/bert-parallel-pipeline.png?raw=true"&gt;&lt;img src=".github/bert-parallel-pipeline.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-does-the-server-need-two-ports" class="anchor" aria-hidden="true" href="#q-why-does-the-server-need-two-ports"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why does the server need two ports?&lt;/h5&gt;
&lt;p&gt;One port is for pushing text data into the server, the other port is for publishing the encoded result to the client(s). In this way, we get rid of back-chatter, meaning that at every level recipients never talk back to senders. The overall message flow is strictly one-way, as depicted in the above figure. Killing back-chatter is essential to real scalability, allowing us to use &lt;code&gt;BertClient&lt;/code&gt; in an asynchronous way.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-tensorflow-on-the-client-side" class="anchor" aria-hidden="true" href="#q-do-i-need-tensorflow-on-the-client-side"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need Tensorflow on the client side?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; No. Think of &lt;code&gt;BertClient&lt;/code&gt; as a general feature extractor, whose output can be fed to &lt;em&gt;any&lt;/em&gt; ML models, e.g. &lt;code&gt;scikit-learn&lt;/code&gt;, &lt;code&gt;pytorch&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt;. The only file that client need is &lt;a href="service/client.py"&gt;&lt;code&gt;client.py&lt;/code&gt;&lt;/a&gt;. Copy this file to your project and import it, then you are ready to go.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-multilingual-bert-model-provided-by-google" class="anchor" aria-hidden="true" href="#q-can-i-use-multilingual-bert-model-provided-by-google"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use multilingual BERT model provided by Google?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-my-own-fine-tuned-bert-model" class="anchor" aria-hidden="true" href="#q-can-i-use-my-own-fine-tuned-bert-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use my own fine-tuned BERT model?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes. In fact, this is suggested. Make sure you have the following three items in &lt;code&gt;model_dir&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-run-it-in-python-2" class="anchor" aria-hidden="true" href="#q-can-i-run-it-in-python-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I run it in python 2?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Server side no, client side yes. This is based on the consideration that python 2.x might still be a major piece in some tech stack. Migrating the whole downstream stack to python 3 for supporting &lt;code&gt;bert-as-service&lt;/code&gt; can take quite some effort. On the other hand, setting up &lt;code&gt;BertServer&lt;/code&gt; is just a one-time thing, which can be even &lt;a href="#run-bert-service-on-nvidia-docker"&gt;run in a docker container&lt;/a&gt;. To ease the integration, we support python 2 on the client side so that you can directly use &lt;code&gt;BertClient&lt;/code&gt; as a part of your python 2 project, whereas the server side should always be hosted with python 3.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-do-i-need-to-do-segmentation-for-chinese" class="anchor" aria-hidden="true" href="#q-do-i-need-to-do-segmentation-for-chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Do I need to do segmentation for Chinese?&lt;/h5&gt;
&lt;p&gt;No, if you are using &lt;a href="https://github.com/google-research/bert#pre-trained-models"&gt;the pretrained Chinese BERT released by Google&lt;/a&gt; you don't need word segmentation. As this Chinese BERT is character-based model. It won't recognize word/phrase even if you intentionally add space in-between. To see that more clearly, this is what the BERT model actually receives after tokenization:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc.encode([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hey you&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;whats up?&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;你好么？&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;我 还 可以&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;tokens: [CLS] hey you [SEP]
input_ids: 101 13153 8357 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] what ##s up ? [SEP]
input_ids: 101 9100 8118 8644 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] 你 好 么 ？ [SEP]
input_ids: 101 872 1962 720 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

tokens: [CLS] 我 还 可 以 [SEP]
input_ids: 101 2769 6820 1377 809 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That means the word embedding is actually the character embedding for Chinese-BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-why-my-english-word-is-tokenized-to-something" class="anchor" aria-hidden="true" href="#q-why-my-english-word-is-tokenized-to-something"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why my (English) word is tokenized to &lt;code&gt;##something&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;Because your word is out-of-vocabulary (OOV). The tokenizer from Google uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;input&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;unaffable&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
tokenizer_output &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;un&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;##aff&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;##able&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-use-my-own-tokenizer" class="anchor" aria-hidden="true" href="#q-can-i-use-my-own-tokenizer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I use my own tokenizer?&lt;/h5&gt;
&lt;p&gt;Yes. If you already tokenize the sentence on your own, simply send use &lt;code&gt;encode&lt;/code&gt; with &lt;code&gt;List[List[Str]]&lt;/code&gt; as input and turn on &lt;code&gt;is_tokenized&lt;/code&gt;, i.e. &lt;code&gt;bc.encode(texts, is_tokenized=True)&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-i-encounter-zmqerrorzmqerror-operation-cannot-be-accomplished-in-current-state-when-using-bertclient-what-should-i-do" class="anchor" aria-hidden="true" href="#q-i-encounter-zmqerrorzmqerror-operation-cannot-be-accomplished-in-current-state-when-using-bertclient-what-should-i-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; I encounter &lt;code&gt;zmq.error.ZMQError: Operation cannot be accomplished in current state&lt;/code&gt; when using &lt;code&gt;BertClient&lt;/code&gt;, what should I do?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is often due to the misuse of &lt;code&gt;BertClient&lt;/code&gt; in multi-thread/process environment. Note that you can’t reuse one &lt;code&gt;BertClient&lt;/code&gt; among multiple threads/processes, you have to make a separate instance for each thread/process. For example, the following won't work at all:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; BAD example&lt;/span&gt;
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc1/Thread1 scope:&lt;/span&gt;
bc.encode(lst_str)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc2/Thread2 scope:&lt;/span&gt;
bc.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead, please do:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc1/Thread1 scope:&lt;/span&gt;
bc1 &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc1.encode(lst_str)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in Proc2/Thread2 scope:&lt;/span&gt;
bc2 &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
bc2.encode(lst_str)&lt;/pre&gt;&lt;/div&gt;
&lt;h5&gt;&lt;a id="user-content-q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-" class="anchor" aria-hidden="true" href="#q-after-running-the-server-i-have-several-garbage-tmpxxxx-folders-how-can-i-change-this-behavior-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; After running the server, I have several garbage &lt;code&gt;tmpXXXX&lt;/code&gt; folders. How can I change this behavior ?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; These folders are used by ZeroMQ to store sockets. You can choose a different location by setting the environment variable &lt;code&gt;ZEROMQ_SOCK_TMP_DIR&lt;/code&gt; :
&lt;code&gt;export ZEROMQ_SOCK_TMP_DIR=/tmp/&lt;/code&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong" class="anchor" aria-hidden="true" href="#q-the-cosine-similarity-of-two-sentence-vectors-is-unreasonably-high-eg-always--08-whats-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; The cosine similarity of two sentence vectors is unreasonably high (e.g. always &amp;gt; 0.8), what's wrong?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; A decent representation for a downstream task doesn't mean that it will be meaningful in terms of cosine distance. Since cosine distance is a linear space where all dimensions are weighted equally. if you want to use cosine distance anyway, then please focus on the rank not the absolute value. Namely, do not use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if cosine(A, B) &amp;gt; 0.9, then A and B are similar
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please consider the following instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if cosine(A, B) &amp;gt; cosine(A, C), then A is more similar to B than C.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The graph below illustrates the pairwise similarity of 3000 Chinese sentences randomly sampled from web (char. length &amp;lt; 25). We compute cosine similarity based on the sentence vectors and &lt;a href="https://en.wikipedia.org/wiki/ROUGE_(metric)" rel="nofollow"&gt;Rouge-L&lt;/a&gt; based on the raw text. The diagonal (self-correlation) is removed for the sake of clarity. As one can see, there is some positive correlation between these two metrics.&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/cosine-vs-rougel.png?raw=true"&gt;&lt;img src=".github/cosine-vs-rougel.png?raw=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;   
&lt;h5&gt;&lt;a id="user-content-q-im-getting-bad-performance-what-should-i-do" class="anchor" aria-hidden="true" href="#q-im-getting-bad-performance-what-should-i-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; I'm getting bad performance, what should I do?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This often suggests that the pretrained BERT could not generate a descent representation of your downstream task. Thus, you can fine-tune the model on the downstream task and then use &lt;code&gt;bert-as-service&lt;/code&gt; to serve the fine-tuned BERT. Note that, &lt;code&gt;bert-as-service&lt;/code&gt; is just a feature extraction service based on BERT. Nothing stops you from using a fine-tuned BERT.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-run-the-server-side-on-cpu-only-machine" class="anchor" aria-hidden="true" href="#q-can-i-run-the-server-side-on-cpu-only-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I run the server side on CPU-only machine?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, please run &lt;code&gt;bert-serving-start -cpu -max_batch_size 16&lt;/code&gt;. Note that, CPU does not scale as good as GPU on large batches, therefore the &lt;code&gt;max_batch_size&lt;/code&gt; on the server side needs to be smaller, e.g. 16 or 32.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-how-can-i-choose-num_worker" class="anchor" aria-hidden="true" href="#q-how-can-i-choose-num_worker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; How can I choose &lt;code&gt;num_worker&lt;/code&gt;?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Generally, the number of workers should be less than or equal to the number of GPU/CPU you have. Otherwise, multiple workers will be allocated to one GPU/CPU, which may not scale well (and may cause out-of-memory on GPU).&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-q-can-i-specify-which-gpu-to-use" class="anchor" aria-hidden="true" href="#q-can-i-specify-which-gpu-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Q:&lt;/strong&gt; Can I specify which GPU to use?&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, you can specifying &lt;code&gt;-device_map&lt;/code&gt; as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-start -device_map 0 1 4 -num_worker 4 -model_dir ...&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will start four workers and allocate them to GPU0, GPU1, GPU4 and again GPU0, respectively. In general, if &lt;code&gt;num_worker&lt;/code&gt; &amp;gt; &lt;code&gt;device_map&lt;/code&gt;, then devices will be reused and shared by the workers (may scale suboptimally or cause OOM); if &lt;code&gt;num_worker&lt;/code&gt; &amp;lt; &lt;code&gt;device_map&lt;/code&gt;, only &lt;code&gt;device_map[:num_worker]&lt;/code&gt; will be used.&lt;/p&gt;
&lt;p&gt;Note, &lt;code&gt;device_map&lt;/code&gt; is ignored when running on CPU.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-zap-benchmark" class="anchor" aria-hidden="true" href="#zap-benchmark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="zap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png"&gt;⚡️&lt;/g-emoji&gt; Benchmark&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bert-as-service.readthedocs.io/en/latest/section/benchmark.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95c38ab3eb5e58dcd9c3c931c7ef216fe77552d0/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626572742d61732d736572766963652f62616467652f3f76657273696f6e3d6c6174657374267374796c653d666f722d7468652d6261646765" alt="ReadTheDoc" data-canonical-src="https://readthedocs.org/projects/bert-as-service/badge/?version=latest&amp;amp;style=for-the-badge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The primary goal of benchmarking is to test the scalability and the speed of this service, which is crucial for using it in a dev/prod environment. Benchmark was done on Tesla M40 24GB, experiments were repeated 10 times and the average value is reported.&lt;/p&gt;
&lt;p&gt;To reproduce the results, please run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bert-serving-benchmark --help&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Common arguments across all experiments are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;num_worker&lt;/td&gt;
&lt;td&gt;1,2,4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max_seq_len&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;client_batch_size&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max_batch_size&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;num_client&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-max_seq_len" class="anchor" aria-hidden="true" href="#speed-wrt-max_seq_len"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;max_seq_len&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;max_seq_len&lt;/code&gt; is a parameter on the server side, which controls the maximum length of a sequence that a BERT model can handle. Sequences larger than &lt;code&gt;max_seq_len&lt;/code&gt; will be truncated on the left side. Thus, if your client want to send long sequences to the model, please make sure the server can handle them correctly.&lt;/p&gt;
&lt;p&gt;Performance-wise, longer sequences means slower speed and  more chance of OOM, as the multi-head self-attention (the core unit of BERT) needs to do dot products and matrix multiplications between every two symbols in the sequence.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/max_seq_len.png?raw=true"&gt;&lt;img src=".github/max_seq_len.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;max_seq_len&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;903&lt;/td&gt;
&lt;td&gt;1774&lt;/td&gt;
&lt;td&gt;3254&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1687&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;td&gt;231&lt;/td&gt;
&lt;td&gt;435&lt;/td&gt;
&lt;td&gt;768&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;160&lt;/td&gt;
&lt;td&gt;119&lt;/td&gt;
&lt;td&gt;237&lt;/td&gt;
&lt;td&gt;464&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;54&lt;/td&gt;
&lt;td&gt;108&lt;/td&gt;
&lt;td&gt;212&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-client_batch_size" class="anchor" aria-hidden="true" href="#speed-wrt-client_batch_size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;client_batch_size&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;client_batch_size&lt;/code&gt; is the number of sequences from a client when invoking &lt;code&gt;encode()&lt;/code&gt;. For performance reason, please consider encoding sequences in batch rather than encoding them one by one.&lt;/p&gt;
&lt;p&gt;For example, do:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; prepare your sent in advance&lt;/span&gt;
bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
my_sentences &lt;span class="pl-k"&gt;=&lt;/span&gt; [s &lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; my_corpus.iter()]
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; doing encoding in one-shot&lt;/span&gt;
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; bc.encode(my_sentences)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;DON'T:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bc &lt;span class="pl-k"&gt;=&lt;/span&gt; BertClient()
vec &lt;span class="pl-k"&gt;=&lt;/span&gt; []
&lt;span class="pl-k"&gt;for&lt;/span&gt; s &lt;span class="pl-k"&gt;in&lt;/span&gt; my_corpus.iter():
    vec.append(bc.encode(s))&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It's even worse if you put &lt;code&gt;BertClient()&lt;/code&gt; inside the loop. Don't do that.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/client_batch_size.png?raw=true"&gt;&lt;img src=".github/client_batch_size.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;client_batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;75&lt;/td&gt;
&lt;td&gt;74&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;206&lt;/td&gt;
&lt;td&gt;205&lt;/td&gt;
&lt;td&gt;201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;274&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;267&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;332&lt;/td&gt;
&lt;td&gt;329&lt;/td&gt;
&lt;td&gt;330&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;td&gt;365&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;382&lt;/td&gt;
&lt;td&gt;383&lt;/td&gt;
&lt;td&gt;383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;432&lt;/td&gt;
&lt;td&gt;766&lt;/td&gt;
&lt;td&gt;762&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;862&lt;/td&gt;
&lt;td&gt;1517&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;917&lt;/td&gt;
&lt;td&gt;1681&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4096&lt;/td&gt;
&lt;td&gt;481&lt;/td&gt;
&lt;td&gt;943&lt;/td&gt;
&lt;td&gt;1809&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-num_client" class="anchor" aria-hidden="true" href="#speed-wrt-num_client"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;num_client&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;num_client&lt;/code&gt; represents the number of concurrent clients connected to the server at the same time.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/num_clients.png?raw=true"&gt;&lt;img src=".github/num_clients.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;num_client&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1759&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;261&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;1028&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;133&lt;/td&gt;
&lt;td&gt;267&lt;/td&gt;
&lt;td&gt;533&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;67&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;td&gt;136&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As one can observe, 1 clients 1 GPU = 381 seqs/s, 2 clients 2 GPU 402 seqs/s, 4 clients 4 GPU 413 seqs/s. This shows the efficiency of our parallel pipeline and job scheduling, as the service can leverage the GPU time  more exhaustively as concurrent requests increase.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-max_batch_size" class="anchor" aria-hidden="true" href="#speed-wrt-max_batch_size"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;max_batch_size&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;max_batch_size&lt;/code&gt; is a parameter on the server side, which controls the maximum number of samples per batch per worker. If a incoming batch from client is larger than &lt;code&gt;max_batch_size&lt;/code&gt;, the server will split it into small batches so that each of them is less or equal than &lt;code&gt;max_batch_size&lt;/code&gt; before sending it to workers.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/max_batch_size.png?raw=true"&gt;&lt;img src=".github/max_batch_size.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;max_batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;450&lt;/td&gt;
&lt;td&gt;887&lt;/td&gt;
&lt;td&gt;1726&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;897&lt;/td&gt;
&lt;td&gt;1759&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;931&lt;/td&gt;
&lt;td&gt;1816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;473&lt;/td&gt;
&lt;td&gt;919&lt;/td&gt;
&lt;td&gt;1688&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;464&lt;/td&gt;
&lt;td&gt;866&lt;/td&gt;
&lt;td&gt;1483&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt-pooling_layer" class="anchor" aria-hidden="true" href="#speed-wrt-pooling_layer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;pooling_layer&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;pooling_layer&lt;/code&gt; determines the encoding layer that pooling operates on. For example, in a 12-layer BERT model, &lt;code&gt;-1&lt;/code&gt; represents the layer closed to the output, &lt;code&gt;-12&lt;/code&gt; represents the layer closed to the embedding layer. As one can observe below, the depth of the pooling layer affects the speed.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/pooling_layer.png?raw=true"&gt;&lt;img src=".github/pooling_layer.png?raw=true" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;pooling_layer&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;1 GPU&lt;/th&gt;
&lt;th&gt;2 GPU&lt;/th&gt;
&lt;th&gt;4 GPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[-1]&lt;/td&gt;
&lt;td&gt;438&lt;/td&gt;
&lt;td&gt;844&lt;/td&gt;
&lt;td&gt;1568&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-2]&lt;/td&gt;
&lt;td&gt;475&lt;/td&gt;
&lt;td&gt;916&lt;/td&gt;
&lt;td&gt;1686&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-3]&lt;/td&gt;
&lt;td&gt;516&lt;/td&gt;
&lt;td&gt;995&lt;/td&gt;
&lt;td&gt;1823&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-4]&lt;/td&gt;
&lt;td&gt;569&lt;/td&gt;
&lt;td&gt;1076&lt;/td&gt;
&lt;td&gt;1986&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-5]&lt;/td&gt;
&lt;td&gt;633&lt;/td&gt;
&lt;td&gt;1193&lt;/td&gt;
&lt;td&gt;2184&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-6]&lt;/td&gt;
&lt;td&gt;711&lt;/td&gt;
&lt;td&gt;1340&lt;/td&gt;
&lt;td&gt;2430&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-7]&lt;/td&gt;
&lt;td&gt;820&lt;/td&gt;
&lt;td&gt;1528&lt;/td&gt;
&lt;td&gt;2729&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-8]&lt;/td&gt;
&lt;td&gt;945&lt;/td&gt;
&lt;td&gt;1772&lt;/td&gt;
&lt;td&gt;3104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-9]&lt;/td&gt;
&lt;td&gt;1128&lt;/td&gt;
&lt;td&gt;2047&lt;/td&gt;
&lt;td&gt;3622&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-10]&lt;/td&gt;
&lt;td&gt;1392&lt;/td&gt;
&lt;td&gt;2542&lt;/td&gt;
&lt;td&gt;4241&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-11]&lt;/td&gt;
&lt;td&gt;1523&lt;/td&gt;
&lt;td&gt;2737&lt;/td&gt;
&lt;td&gt;4752&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-12]&lt;/td&gt;
&lt;td&gt;1568&lt;/td&gt;
&lt;td&gt;2985&lt;/td&gt;
&lt;td&gt;5303&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-speed-wrt--fp16-and--xla" class="anchor" aria-hidden="true" href="#speed-wrt--fp16-and--xla"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed wrt. &lt;code&gt;-fp16&lt;/code&gt; and &lt;code&gt;-xla&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;bert-as-service&lt;/code&gt; supports two additional optimizations: half-precision and XLA, which can be turned on by adding &lt;code&gt;-fp16&lt;/code&gt; and &lt;code&gt;-xla&lt;/code&gt; to &lt;code&gt;bert-serving-start&lt;/code&gt;, respectively. To enable these two options, you have to meet the following requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;your GPU supports FP16 instructions;&lt;/li&gt;
&lt;li&gt;your Tensorflow is self-compiled with XLA and &lt;code&gt;-march=native&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;your CUDA and cudnn are not too old.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On Tesla V100 with &lt;code&gt;tensorflow=1.13.0-rc0&lt;/code&gt; it gives:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/fp16-xla.svg"&gt;&lt;img src=".github/fp16-xla.svg" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FP16 achieves ~1.4x speedup (round-trip) comparing to the FP32 counterpart. To reproduce the result, please run &lt;code&gt;python example/example1.py&lt;/code&gt;.&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h2&gt;
&lt;p align="right"&gt;&lt;a href="#bert-as-service"&gt;&lt;sup&gt;▴ Back to top&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use bert-as-service in a scientific publication, we would appreciate references to the following BibTex entry:&lt;/p&gt;
&lt;div class="highlight highlight-text-tex-latex"&gt;&lt;pre&gt;@misc{xiao2018bertservice,
  title={bert-as-service},
  author={Xiao, Han},
  howpublished={&lt;span class="pl-c1"&gt;\url&lt;/span&gt;{https://github.com/hanxiao/bert-as-service}},
  year={2018}
}&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hanxiao</author><guid isPermaLink="false">https://github.com/hanxiao/bert-as-service</guid><pubDate>Tue, 05 Nov 2019 00:18:00 GMT</pubDate></item><item><title>ansible/ansible #19 in Python, This week</title><link>https://github.com/ansible/ansible</link><description>&lt;p&gt;&lt;i&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications — automate in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com/ansible/&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible" rel="nofollow"&gt;&lt;img alt="PyPI version" src="https://camo.githubusercontent.com/1700ed8e65665052f4e72ba6ae9e1f1d7fddc6c6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f616e7369626c652e737667" data-canonical-src="https://img.shields.io/pypi/v/ansible.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/" rel="nofollow"&gt;&lt;img alt="Docs badge" src="https://camo.githubusercontent.com/dc37b81ae5ef1245837ee1f1547892e8345ccd4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/docs-latest-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;&lt;img alt="Chat badge" src="https://camo.githubusercontent.com/a36ab54aea33fa40f9d063c8804b9bbf1b6fbd47/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d4952432d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/chat-IRC-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://app.shippable.com/projects/573f79d02a8192902e20e34b" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/c4dd185960fb101604717a4c8965ac9ba2725e69/68747470733a2f2f6170692e736869707061626c652e636f6d2f70726f6a656374732f3537336637396430326138313932393032653230653334622f62616467653f6272616e63683d646576656c" data-canonical-src="https://api.shippable.com/projects/573f79d02a8192902e20e34b/badge?branch=devel" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/code_of_conduct.html" rel="nofollow"&gt;&lt;img alt="Ansible Code of Conduct" src="https://camo.githubusercontent.com/412f4c0b8d7289e25a69d8568bd02c1bf976f9fd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532306f66253230636f6e647563742d416e7369626c652d73696c7665722e737667" data-canonical-src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information" rel="nofollow"&gt;&lt;img alt="Ansible mailing lists" src="https://camo.githubusercontent.com/74dd4958c493abf9d3105cbcd020e55aa5df90c9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61696c696e672532306c697374732d416e7369626c652d6f72616e67652e737667" data-canonical-src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg" style="max-width:100%;"&gt;
&lt;/a&gt; &lt;a href="COPYING"&gt;&lt;img alt="Repository License" src="https://camo.githubusercontent.com/0ac7552afd56fbe0c2ce6722d54f68857aa92b82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c25323076332e302d627269676874677265656e2e737667" data-canonical-src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-ansible"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-ansible" class="anchor" aria-hidden="true" href="#ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ansible&lt;/h2&gt;
&lt;p&gt;Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on &lt;a href="https://ansible.com/" rel="nofollow"&gt;the Ansible website&lt;/a&gt;.&lt;/p&gt;
&lt;a name="user-content-design-principles"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-design-principles" class="anchor" aria-hidden="true" href="#design-principles"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Design Principles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Have a dead simple setup process and a minimal learning curve.&lt;/li&gt;
&lt;li&gt;Manage machines very quickly and in parallel.&lt;/li&gt;
&lt;li&gt;Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.&lt;/li&gt;
&lt;li&gt;Describe infrastructure in a language that is both machine and human
friendly.&lt;/li&gt;
&lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt;
&lt;li&gt;Manage new remote machines instantly, without bootstrapping any
software.&lt;/li&gt;
&lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt;
&lt;li&gt;Be usable as non-root.&lt;/li&gt;
&lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-use-ansible"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-use-ansible" class="anchor" aria-hidden="true" href="#use-ansible"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use Ansible&lt;/h3&gt;
&lt;p&gt;You can install a released version of Ansible via &lt;code&gt;pip&lt;/code&gt;, a package manager, or
our &lt;a href="https://releases.ansible.com/ansible/" rel="nofollow"&gt;release repository&lt;/a&gt;. See our
&lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html" rel="nofollow"&gt;installation guide&lt;/a&gt; for details on installing Ansible
on a variety of platforms.&lt;/p&gt;
&lt;p&gt;Red Hat offers supported builds of &lt;a href="https://www.ansible.com/ansible-engine" rel="nofollow"&gt;Ansible Engine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved
in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt;
&lt;a name="user-content-get-involved"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-get-involved" class="anchor" aria-hidden="true" href="#get-involved"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Get Involved&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/latest/community" rel="nofollow"&gt;Community
Information&lt;/a&gt; for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.&lt;/li&gt;
&lt;li&gt;Join a &lt;a href="https://github.com/ansible/community/wiki"&gt;Working Group&lt;/a&gt;, an organized community devoted to a specific technology domain or platform.&lt;/li&gt;
&lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt;
&lt;li&gt;Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.&lt;/li&gt;
&lt;li&gt;For a list of email lists, IRC channels and Working Groups, see the
&lt;a href="https://docs.ansible.com/ansible/latest/community/communication.html" rel="nofollow"&gt;Communication page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-branch-info"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-branch-info" class="anchor" aria-hidden="true" href="#branch-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Branch Info&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt;
&lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/latest/dev_guide/developing_modules_general.html#common-environment-setup" rel="nofollow"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt;
&lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/release_and_maintenance.html" rel="nofollow"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-roadmap"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-roadmap" class="anchor" aria-hidden="true" href="#roadmap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Roadmap&lt;/h3&gt;
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/" rel="nofollow"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt;
&lt;a name="user-content-authors"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h3&gt;
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt;
and has contributions from over 4600 users (and growing). Thanks everyone!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ansible.com" rel="nofollow"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com" rel="nofollow"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-license"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;GNU General Public License v3.0&lt;/p&gt;
&lt;p&gt;See &lt;a href="COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>ansible</author><guid isPermaLink="false">https://github.com/ansible/ansible</guid><pubDate>Tue, 05 Nov 2019 00:19:00 GMT</pubDate></item><item><title>psf/black #20 in Python, This week</title><link>https://github.com/psf/black</link><description>&lt;p&gt;&lt;i&gt;The uncompromising Python code formatter&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png"&gt;&lt;img src="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png" alt="Black Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-the-uncompromising-code-formatter" class="anchor" aria-hidden="true" href="#the-uncompromising-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Uncompromising Code Formatter&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;a href="https://travis-ci.com/psf/black" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/0d62c6ce125db151bb0bc13cbc834c0d0522ed88/68747470733a2f2f7472617669732d63692e636f6d2f7073662f626c61636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/psf/black.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://black.readthedocs.io/en/stable/?badge=stable" rel="nofollow"&gt;&lt;img alt="Documentation Status" src="https://camo.githubusercontent.com/ab197284ad0cd9cae552157b75f815b640eba827/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626c61636b2f62616467652f3f76657273696f6e3d737461626c65" data-canonical-src="https://readthedocs.org/projects/black/badge/?version=stable" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/psf/black?branch=master" rel="nofollow"&gt;&lt;img alt="Coverage Status" src="https://camo.githubusercontent.com/b77842b75f031fbf9bbf690eb55585ae55a8321d/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7073662f626c61636b2f62616467652e7376673f6272616e63683d6d6173746572" data-canonical-src="https://coveralls.io/repos/github/psf/black/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black/blob/master/LICENSE"&gt;&lt;img alt="License: MIT" src="https://camo.githubusercontent.com/14a9abb7e83098f2949f26d2190e04fb1bd52c06/68747470733a2f2f626c61636b2e72656164746865646f63732e696f2f656e2f737461626c652f5f7374617469632f6c6963656e73652e737667" data-canonical-src="https://black.readthedocs.io/en/stable/_static/license.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/black/" rel="nofollow"&gt;&lt;img alt="PyPI" src="https://camo.githubusercontent.com/5f6551edb3716d9b0a5659caf441d987bb1527a6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626c61636b" data-canonical-src="https://img.shields.io/pypi/v/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/black" rel="nofollow"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/7b3026c6e1fecc9574cb4b93a3925e392bee087d/68747470733a2f2f706570792e746563682f62616467652f626c61636b" data-canonical-src="https://pepy.tech/badge/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Any color you like.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed,
determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time
and mental energy for more important matters.&lt;/p&gt;
&lt;p&gt;Blackened code looks the same regardless of the project you're reading. Formatting
becomes transparent after a while and you can focus on the content instead.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt;
&lt;p&gt;Try it out now using the &lt;a href="https://black.now.sh" rel="nofollow"&gt;Black Playground&lt;/a&gt;. Watch the
&lt;a href="https://youtu.be/esZLCuWs_2Y" rel="nofollow"&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Contents:&lt;/em&gt; &lt;strong&gt;&lt;a href="#installation-and-usage"&gt;Installation and usage&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#the-black-code-style"&gt;Code style&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#pyprojecttoml"&gt;pyproject.toml&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#editor-integration"&gt;Editor integration&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#blackd"&gt;blackd&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#version-control-integration"&gt;Version control integration&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#ignoring-unmodified-files"&gt;Ignoring unmodified files&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#used-by"&gt;Used by&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#testimonials"&gt;Testimonials&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#show-your-style"&gt;Show your style&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#contributing-to-black"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#change-log"&gt;Change Log&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.6.0+ to
run but you can reformat Python 2 code with it, too.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-command-line-options" class="anchor" aria-hidden="true" href="#command-line-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command line options&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; doesn't provide many options. You can list them by running &lt;code&gt;black --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;black [OPTIONS] [SRC]...

Options:
  -c, --code TEXT                 Format the code passed in as a string.
  -l, --line-length INTEGER       How many characters per line to allow.
                                  [default: 88]
  -t, --target-version [py27|py33|py34|py35|py36|py37|py38]
                                  Python versions that should be supported by
                                  Black's output. [default: per-file auto-
                                  detection]
  --py36                          Allow using Python 3.6-only syntax on all
                                  input files.  This will put trailing commas
                                  in function signatures and calls also after
                                  *args and **kwargs. Deprecated; use
                                  --target-version instead. [default: per-file
                                  auto-detection]
  --pyi                           Format all input files like typing stubs
                                  regardless of file extension (useful when
                                  piping source on standard input).
  -S, --skip-string-normalization
                                  Don't normalize string quotes or prefixes.
  --check                         Don't write the files back, just return the
                                  status.  Return code 0 means nothing would
                                  change.  Return code 1 means some files
                                  would be reformatted.  Return code 123 means
                                  there was an internal error.
  --diff                          Don't write the files back, just output a
                                  diff for each file on stdout.
  --fast / --safe                 If --fast given, skip temporary sanity
                                  checks. [default: --safe]
  --include TEXT                  A regular expression that matches files and
                                  directories that should be included on
                                  recursive searches.  An empty value means
                                  all files are included regardless of the
                                  name.  Use forward slashes for directories
                                  on all platforms (Windows, too).  Exclusions
                                  are calculated first, inclusions later.
                                  [default: \.pyi?$]
  --exclude TEXT                  A regular expression that matches files and
                                  directories that should be excluded on
                                  recursive searches.  An empty value means no
                                  paths are excluded. Use forward slashes for
                                  directories on all platforms (Windows, too).
                                  Exclusions are calculated first, inclusions
                                  later.  [default: /(\.eggs|\.git|\.hg|\.mypy
                                  _cache|\.nox|\.tox|\.venv|_build|buck-
                                  out|build|dist)/]
  -q, --quiet                     Don't emit non-error messages to stderr.
                                  Errors are still emitted, silence those with
                                  2&amp;gt;/dev/null.
  -v, --verbose                   Also emit messages to stderr about files
                                  that were not changed or were ignored due to
                                  --exclude=.
  --version                       Show the version and exit.
  --config PATH                   Read configuration from PATH.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a well-behaved Unix-style command-line tool:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it does nothing if no sources are passed to it;&lt;/li&gt;
&lt;li&gt;it will read from standard input and write to standard output if &lt;code&gt;-&lt;/code&gt; is used as the
filename;&lt;/li&gt;
&lt;li&gt;it only outputs messages to users on standard error;&lt;/li&gt;
&lt;li&gt;exits with code 0 unless an internal error occurred (or &lt;code&gt;--check&lt;/code&gt; was used).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-note-this-is-a-beta-product" class="anchor" aria-hidden="true" href="#note-this-is-a-beta-product"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NOTE: This is a beta product&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href="#used-by"&gt;successfully used&lt;/a&gt; by many projects, small and big. It
also sports a decent test suite. However, it is still very new. Things will probably be
wonky for a while. This is made explicit by the "Beta" trove classifier, as well as by
the "b" in the version number. What this means for you is that &lt;strong&gt;until the formatter
becomes stable, you should expect some formatting to change in the future&lt;/strong&gt;. That being
said, no drastic stylistic changes are planned, mostly responses to bug reports.&lt;/p&gt;
&lt;p&gt;Also, as a temporary safety measure, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still
produces a valid AST that is equivalent to the original. This slows it down. If you're
feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-black-code-style" class="anchor" aria-hidden="true" href="#the-black-code-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; reformats entire files in place. It is not configurable. It doesn't take
previous formatting into account. It doesn't reformat blocks that start with
&lt;code&gt;# fmt: off&lt;/code&gt; and end with &lt;code&gt;# fmt: on&lt;/code&gt;. &lt;code&gt;# fmt: on/off&lt;/code&gt; have to be on the same level of
indentation. It also recognizes &lt;a href="https://github.com/google/yapf"&gt;YAPF&lt;/a&gt;'s block comments
to the same effect, as a courtesy for straddling code.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-black-wraps-lines" class="anchor" aria-hidden="true" href="#how-black-wraps-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How &lt;em&gt;Black&lt;/em&gt; wraps lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; ignores previous formatting and applies uniform horizontal and vertical
whitespace to your code. The rules for horizontal whitespace can be summarized as: do
whatever makes &lt;code&gt;pycodestyle&lt;/code&gt; happy. The coding style used by &lt;em&gt;Black&lt;/em&gt; can be viewed as a
strict subset of PEP 8.&lt;/p&gt;
&lt;p&gt;As for vertical whitespace, &lt;em&gt;Black&lt;/em&gt; tries to render one full expression or simple
statement per line. If this fits the allotted line length, great.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;2&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;3&lt;/span&gt;,
]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If not, &lt;em&gt;Black&lt;/em&gt; will look at the contents of the first outer matching brackets and put
that in a separate indented line.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

ImportantClass.important_method(exc, limit, lookup_lines, capture_locals, extra_argument)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

ImportantClass.important_method(
    exc, limit, lookup_lines, capture_locals, extra_argument
)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If that still doesn't fit the bill, it will decompose the internal expression further
using the same rule, indenting matching brackets every time. If the contents of the
matching brackets pair are comma-separated (like an argument list, or a dict literal,
and so on) then &lt;em&gt;Black&lt;/em&gt; will first try to keep them on the same line with the matching
brackets. If that doesn't work, it will put all of them in separate lines.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(&lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;, &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike, &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;w&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(
    &lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike,
    &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;,
):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;w&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You might have noticed that closing brackets are always dedented and that a trailing
comma is always added. Such formatting produces smaller diffs; when you add or remove an
element, it's always just one line. Also, having the closing bracket dedented provides a
clear delimiter between two distinct sections of the code that otherwise share the same
indentation level (like the arguments list and the docstring in the example above).&lt;/p&gt;
&lt;p&gt;If a data structure literal (tuple, list, set, dict) or a line of "from" imports cannot
fit in the allotted length, it's always split into one element per line. This minimizes
diffs as well as enables readers of code to find which commit introduced a particular
entry. This also makes &lt;em&gt;Black&lt;/em&gt; compatible with &lt;a href="https://pypi.org/p/isort/" rel="nofollow"&gt;isort&lt;/a&gt; with
the following configuration.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;A compatible `.isort.cfg`&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;[settings]
multi_line_output=3
include_trailing_comma=True
force_grid_wrap=0
use_parentheses=True
line_length=88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equivalent command line is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ isort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88 [ file.py ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-line-length" class="anchor" aria-hidden="true" href="#line-length"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line length&lt;/h3&gt;
&lt;p&gt;You probably noticed the peculiar default line length. &lt;em&gt;Black&lt;/em&gt; defaults to 88 characters
per line, which happens to be 10% over 80. This number was found to produce
significantly shorter files than sticking with 80 (the most popular), or even 79 (used
by the standard library). In general,
&lt;a href="https://youtu.be/wf-BqAjZb8M?t=260" rel="nofollow"&gt;90-ish seems like the wise choice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're paid by the line of code you write, you can pass &lt;code&gt;--line-length&lt;/code&gt; with a lower
number. &lt;em&gt;Black&lt;/em&gt; will try to respect that. However, sometimes it won't be able to without
breaking other rules. In those rare cases, auto-formatted code will exceed your allotted
limit.&lt;/p&gt;
&lt;p&gt;You can also increase it, but remember that people with sight disabilities find it
harder to work with line lengths exceeding 100 characters. It also adversely affects
side-by-side diff review on typical screen resolutions. Long lines also make it harder
to present code neatly in documentation or talk slides.&lt;/p&gt;
&lt;p&gt;If you're using Flake8, you can bump &lt;code&gt;max-line-length&lt;/code&gt; to 88 and forget about it.
Alternatively, use &lt;a href="https://github.com/PyCQA/flake8-bugbear"&gt;Bugbear&lt;/a&gt;'s B950 warning
instead of E501 and keep the max line length at 80 which you are probably already using.
You'd do it like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-ini"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;[flake8]&lt;/span&gt;
&lt;span class="pl-k"&gt;max-line-length&lt;/span&gt; = 80
...
&lt;span class="pl-k"&gt;select&lt;/span&gt; = C,E,F,W,B,B950
&lt;span class="pl-k"&gt;ignore&lt;/span&gt; = E203, E501, W503&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll find &lt;em&gt;Black&lt;/em&gt;'s own .flake8 config file is configured like this. Explanation of
why W503 and E203 are disabled can be found further in this documentation. And if you're
curious about the reasoning behind B950,
&lt;a href="https://github.com/PyCQA/flake8-bugbear#opinionated-warnings"&gt;Bugbear's documentation&lt;/a&gt;
explains it. The tl;dr is "it's like highway speed limits, we won't bother you if you
overdo it by a few km/h".&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-empty-lines" class="anchor" aria-hidden="true" href="#empty-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Empty lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; avoids spurious vertical whitespace. This is in the spirit of PEP 8 which says
that in-function vertical whitespace should only be used sparingly.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will allow single empty lines inside functions, and single and double empty
lines on module level left by the original editors, except when they're within
parenthesized expressions. Since such expressions are always reformatted to fit minimal
space, this whitespace is lost.&lt;/p&gt;
&lt;p&gt;It will also insert proper spacing before and after function definitions. It's one line
before and after inner functions and two lines before and after module-level functions
and classes. &lt;em&gt;Black&lt;/em&gt; will not put empty lines between function/class definitions and
standalone comments that immediately precede the given function/class.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will enforce single empty lines between a class-level docstring and the first
following field or method. This conforms to
&lt;a href="https://www.python.org/dev/peps/pep-0257/#multi-line-docstrings" rel="nofollow"&gt;PEP 257&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; won't insert empty lines after function docstrings unless that empty line is
required due to an inner function starting immediately after.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-trailing-commas" class="anchor" aria-hidden="true" href="#trailing-commas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trailing commas&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will add trailing commas to expressions that are split by comma where each
element is on its own line. This includes function signatures.&lt;/p&gt;
&lt;p&gt;Unnecessary trailing commas are removed if an expression fits in one line. This makes it
1% more likely that your line won't exceed the allotted line length limit. Moreover, in
this scenario, if you added another argument to your call, you'd probably fit it in the
same line anyway. That doesn't make diffs any larger.&lt;/p&gt;
&lt;p&gt;One exception to removing trailing commas is tuple expressions with just one element. In
this case &lt;em&gt;Black&lt;/em&gt; won't touch the single trailing comma as this would unexpectedly
change the underlying data type. Note that this is also the case when commas are used
while indexing. This is a tuple in disguise: &lt;code&gt;numpy_array[3, ]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One exception to adding trailing commas is function signatures containing &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;*args&lt;/code&gt;,
or &lt;code&gt;**kwargs&lt;/code&gt;. In this case a trailing comma is only safe to use on Python 3.6. &lt;em&gt;Black&lt;/em&gt;
will detect if your file is already 3.6+ only and use trailing commas in this situation.
If you wonder how it knows, it looks for f-strings and existing use of trailing commas
in function signatures that have stars in them. In other words, if you'd like a trailing
comma in this situation and &lt;em&gt;Black&lt;/em&gt; didn't recognize it was safe to do so, put it there
manually and &lt;em&gt;Black&lt;/em&gt; will keep it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-strings" class="anchor" aria-hidden="true" href="#strings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Strings&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; prefers double quotes (&lt;code&gt;"&lt;/code&gt; and &lt;code&gt;"""&lt;/code&gt;) over single quotes (&lt;code&gt;'&lt;/code&gt; and &lt;code&gt;'''&lt;/code&gt;). It
will replace the latter with the former as long as it does not result in more backslash
escapes than before.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; also standardizes string prefixes, making them always lowercase. On top of that,
if your code is already Python 3.6+ only or it's using the &lt;code&gt;unicode_literals&lt;/code&gt; future
import, &lt;em&gt;Black&lt;/em&gt; will remove &lt;code&gt;u&lt;/code&gt; from the string prefix as it is meaningless in those
scenarios.&lt;/p&gt;
&lt;p&gt;The main reason to standardize on a single form of quotes is aesthetics. Having one kind
of quotes everywhere reduces reader distraction. It will also enable a future version of
&lt;em&gt;Black&lt;/em&gt; to merge consecutive string literals that ended up on the same line (see
&lt;a href="https://github.com/psf/black/issues/26"&gt;#26&lt;/a&gt; for details).&lt;/p&gt;
&lt;p&gt;Why settle on double quotes? They anticipate apostrophes in English text. They match the
docstring standard described in
&lt;a href="https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring" rel="nofollow"&gt;PEP 257&lt;/a&gt;. An empty
string in double quotes (&lt;code&gt;""&lt;/code&gt;) is impossible to confuse with a one double-quote
regardless of fonts and syntax highlighting used. On top of this, double quotes for
strings are consistent with C which Python interacts a lot with.&lt;/p&gt;
&lt;p&gt;On certain keyboard layouts like US English, typing single quotes is a bit easier than
double quotes. The latter requires use of the Shift key. My recommendation here is to
keep using whatever is faster to type and let &lt;em&gt;Black&lt;/em&gt; handle the transformation.&lt;/p&gt;
&lt;p&gt;If you are adopting &lt;em&gt;Black&lt;/em&gt; in a large project with pre-existing string conventions
(like the popular
&lt;a href="https://stackoverflow.com/a/56190" rel="nofollow"&gt;"single quotes for data, double quotes for human-readable strings"&lt;/a&gt;),
you can pass &lt;code&gt;--skip-string-normalization&lt;/code&gt; on the command line. This is meant as an
adoption helper, avoid using this for new projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-numeric-literals" class="anchor" aria-hidden="true" href="#numeric-literals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numeric literals&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; standardizes most numeric literals to use lowercase letters for the syntactic
parts and uppercase letters for the digits themselves: &lt;code&gt;0xAB&lt;/code&gt; instead of &lt;code&gt;0XAB&lt;/code&gt; and
&lt;code&gt;1e10&lt;/code&gt; instead of &lt;code&gt;1E10&lt;/code&gt;. Python 2 long literals are styled as &lt;code&gt;2L&lt;/code&gt; instead of &lt;code&gt;2l&lt;/code&gt; to
avoid confusion between &lt;code&gt;l&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-line-breaks--binary-operators" class="anchor" aria-hidden="true" href="#line-breaks--binary-operators"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line breaks &amp;amp; binary operators&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will break a line before a binary operator when splitting a block of code over
multiple lines. This is so that &lt;em&gt;Black&lt;/em&gt; is compliant with the recent changes in the
&lt;a href="https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator" rel="nofollow"&gt;PEP 8&lt;/a&gt;
style guide, which emphasizes that this approach improves readability.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;W503 line break before binary operator&lt;/code&gt; warnings in style
guide enforcement tools like Flake8. Since &lt;code&gt;W503&lt;/code&gt; is not PEP 8 compliant, you should
tell Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-slices" class="anchor" aria-hidden="true" href="#slices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slices&lt;/h3&gt;
&lt;p&gt;PEP 8
&lt;a href="https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements" rel="nofollow"&gt;recommends&lt;/a&gt;
to treat &lt;code&gt;:&lt;/code&gt; in slices as a binary operator with the lowest priority, and to leave an
equal amount of space on either side, except if a parameter is omitted (e.g.
&lt;code&gt;ham[1 + 1 :]&lt;/code&gt;). It also states that for extended slices, both &lt;code&gt;:&lt;/code&gt; operators have to
have the same amount of spacing, except if a parameter is omitted (&lt;code&gt;ham[1 + 1 ::]&lt;/code&gt;).
&lt;em&gt;Black&lt;/em&gt; enforces these rules consistently.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;E203 whitespace before ':'&lt;/code&gt; warnings in style guide
enforcement tools like Flake8. Since &lt;code&gt;E203&lt;/code&gt; is not PEP 8 compliant, you should tell
Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-parentheses" class="anchor" aria-hidden="true" href="#parentheses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parentheses&lt;/h3&gt;
&lt;p&gt;Some parentheses are optional in the Python grammar. Any expression can be wrapped in a
pair of parentheses to form an atom. There are a few interesting cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;while (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;for (...) in (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;assert (...), (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from X import (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;assignments like:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;target = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;target: type = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;some, *un, packing = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;augmented += (...)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In those cases, parentheses are removed when the entire statement fits in one line, or
if the inner expression doesn't have any delimiters to further split on. If there is
only a single delimiter and the expression starts or ends with a bracket, the
parenthesis can also be successfully omitted since the existing bracket pair will
organize the expression neatly anyway. Otherwise, the parentheses are added.&lt;/p&gt;
&lt;p&gt;Please note that &lt;em&gt;Black&lt;/em&gt; does not add or remove any additional nested parentheses that
you might want to have for clarity or further code organization. For example those
parentheses are not going to be removed:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; (this &lt;span class="pl-k"&gt;or&lt;/span&gt; that)
decision &lt;span class="pl-k"&gt;=&lt;/span&gt; (maybe.this() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; (maybe.that() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-call-chains" class="anchor" aria-hidden="true" href="#call-chains"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Call chains&lt;/h3&gt;
&lt;p&gt;Some popular APIs, like ORMs, use call chaining. This API style is known as a
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interface&lt;/a&gt;. &lt;em&gt;Black&lt;/em&gt; formats
those by treating dots that follow a call or an indexing operation like a very low
priority delimiter. It's easier to show the behavior than to explain it. Look at the
example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;example&lt;/span&gt;(&lt;span class="pl-smi"&gt;session&lt;/span&gt;):
    result &lt;span class="pl-k"&gt;=&lt;/span&gt; (
        session.query(models.Customer.id)
        .filter(
            models.Customer.account_id &lt;span class="pl-k"&gt;==&lt;/span&gt; account_id,
            models.Customer.email &lt;span class="pl-k"&gt;==&lt;/span&gt; email_address,
        )
        .order_by(models.Customer.id.asc())
        .all()
    )&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typing-stub-files" class="anchor" aria-hidden="true" href="#typing-stub-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typing stub files&lt;/h3&gt;
&lt;p&gt;PEP 484 describes the syntax for type hints in Python. One of the use cases for typing
is providing type annotations for modules which cannot contain them directly (they might
be written in C, or they might be third-party, or their implementation may be overly
dynamic, and so on).&lt;/p&gt;
&lt;p&gt;To solve this,
&lt;a href="https://www.python.org/dev/peps/pep-0484/#stub-files" rel="nofollow"&gt;stub files with the &lt;code&gt;.pyi&lt;/code&gt; file extension&lt;/a&gt;
can be used to describe typing information for an external module. Those stub files omit
the implementation of classes and functions they describe, instead they only contain the
structure of the file (listing globals, functions, and classes with their members). The
recommended code style for those files is more terse than PEP 8:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; on the same line as the class/function signature;&lt;/li&gt;
&lt;li&gt;avoid vertical whitespace between consecutive module-level functions, names, or
methods and fields within a single class;&lt;/li&gt;
&lt;li&gt;use a single blank line between top-level class definitions, or none if the classes
are very small.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; enforces the above rules. There are additional guidelines for formatting &lt;code&gt;.pyi&lt;/code&gt;
file that are not enforced yet but might be in a future version of the formatter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all function bodies should be empty (contain &lt;code&gt;...&lt;/code&gt; instead of the body);&lt;/li&gt;
&lt;li&gt;do not use docstrings;&lt;/li&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; over &lt;code&gt;pass&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;for arguments with a default, use &lt;code&gt;...&lt;/code&gt; instead of the actual default;&lt;/li&gt;
&lt;li&gt;avoid using string literals in type annotations, stub files support forward references
natively (like Python 3.7 code with &lt;code&gt;from __future__ import annotations&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;use variable annotations instead of type comments, even for stubs that target older
versions of Python;&lt;/li&gt;
&lt;li&gt;for arguments that default to &lt;code&gt;None&lt;/code&gt;, use &lt;code&gt;Optional[]&lt;/code&gt; explicitly;&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;float&lt;/code&gt; instead of &lt;code&gt;Union[int, float]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pyprojecttoml" class="anchor" aria-hidden="true" href="#pyprojecttoml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyproject.toml&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options
from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom
&lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; patterns for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you're asking yourself "Do I need to configure anything?" the answer is
"No". &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-on-earth-is-a-pyprojecttoml-file" class="anchor" aria-hidden="true" href="#what-on-earth-is-a-pyprojecttoml-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What on Earth is a &lt;code&gt;pyproject.toml&lt;/code&gt; file?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.python.org/dev/peps/pep-0518/" rel="nofollow"&gt;PEP 518&lt;/a&gt; defines &lt;code&gt;pyproject.toml&lt;/code&gt; as a
configuration file to store build system requirements for Python projects. With the help
of tools like &lt;a href="https://poetry.eustace.io/" rel="nofollow"&gt;Poetry&lt;/a&gt; or
&lt;a href="https://flit.readthedocs.io/en/latest/" rel="nofollow"&gt;Flit&lt;/a&gt; it can fully replace the need for
&lt;code&gt;setup.py&lt;/code&gt; and &lt;code&gt;setup.cfg&lt;/code&gt; files.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-where-black-looks-for-the-file" class="anchor" aria-hidden="true" href="#where-black-looks-for-the-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where &lt;em&gt;Black&lt;/em&gt; looks for the file&lt;/h3&gt;
&lt;p&gt;By default &lt;em&gt;Black&lt;/em&gt; looks for &lt;code&gt;pyproject.toml&lt;/code&gt; starting from the common base directory of
all files and directories passed on the command line. If it's not there, it looks in
parent directories. It stops looking when it finds the file, or a &lt;code&gt;.git&lt;/code&gt; directory, or a
&lt;code&gt;.hg&lt;/code&gt; directory, or the root of the file system, whichever comes first.&lt;/p&gt;
&lt;p&gt;If you're formatting standard input, &lt;em&gt;Black&lt;/em&gt; will look for configuration starting from
the current working directory.&lt;/p&gt;
&lt;p&gt;You can also explicitly specify the path to a particular file that you want with
&lt;code&gt;--config&lt;/code&gt;. In this situation &lt;em&gt;Black&lt;/em&gt; will not look for any other file.&lt;/p&gt;
&lt;p&gt;If you're running with &lt;code&gt;--verbose&lt;/code&gt;, you will see a blue message if a file was found and
used.&lt;/p&gt;
&lt;p&gt;Please note &lt;code&gt;blackd&lt;/code&gt; will not use &lt;code&gt;pyproject.toml&lt;/code&gt; configuration.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-configuration-format" class="anchor" aria-hidden="true" href="#configuration-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration format&lt;/h3&gt;
&lt;p&gt;As the file extension suggests, &lt;code&gt;pyproject.toml&lt;/code&gt; is a
&lt;a href="https://github.com/toml-lang/toml"&gt;TOML&lt;/a&gt; file. It contains separate sections for
different tools. &lt;em&gt;Black&lt;/em&gt; is using the &lt;code&gt;[tool.black]&lt;/code&gt; section. The option keys are the
same as long names of options on the command line.&lt;/p&gt;
&lt;p&gt;Note that you have to use single-quoted strings in TOML for regular expressions. It's
the equivalent of r-strings in Python. Multiline strings are treated as verbose regular
expressions by Black. Use &lt;code&gt;[ ]&lt;/code&gt; to denote a significant space character.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example `pyproject.toml`&lt;/summary&gt;
&lt;div class="highlight highlight-source-toml"&gt;&lt;pre&gt;[&lt;span class="pl-en"&gt;tool&lt;/span&gt;.&lt;span class="pl-en"&gt;black&lt;/span&gt;]
&lt;span class="pl-smi"&gt;line-length&lt;/span&gt; = &lt;span class="pl-c1"&gt;88&lt;/span&gt;
&lt;span class="pl-smi"&gt;target-version&lt;/span&gt; = [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;py37&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
&lt;span class="pl-smi"&gt;include&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;\.pyi?$&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-smi"&gt;exclude&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;(&lt;/span&gt;
&lt;span class="pl-s"&gt;  /(&lt;/span&gt;
&lt;span class="pl-s"&gt;      \.eggs         # exclude a few common directories in the&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.git          # root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.hg&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.mypy_cache&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.tox&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.venv&lt;/span&gt;
&lt;span class="pl-s"&gt;    | _build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | buck-out&lt;/span&gt;
&lt;span class="pl-s"&gt;    | build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | dist&lt;/span&gt;
&lt;span class="pl-s"&gt;  )/&lt;/span&gt;
&lt;span class="pl-s"&gt;  | foo.py           # also separately exclude a file named foo.py in&lt;/span&gt;
&lt;span class="pl-s"&gt;                     # the root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;)&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-lookup-hierarchy" class="anchor" aria-hidden="true" href="#lookup-hierarchy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lookup hierarchy&lt;/h3&gt;
&lt;p&gt;Command-line options have defaults that you can see in &lt;code&gt;--help&lt;/code&gt;. A &lt;code&gt;pyproject.toml&lt;/code&gt; can
override those defaults. Finally, options provided by the user on the command line
override both.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will only ever use one &lt;code&gt;pyproject.toml&lt;/code&gt; file during an entire run. It doesn't
look for multiple files, and doesn't compose configuration from different levels of the
file hierarchy.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-editor-integration" class="anchor" aria-hidden="true" href="#editor-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Editor integration&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-emacs" class="anchor" aria-hidden="true" href="#emacs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Emacs&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/proofit404/blacken"&gt;proofit404/blacken&lt;/a&gt; or
&lt;a href="https://github.com/jorgenschaefer/elpy"&gt;Elpy&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pycharmintellij-idea" class="anchor" aria-hidden="true" href="#pycharmintellij-idea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyCharm/IntelliJ IDEA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Locate your &lt;code&gt;black&lt;/code&gt; installation folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS / Linux / BSD:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;which black&lt;/span&gt;
&lt;span class="pl-c1"&gt;/usr/local/bin/black  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Windows:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;where black&lt;/span&gt;
&lt;span class="pl-c1"&gt;%LocalAppData%\Programs\Python\Python36-32\Scripts\black.exe  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Open External tools in PyCharm/IntelliJ IDEA&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PyCharm -&amp;gt; Preferences -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;On Windows / Linux / BSD:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;File -&amp;gt; Settings -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;p&gt;Click the + icon to add a new external tool with the following values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;Description: Black is the uncompromising Python code formatter.&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;"$FilePath$"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Format the currently opened file by selecting &lt;code&gt;Tools -&amp;gt; External Tools -&amp;gt; black&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, you can set a keyboard shortcut by navigating to
&lt;code&gt;Preferences or Settings -&amp;gt; Keymap -&amp;gt; External Tools -&amp;gt; External Tools - Black&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optionally, run &lt;em&gt;Black&lt;/em&gt; on every file save:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have the
&lt;a href="https://plugins.jetbrains.com/plugin/7177-file-watchers" rel="nofollow"&gt;File Watcher&lt;/a&gt; plugin
installed.&lt;/li&gt;
&lt;li&gt;Go to &lt;code&gt;Preferences or Settings -&amp;gt; Tools -&amp;gt; File Watchers&lt;/code&gt; and click &lt;code&gt;+&lt;/code&gt; to add a
new watcher:
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;File type: Python&lt;/li&gt;
&lt;li&gt;Scope: Project Files&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Output paths to refresh: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Working directory: &lt;code&gt;$ProjectFileDir$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Uncheck "Auto-save edited files to trigger the watcher"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-wing-ide" class="anchor" aria-hidden="true" href="#wing-ide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wing IDE&lt;/h3&gt;
&lt;p&gt;Wing supports black via the OS Commands tool, as explained in the Wing documentation on
&lt;a href="https://wingware.com/doc/edit/pep8" rel="nofollow"&gt;pep8 formatting&lt;/a&gt;. The detailed procedure is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Make sure it runs from the command line, e.g.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;black --help&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;In Wing IDE, activate the &lt;strong&gt;OS Commands&lt;/strong&gt; panel and define the command &lt;strong&gt;black&lt;/strong&gt; to
execute black on the currently selected file:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use the Tools -&amp;gt; OS Commands menu selection&lt;/li&gt;
&lt;li&gt;click on &lt;strong&gt;+&lt;/strong&gt; in &lt;strong&gt;OS Commands&lt;/strong&gt; -&amp;gt; New: Command line..
&lt;ul class="contains-task-list"&gt;
&lt;li&gt;Title: black&lt;/li&gt;
&lt;li&gt;Command Line: black %s&lt;/li&gt;
&lt;li&gt;I/O Encoding: Use Default&lt;/li&gt;
&lt;li&gt;Key Binding: F1&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Raise OS Commands when executed&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Auto-save files before execution&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Line mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Select a file in the editor and press &lt;strong&gt;F1&lt;/strong&gt; , or whatever key binding you selected
in step 3, to reformat the file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-vim" class="anchor" aria-hidden="true" href="#vim"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vim&lt;/h3&gt;
&lt;p&gt;Commands and shortcuts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;:Black&lt;/code&gt; to format the entire file (ranges not supported);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackUpgrade&lt;/code&gt; to upgrade &lt;em&gt;Black&lt;/em&gt; inside the virtualenv;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackVersion&lt;/code&gt; to get the current version of &lt;em&gt;Black&lt;/em&gt; inside the virtualenv.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;g:black_fast&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_linelength&lt;/code&gt; (defaults to &lt;code&gt;88&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_skip_string_normalization&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_virtualenv&lt;/code&gt; (defaults to &lt;code&gt;~/.vim/black&lt;/code&gt; or &lt;code&gt;~/.local/share/nvim/black&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install with &lt;a href="https://github.com/junegunn/vim-plug"&gt;vim-plug&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plug 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with &lt;a href="https://github.com/VundleVim/Vundle.vim"&gt;Vundle&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plugin 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can copy the plugin from
&lt;a href="https://github.com/psf/black/tree/master/plugin/black.vim"&gt;plugin/black.vim&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/.vim/pack/python/start/black/plugin
curl https://raw.githubusercontent.com/psf/black/master/plugin/black.vim -o ~/.vim/pack/python/start/black/plugin/black.vim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me know if this requires any changes to work with Vim 8's builtin &lt;code&gt;packadd&lt;/code&gt;, or
Pathogen, and so on.&lt;/p&gt;
&lt;p&gt;This plugin &lt;strong&gt;requires Vim 7.0+ built with Python 3.6+ support&lt;/strong&gt;. It needs Python 3.6 to
be able to run &lt;em&gt;Black&lt;/em&gt; inside the Vim process which is much faster than calling an
external command.&lt;/p&gt;
&lt;p&gt;On first run, the plugin creates its own virtualenv using the right Python version and
automatically installs &lt;em&gt;Black&lt;/em&gt;. You can upgrade it later by calling &lt;code&gt;:BlackUpgrade&lt;/code&gt; and
restarting Vim.&lt;/p&gt;
&lt;p&gt;If you need to do anything special to make your virtualenv work and install &lt;em&gt;Black&lt;/em&gt; (for
example you want to run a version from master), create a virtualenv manually and point
&lt;code&gt;g:black_virtualenv&lt;/code&gt; to it. The plugin will use it.&lt;/p&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on save, add the following line to &lt;code&gt;.vimrc&lt;/code&gt; or &lt;code&gt;init.vim&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autocmd BufWritePre *.py execute ':Black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on a key press (e.g. F9 below), add this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nnoremap &amp;lt;F9&amp;gt; :Black&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;How to get Vim with Python 3.6?&lt;/strong&gt; On Ubuntu 17.10 Vim comes with Python 3.6 by
default. On macOS with Homebrew run: &lt;code&gt;brew install vim --with-python3&lt;/code&gt;. When building
Vim from source, use: &lt;code&gt;./configure --enable-python3interp=yes&lt;/code&gt;. There's many guides
online how to do this.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h3&gt;
&lt;p&gt;Use the
&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python" rel="nofollow"&gt;Python extension&lt;/a&gt;
(&lt;a href="https://code.visualstudio.com/docs/python/editing#_formatting" rel="nofollow"&gt;instructions&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sublimetext-3" class="anchor" aria-hidden="true" href="#sublimetext-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SublimeText 3&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/jgirardet/sublack"&gt;sublack plugin&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jupyter-notebook-magic" class="anchor" aria-hidden="true" href="#jupyter-notebook-magic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook Magic&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/csurfer/blackcellmagic"&gt;blackcellmagic&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-language-server" class="anchor" aria-hidden="true" href="#python-language-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Language Server&lt;/h3&gt;
&lt;p&gt;If your editor supports the &lt;a href="https://langserver.org/" rel="nofollow"&gt;Language Server Protocol&lt;/a&gt; (Atom,
Sublime Text, Visual Studio Code and many more), you can use the
&lt;a href="https://github.com/palantir/python-language-server"&gt;Python Language Server&lt;/a&gt; with the
&lt;a href="https://github.com/rupert/pyls-black"&gt;pyls-black&lt;/a&gt; plugin.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-atomnuclide" class="anchor" aria-hidden="true" href="#atomnuclide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Atom/Nuclide&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://atom.io/packages/python-black" rel="nofollow"&gt;python-black&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-kakoune" class="anchor" aria-hidden="true" href="#kakoune"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kakoune&lt;/h3&gt;
&lt;p&gt;Add the following hook to your kakrc, then run black with &lt;code&gt;:format&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hook global WinSetOption filetype=python %{
    set-option window formatcmd 'black -q  -'
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-other-editors" class="anchor" aria-hidden="true" href="#other-editors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other editors&lt;/h3&gt;
&lt;p&gt;Other editors will require external contributions.&lt;/p&gt;
&lt;p&gt;Patches welcome! &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;🍰&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;Any tool that can pipe code through &lt;em&gt;Black&lt;/em&gt; using its stdio mode (just
&lt;a href="https://www.tldp.org/LDP/abs/html/special-chars.html#DASHREF2" rel="nofollow"&gt;use &lt;code&gt;-&lt;/code&gt; as the file name&lt;/a&gt;).
The formatted code will be returned on stdout (unless &lt;code&gt;--check&lt;/code&gt; was passed). &lt;em&gt;Black&lt;/em&gt;
will still emit messages on stderr but that shouldn't affect your use case.&lt;/p&gt;
&lt;p&gt;This can be used for example with PyCharm's or IntelliJ's
&lt;a href="https://www.jetbrains.com/help/pycharm/file-watchers.html" rel="nofollow"&gt;File Watchers&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-blackd" class="anchor" aria-hidden="true" href="#blackd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;blackd&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is a small HTTP server that exposes &lt;em&gt;Black&lt;/em&gt;'s functionality over a simple
protocol. The main benefit of using it is to avoid paying the cost of starting up a new
&lt;em&gt;Black&lt;/em&gt; process every time you want to blacken a file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage-1" class="anchor" aria-hidden="true" href="#usage-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is not packaged alongside &lt;em&gt;Black&lt;/em&gt; by default because it has additional
dependencies. You will need to do &lt;code&gt;pip install black[d]&lt;/code&gt; to install it.&lt;/p&gt;
&lt;p&gt;You can start the server on the default port, binding only to the local interface by
running &lt;code&gt;blackd&lt;/code&gt;. You will see a single line mentioning the server's version, and the
host and port it's listening on. &lt;code&gt;blackd&lt;/code&gt; will then print an access log similar to most
web servers on standard output, merged with any exception traces caused by invalid
formatting requests.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; provides even less options than &lt;em&gt;Black&lt;/em&gt;. You can see them by running
&lt;code&gt;blackd --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;Usage: blackd [OPTIONS]

Options:
  --bind-host TEXT                Address to bind the server to.
  --bind-port INTEGER             Port to listen on
  --version                       Show the version and exit.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no official blackd client tool (yet!). You can test that blackd is working
using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blackd --bind-port 9090 &amp;amp;  # or let blackd choose a port
curl -s -XPOST "localhost:9090" -d "print('valid')"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-protocol" class="anchor" aria-hidden="true" href="#protocol"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Protocol&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; only accepts &lt;code&gt;POST&lt;/code&gt; requests at the &lt;code&gt;/&lt;/code&gt; path. The body of the request should
contain the python source code to be formatted, encoded according to the &lt;code&gt;charset&lt;/code&gt; field
in the &lt;code&gt;Content-Type&lt;/code&gt; request header. If no &lt;code&gt;charset&lt;/code&gt; is specified, &lt;code&gt;blackd&lt;/code&gt; assumes
&lt;code&gt;UTF-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are a few HTTP headers that control how the source is formatted. These correspond
to command line flags for &lt;em&gt;Black&lt;/em&gt;. There is one exception to this: &lt;code&gt;X-Protocol-Version&lt;/code&gt;
which if present, should have the value &lt;code&gt;1&lt;/code&gt;, otherwise the request is rejected with
&lt;code&gt;HTTP 501&lt;/code&gt; (Not Implemented).&lt;/p&gt;
&lt;p&gt;The headers controlling how code is formatted are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X-Line-Length&lt;/code&gt;: corresponds to the &lt;code&gt;--line-length&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Skip-String-Normalization&lt;/code&gt;: corresponds to the &lt;code&gt;--skip-string-normalization&lt;/code&gt;
command line flag. If present and its value is not the empty string, no string
normalization will be performed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Fast-Or-Safe&lt;/code&gt;: if set to &lt;code&gt;fast&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--fast&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Python-Variant&lt;/code&gt;: if set to &lt;code&gt;pyi&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--pyi&lt;/code&gt; command line flag. Otherwise, its value must correspond to a Python version or
a set of comma-separated Python versions, optionally prefixed with &lt;code&gt;py&lt;/code&gt;. For example,
to request code that is compatible with Python 3.5 and 3.6, set the header to
&lt;code&gt;py3.5,py3.6&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Diff&lt;/code&gt;: corresponds to the &lt;code&gt;--diff&lt;/code&gt; command line flag. If present, a diff of the
formats will be output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of these headers are set to invalid values, &lt;code&gt;blackd&lt;/code&gt; returns a &lt;code&gt;HTTP 400&lt;/code&gt; error
response, mentioning the name of the problematic header in the message body.&lt;/p&gt;
&lt;p&gt;Apart from the above, &lt;code&gt;blackd&lt;/code&gt; can produce the following response codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;HTTP 204&lt;/code&gt;: If the input is already well-formatted. The response body is empty.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 200&lt;/code&gt;: If formatting was needed on the input. The response body contains the
blackened Python code, and the &lt;code&gt;Content-Type&lt;/code&gt; header is set accordingly.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 400&lt;/code&gt;: If the input contains a syntax error. Details of the error are returned in
the response body.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 500&lt;/code&gt;: If there was any kind of error while trying to format the input. The
response body contains a textual representation of the error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The response headers include a &lt;code&gt;X-Black-Version&lt;/code&gt; header containing the version of
&lt;em&gt;Black&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-version-control-integration" class="anchor" aria-hidden="true" href="#version-control-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Version control integration&lt;/h2&gt;
&lt;p&gt;Use &lt;a href="https://pre-commit.com/" rel="nofollow"&gt;pre-commit&lt;/a&gt;. Once you
&lt;a href="https://pre-commit.com/#install" rel="nofollow"&gt;have it installed&lt;/a&gt;, add this to the
&lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; in your repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-yaml"&gt;&lt;pre&gt;&lt;span class="pl-ent"&gt;repos&lt;/span&gt;:
  - &lt;span class="pl-ent"&gt;repo&lt;/span&gt;: &lt;span class="pl-s"&gt;https://github.com/psf/black&lt;/span&gt;
    &lt;span class="pl-ent"&gt;rev&lt;/span&gt;: &lt;span class="pl-s"&gt;stable&lt;/span&gt;
    &lt;span class="pl-ent"&gt;hooks&lt;/span&gt;:
      - &lt;span class="pl-ent"&gt;id&lt;/span&gt;: &lt;span class="pl-s"&gt;black&lt;/span&gt;
        &lt;span class="pl-ent"&gt;language_version&lt;/span&gt;: &lt;span class="pl-s"&gt;python3.6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then run &lt;code&gt;pre-commit install&lt;/code&gt; and you're ready to go.&lt;/p&gt;
&lt;p&gt;Avoid using &lt;code&gt;args&lt;/code&gt; in the hook. Instead, store necessary configuration in
&lt;code&gt;pyproject.toml&lt;/code&gt; so that editors and command-line usage of Black all behave consistently
for your project. See &lt;em&gt;Black&lt;/em&gt;'s own &lt;a href="/pyproject.toml"&gt;pyproject.toml&lt;/a&gt; for an example.&lt;/p&gt;
&lt;p&gt;If you're already using Python 3.7, switch the &lt;code&gt;language_version&lt;/code&gt; accordingly. Finally,
&lt;code&gt;stable&lt;/code&gt; is a tag that is pinned to the latest release on PyPI. If you'd rather run on
master, this is also an option.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ignoring-unmodified-files" class="anchor" aria-hidden="true" href="#ignoring-unmodified-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ignoring unmodified files&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; remembers files it has already formatted, unless the &lt;code&gt;--diff&lt;/code&gt; flag is used or
code is passed via standard input. This information is stored per-user. The exact
location of the file depends on the &lt;em&gt;Black&lt;/em&gt; version and the system on which &lt;em&gt;Black&lt;/em&gt; is
run. The file is non-portable. The standard location on common operating systems is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows:
&lt;code&gt;C:\\Users\&amp;lt;username&amp;gt;\AppData\Local\black\black\Cache\&amp;lt;version&amp;gt;\cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;macOS:
&lt;code&gt;/Users/&amp;lt;username&amp;gt;/Library/Caches/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Linux:
&lt;code&gt;/home/&amp;lt;username&amp;gt;/.cache/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;file-mode&lt;/code&gt; is an int flag that determines whether the file was formatted as 3.6+ only,
as .pyi, and whether string normalization was omitted.&lt;/p&gt;
&lt;p&gt;To override the location of these files on macOS or Linux, set the environment variable
&lt;code&gt;XDG_CACHE_HOME&lt;/code&gt; to your preferred location. For example, if you want to put the cache
in the directory you're running &lt;em&gt;Black&lt;/em&gt; from, set &lt;code&gt;XDG_CACHE_HOME=.cache&lt;/code&gt;. &lt;em&gt;Black&lt;/em&gt; will
then write the above files to &lt;code&gt;.cache/black/&amp;lt;version&amp;gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-used-by" class="anchor" aria-hidden="true" href="#used-by"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Used by&lt;/h2&gt;
&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent
code style: pytest, tox, Pyramid, Django Channels, Hypothesis, attrs, SQLAlchemy,
Poetry, PyPA applications (Warehouse, Pipenv, virtualenv), pandas, Pillow, every Datadog
Agent Integration.&lt;/p&gt;
&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testimonials" class="anchor" aria-hidden="true" href="#testimonials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testimonials&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;,
&lt;a href="https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips" rel="nofollow"&gt;writer&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don't have to be.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href="https://www.attrs.org/" rel="nofollow"&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of
Twisted and CPython:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An auto-formatter that doesn't suck is all I want for Xmas!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;Django&lt;/a&gt; core developer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At least the name is good.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href="http://python-requests.org/" rel="nofollow"&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and
&lt;a href="https://docs.pipenv.org/" rel="nofollow"&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-show-your-style" class="anchor" aria-hidden="true" href="#show-your-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Show your style&lt;/h2&gt;
&lt;p&gt;Use the badge in your project's README.md:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;[![&lt;span class="pl-e"&gt;Code style: black&lt;/span&gt;](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like this:
&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-black" class="anchor" aria-hidden="true" href="#contributing-to-black"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to &lt;em&gt;Black&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;In terms of inspiration, &lt;em&gt;Black&lt;/em&gt; is about as configurable as &lt;em&gt;gofmt&lt;/em&gt;. This is
deliberate.&lt;/p&gt;
&lt;p&gt;Bug reports and fixes are always welcome! However, before you suggest a new feature or
configuration knob, ask yourself why you want it. If it enables better integration with
some workflow, fixes an inconsistency, speeds things up, and so on - go for it! On the
other hand, if your answer is "because I don't like a particular formatting" then you're
not ready to embrace &lt;em&gt;Black&lt;/em&gt; yet. Such changes are unlikely to get accepted. You can
still try but prepare to be disappointed.&lt;/p&gt;
&lt;p&gt;More details can be found in &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1910b0" class="anchor" aria-hidden="true" href="#1910b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.10b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 572 assignment expressions (#711)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 570 positional-only arguments (#943)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for async generators (#593)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for pre-splitting collections by putting an explicit trailing comma
inside (#826)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;black -c&lt;/code&gt; as a way to format code passed from the command line (#761)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;--safe now works with Python 2 code (#840)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed grammar selection for Python 2-specific code (#765)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed feature detection for trailing commas in function definitions and call sites
(#763)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt;/&lt;code&gt;# fmt: on&lt;/code&gt; comment pairs placed multiple times within the same block of
code now behave correctly (#1005)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Windows machines with more than 61 cores (#838)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on standalone comments prepended with a backslash (#767)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on &lt;code&gt;from&lt;/code&gt; ... &lt;code&gt;import&lt;/code&gt; blocks with comments (#829)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Python 3.7 on some platform configurations (#494)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails on comments in from-imports (#671)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails when the file starts with a backslash (#922)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer merges regular comments with type comments (#1027)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer splits long lines that contain type comments (#997)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;removed unnecessary parentheses around &lt;code&gt;yield&lt;/code&gt; expressions (#834)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around long tuples in unpacking assignments (#832)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around complex powers when they are prefixed by a unary operator
(#646)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed bug that led &lt;em&gt;Black&lt;/em&gt; format some code with a line length target of 1 (#762)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer introduces quotes in f-string subexpressions on string boundaries
(#863)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if &lt;em&gt;Black&lt;/em&gt; puts parenthesis around a single expression, it moves comments to the
wrapped expression instead of after the brackets (#872)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now returns the version of &lt;em&gt;Black&lt;/em&gt; in the response headers (#1013)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; can now output the diff of formats on source code when the &lt;code&gt;X-Diff&lt;/code&gt; header is
provided (#969)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-193b0" class="anchor" aria-hidden="true" href="#193b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.3b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;new option &lt;code&gt;--target-version&lt;/code&gt; to control which Python versions &lt;em&gt;Black&lt;/em&gt;-formatted code
should target (#618)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;deprecated &lt;code&gt;--py36&lt;/code&gt; (use &lt;code&gt;--target-version=py36&lt;/code&gt; instead) (#724)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer normalizes numeric literals to include &lt;code&gt;_&lt;/code&gt; separators (#696)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;long &lt;code&gt;del&lt;/code&gt; statements are now split into multiple lines (#698)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;type comments are no longer mangled in function signatures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;improved performance of formatting deeply nested data structures (#509)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now properly formats multiple files in parallel on Windows (#632)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now creates cache files atomically which allows it to be used in parallel
pipelines (like &lt;code&gt;xargs -P8&lt;/code&gt;) (#673)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now correctly indents comments in files that were previously formatted with
tabs (#262)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now supports CORS (#622)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-189b0" class="anchor" aria-hidden="true" href="#189b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.9b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are now formatted by &lt;em&gt;Black&lt;/em&gt; (#452, #461, #464, #469):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are normalized to include &lt;code&gt;_&lt;/code&gt; separators on Python 3.6+ code&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-numeric-underscore-normalization&lt;/code&gt; to disable the above behavior and
leave numeric underscores as they were in the input&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code with &lt;code&gt;_&lt;/code&gt; in numeric literals is recognized as Python 3.6+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;most letters in numeric literals are lowercased (e.g., in &lt;code&gt;1e10&lt;/code&gt;, &lt;code&gt;0x01&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hexadecimal digits are always uppercased (e.g. &lt;code&gt;0xBADC0DE&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;blackd&lt;/code&gt;, see &lt;a href="#blackd"&gt;its documentation&lt;/a&gt; for more info (#349)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjacent string literals are now correctly split into multiple lines (#463)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;trailing comma is now added to single imports that don't fit on a line (#250)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cache is now populated when &lt;code&gt;--check&lt;/code&gt; is successful for a file which speeds up
consecutive checks of properly formatted unmodified files (#448)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whitespace at the beginning of the file is now removed (#399)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed mangling &lt;a href="http://mpastell.com/pweave/" rel="nofollow"&gt;pweave&lt;/a&gt; and
&lt;a href="https://pythonhosted.org/spyder/" rel="nofollow"&gt;Spyder IDE&lt;/a&gt; special comments (#532)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when unpacking big tuples (#267)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of &lt;code&gt;__future__&lt;/code&gt; imports with renames (#389)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed scope of &lt;code&gt;# fmt: off&lt;/code&gt; when directly preceding &lt;code&gt;yield&lt;/code&gt; and other nodes (#385)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of lambda expressions with default arguments (#468)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;async for&lt;/code&gt; statements: &lt;em&gt;Black&lt;/em&gt; no longer breaks them into separate lines (#372)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;note: the Vim plugin stopped registering &lt;code&gt;,=&lt;/code&gt; as a default chord as it turned out to
be a bad idea (#415)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b4" class="anchor" aria-hidden="true" href="#186b4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hotfix: don't freeze when multiple comments directly precede &lt;code&gt;# fmt: off&lt;/code&gt; (#371)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b3" class="anchor" aria-hidden="true" href="#186b3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) now have blank lines added after constants (#340)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are now much more dependable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they now work also within bracket pairs (#329)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now correctly work across function/class boundaries (#335)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now work when an indentation block starts with empty lines or misaligned
comments (#334)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;made Click not fail on invalid environments; note that Click is right but the
likelihood we'll need to access non-ASCII file paths when dealing with Python source
code is low (#277)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper formatting of f-strings with quotes inside interpolated expressions
(#322)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown when long list literals where found in a file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown on AST nodes with very many siblings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed cannibalizing backslashes during string normalization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed a crash due to symbolic links pointing outside of the project directory (#338)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b2" class="anchor" aria-hidden="true" href="#186b2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--config&lt;/code&gt; (#65)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;-h&lt;/code&gt; equivalent to &lt;code&gt;--help&lt;/code&gt; (#316)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper unmodified file caching when &lt;code&gt;-S&lt;/code&gt; was used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra space in string unpacking (#305)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of empty triple quoted strings (#313)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown in comment placement calculation on lines without comments&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b1" class="anchor" aria-hidden="true" href="#186b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output human-facing information on stdout (#299)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output cake emoji on non-zero return code (#300)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b0" class="anchor" aria-hidden="true" href="#186b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; (#270)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-string-normalization&lt;/code&gt; (#118)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--verbose&lt;/code&gt; (#283)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the header output in &lt;code&gt;--diff&lt;/code&gt; now actually conforms to the unified diff spec&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed long trivial assignments being wrapped in unnecessary parentheses (#273)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary parentheses when a line contained multiline strings (#232)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed stdin handling not working correctly if an old version of Click was used (#276)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now preserves line endings when formatting a file in place (#258)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b1" class="anchor" aria-hidden="true" href="#185b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--pyi&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--py36&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python grammar pickle caches are stored with the formatting caches, making &lt;em&gt;Black&lt;/em&gt;
work in environments where site-packages is not user-writable (#192)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now enforces a PEP 257 empty line after a class-level docstring (and/or
fields) and the first method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when standalone comments were present in a trailer that
was omitted from line splitting on a large expression (#237)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed optional parentheses being removed within &lt;code&gt;# fmt: off&lt;/code&gt; sections (#224)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when stars in very long imports were incorrectly wrapped
in optional parentheses (#234)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when inline comments were moved around in a trailer that was
omitted from line splitting on a large expression (#238)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a class declaration and the first method if no class
docstring or fields are present (#219)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a function signature and an inner function or inner
class (#196)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b0" class="anchor" aria-hidden="true" href="#185b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;call chains are now formatted according to the
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interfaces&lt;/a&gt; style (#67)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;data structure literals (tuples, lists, dictionaries, and sets) are now also always
exploded like imports when they don't fit in a single line (#152)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;slices are now formatted according to PEP 8 (#178)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;parentheses are now also managed automatically on the right-hand side of assignments
and return statements (#140)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;math operators now use their respective priorities for delimiting multiline
expressions (#148)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;optional parentheses are now omitted on expressions that start or end with a bracket
and only contain a single operator (#177)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;empty parentheses in a class definition are now removed (#145, #180)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;string prefixes are now standardized to lowercase and &lt;code&gt;u&lt;/code&gt; is removed on Python 3.6+
only code and Python 2.7+ code with the &lt;code&gt;unicode_literals&lt;/code&gt; future import (#188, #198,
#199)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) are now formatted in a style that is consistent with PEP
484 (#207, #210)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;progress when reformatting many files is now reported incrementally&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed trailers (content with brackets) being unnecessarily exploded into their own
lines after a dedented closing bracket (#119)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed an invalid trailing comma sometimes left in imports (#185)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed non-deterministic formatting when multiple pairs of removable parentheses were
used (#183)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed multiline strings being unnecessarily wrapped in optional parentheses in long
assignments (#215)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed not splitting long from-imports with only a single name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed Python 3.6+ file discovery by also looking at function calls with unpacking.
This fixed non-deterministic formatting if trailing commas where used both in function
signatures with stars and function calls with stars but the former would be
reformatted to a single line.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash on dealing with optional parentheses (#193)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed "is", "is not", "in", and "not in" not considered operators for splitting
purposes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash when dead symlinks where encountered&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a4" class="anchor" aria-hidden="true" href="#184a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;don't populate the cache on &lt;code&gt;--check&lt;/code&gt; (#175)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a3" class="anchor" aria-hidden="true" href="#184a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added a "cache"; files already reformatted that haven't changed on disk won't be
reformatted again (#109)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;--check&lt;/code&gt; and &lt;code&gt;--diff&lt;/code&gt; are no longer mutually exclusive (#149)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generalized star expression handling, including double stars; this fixes
multiplication making expressions "unsafe" for trailing commas (#132)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer enforces putting empty lines behind control flow statements (#90)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now splits imports like "Mode 3 + trailing comma" of isort (#127)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed comment indentation when a standalone comment closes a block (#16, #32)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed standalone comments receiving extra empty lines if immediately preceding a
class, def, or decorator (#56, #154)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;--diff&lt;/code&gt; not showing entire path (#130)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of complex expressions after star and double stars in function calls
(#2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting on comma in lambda arguments (#133)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing splits of ternary expressions (#141)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a2" class="anchor" aria-hidden="true" href="#184a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of unaligned standalone comments (#99, #112)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed placement of dictionary unpacking inside dictionary literals (#111)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vim plugin now works on Windows, too&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when encountering unnecessarily escaped quotes in a string
(#120)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a1" class="anchor" aria-hidden="true" href="#184a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--quiet&lt;/code&gt; (#78)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added automatic parentheses management (#4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;a href="https://pre-commit.com" rel="nofollow"&gt;pre-commit&lt;/a&gt; integration (#103, #104)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed reporting on &lt;code&gt;--check&lt;/code&gt; with multiple files (#101, #102)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed removing backslash escapes from raw strings (#100, #105)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a0" class="anchor" aria-hidden="true" href="#184a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--diff&lt;/code&gt; (#87)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;add line breaks before all delimiters, except in cases like commas, to better comply
with PEP 8 (#73)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;standardize string literals to use double quotes (almost) everywhere (#75)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed handling of standalone comments within nested bracketed expressions; &lt;em&gt;Black&lt;/em&gt;
will no longer produce super long lines or put all standalone comments at the end of
the expression (#22)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: don't crash and burn on empty lines with trailing whitespace
(#80)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: &lt;code&gt;# yapf: disable&lt;/code&gt; usage as trailing comment would cause
&lt;em&gt;Black&lt;/em&gt; to not emit the rest of the file (#95)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when CTRL+C is pressed while formatting many files, &lt;em&gt;Black&lt;/em&gt; no longer freaks out with
a flurry of asyncio-related exceptions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only allow up to two empty lines on module level and only single empty lines within
functions (#74)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a4" class="anchor" aria-hidden="true" href="#183a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are implemented (#5)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;automatic detection of deprecated Python 2 forms of print statements and exec
statements in the formatted file (#49)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use proper spaces for complex expressions in default values of typed function
arguments (#60)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only return exit code 1 when --check is used (#50)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't remove single trailing commas from square bracket indexing (#59)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't omit whitespace if the previous factor leaf wasn't a math operator (#55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in kwarg unpacking if it's the first argument (#46)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in
&lt;a href="http://www.sphinx-doc.org/en/stable/ext/autodoc.html#directive-autoattribute" rel="nofollow"&gt;Sphinx auto-attribute comments&lt;/a&gt;
(#68)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a3" class="anchor" aria-hidden="true" href="#183a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;don't remove single empty lines outside of bracketed expressions (#19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added ability to pipe formatting from stdin to stdin (#25)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;restored ability to format code with legacy usage of &lt;code&gt;async&lt;/code&gt; as a name (#20, #42)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;even better handling of numpy-style array indexing (#33, again)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a2" class="anchor" aria-hidden="true" href="#183a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;changed positioning of binary operators to occur at beginning of lines instead of at
the end, following
&lt;a href="https://github.com/python/peps/commit/c59c4376ad233a62ca4b3a6060c81368bd21e85b"&gt;a recent change to PEP 8&lt;/a&gt;
(#21)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ignore empty bracket pairs while splitting. This avoids very weirdly looking
formattings (#34, #35)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;remove a trailing comma if there is a single argument to a call&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if top level functions were separated by a comment, don't put four empty lines after
the upper function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting of newlines with imports&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unintentional folding of post scriptum standalone comments into last statement
if it was a simple statement (#18, #28)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing space in numpy-style array indexing (#33)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after star-based unary expressions (#31)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a1" class="anchor" aria-hidden="true" href="#183a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--check&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only put trailing commas in function signatures and calls if it's safe to do so. If
the file is Python 3.6+ it's always safe, otherwise only safe if there are no &lt;code&gt;*args&lt;/code&gt;
or &lt;code&gt;**kwargs&lt;/code&gt; used in the signature or call. (#8)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid spacing of dots in relative imports (#6, #13)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting after comma on unpacked variables in for-loops (#23)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space in parenthesized set expressions (#7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after opening parentheses and in default arguments (#14, #17)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after unary operators when the operand was a complex expression
(#15)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a0" class="anchor" aria-hidden="true" href="#183a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;first published version, Happy &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;🍰&lt;/g-emoji&gt; Day 2018!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;alpha quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date-versioned (see: &lt;a href="https://calver.org/" rel="nofollow"&gt;https://calver.org/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;Glued together by &lt;a href="mailto:lukasz@langa.pl"&gt;Łukasz Langa&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Maintained with &lt;a href="mailto:carolcode@willingconsulting.com"&gt;Carol Willing&lt;/a&gt;,
&lt;a href="mailto:carl@oddbird.net"&gt;Carl Meyer&lt;/a&gt;,
&lt;a href="mailto:jelle.zijlstra@gmail.com"&gt;Jelle Zijlstra&lt;/a&gt;,
&lt;a href="mailto:mail@autophagy.io"&gt;Mika Naylor&lt;/a&gt;, and
&lt;a href="mailto:zsol.zsol@gmail.com"&gt;Zsolt Dollenstein&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Multiple contributions by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mailto:cryptolabour@gmail.com"&gt;Abdur-Rahmaan Janhangeer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@adamj.eu"&gt;Adam Johnson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@grande.coffee"&gt;Alexander Huynh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andrew.thorp.dev@gmail.com"&gt;Andrew Thorp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:dyuuus@yandex.ru"&gt;Andrey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andy@andyfreeland.net"&gt;Andy Freeland&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asottile@umich.edu"&gt;Anthony Sottile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:arjaan.buijk@gmail.com"&gt;Arjaan Buijk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:proofit404@gmail.com"&gt;Artem Malyshev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asgerdrewsen@gmail.com"&gt;Asger Hautop Drewsen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:raf@durin42.com"&gt;Augie Fackler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:aviskarkc10@gmail.com"&gt;Aviskar KC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@benjam.info"&gt;Benjamin Woodruff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:brandtbucher@gmail.com"&gt;Brandt Bucher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Charles Reid&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:christian@python.org"&gt;Christian Heimes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:chuck.wooters@microsoft.com"&gt;Chuck Wooters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@thequod.de"&gt;Daniel Hahler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:polycitizen@gmail.com"&gt;Daniel M. Capella&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Daniele Esposti&lt;/li&gt;
&lt;li&gt;dylanjblack&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:eli@treuherz.com"&gt;Eli Treuherz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:fthiery@gmail.com"&gt;Florent Thiery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hauntsaninja&lt;/li&gt;
&lt;li&gt;Hugo van Kemenade&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ivan.katanic@gmail.com"&gt;Ivan Katanić&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@jasonfried.info"&gt;Jason Fried&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ijkl@netc.fr"&gt;jgirardet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jma353@cornell.edu"&gt;Joe Antonakakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jon.dufresne@gmail.com"&gt;Jon Dufresne&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ojiidotch@gmail.com"&gt;Jonas Obrist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:joshbode@fastmail.com"&gt;Josh Bode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hello@juanlu.space"&gt;Juan Luis Cano Rodríguez&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:katie@glasnt.com"&gt;Katie McLaughlin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lawrence Chan&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@linusgroh.de"&gt;Linus Groh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:luka.sterbic@gmail.com"&gt;Luka Sterbic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mariatta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:vaneseltine@gmail.com"&gt;Matt VanEseltine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael.flaxman@gmail.com"&gt;Michael Flaxman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sully@msully.net"&gt;Michael J. Sullivan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael@mcclimon.org"&gt;Michael McClimon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miggaiowski@gmail.com"&gt;Miguel Gaiowski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:roshi@fedoraproject.org"&gt;Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:minho42@gmail.com"&gt;Min ho Kim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miroslav@miki725.com"&gt;Miroslav Shubernetskiy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:neraste.herr10@gmail.com"&gt;Neraste&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ofekmeister@gmail.com"&gt;Ofek Lev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:osaetindaniel@gmail.com"&gt;Osaetin Daniel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:Pablogsal@gmail.com"&gt;Pablo Galindo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@peterbe.com"&gt;Peter Bengtsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pmacosta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:rishijha424@gmail.com"&gt;Rishikesh Jha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hi@stavros.io"&gt;Stavros Korokithakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sirosen@globus.org"&gt;Stephen Rosen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:snlkapil@gmail.com"&gt;Sunil Kapil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:thomas.c.lu@gmail.com"&gt;Thom Lu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:tom@tomchristie.com"&gt;Tom Christie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:uranusjr@gmail.com"&gt;Tzu-ping Chung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ukshah2@illinois.edu"&gt;Utsav Shah&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vezeli&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sharma.vishwas88@gmail.com"&gt;Vishwas B Sharma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:yngve@hoiseth.net"&gt;Yngve Høiseth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:1998uriyyo@gmail.com"&gt;Yurii Karabas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>psf</author><guid isPermaLink="false">https://github.com/psf/black</guid><pubDate>Tue, 05 Nov 2019 00:20:00 GMT</pubDate></item><item><title>huggingface/transformers #21 in Python, This week</title><link>https://github.com/huggingface/transformers</link><description>&lt;p&gt;&lt;i&gt;🤗 Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;br&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://circleci.com/gh/huggingface/transformers" rel="nofollow"&gt;
        &lt;img alt="Build" src="https://camo.githubusercontent.com/045b8639882280ff5cd38c403499977386c25134/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f68756767696e67666163652f7472616e73666f726d6572732f6d6173746572" data-canonical-src="https://img.shields.io/circleci/build/github/huggingface/transformers/master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/blob/master/LICENSE"&gt;
        &lt;img alt="GitHub" src="https://camo.githubusercontent.com/440e73b137335cc0088bb06e6c90cc7b503b14a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68756767696e67666163652f7472616e73666f726d6572732e7376673f636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://huggingface.co/transformers/index.html" rel="nofollow"&gt;
        &lt;img alt="Documentation" src="https://camo.githubusercontent.com/b104c21f478c4d4a37f63292ab2898047f19ee24/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f68756767696e67666163652e636f2f7472616e73666f726d6572732f696e6465782e68746d6c2e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65" data-canonical-src="https://img.shields.io/website/http/huggingface.co/transformers/index.html.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/releases"&gt;
        &lt;img alt="GitHub release" src="https://camo.githubusercontent.com/8409fd8716dd1a11afa7ab38e1218b34918164eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f68756767696e67666163652f7472616e73666f726d6572732e737667" data-canonical-src="https://img.shields.io/github/release/huggingface/transformers.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch" class="anchor" aria-hidden="true" href="#state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;p&gt;State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch
&lt;/p&gt;&lt;/h3&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers (formerly known as &lt;code&gt;pytorch-transformers&lt;/code&gt; and &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;As easy to use as pytorch-transformers&lt;/li&gt;
&lt;li&gt;As powerful and concise as Keras&lt;/li&gt;
&lt;li&gt;High performance on NLU and NLG tasks&lt;/li&gt;
&lt;li&gt;Low barrier to entry for educators and practitioners&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;State-of-the-art NLP for everyone&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning researchers&lt;/li&gt;
&lt;li&gt;Hands-on practitioners&lt;/li&gt;
&lt;li&gt;AI/ML/NLP teachers and educators&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lower compute costs, smaller carbon footprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Researchers can share trained models instead of always retraining&lt;/li&gt;
&lt;li&gt;Practitioners can reduce compute time and production costs&lt;/li&gt;
&lt;li&gt;10 architectures with over 30 pretrained models, some in more than 100 languages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Choose the right framework for every part of a model's lifetime&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train state-of-the-art models in 3 lines of code&lt;/li&gt;
&lt;li&gt;Deep interoperability between TensorFlow 2.0 and PyTorch models&lt;/li&gt;
&lt;li&gt;Move a single model between TF2.0/PyTorch frameworks at will&lt;/li&gt;
&lt;li&gt;Seamlessly pick the right framework for training, evaluation, production&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Section&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;How to install the package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#model-architectures"&gt;Model architectures&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Architectures (with pretrained weights)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#online-demo"&gt;Online demo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimenting with this repo’s text generation capabilities&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour"&gt;Quick tour: Usage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tokenizers &amp;amp; models usage: Bert and GPT-2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Quick-tour-TF-20-training-and-PyTorch-interoperability"&gt;Quick tour: TF 2.0 and PyTorch &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Train a TF 2.0 model in 10 lines of code, load it in PyTorch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;Quick tour: Fine-tuning/usage scripts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Using provided scripts: GLUE, SQuAD and Text generation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-transformers-to-transformers"&gt;Migrating from pytorch-transformers to transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-transformers to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-pretrained-bert-to-transformers"&gt;Migrating from pytorch-pretrained-bert to pytorch-transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-pretrained-bert to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;Documentation&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.1.1" rel="nofollow"&gt;(v2.1.1)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.0.0" rel="nofollow"&gt;(v2.0.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.2.0" rel="nofollow"&gt;(v1.2.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.1.0" rel="nofollow"&gt;(v1.1.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.0.0" rel="nofollow"&gt;(v1.0.0)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Full API documentation and more&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+), PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-with-pip" class="anchor" aria-hidden="true" href="#with-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;With pip&lt;/h3&gt;
&lt;p&gt;First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers can be installed using pip as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install transformers&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-from-source" class="anchor" aria-hidden="true" href="#from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From source&lt;/h3&gt;
&lt;p&gt;Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install [--editable] &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h3&gt;
&lt;p&gt;A series of tests are included for the library and the example scripts. Library tests can be found in the &lt;a href="https://github.com/huggingface/transformers/tree/master/transformers/tests"&gt;tests folder&lt;/a&gt; and examples tests in the &lt;a href="https://github.com/huggingface/transformers/tree/master/examples"&gt;examples folder&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These tests can be run using &lt;code&gt;pytest&lt;/code&gt; (install pytest if needed with &lt;code&gt;pip install pytest&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.&lt;/p&gt;
&lt;p&gt;You can run the tests from the root of the cloned repository with the commands:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m pytest -sv ./transformers/tests/
python -m pytest -sv ./examples/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-do-you-want-to-run-a-transformer-model-on-a-mobile-device" class="anchor" aria-hidden="true" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do you want to run a Transformer model on a mobile device?&lt;/h3&gt;
&lt;p&gt;You should check out our &lt;a href="https://github.com/huggingface/swift-coreml-transformers"&gt;&lt;code&gt;swift-coreml-transformers&lt;/code&gt;&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains &lt;code&gt;GPT-2&lt;/code&gt;, &lt;code&gt;DistilGPT-2&lt;/code&gt;, &lt;code&gt;BERT&lt;/code&gt;, and &lt;code&gt;DistilBERT&lt;/code&gt;) to CoreML models that run on iOS devices.&lt;/p&gt;
&lt;p&gt;At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-architectures" class="anchor" aria-hidden="true" href="#model-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model architectures&lt;/h2&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers currently provides 10 NLU/NLG architectures:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/bert"&gt;BERT&lt;/a&gt;&lt;/strong&gt; (from Google) released with the paper &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/openai/finetune-transformer-lm"&gt;GPT&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt; by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;GPT-2&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt; by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/kimiyoung/transformer-xl"&gt;Transformer-XL&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1901.02860" rel="nofollow"&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt; by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/zihangdai/xlnet/"&gt;XLNet&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1906.08237" rel="nofollow"&gt;​XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt; by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/XLM/"&gt;XLM&lt;/a&gt;&lt;/strong&gt; (from Facebook) released together with the paper &lt;a href="https://arxiv.org/abs/1901.07291" rel="nofollow"&gt;Cross-lingual Language Model Pretraining&lt;/a&gt; by Guillaume Lample and Alexis Conneau.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta"&gt;RoBERTa&lt;/a&gt;&lt;/strong&gt; (from Facebook), released together with the paper a &lt;a href="https://arxiv.org/abs/1907.11692" rel="nofollow"&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt; by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilBERT&lt;/a&gt;&lt;/strong&gt; (from HuggingFace), released together with the paper &lt;a href="https://arxiv.org/abs/1910.01108" rel="nofollow"&gt;DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter&lt;/a&gt; by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilGPT2&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/salesforce/ctrl/"&gt;CTRL&lt;/a&gt;&lt;/strong&gt; (from Salesforce) released with the paper &lt;a href="https://arxiv.org/abs/1909.05858" rel="nofollow"&gt;CTRL: A Conditional Transformer Language Model for Controllable Generation&lt;/a&gt; by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.&lt;/li&gt;
&lt;li&gt;Want to contribute a new model? We have added a &lt;strong&gt;detailed guide and templates&lt;/strong&gt; to guide you in the process of adding a new model. You can find them in the &lt;a href="./templates"&gt;&lt;code&gt;templates&lt;/code&gt;&lt;/a&gt; folder of the repository. Be sure to check the &lt;a href="./CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; and contact the maintainers or open an issue to collect feedbacks before starting your PR.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the &lt;a href="https://huggingface.co/transformers/examples.html" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-online-demo" class="anchor" aria-hidden="true" href="#online-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online demo&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://transformer.huggingface.co" rel="nofollow"&gt;Write With Transformer&lt;/a&gt;&lt;/strong&gt;, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo’s text generation capabilities.
You can use it to experiment with completions generated by &lt;code&gt;GPT2Model&lt;/code&gt;, &lt;code&gt;TransfoXLModel&lt;/code&gt;, and &lt;code&gt;XLNetModel&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“&lt;g-emoji class="g-emoji" alias="unicorn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f984.png"&gt;🦄&lt;/g-emoji&gt; Write with transformer is to writing what calculators are to calculus.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67" alt="write_with_transformer" data-canonical-src="https://transformer.huggingface.co/front/assets/thumbnail-large.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour" class="anchor" aria-hidden="true" href="#quick-tour"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour&lt;/h2&gt;
&lt;p&gt;Let's do a very quick overview of the model architectures in &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;full documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Transformers has a unified API&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; for 8 transformer architectures and 30 pretrained weights.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;          Model          | Tokenizer          | Pretrained weights shortcut&lt;/span&gt;
&lt;span class="pl-c1"&gt;MODELS&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [(BertModel,       BertTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;openai-gpt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (GPT2Model,       GPT2Tokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;gpt2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (CTRLModel,       CTRLTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ctrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (TransfoXLModel,  TransfoXLTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;transfo-xl-wt103&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLNetModel,      XLNetTokenizer,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlnet-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLMModel,        XLMTokenizer,        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlm-mlm-enfr-1024&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (DistilBertModel, DistilBertTokenizer, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;distilbert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (RobertaModel,    RobertaTokenizer,    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;roberta-base&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's encode some text in a sequence of hidden-states using each model:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class, tokenizer_class, pretrained_weights &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;MODELS&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer_class.from_pretrained(pretrained_weights)
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Encode text&lt;/span&gt;
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Here is some text to encode&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)])  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Add special tokens takes care of adding [CLS], [SEP], &amp;lt;s&amp;gt;... tokens in the right way for each model.&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; torch.no_grad():
        last_hidden_states &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models outputs are now tuples&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.&lt;/span&gt;
&lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,
                      BertForQuestionAnswering]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; All the classes for an architecture can be initiated from pretrained weights for this architecture&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Note that additional weights added for fine-tuning are only initialized&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; and need to be trained on the down-stream task&lt;/span&gt;
pretrained_weights &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(pretrained_weights)
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models can return full list of hidden-states &amp;amp; attentions weights at each layer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights,
                                        &lt;span class="pl-v"&gt;output_hidden_states&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;,
                                        &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Let's see all hidden-states and attentions on this text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)])
    all_hidden_states, all_attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;:]

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models are compatible with Torchscript&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights, &lt;span class="pl-v"&gt;torchscript&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    traced_model &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.jit.trace(model, (input_ids,))

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Simple serialization for models and tokenizers&lt;/span&gt;
    model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;
    tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; SOTA examples for GLUE, SQUAD, text generation...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-tf-20-training-and-pytorch-interoperability" class="anchor" aria-hidden="true" href="#quick-tour-tf-20-training-and-pytorch-interoperability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour TF 2.0 training and PyTorch interoperability&lt;/h2&gt;
&lt;p&gt;Let's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers and then loaded in PyTorch for fast inspection/tests.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow_datasets
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load dataset, tokenizer, model from pretrained model/vocabulary&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model &lt;span class="pl-k"&gt;=&lt;/span&gt; TFBertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; tensorflow_datasets.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;glue/mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare dataset for GLUE as a tf.data.Dataset instance&lt;/span&gt;
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;validation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; train_dataset.shuffle(&lt;span class="pl-c1"&gt;100&lt;/span&gt;).batch(&lt;span class="pl-c1"&gt;32&lt;/span&gt;).repeat(&lt;span class="pl-c1"&gt;2&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; valid_dataset.batch(&lt;span class="pl-c1"&gt;64&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule &lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.optimizers.Adam(&lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-5&lt;/span&gt;, &lt;span class="pl-v"&gt;epsilon&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1e-08&lt;/span&gt;, &lt;span class="pl-v"&gt;clipnorm&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1.0&lt;/span&gt;)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.losses.SparseCategoricalCrossentropy(&lt;span class="pl-v"&gt;from_logits&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
metric &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.metrics.SparseCategoricalAccuracy(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;accuracy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model.compile(&lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;optimizer, &lt;span class="pl-v"&gt;loss&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;loss, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[metric])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train and evaluate using tf.keras.Model.fit()&lt;/span&gt;
history &lt;span class="pl-k"&gt;=&lt;/span&gt; model.fit(train_dataset, &lt;span class="pl-v"&gt;epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-v"&gt;steps_per_epoch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;115&lt;/span&gt;,
                    &lt;span class="pl-v"&gt;validation_data&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;valid_dataset, &lt;span class="pl-v"&gt;validation_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load the TensorFlow model in PyTorch for inspection&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
pytorch_model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;from_tf&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task&lt;/span&gt;
sentence_0 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;This research was consistent with his findings.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were not compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
inputs_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_1, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
inputs_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_2, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

pred_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()
pred_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_1 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_1 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_2 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_2 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-of-the-fine-tuningusage-scripts" class="anchor" aria-hidden="true" href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour of the fine-tuning/usage scripts&lt;/h2&gt;
&lt;p&gt;The library comprises several example scripts with SOTA performances for NLU and NLG tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (&lt;em&gt;sequence-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (&lt;em&gt;token-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation&lt;/li&gt;
&lt;li&gt;other model-specific examples (see the documentation).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are three quick usage examples for these scripts:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification" class="anchor" aria-hidden="true" href="#run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: Fine-tuning on GLUE tasks for sequence classification&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://gluebenchmark.com/" rel="nofollow"&gt;General Language Understanding Evaluation (GLUE) benchmark&lt;/a&gt; is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.&lt;/p&gt;
&lt;p&gt;Before running anyone of these GLUE tasks you should download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You should also install the additional packages required by the examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r ./examples/requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name &lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt;/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.&lt;/p&gt;
&lt;p&gt;The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-xlnet-model-on-the-sts-b-regression-task" class="anchor" aria-hidden="true" href="#fine-tuning-xlnet-model-on-the-sts-b-regression-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning XLNet model on the STS-B regression task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python ./examples/run_glue.py \
    --model_type xlnet \
    --model_name_or_path xlnet-large-cased \
    --do_train  \
    --do_eval   \
    --task_name=sts-b     \
    --data_dir=&lt;span class="pl-smi"&gt;${GLUE_DIR}&lt;/span&gt;/STS-B  \
    --output_dir=./proc_data/sts-b-110   \
    --max_seq_length=128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --gradient_accumulation_steps=1 \
    --max_steps=1200  \
    --model_name=xlnet-large-cased   \
    --overwrite_output_dir   \
    --overwrite_cache \
    --warmup_steps=120&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On this machine we thus have a batch size of 32, please increase &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of &lt;code&gt;+0.917&lt;/code&gt; on the development set.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-bert-model-on-the-mrpc-classification-task" class="anchor" aria-hidden="true" href="#fine-tuning-bert-model-on-the-mrpc-classification-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning Bert model on the MRPC classification task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &amp;gt; 92.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name MRPC \
    --do_train   \
    --do_eval   \
    --do_lower_case   \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC/   \
    --max_seq_length 128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5   \
    --num_train_epochs 3.0  \
    --output_dir /tmp/mrpc_output/ \
    --overwrite_output_dir   \
    --overwrite_cache \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  acc = 0.8823529411764706
  acc_and_f1 = 0.901702786377709
  eval_loss = 0.3418912578906332
  f1 = 0.9210526315789473
  global_step = 174
  loss = 0.07231863956341798&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run_squadpy-fine-tuning-on-squad-for-question-answering" class="anchor" aria-hidden="true" href="#run_squadpy-fine-tuning-on-squad-for-question-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: Fine-tuning on SQuAD for question-answering&lt;/h3&gt;
&lt;p&gt;This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &amp;gt; 93 on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
    --predict_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../models/wwm_uncased_finetuned_squad/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 86.91579943235573, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 93.1532499015869}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the model provided as &lt;code&gt;bert-large-uncased-whole-word-masking-finetuned-squad&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet" class="anchor" aria-hidden="true" href="#run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet&lt;/h3&gt;
&lt;p&gt;A conditional generation script is also included to generate text from a prompt.
The generation script includes the &lt;a href="https://github.com/rusiaaman/XLNet-gen#methodology"&gt;tricks&lt;/a&gt; proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).&lt;/p&gt;
&lt;p&gt;Here is how to run the script with the small version of OpenAI GPT-2 model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=gpt2 \
    --length=20 \
    --model_name_or_path=gpt2 \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and from the Salesforce CTRL model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=ctrl \
    --length=20 \
    --model_name_or_path=ctrl \
    --temperature=0 \
    --repetition_penalty=1.2 \&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-transformers-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-transformers-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-transformers to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-transformers&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed" class="anchor" aria-hidden="true" href="#positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Positional order of some models' keywords inputs (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) changed&lt;/h3&gt;
&lt;p&gt;To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models &lt;strong&gt;keywords inputs&lt;/strong&gt; (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) has been changed.&lt;/p&gt;
&lt;p&gt;If you used to call the models with keyword names for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)&lt;/code&gt;, this should not cause any change.&lt;/p&gt;
&lt;p&gt;If you used to call the models with positional inputs for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask, token_type_ids)&lt;/code&gt;, you may have to double check the exact order of input arguments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-pretrained-bert-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-pretrained-bert-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-pretrained-bert to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-models-always-output-tuples" class="anchor" aria-hidden="true" href="#models-always-output-tuples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models always output &lt;code&gt;tuples&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The main breaking change when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; is that every model's forward method always outputs a &lt;code&gt;tuple&lt;/code&gt; with various elements depending on the model and the configuration parameters.&lt;/p&gt;
&lt;p&gt;The exact content of the tuples for each model is detailed in the models' docstrings and the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; conversion example for a &lt;code&gt;BertForSequenceClassification&lt;/code&gt; classification model:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's load our model&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you used to have this line in pytorch-pretrained-bert:&lt;/span&gt;
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Now just use this line in transformers to extract the loss from the output tuple:&lt;/span&gt;
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; In transformers you can also have access to the logits:&lt;/span&gt;
loss, logits &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[:&lt;span class="pl-c1"&gt;2&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss, logits, attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-using-hidden-states" class="anchor" aria-hidden="true" href="#using-hidden-states"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using hidden states&lt;/h3&gt;
&lt;p&gt;By enabling the configuration option &lt;code&gt;output_hidden_states&lt;/code&gt;, it was possible to retrieve the last hidden states of the encoder. In &lt;code&gt;pytorch-transformers&lt;/code&gt; as well as &lt;code&gt;transformers&lt;/code&gt; the return value has changed slightly: &lt;code&gt;all_hidden_states&lt;/code&gt; now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serialization" class="anchor" aria-hidden="true" href="#serialization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serialization&lt;/h3&gt;
&lt;p&gt;Breaking change in the &lt;code&gt;from_pretrained()&lt;/code&gt; method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Models are now set in evaluation mode by default when instantiated with the &lt;code&gt;from_pretrained()&lt;/code&gt; method. To train them, don't forget to set them back in training mode (&lt;code&gt;model.train()&lt;/code&gt;) to activate the dropout modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The additional &lt;code&gt;*input&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; arguments supplied to the &lt;code&gt;from_pretrained()&lt;/code&gt; method used to be directly passed to the underlying model's class &lt;code&gt;__init__()&lt;/code&gt; method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous &lt;code&gt;BertForSequenceClassification&lt;/code&gt; examples. We are working on a way to mitigate this breaking change in &lt;a href="https://github.com/huggingface/transformers/pull/866"&gt;#866&lt;/a&gt; by forwarding the the model's &lt;code&gt;__init__()&lt;/code&gt; method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method &lt;code&gt;save_pretrained(save_directory)&lt;/code&gt; if you were using any other serialization method before.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Let's load a model and tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Do some stuff to our model and tokenizer&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Ex: add new tokens to the vocabulary and embeddings of our model&lt;/span&gt;
tokenizer.add_tokens([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_1]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_2]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
model.resize_token_embeddings(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokenizer))
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train our model&lt;/span&gt;
train(model)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Now let's save our model and tokenizer to a directory&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Reload the model and the tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules" class="anchor" aria-hidden="true" href="#optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimizers: BertAdam &amp;amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules&lt;/h3&gt;
&lt;p&gt;The two optimizers previously included, &lt;code&gt;BertAdam&lt;/code&gt; and &lt;code&gt;OpenAIAdam&lt;/code&gt;, have been replaced by a single &lt;code&gt;AdamW&lt;/code&gt; optimizer which has a few differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it only implements weights decay correction,&lt;/li&gt;
&lt;li&gt;schedules are now externals (see below),&lt;/li&gt;
&lt;li&gt;gradient clipping is now also external (see below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The new optimizer &lt;code&gt;AdamW&lt;/code&gt; matches PyTorch &lt;code&gt;Adam&lt;/code&gt; optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.&lt;/p&gt;
&lt;p&gt;The schedules are now standard &lt;a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" rel="nofollow"&gt;PyTorch learning rate schedulers&lt;/a&gt; and not part of the optimizer anymore.&lt;/p&gt;
&lt;p&gt;Here is a conversion examples from &lt;code&gt;BertAdam&lt;/code&gt; with a linear warmup and decay schedule to &lt;code&gt;AdamW&lt;/code&gt; and the same schedule:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Parameters:&lt;/span&gt;
lr &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1e-3&lt;/span&gt;
max_grad_norm &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1.0&lt;/span&gt;
num_total_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1000&lt;/span&gt;
num_warmup_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;
warmup_proportion &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_warmup_steps) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_total_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 0.1&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Previously BertAdam optimizer was instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertAdam(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;schedule&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;warmup_linear&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;warmup&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;warmup_proportion, &lt;span class="pl-v"&gt;t_total&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_total_steps)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    optimizer.step()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## In Transformers, optimizer and schedules are splitted and instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; AdamW(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;correct_bias&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To reproduce BertAdam specific behavior set correct_bias=False&lt;/span&gt;
scheduler &lt;span class="pl-k"&gt;=&lt;/span&gt; WarmupLinearSchedule(optimizer, &lt;span class="pl-v"&gt;warmup_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_warmup_steps, &lt;span class="pl-v"&gt;t_total&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_total_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; PyTorch scheduler&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    model.train()
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Gradient clipping is not in AdamW anymore (so you can use amp without issue)&lt;/span&gt;
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;We now have a paper you can cite for the &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;🤗&lt;/g-emoji&gt; Transformers library:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huggingface</author><guid isPermaLink="false">https://github.com/huggingface/transformers</guid><pubDate>Tue, 05 Nov 2019 00:21:00 GMT</pubDate></item><item><title>WenDesi/lihang_book_algorithm #22 in Python, This week</title><link>https://github.com/WenDesi/lihang_book_algorithm</link><description>&lt;p&gt;&lt;i&gt;致力于将李航博士《统计学习方法》一书中所有算法实现一遍&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;h1&gt;&lt;a id="user-content-lihang_book_algorithm" class="anchor" aria-hidden="true" href="#lihang_book_algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;lihang_book_algorithm&lt;/h1&gt;
&lt;p&gt;被李航老师肯定啦！！开心！
&lt;br&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/WenDesi/lihang_book_algorithm/master/weibo.png"&gt;&lt;img src="https://raw.githubusercontent.com/WenDesi/lihang_book_algorithm/master/weibo.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;我这里不介绍任何机器学习算法的原理，只是将《统计学习方法》中每一章的算法用我自己的方式实现一遍。
除了李航书上的算法外，还实现了一些其他机器学习的算法。&lt;/p&gt;
&lt;p&gt;那么我们就按章节来了&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-章节" class="anchor" aria-hidden="true" href="#章节"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;章节&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-第二章-感知器模型" class="anchor" aria-hidden="true" href="#第二章-感知器模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二章 感知器模型&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/51923546" rel="nofollow"&gt;李航《统计学习方法》第二章——用Python实现感知器模型（MNIST数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/perceptron/binary_perceptron.py"&gt;perceptron/binary_perceptron.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第三章-k近邻法" class="anchor" aria-hidden="true" href="#第三章-k近邻法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三章 K近邻法&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/51933044" rel="nofollow"&gt;李航《统计学习方法》第三章——用Python实现KNN算法（MNIST数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/knn/knn.py"&gt;knn/knn.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第四章-朴素贝叶斯" class="anchor" aria-hidden="true" href="#第四章-朴素贝叶斯"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四章 朴素贝叶斯&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/51967839" rel="nofollow"&gt;李航《统计学习方法》第四章——用Python实现朴素贝叶斯分类器（MNIST数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/naive_bayes/naive_bayes.py"&gt;naive_bayes/naive_bayes.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第五章-决策树" class="anchor" aria-hidden="true" href="#第五章-决策树"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五章 决策树&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/52849400" rel="nofollow"&gt;李航《统计学习方法》第五章——用Python实现决策树（MNIST数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/decision_tree/decision_tree.py"&gt;decision_tree/decision_tree.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第六章-逻辑斯提回归" class="anchor" aria-hidden="true" href="#第六章-逻辑斯提回归"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第六章 逻辑斯提回归&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53084871" rel="nofollow"&gt;李航《统计学习方法》第六章——用Python实现逻辑斯谛回归（MNIST数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/logistic_regression/logistic_regression.py"&gt;logistic_regression/logistic_regression.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第六章-最大熵模型" class="anchor" aria-hidden="true" href="#第六章-最大熵模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第六章 最大熵模型&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53106579" rel="nofollow"&gt;李航《统计学习方法》第六章——用Python实现最大熵模型（MNIST数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/maxENT/maxENT.py"&gt;maxENT/maxENT.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第七章-支持向量机" class="anchor" aria-hidden="true" href="#第七章-支持向量机"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第七章 支持向量机&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53156589" rel="nofollow"&gt;李航《统计学习方法》第七章——用Python实现支持向量机模型（伪造数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/svm/svm.py"&gt;svm/svm.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第八章-提升方法" class="anchor" aria-hidden="true" href="#第八章-提升方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第八章 提升方法&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53195725" rel="nofollow"&gt;李航《统计学习方法》第八章——用Python+Cpp实现AdaBoost算法（MNIST数据集）&lt;/a&gt;
&lt;br&gt;纯Python代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/adaboost.py"&gt;AdaBoost/adaboost.py&lt;/a&gt;
&lt;br&gt;Python C++代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/adaboost_cpp.py"&gt;AdaBoost/adaboost_cpp.py&lt;/a&gt;,&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/Sign/Sign/sign.h"&gt;AdaBoost/Sign/Sign/sign.h&lt;/a&gt;,&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/Sign/Sign/sign.cpp"&gt;AdaBoost/Sign/Sign/sign.cpp&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第十章-隐马尔科夫模型" class="anchor" aria-hidden="true" href="#第十章-隐马尔科夫模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第十章 隐马尔科夫模型&lt;/h3&gt;
&lt;p&gt;博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/75212599" rel="nofollow"&gt;李航《统计学习方法》第十章——用Python实现隐马尔科夫模型&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/hmm/hmm.py"&gt;hmm/hmm.py&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-额外章节" class="anchor" aria-hidden="true" href="#额外章节"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;额外章节&lt;/h2&gt;
&lt;p&gt;###softmax分类器
博客：&lt;a href="http://blog.csdn.net/wds2006sdo/article/details/53699778" rel="nofollow"&gt;python 实现 softmax分类器（MNIST数据集）&lt;/a&gt;
&lt;br&gt;代码：&lt;a href="https://github.com/WenDesi/lihang_book_algorithm/blob/master/softmax/softmax.py"&gt;softmax/softmax.py&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>WenDesi</author><guid isPermaLink="false">https://github.com/WenDesi/lihang_book_algorithm</guid><pubDate>Tue, 05 Nov 2019 00:22:00 GMT</pubDate></item><item><title>dbolya/yolact #23 in Python, This week</title><link>https://github.com/dbolya/yolact</link><description>&lt;p&gt;&lt;i&gt;A simple, fully convolutional model for real-time instance segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-you-only-look-at-coefficients" class="anchor" aria-hidden="true" href="#you-only-look-at-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Y&lt;/strong&gt;ou &lt;strong&gt;O&lt;/strong&gt;nly &lt;strong&gt;L&lt;/strong&gt;ook &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;C&lt;/strong&gt;oefficien&lt;strong&gt;T&lt;/strong&gt;s&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗
    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝
     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   
      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   
       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   
       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple, fully convolutional model for real-time instance segmentation. This is the code for &lt;a href="https://arxiv.org/abs/1904.02689" rel="nofollow"&gt;our paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-iccv-update-v11-released-check-out-the-iccv-trailer-here" class="anchor" aria-hidden="true" href="#iccv-update-v11-released-check-out-the-iccv-trailer-here"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ICCV update (v1.1) released! Check out the ICCV trailer here:&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=0pMfmo8qfpQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c9f0f1403e25276c0beea78732b5cec6c9b610ab/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f30704d666d6f38716670512f302e6a7067" alt="IMAGE ALT TEXT HERE" data-canonical-src="https://img.youtube.com/vi/0pMfmo8qfpQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Read &lt;a href="CHANGELOG.md"&gt;the changelog&lt;/a&gt; for details on, well, what changed. Oh, and the paper got updated too with pascal results and an appendix with box mAP.&lt;/p&gt;
&lt;p&gt;Some examples from our base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's &lt;code&gt;test-dev&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_0.png"&gt;&lt;img src="data/yolact_example_0.png" alt="Example 0" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_1.png"&gt;&lt;img src="data/yolact_example_1.png" alt="Example 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_2.png"&gt;&lt;img src="data/yolact_example_2.png" alt="Example 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Set up a Python3 environment.&lt;/li&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org/" rel="nofollow"&gt;Pytorch&lt;/a&gt; 1.0.1 (or higher) and TorchVision.&lt;/li&gt;
&lt;li&gt;Install some other packages:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Cython needs to be installed before pycocotools&lt;/span&gt;
pip install cython
pip install opencv-python pillow pycocotools matplotlib &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Clone this repository and enter it:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/dbolya/yolact.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolact&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into &lt;code&gt;./data/coco&lt;/code&gt;.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to evaluate YOLACT on &lt;code&gt;test-dev&lt;/code&gt;, download &lt;code&gt;test-dev&lt;/code&gt; with this script.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO_test.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluation&lt;/h1&gt;
&lt;p&gt;As of April 5th, 2019 here are our latest models along with their FPS on a Titan Xp and mAP on &lt;code&gt;test-dev&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Image Size&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;FPS&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th&gt;Weights&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet50-FPN&lt;/td&gt;
&lt;td align="center"&gt;42.5&lt;/td&gt;
&lt;td align="center"&gt;28.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing" rel="nofollow"&gt;yolact_resnet50_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Darknet53-FPN&lt;/td&gt;
&lt;td align="center"&gt;40.0&lt;/td&gt;
&lt;td align="center"&gt;28.7&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing" rel="nofollow"&gt;yolact_darknet53_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;33.0&lt;/td&gt;
&lt;td align="center"&gt;29.8&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing" rel="nofollow"&gt;yolact_base_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;700&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;23.6&lt;/td&gt;
&lt;td align="center"&gt;31.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing" rel="nofollow"&gt;yolact_im700_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To evalute the model, put the corresponding weights file in the &lt;code&gt;./weights&lt;/code&gt; directory and run one of the following commands.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quantitative-results-on-coco" class="anchor" aria-hidden="true" href="#quantitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This should get 29.92 validation mask mAP last time I checked.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.&lt;/span&gt;
python run_coco_eval.py

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-qualitative-results-on-coco" class="anchor" aria-hidden="true" href="#qualitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qualitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-on-coco" class="anchor" aria-hidden="true" href="#benchmarking-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run just the raw model on the first 1k images of the validation set&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-images" class="anchor" aria-hidden="true" href="#images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Images&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on the specified image.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process an image and save it to another file.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a whole folder of images.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a video in real-time. "--video_multiframe" will process that many frames at once for improved performance.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you want, use "--display_fps" to draw the FPS directly on the frame.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can tell, &lt;code&gt;eval.py&lt;/code&gt; can do a ton of stuff. Run the &lt;code&gt;--help&lt;/code&gt; command to see everything it can do.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python eval.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;By default, we train on COCO. Make sure to download the entire dataset using the commands above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To train, grab an imagenet-pretrained model and put it in &lt;code&gt;./weights&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;For Resnet101, download &lt;code&gt;resnet101_reducedfc.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Resnet50, download &lt;code&gt;resnet50-19c8e357.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Darknet53, download &lt;code&gt;darknet53.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run one of the training commands below.
&lt;ul&gt;
&lt;li&gt;Note that you can press ctrl+c while training and it will save an &lt;code&gt;*_interrupt.pth&lt;/code&gt; file at the current iteration.&lt;/li&gt;
&lt;li&gt;All weights are saved in the &lt;code&gt;./weights&lt;/code&gt; directory by default with the file name &lt;code&gt;&amp;lt;config&amp;gt;_&amp;lt;epoch&amp;gt;_&amp;lt;iter&amp;gt;.pth&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains using the base config with a batch size of 8 (the default).&lt;/span&gt;
python train.py --config=yolact_base_config

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.&lt;/span&gt;
python train.py --config=yolact_base_config --batch_size=5

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.&lt;/span&gt;
python train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Use the help option to see a description of all available command line arguments&lt;/span&gt;
python train.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-support" class="anchor" aria-hidden="true" href="#multi-gpu-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU Support&lt;/h2&gt;
&lt;p&gt;YOLACT now supports multiple GPUs seamlessly during training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before running any of the scripts, run: &lt;code&gt;export CUDA_VISIBLE_DEVICES=[gpus]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).&lt;/li&gt;
&lt;li&gt;You should still do this if only using 1 GPU.&lt;/li&gt;
&lt;li&gt;You can check the indices of your GPUs with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then, simply set the batch size to &lt;code&gt;8*num_gpus&lt;/code&gt; with the training commands above. The training script will automatically scale the hyperparameters to the right values.
&lt;ul&gt;
&lt;li&gt;If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.&lt;/li&gt;
&lt;li&gt;If you want to allocate the images per GPU specific for different GPUs, you can use &lt;code&gt;--batch_alloc=[alloc]&lt;/code&gt; where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to &lt;code&gt;batch_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Logging&lt;/h2&gt;
&lt;p&gt;YOLACT now logs training and validation information by default. You can disable this with &lt;code&gt;--no_log&lt;/code&gt;. A guide on how to visualize these logs is coming soon, but now you can look at &lt;code&gt;LogVizualizer&lt;/code&gt; in &lt;code&gt;utils/logger.py&lt;/code&gt; for help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pascal-sbd" class="anchor" aria-hidden="true" href="#pascal-sbd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pascal SBD&lt;/h2&gt;
&lt;p&gt;We also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the dataset from &lt;a href="http://home.bharathh.info/pubs/codes/SBD/download.html" rel="nofollow"&gt;here&lt;/a&gt;. It's the first link in the top "Overview" section (and the file is called &lt;code&gt;benchmark.tgz&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Extract the dataset somewhere. In the dataset there should be a folder called &lt;code&gt;dataset/img&lt;/code&gt;. Create the directory &lt;code&gt;./data/sbd&lt;/code&gt; (where &lt;code&gt;.&lt;/code&gt; is YOLACT's root) and copy &lt;code&gt;dataset/img&lt;/code&gt; to &lt;code&gt;./data/sbd/img&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Download the COCO-style annotations from &lt;a href="https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Extract the annotations into &lt;code&gt;./data/sbd/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Now you can train using &lt;code&gt;--config=yolact_resnet50_pascal_config&lt;/code&gt;. Check that config to see how to extend it to other models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in &lt;code&gt;./scripts/convert_sbd.py&lt;/code&gt;, but you'll have to check how it works to be able to use it because I don't actually remember at this point.&lt;/p&gt;
&lt;p&gt;If you want to verify our results, you can download our &lt;code&gt;yolact_resnet50_pascal_config&lt;/code&gt; weights from &lt;a href="https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S" rel="nofollow"&gt;here&lt;/a&gt;. This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the "all" AP isn't the same as the "vol" AP reported in others papers for pascal (they use an averages of the thresholds from &lt;code&gt;0.1 - 0.9&lt;/code&gt; in increments of &lt;code&gt;0.1&lt;/code&gt; instead of what COCO uses).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h2&gt;
&lt;p&gt;You can also train on your own dataset by following these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found &lt;a href="http://cocodataset.org/#format-data" rel="nofollow"&gt;here&lt;/a&gt;. Note that we don't use some fields, so the following may be omitted:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;liscense&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;code&gt;image&lt;/code&gt;: &lt;code&gt;license, flickr_url, coco_url, date_captured&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (we use our own format for categories, see below)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create a definition for your dataset under &lt;code&gt;dataset_base&lt;/code&gt; in &lt;code&gt;data/config.py&lt;/code&gt; (see the comments in &lt;code&gt;dataset_base&lt;/code&gt; for an explanation of each field):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;my_custom_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; dataset_base.copy({
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;My Dataset&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;has_gt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;class_names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;...&lt;/span&gt;)
})&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A couple things to note:
&lt;ul&gt;
&lt;li&gt;Class IDs in the annotation file should start at 1 and increase sequentially on the order of &lt;code&gt;class_names&lt;/code&gt;. If this isn't the case for your annotation file (like in COCO), see the field &lt;code&gt;label_map&lt;/code&gt; in &lt;code&gt;dataset_base&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you do not want to create a validation split, use the same image path and annotations file for validation. By default (see &lt;code&gt;python train.py --help&lt;/code&gt;), &lt;code&gt;train.py&lt;/code&gt; will output validation mAP for the first 5000 images in the dataset every 2 epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally, in &lt;code&gt;yolact_base_config&lt;/code&gt; in the same file, change the value for &lt;code&gt;'dataset'&lt;/code&gt; to &lt;code&gt;'my_custom_dataset'&lt;/code&gt; or whatever you named the config object above. Then you can use any of the training commands in the previous section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-creating-a-custom-dataset-from-scratch" class="anchor" aria-hidden="true" href="#creating-a-custom-dataset-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a Custom Dataset from Scratch&lt;/h4&gt;
&lt;p&gt;See &lt;a href="https://github.com/dbolya/yolact/issues/70#issuecomment-504283008"&gt;this nice post by @Amit12690&lt;/a&gt; for tips on how to annotate a custom dataset and prepare it for use with YOLACT.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;If you use YOLACT or this code base in your work, please cite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{bolya-iccv2019,
  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  title     = {YOLACT: {Real-time} Instance Segmentation},
  booktitle = {ICCV},
  year      = {2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;For questions about our paper or code, please contact &lt;a href="mailto:dbolya@ucdavis.edu"&gt;Daniel Bolya&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dbolya</author><guid isPermaLink="false">https://github.com/dbolya/yolact</guid><pubDate>Tue, 05 Nov 2019 00:23:00 GMT</pubDate></item><item><title>pytorch/examples #24 in Python, This week</title><link>https://github.com/pytorch/examples</link><description>&lt;p&gt;&lt;i&gt;A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-examples" class="anchor" aria-hidden="true" href="#pytorch-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Examples&lt;/h1&gt;
&lt;p&gt;A repository showcasing examples of using &lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mnist"&gt;Image classification (MNIST) using Convnets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="word_language_model"&gt;Word level Language Modeling using LSTM RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="imagenet"&gt;Training Imagenet Classifiers with Residual Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="dcgan"&gt;Generative Adversarial Networks (DCGAN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vae"&gt;Variational Auto-Encoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="super_resolution"&gt;Superresolution using an efficient sub-pixel convolutional neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mnist_hogwild"&gt;Hogwild training of shared ConvNets across multiple processes on MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning"&gt;Training a CartPole to balance in OpenAI Gym with actor-critic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="snli"&gt;Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="time_sequence_prediction"&gt;Time sequence prediction - use an LSTM to learn Sine waves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="fast_neural_style"&gt;Implement the Neural Style Transfer algorithm on images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="cpp"&gt;Several examples illustrating the C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, a list of good examples hosted in their own repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenNMT/OpenNMT-py"&gt;Neural Machine Translation using sequence-to-sequence RNN with attention (OpenNMT)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/examples</guid><pubDate>Tue, 05 Nov 2019 00:24:00 GMT</pubDate></item><item><title>fighting41love/funNLP #25 in Python, This week</title><link>https://github.com/fighting41love/funNLP</link><description>&lt;p&gt;&lt;i&gt;中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&amp;摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT &amp; ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;最近需要从文本中抽取结构化信息，用到了很多github上的包，遂整理了一下，后续会不断更新。&lt;/p&gt;
&lt;p&gt;很多包非常有趣，值得收藏，满足大家的收集癖！
如果觉得有用，请分享并star，谢谢！&lt;/p&gt;
&lt;p&gt;涉及内容包括：&lt;strong&gt;中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&amp;amp;摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT &amp;amp; ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、UER：基于不同语料+编码器+目标任务的中文预训练模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库、快速转化「中文数字」和「阿拉伯数字」、百度知道问答语料库、基于知识图谱的问答系统、jieba_fast 加速版的jieba、正则表达式教程、中文阅读理解数据集、基于BERT等最新语言模型的抽取式摘要提取、Python利用深度学习进行文本摘要的综合指南、知识图谱深度学习相关资料整理、维基大规模平行文本语料、StanfordNLP 0.2.0：纯Python版自然语言处理包、NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具、端到端的封闭域对话系统、中文命名实体识别：NeuroNER vs. BertNER、新闻事件线索抽取、2019年百度的三元组抽取比赛：“科学空间队”源码、基于依存句法的开放域文本知识三元组抽取和知识库构建、中文的GPT2训练代码、ML-NLP - 机器学习(Machine Learning)NLP面试中常考到的知识点和代码实现、nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查、XLM：Facebook的跨语言预训练语言模型、用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取、中文自然语言处理相关的开放任务-数据集-当前最佳结果、CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统、抽象知识图谱、MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. textfilter: 中英文敏感词过滤&lt;/strong&gt;  &lt;a href="https://github.com/observerss/textfilter"&gt;observerss/textfilter&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; &amp;gt;&amp;gt;&amp;gt; f = DFAFilter()
 &amp;gt;&amp;gt;&amp;gt; f.add("sexy")
 &amp;gt;&amp;gt;&amp;gt; f.filter("hello sexy baby")
 hello **** baby
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;敏感词包括政治、脏话等话题词汇。其原理主要是基于词典的查找（项目中的keyword文件），内容很劲爆。。。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. langid：97种语言检测&lt;/strong&gt; &lt;a href="https://github.com/saffsd/langid.py"&gt;https://github.com/saffsd/langid.py&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install langid&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import langid
&amp;gt;&amp;gt;&amp;gt; langid.classify("This is a test")
('en', -54.41310358047485)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. langdetect：另一个语言检测&lt;/strong&gt;&lt;a href="https://code.google.com/archive/p/language-detection/" rel="nofollow"&gt;https://code.google.com/archive/p/language-detection/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install langdetect&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;from langdetect import detect
from langdetect import detect_langs

s1 = "本篇博客主要介绍两款语言探测工具，用于区分文本到底是什么语言，"
s2 = 'We are pleased to introduce today a new technology'
print(detect(s1))
print(detect(s2))
print(detect_langs(s3))    # detect_langs()输出探测出的所有语言类型及其所占的比例
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;输出结果如下： 注：语言类型主要参考的是ISO 639-1语言编码标准，详见&lt;a href="https://baike.baidu.com/item/ISO%20639-1" rel="nofollow"&gt;ISO 639-1百度百科&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;跟上一个语言检测比较，准确率低，效率高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. phone 中国手机归属地查询：&lt;/strong&gt; &lt;a href="https://github.com/ls0f/phone"&gt;ls0f/phone&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;from phone import Phone
p  = Phone()
p.find(18100065143)
#return {'phone': '18100065143', 'province': '上海', 'city': '上海', 'zip_code': '200000', 'area_code': '021', 'phone_type': '电信'}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;支持号段: 13*,15*,18*,14[5,7],17[0,6,7,8]&lt;/p&gt;
&lt;p&gt;记录条数: 360569 (updated:2017年4月)&lt;/p&gt;
&lt;p&gt;作者提供了数据&lt;a href="https://github.com/lovedboy/phone/raw/master/phone/phone.dat"&gt;phone.dat&lt;/a&gt; 方便非python用户Load数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. phone国际手机、电话归属地查询：&lt;/strong&gt;&lt;a href="https://github.com/AfterShip/phone"&gt;AfterShip/phone&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;npm install phone&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;import phone from 'phone';
phone('+852 6569-8900'); // return ['+85265698900', 'HKG']
phone('(817) 569-8900'); // return ['+18175698900, 'USA']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;6. ngender 根据名字判断性别：&lt;/strong&gt;&lt;a href="https://github.com/observerss/ngender"&gt;observerss/ngender&lt;/a&gt; 基于朴素贝叶斯计算的概率&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install ngender&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import ngender
&amp;gt;&amp;gt;&amp;gt; ngender.guess('赵本山')
('male', 0.9836229687547046)
&amp;gt;&amp;gt;&amp;gt; ngender.guess('宋丹丹')
('female', 0.9759486128949907)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;7. 抽取email的正则表达式&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;email_pattern = '^[*#\u4e00-\u9fa5 a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\.[a-zA-Z0-9-]+)*\.[a-zA-Z0-9]{2,6}$'
emails = re.findall(email_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;8. 抽取phone_number的正则表达式&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;cellphone_pattern = '^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\d{8}$'
phoneNumbers = re.findall(cellphone_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;9. 抽取身份证号的正则表达式&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IDCards_pattern = r'^([1-9]\d{5}[12]\d{3}(0[1-9]|1[012])(0[1-9]|[12][0-9]|3[01])\d{3}[0-9xX])$'
IDs = re.findall(IDCards_pattern, text, flags=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;10.  人名语料库：&lt;/strong&gt; &lt;a href="https://github.com/wainshine/Chinese-Names-Corpus"&gt;wainshine/Chinese-Names-Corpus&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;人名抽取功能 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;中文（现代、古代）名字、日文名字、中文的姓和名、称呼（大姨妈、小姨妈等）、英文-&amp;gt;中文名字（李约翰）、成语词典
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;（可用于中文分词、姓名识别）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;11. 中文缩写库：&lt;/strong&gt;&lt;a href="https://github.com/zhangyics/Chinese-abbreviation-dataset/blob/master/dev_set.txt"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;全国人大: 全国/n 人民/n 代表大会/n
中国: 中华人民共和国/ns
女网赛: 女子/n 网球/n 比赛/vn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;12. 汉语拆字词典：&lt;/strong&gt;&lt;a href="https://github.com/kfcd/chaizi"&gt;kfcd/chaizi&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;漢字	拆法 (一)	拆法 (二)	拆法 (三)
拆	手 斥	扌 斥	才 斥
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;13. 词汇情感值：&lt;/strong&gt;&lt;a href="https://github.com/rainarch/SentiBridge/blob/master/Entity_Emotion_Express/CCF_data/pair_mine_result"&gt;rainarch/SentiBridge&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;山泉水	充沛	0.400704566541	0.370067395878
视野	        宽广	0.305762728932	0.325320747491
大峡谷	惊险	0.312137906517	0.378594957281
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;14. 中文词库、停用词、敏感词&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/Chinese_from_dongxiexidian"&gt;dongxiexidian/Chinese&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;此package的敏感词库分类更细：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;反动词库&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;敏感词库表统计&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;暴恐词库&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;民生词库&lt;/a&gt;， &lt;a href="https://github.com/fighting41love/funNLP/tree/master/data/%E6%95%8F%E6%84%9F%E8%AF%8D%E5%BA%93"&gt;色情词库&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;15. 汉字转拼音：&lt;/strong&gt;&lt;a href="https://github.com/mozillazg/python-pinyin"&gt;mozillazg/python-pinyin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文本纠错会用到&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;16. 中文繁简体互转：&lt;/strong&gt;&lt;a href="https://github.com/skydark/nstools/tree/master/zhtools"&gt;skydark/nstools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;17. 英文模拟中文发音引擎&lt;/strong&gt; funny chinese text to speech enginee：&lt;a href="https://github.com/tinyfool/ChineseWithEnglish"&gt;tinyfool/ChineseWithEnglish&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;say wo i ni
#说：我爱你
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;相当于用英文音标，模拟中文发音。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;18. 汪峰歌词生成器：&lt;/strong&gt;&lt;a href="https://github.com/phunterlau/wangfeng-rnn"&gt;phunterlau/wangfeng-rnn&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;我在这里中的夜里
就像一场是一种生命的意旪
就像我的生活变得在我一样
可我们这是一个知道
我只是一天你会怎吗
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;19. 同义词库、反义词库、否定词库：&lt;/strong&gt;&lt;a href="https://github.com/guotong1988/chinese_dictionary"&gt;guotong1988/chinese_dictionary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;20. 无空格英文串分割、抽取单词：&lt;/strong&gt;&lt;a href="https://github.com/keredson/wordninja"&gt;wordinja&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import wordninja
&amp;gt;&amp;gt;&amp;gt; wordninja.split('derekanderson')
['derek', 'anderson']
&amp;gt;&amp;gt;&amp;gt; wordninja.split('imateapot')
['im', 'a', 'teapot']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;21. IP地址正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;22. 腾讯QQ号正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1-9]([0-9]{5,11})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;23. 国内固话号码正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[0-9-()（）]{7,18}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;24. 用户名正则表达式：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[A-Za-z0-9_\-\u4e00-\u9fa5]+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;25. 汽车品牌、汽车零件相关词汇：&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;见本repo的data文件 [data](https://github.com/fighting41love/funNLP/tree/master/data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;26. 时间抽取：&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;已集成到 python package &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;cocoNLP&lt;/a&gt;中，欢迎试用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;在2016年6月7日9:44执行測試，结果如下

Hi，all。下周一下午三点开会

&amp;gt;&amp;gt; 2016-06-13 15:00:00-false

周一开会

&amp;gt;&amp;gt; 2016-06-13 00:00:00-true

下下周一开会

&amp;gt;&amp;gt; 2016-06-20 00:00:00-true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://github.com/shinyke/Time-NLP"&gt;java version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanzecheng/Time_NLP"&gt;python version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;27. 各种中文词向量：&lt;/strong&gt; &lt;a href="https://github.com/Embedding/Chinese-Word-Vectors"&gt;github repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文词向量大全&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;28. 公司名字大全：&lt;/strong&gt; &lt;a href="https://github.com/wainshine/Company-Names-Corpus"&gt;github repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;29. 古诗词库：&lt;/strong&gt; &lt;a href="https://github.com/panhaiqi/AncientPoetry"&gt;github repo&lt;/a&gt; &lt;a href="https://github.com/chinese-poetry/chinese-poetry"&gt;更全的古诗词库&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;30. THU整理的词库：&lt;/strong&gt; &lt;a href="http://thuocl.thunlp.org/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;已整理到本repo的data文件夹中.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;31. 中文聊天语料&lt;/strong&gt; &lt;a href="https://github.com/codemayq/chaotbot_corpus_Chinese"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;该库搜集了包含:豆瓣多轮, PTT八卦语料, 青云语料, 电视剧对白语料, 贴吧论坛回帖语料,微博语料,小黄鸡语料
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;32. 中文谣言数据:&lt;/strong&gt; &lt;a href="https://github.com/thunlp/Chinese_Rumor_Dataset"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;该数据文件中，每一行为一条json格式的谣言数据，字段释义如下：

rumorCode: 该条谣言的唯一编码，可以通过该编码直接访问该谣言举报页面。
title: 该条谣言被举报的标题内容
informerName: 举报者微博名称
informerUrl: 举报者微博链接
rumormongerName: 发布谣言者的微博名称
rumormongerUr: 发布谣言者的微博链接
rumorText: 谣言内容
visitTimes: 该谣言被访问次数
result: 该谣言审查结果
publishTime: 该谣言被举报时间
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;33. 情感波动分析：&lt;/strong&gt;&lt;a href="https://github.com/CasterWx/python-girlfriend-mood/"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;词库已整理到本repo的data文件夹中.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;本repo项目是一个通过与人对话获得其情感值波动图谱, 内用词库在data文件夹中.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;34. 百度中文问答数据集&lt;/strong&gt;：&lt;a href="https://pan.baidu.com/s/1QUsKcFWZ7Tg1dk_AbldZ1A" rel="nofollow"&gt;链接&lt;/a&gt; 提取码: 2dva&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;35. 句子、QA相似度匹配:MatchZoo&lt;/strong&gt;  &lt;a href="https://github.com/NTMC-Community/MatchZoo"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文本相似度匹配算法的集合，包含多个深度学习的方法，值得尝试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;36. bert资源：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bert论文中文翻译: &lt;a href="https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;bert原作者的slides: &lt;a href="https://pan.baidu.com/s/1OSPsIu2oh1iJ-bcXoDZpJQ" rel="nofollow"&gt;link&lt;/a&gt;
提取码: iarj&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;文本分类实践: &lt;a href="https://github.com/NLPScott/bert-Chinese-classification-task"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert tutorial文本分类教程: &lt;a href="https://github.com/Socialbird-AILab/BERT-Classification-Tutorial"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert pytorch实现:  &lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert用于中文命名实体识别 tensorflow版本: &lt;a href="https://github.com/macanv/BERT-BiLSTM-CRF-NER"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT生成句向量，BERT做文本分类、文本相似度计算&lt;a href="https://github.com/terrifyzhao/bert-utils"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert 基于 keras 的封装分类标注框架 Kashgari，几分钟即可搭建一个分类或者序列标注模型: &lt;a href="https://github.com/BrikerMan/Kashgari"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bert、ELMO的图解： &lt;a href="https://jalammar.github.io/illustrated-bert/" rel="nofollow"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT: Pre-trained models and downstream applications: &lt;a href="https://github.com/asyml/texar/tree/master/examples/bert"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;37. Texar - Toolkit for Text Generation and Beyond&lt;/strong&gt;: &lt;a href="https://github.com/asyml/texar"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于Tensorflow的开源工具包，旨在支持广泛的机器学习，特别是文本生成任务，如机器翻译、对话、摘要、内容处置、语言建模等&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;38. 中文事件抽取：&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ComplexEventExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文复合事件抽取，包括条件事件、因果事件、顺承事件、反转事件等事件抽取，并形成事理图谱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;39. cocoNLP:&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/cocoNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;人名、地址、邮箱、手机号、手机归属地 等信息的抽取，rake短语抽取算法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install cocoNLP&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from cocoNLP.extractor import extractor

&amp;gt;&amp;gt;&amp;gt; ex = extractor()

&amp;gt;&amp;gt;&amp;gt; text = '急寻特朗普，男孩，于2018年11月27号11时在陕西省安康市汉滨区走失。丢失发型短发，...如有线索，请迅速与警方联系：18100065143，132-6156-2938，baizhantang@sina.com.cn 和yangyangfuture at gmail dot com'

# 抽取邮箱
&amp;gt;&amp;gt;&amp;gt; emails = ex.extract_email(text)
&amp;gt;&amp;gt;&amp;gt; print(emails)

['baizhantang@sina.com.cn', 'yangyangfuture@gmail.com.cn']
# 抽取手机号
&amp;gt;&amp;gt;&amp;gt; cellphones = ex.extract_cellphone(text,nation='CHN')
&amp;gt;&amp;gt;&amp;gt; print(cellphones)

['18100065143', '13261562938']
# 抽取手机归属地、运营商
&amp;gt;&amp;gt;&amp;gt; cell_locs = [ex.extract_cellphone_location(cell,'CHN') for cell in cellphones]
&amp;gt;&amp;gt;&amp;gt; print(cell_locs)

cellphone_location [{'phone': '18100065143', 'province': '上海', 'city': '上海', 'zip_code': '200000', 'area_code': '021', 'phone_type': '电信'}]
# 抽取地址信息
&amp;gt;&amp;gt;&amp;gt; locations = ex.extract_locations(text)
&amp;gt;&amp;gt;&amp;gt; print(locations)
['陕西省安康市汉滨区', '安康市汉滨区', '汉滨区']
# 抽取时间点
&amp;gt;&amp;gt;&amp;gt; times = ex.extract_time(text)
&amp;gt;&amp;gt;&amp;gt; print(times)
time {"type": "timestamp", "timestamp": "2018-11-27 11:00:00"}
# 抽取人名
&amp;gt;&amp;gt;&amp;gt; name = ex.extract_name(text)
&amp;gt;&amp;gt;&amp;gt; print(name)
特朗普

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;40. 国内电话号码正则匹配（三大运营商+虚拟等）:&lt;/strong&gt; &lt;a href="https://github.com/VincentSit/ChinaMobilePhoneNumberRegex"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;41. 清华大学XLORE:中英文跨语言百科知识图谱:&lt;/strong&gt; &lt;a href="https://xlore.org/download.html" rel="nofollow"&gt;link&lt;/a&gt;&lt;br&gt;
上述链接中包含了所有实体及关系的TTL文件，更多数据将在近期发布。
概念，实例，属性和上下位关系数目&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;百度&lt;/th&gt;
&lt;th&gt;中文维基&lt;/th&gt;
&lt;th&gt;英文维基&lt;/th&gt;
&lt;th&gt;总数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;概念数量&lt;/td&gt;
&lt;td&gt;32,009&lt;/td&gt;
&lt;td&gt;150,241&lt;/td&gt;
&lt;td&gt;326,518&lt;/td&gt;
&lt;td&gt;508,768&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;实例数量&lt;/td&gt;
&lt;td&gt;1,629,591&lt;/td&gt;
&lt;td&gt;640,622&lt;/td&gt;
&lt;td&gt;1,235,178&lt;/td&gt;
&lt;td&gt;3,505,391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;属性数量&lt;/td&gt;
&lt;td&gt;157,370&lt;/td&gt;
&lt;td&gt;45,190&lt;/td&gt;
&lt;td&gt;26,723&lt;/td&gt;
&lt;td&gt;229.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;InstanceOf&lt;/td&gt;
&lt;td&gt;7,584,931&lt;/td&gt;
&lt;td&gt;1,449,925&lt;/td&gt;
&lt;td&gt;3,032,515&lt;/td&gt;
&lt;td&gt;12,067,371&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SubClassOf&lt;/td&gt;
&lt;td&gt;2,784&lt;/td&gt;
&lt;td&gt;191,577&lt;/td&gt;
&lt;td&gt;555,538&lt;/td&gt;
&lt;td&gt;749,899&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;跨语言连接（概念/实例）&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;百度&lt;/th&gt;
&lt;th&gt;中文维基&lt;/th&gt;
&lt;th&gt;英文维基&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;百度&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;10,216/336,890&lt;/td&gt;
&lt;td&gt;4,846/303,108&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;中文维基&lt;/td&gt;
&lt;td&gt;10,216/336,890&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;28,921/454,579&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;英文维基&lt;/td&gt;
&lt;td&gt;4,846/303,108&lt;/td&gt;
&lt;td&gt;28,921/454,579&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;42. 清华大学人工智能技术系列报告：&lt;/strong&gt; &lt;a href="https://reports.aminer.cn" rel="nofollow"&gt;link&lt;/a&gt;&lt;br&gt;
每年会出AI领域相关的报告，内容包含&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自然语言处理 &lt;a href="https://static.aminer.cn/misc/article/nlp.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;知识图谱 &lt;a href="https://www.aminer.cn/research_report/5c3d5a8709%20e961951592a49d?download=true&amp;amp;pathname=knowledgegraph.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;数据挖掘 &lt;a href="https://www.aminer.cn/research_report/5c3d5a5cecb160952fa10b76?download=true&amp;amp;pathname=datamining.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;自动驾驶 &lt;a href="https://static.aminer.cn/misc/article/selfdriving.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;机器翻译 &lt;a href="https://static.aminer.cn/misc/article/translation.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;区块链 &lt;a href="https://static.aminer.cn/misc/article/blockchain_public.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;机器人 &lt;a href="https://static.aminer.cn/misc/article/robotics_beta.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;计算机图形学 &lt;a href="https://static.aminer.cn/misc/article/cg.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3D打印 &lt;a href="https://static.aminer.cn/misc/article/3d.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人脸识别 &lt;a href="https://static.aminer.cn/misc/article/facerecognition.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人工智能芯片 &lt;a href="https://static.aminer.cn/misc/article/aichip.pdf" rel="nofollow"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;等等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;43.自然语言生成方面:&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://ehudreiter.com" rel="nofollow"&gt;Ehud Reiter教授的博客&lt;/a&gt;  北大万小军教授强力推荐，该博客对NLG技术、评价与应用进行了深入的探讨与反思。&lt;br&gt;
&lt;a href="https://github.com/ChenChengKuan/awesome-text-generation"&gt;文本生成相关资源大列表&lt;/a&gt;&lt;br&gt;
&lt;a href="https://drive.google.com/file/d/1Mdna3q986k6OoJNsfAHznTtnMAEVzv5z/view" rel="nofollow"&gt;自然语言生成：让机器掌握自动创作的本领 - 开放域对话生成及在微软小冰中的实践&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/harvardnlp/Talk-Latent/blob/master/main.pdf"&gt;文本生成控制&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;44.:&lt;/strong&gt;
&lt;a href="https://github.com/fxsjy/jieba"&gt;jieba&lt;/a&gt;和&lt;a href="https://github.com/hankcs/pyhanlp"&gt;hanlp&lt;/a&gt;就不必介绍了吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;45.NLP太难了系列:&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/hardNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;来到杨过曾经生活过的地方，小龙女动情地说：“我也想过过过儿过过的生活。” ​​​&lt;/li&gt;
&lt;li&gt;来到儿子等校车的地方，邓超对孙俪说：“我也想等等等等等过的那辆车。”&lt;/li&gt;
&lt;li&gt;赵敏说：我也想控忌忌己不想无忌。&lt;/li&gt;
&lt;li&gt;你也想犯范范范玮琪犯过的错吗&lt;/li&gt;
&lt;li&gt;对叙打击是一次性行为？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;46.自动对联数据及机器人:&lt;/strong&gt;&lt;br&gt;
&lt;a href="https://github.com/wb14123/couplet-dataset"&gt;70万对联数据 link&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/wb14123/seq2seq-couplet"&gt;代码 link&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;上联&lt;/th&gt;
&lt;th&gt;下联&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;殷勤怕负三春意&lt;/td&gt;
&lt;td&gt;潇洒难书一字愁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;如此清秋何吝酒&lt;/td&gt;
&lt;td&gt;这般明月不须钱&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;47.用户名黑名单列表：&lt;/strong&gt; &lt;a href="https://github.com/marteinn/The-Big-Username-Blacklist"&gt;github&lt;/a&gt;
包含了用户名禁用列表，比如: &lt;a href="https://github.com/marteinn/The-Big-Username-Blacklist/blob/master/list_raw.txt"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;administrator
administration
autoconfig
autodiscover
broadcasthost
domain
editor
guest
host
hostmaster
info
keybase.txt
localdomain
localhost
master
mail
mail0
mail1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;48.罪名法务名词及分类模型:&lt;/strong&gt;   &lt;a href="https://github.com/liuhuanyong/CrimeKgAssitant"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;49.微信公众号语料:&lt;/strong&gt; &lt;a href="https://github.com/nonamestreet/weixin_public_corpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3G语料，包含部分网络抓取的微信公众号的文章，已经去除HTML，只包含了纯文本。每行一篇，是JSON格式，name是微信公众号名字，account是微信公众号ID，title是题目，content是正文&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;50.cs224n深度学习自然语言处理课程：&lt;/strong&gt;&lt;a href="http://web.stanford.edu/class/cs224n/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;课程中模型的pytorch实现 &lt;a href="https://github.com/DSKSD/DeepNLP-models-Pytorch"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;面向深度学习研究人员的自然语言处理实例教程 &lt;a href="https://github.com/graykode/nlp-tutorial"&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;51.中文手写汉字识别：&lt;/strong&gt;&lt;a href="https://github.com/chizhanyuefeng/Chinese_OCR_CNN-RNN-CTC"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;52.中文自然语言处理 语料/数据集：&lt;/strong&gt;&lt;a href="https://github.com/SophonPlus/ChineseNlpCorpus"&gt;github&lt;/a&gt;
&lt;a href="https://github.com/thunlp/THUOCL"&gt;竞品：THUOCL（THU Open Chinese Lexicon）中文词库&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;53.变量命名神器：&lt;/strong&gt;&lt;a href="https://github.com/unbug/codelf"&gt;github&lt;/a&gt; &lt;a href="https://unbug.github.io/codelf/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;54.分词语料库+代码：&lt;/strong&gt;&lt;a href="https://pan.baidu.com/s/1MXZONaLgeaw0_TxZZDAIYQ" rel="nofollow"&gt;百度网盘链接&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提取码: pea6&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GlassyWing/bi-lstm-crf"&gt;keras实现的基于Bi-LSTM + CRF的中文分词+词性标注&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GlassyWing/transformer-word-segmenter"&gt;基于Universal Transformer + CRF 的中文分词和词性标注&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yaoguangluo/NeroParser"&gt;快速神经网络分词包 java version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;55. NLP新书推荐《Natural Language Processing》by Jacob Eisenstein：&lt;/strong&gt; &lt;a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;56. 任务型对话英文数据集：&lt;/strong&gt;   &lt;a href="https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey"&gt;github&lt;/a&gt;&lt;br&gt;
【最全任务型对话数据集】主要介绍了一份任务型对话数据集大全，这份数据集大全涵盖了到目前在任务型对话领域的所有常用数据集的主要信息。此外，为了帮助研究者更好的把握领域进展的脉络，我们以Leaderboard的形式给出了几个数据集上的State-of-the-art实验结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;57. ASR 语音数据集 + 基于深度学习的中文语音识别系统：&lt;/strong&gt;  &lt;a href="https://github.com/nl8590687/ASRT_SpeechRecognition"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data Sets 数据集&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;清华大学THCHS30中文语音数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;data_thchs30.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/data_thchs30.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/data_thchs30.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;test-noise.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/test-noise.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/test-noise.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;resource.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/18/resource.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/18/resource.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Free ST Chinese Mandarin Corpus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ST-CMDS-20170001_1-OS.tar.gz
&lt;a href="http://cn-mirror.openslr.org/resources/38/ST-CMDS-20170001_1-OS.tar.gz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/38/ST-CMDS-20170001_1-OS.tar.gz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AIShell-1 开源版数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;data_aishell.tgz
&lt;a href="http://cn-mirror.openslr.org/resources/33/data_aishell.tgz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/33/data_aishell.tgz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：数据集解压方法&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tar xzf data_aishell.tgz
$ cd data_aishell/wav
$ for tar in *.tar.gz;  do tar xvf $tar; done
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Primewords Chinese Corpus Set 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;primewords_md_2018_set1.tar.gz
&lt;a href="http://cn-mirror.openslr.org/resources/47/primewords_md_2018_set1.tar.gz" rel="nofollow"&gt;OpenSLR国内镜像&lt;/a&gt;
&lt;a href="http://www.openslr.org/resources/47/primewords_md_2018_set1.tar.gz" rel="nofollow"&gt;OpenSLR国外镜像&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;58. 笑声检测器：&lt;/strong&gt;  &lt;a href="https://github.com/ideo/LaughDetection"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;59. Microsoft多语言数字/单位/如日期时间识别包：&lt;/strong&gt; [github](&lt;a href="https://github.com/Microsoft/Recognizers-Text"&gt;https://github.com/Microsoft/Recognizers-Text&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;60. chinese-xinhua 中华新华字典数据库及api，包括常用歇后语、成语、词语和汉字&lt;/strong&gt; &lt;a href="https://github.com/pwxcoo/chinese-xinhua"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;61. 文档图谱自动生成&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/TextGrapher"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TextGrapher - Text Content Grapher based on keyinfo extraction by NLP method。输入一篇文档，将文档进行关键信息提取，进行结构化，并最终组织成图谱组织形式，形成对文章语义信息的图谱化展示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;62. SpaCy 中文模型&lt;/strong&gt; &lt;a href="https://github.com/howl-anderson/Chinese_models_for_SpaCy"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包含Parser, NER, 语法树等功能。有一些英文package使用spacy的英文模型的，如果要适配中文，可能需要使用spacy中文模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;63. Common Voice语音识别数据集新版&lt;/strong&gt;  &lt;a href="https://voice.mozilla.org/en/datasets" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括来自42,000名贡献者超过1,400小时的语音样本，涵github&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;64. 神经网络关系抽取 pytorch&lt;/strong&gt;  &lt;a href="https://github.com/ShulinCao/OpenNRE-PyTorch"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;暂不支持中文&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;65. 基于bert的命名实体识别 pytorch&lt;/strong&gt;  &lt;a href="https://github.com/Kyubyong/bert_ner"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;暂不支持中文&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;66. 关键词(Keyphrase)抽取包 pke&lt;/strong&gt;  &lt;a href="https://github.com/boudinfl/pke"&gt;github&lt;/a&gt;&lt;br&gt;
&lt;a href="http://aclweb.org/anthology/C16-2015" rel="nofollow"&gt;pke: an open source python-based keyphrase extraction toolkit&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;暂不支持中文，我于近期对其进行修改，使其适配中文。
请关注我的github动态，谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;67. 基于医疗领域知识图谱的问答系统&lt;/strong&gt;  &lt;a href="https://github.com/zhihao-chen/QASystemOnMedicalGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;该repo参考了&lt;a href="https://github.com/liuhuanyong/QASystemOnMedicalKG"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;68. 基于依存句法与语义角色标注的事件三元组抽取&lt;/strong&gt;  &lt;a href="https://github.com/liuhuanyong/EventTriplesExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;69. 依存句法分析4万句高质量标注数据&lt;/strong&gt; by 苏州大学汉语依存树库（SUCDT）
&lt;a href="http://hlt.suda.edu.cn/index.php/Nlpcc-2019-shared-task" rel="nofollow"&gt;Homepage&lt;/a&gt;
数据下载详见homepage底部，需要签署协议，需要邮件接收解压密码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;70. cnocr：用来做中文OCR的Python3包，自带了训练好的识别模型&lt;/strong&gt; &lt;a href="https://github.com/breezedeus/cnocr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;71. 中文人物关系知识图谱项目&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/PersonRelationKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中文人物关系图谱构建&lt;/li&gt;
&lt;li&gt;基于知识库的数据回标&lt;/li&gt;
&lt;li&gt;基于远程监督与bootstrapping方法的人物关系抽取&lt;/li&gt;
&lt;li&gt;基于知识图谱的知识问答等应用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;72. 中文nlp竞赛项目及代码汇总&lt;/strong&gt; &lt;a href="https://github.com/geekinglcq/CDCS"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本生成、文本摘要：Byte Cup 2018 国际机器学习竞赛&lt;/li&gt;
&lt;li&gt;知识图谱：瑞金医院MMC人工智能辅助构建知识图谱大赛&lt;/li&gt;
&lt;li&gt;视频识别 问答：2018之江杯全球人工智能大赛 ：视频识别&amp;amp;问答&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;73. 中文字符数据&lt;/strong&gt; &lt;a href="https://github.com/skishore/makemeahanzi"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;简/繁体汉字笔顺&lt;/li&gt;
&lt;li&gt;矢量笔画&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;74. speech-aligner: 从“人声语音”及其“语言文本”，产生音素级别时间对齐标注的工具&lt;/strong&gt; &lt;a href="https://github.com/open-speech/speech-aligner"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;75. AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测&lt;/strong&gt; &lt;a href="https://github.com/Accenture/AmpliGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;埃森哲出品，目前尚不支持中文&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;76. Scattertext 文本可视化(python)&lt;/strong&gt; &lt;a href="https://github.com/JasonKessler/scattertext"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;很好用的工具包，简单修改后可支持中文&lt;/li&gt;
&lt;li&gt;能否分析出某个类别的文本与其他文本的用词差异&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;77. 语言/知识表示工具：BERT &amp;amp; ERNIE&lt;/strong&gt; &lt;a href="https://github.com/PaddlePaddle/LARK"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;百度出品，ERNIE也号称在多项nlp任务中击败了bert&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;78. 中文对比英文自然语言处理NLP的区别综述&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/LQU_HJ4q74lL5oCIk7w5RA" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;79. Synonyms中文近义词工具包&lt;/strong&gt; &lt;a href="https://github.com/huyingxi/Synonyms"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Synonyms 中文近义词工具包，可以用于自然语言理解的很多任务：文本对齐，推荐算法，相似度计算，语义偏移，关键字提取，概念提取，自动摘要，搜索引擎等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;80. HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）&lt;/strong&gt; &lt;a href="https://github.com/blmoistawinde/HarvestText"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;81. word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对&lt;/strong&gt; &lt;a href="https://github.com/Kyubyong/word2word"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;82. 语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库&lt;/strong&gt; &lt;a href="https://github.com/yc9701/pansori"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;83. ASR语音大辞典/词典：&lt;/strong&gt; github&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;84. 构建医疗实体识别的模型，包含词典和语料标注，基于python:&lt;/strong&gt; &lt;a href="https://github.com/yixiu00001/LSTM-CRF-medical"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;85. 单文档非监督的关键词抽取：&lt;/strong&gt; &lt;a href="https://github.com/LIAAD/yake"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;86. Kashgari中使用gpt-2语言模型&lt;/strong&gt; &lt;a href="https://github.com/BrikerMan/Kashgari"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;87.  开源的金融投资数据提取工具&lt;/strong&gt; &lt;a href="https://github.com/PKUJohnson/OpenData"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;88. 文本自动摘要库TextTeaser: 仅支持英文&lt;/strong&gt; &lt;a href="https://github.com/IndigoResearch/textteaser"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;89. 人民日报语料处理工具集&lt;/strong&gt; &lt;a href="https://github.com/howl-anderson/tools_for_corpus_of_people_daily"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;90. 一些关于自然语言的基本模型&lt;/strong&gt; &lt;a href="https://github.com/lpty/nlp_base"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;91. 基于14W歌曲知识库的问答尝试，功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/MusicLyricChatbot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;92. 基于Siamese bilstm模型的相似句子判定模型,提供训练数据集和测试数据集&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/SiameseSentenceSimilarity"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提供了10万个训练样本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;93. 用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论&lt;/strong&gt; &lt;a href="https://github.com/leod/hncynic"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;94. 用BERT进行序列标记和文本分类的模板代码&lt;/strong&gt; &lt;a href="https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;95. LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料&lt;/strong&gt; &lt;a href="https://github.com/dbamman/litbank"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;96. 百度开源的基准信息抽取系统&lt;/strong&gt; &lt;a href="https://github.com/baidu/information-extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;97. 虚假新闻数据集 fake news corpus&lt;/strong&gt; &lt;a href="https://github.com/several27/FakeNewsCorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;98. Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/LAMA"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;99. CommonsenseQA：面向常识的英文QA挑战&lt;/strong&gt; &lt;a href="https://www.tau-nlp.org/commonsenseqa" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;100. 中文知识图谱资料、数据及工具&lt;/strong&gt; &lt;a href="https://github.com/husthuke/awesome-knowledge-graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;101. 各大公司内部里大牛分享的技术文档 PDF 或者 PPT&lt;/strong&gt; &lt;a href="https://github.com/0voice/from_coder_to_expert"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;102. 自然语言生成SQL语句（英文）&lt;/strong&gt; &lt;a href="https://github.com/paulfitz/mlsql"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;103. 中文NLP数据增强（EDA）工具&lt;/strong&gt; &lt;a href="https://github.com/zhanlaoban/eda_nlp_for_Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 英文NLP数据增强工具 &lt;a href="https://github.com/makcedward/nlpaug"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;104. 基于医药知识图谱的智能问答系统&lt;/strong&gt; &lt;a href="https://github.com/YeYzheng/KGQA-Based-On-medicine"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;105. 京东商品知识图谱&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ProductKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于京东网站的1300种商品上下级概念，约10万商品品牌，约65万品牌销售关系，商品描述维度等知识库，基于该知识库可以支持商品属性库构建，商品销售问答，品牌物品生产等知识查询服务，也可用于情感分析等下游应用．&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;106. 基于mongodb存储的军事领域知识图谱问答项目&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/QAonMilitaryKG"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于mongodb存储的军事领域知识图谱问答项目，包括飞行器、太空装备等8大类，100余小类，共计5800项的军事武器知识库，该项目不使用图数据库进行存储，通过jieba进行问句解析，问句实体项识别，基于查询模板完成多类问题的查询，主要是提供一种工业界的问答思想demo。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;107. 基于远监督的中文关系抽取&lt;/strong&gt; &lt;a href="https://github.com/xiaolalala/Distant-Supervised-Chinese-Relation-Extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;108. 语音情感分析&lt;/strong&gt; &lt;a href="https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;109. 中文ULMFiT 情感分析 文本分类 语料及模型&lt;/strong&gt; &lt;a href="https://github.com/bigboNed3/chinese_ulmfit"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;110. 一个拍照做题程序。输入一张包含数学计算题的图片，输出识别出的数学计算式以及计算结果&lt;/strong&gt; &lt;a href="https://github.com/Roujack/mathAI"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;111. 世界各国大规模人名库&lt;/strong&gt; &lt;a href="https://github.com/philipperemy/name-dataset"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;112. 一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人&lt;/strong&gt; &lt;a href="https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用了青云语料10万语料，本repo中也有该语料的链接&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;113. 中文聊天机器人， 根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景&lt;/strong&gt; &lt;a href="https://github.com/zhaoyingjun/chatbot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景。加入seqGAN版本。&lt;/li&gt;
&lt;li&gt;repo中提供了一份质量不太高的语料&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;114. 省市区镇行政区划数据带拼音标注&lt;/strong&gt; &lt;a href="https://github.com/xiangyuecn/AreaCity-JsSpider-StatsGov"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;国家统计局中的省市区镇行政区划数据带拼音标注，高德地图的坐标和行政区域边界范围，在浏览器里面运行js代码采集的2019年发布的最新数据，含采集源码，提供csv格式数据，支持csv转成省市区多级联动js代码&lt;/li&gt;
&lt;li&gt;坐标、边界范围、名称、拼音、行政区等多级地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;115. 教育行业新闻 自动文摘 语料库&lt;/strong&gt; &lt;a href="https://github.com/wonderfulsuccess/chinese_abstractive_corpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;116. 开放了对话机器人、知识图谱、语义理解、自然语言处理工具及数据&lt;/strong&gt; &lt;a href="https://www.ownthink.com/#header-n30" rel="nofollow"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;另一个qa对的机器人 &lt;a href="https://github.com/WenRichard/QAmodel-for-Retrievalchatbot"&gt;Amodel-for-Retrivalchatbot - 客服机器人，Chinese Retreival chatbot（中文检索式机器人）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;117. 中文知识图谱：基于百度百科中文页面，抽取三元组信息，构建中文知识图谱&lt;/strong&gt; &lt;a href="https://github.com/lixiang0/WEB_KG"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;118. masr: 中文语音识别，提供预训练模型，高识别率&lt;/strong&gt; &lt;a href="https://github.com/lukhy/masr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;119. Python音频数据增广库&lt;/strong&gt; &lt;a href="https://github.com/iver56/audiomentations"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;120. 中文全词覆盖BERT及两份阅读理解数据&lt;/strong&gt; &lt;a href="https://github.com/ymcui/Chinese-BERT-wwm"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DRCD数据集&lt;/strong&gt;由中国台湾台达研究院发布，其形式与SQuAD相同，是基于繁体中文的抽取式阅读理解数据集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CMRC 2018数据集&lt;/strong&gt;是哈工大讯飞联合实验室发布的中文机器阅读理解数据。根据给定问题，系统需要从篇章中抽取出片段作为答案，形式与SQuAD相同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;121. ConvLab：开源多域端到端对话系统平台&lt;/strong&gt; &lt;a href="https://github.com/ConvLab/ConvLab"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;122. 中文自然语言处理数据集&lt;/strong&gt; &lt;a href="https://github.com/InsaneLife/ChineseNLPCorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;123. 基于最新版本rasa搭建的对话系统&lt;/strong&gt; &lt;a href="https://github.com/GaoQ1/rasa_chatbot_cn"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;124. 基于TensorFlow和BERT的管道式实体及关系抽取&lt;/strong&gt; &lt;a href="https://github.com/yuanxiaosc/Entity-Relation-Extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entity and Relation Extraction Based on TensorFlow and BERT. 基于TensorFlow和BERT的管道式实体及关系抽取，2019语言与智能技术竞赛信息抽取任务解决方案。Schema based Knowledge Extraction, SKE 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;125. 一个小型的证券知识图谱/知识库&lt;/strong&gt; &lt;a href="https://github.com/lemonhu/stock-knowledge-graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;126. 复盘所有NLP比赛的TOP方案&lt;/strong&gt; &lt;a href="https://github.com/zhpmatrix/nlp-competitions-list-review"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;127. OpenCLaP：多领域开源中文预训练语言模型仓库&lt;/strong&gt; &lt;a href="https://github.com/thunlp/OpenCLaP"&gt;github&lt;/a&gt;
包含如下语言模型及百度百科数据&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;民事文书BERT	bert-base	全部民事文书	2654万篇文书	22554词	370MB&lt;/li&gt;
&lt;li&gt;刑事文书BERT	bert-base	全部刑事文书	663万篇文书	22554词	370MB&lt;/li&gt;
&lt;li&gt;百度百科BERT	bert-base	百度百科	903万篇词条	22166词	367MB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;128. UER：基于不同语料、编码器、目标任务的中文预训练模型仓库（包括BERT、GPT、ELMO等）&lt;/strong&gt; &lt;a href="https://github.com/dbiir/UER-py"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于PyTorch的预训练模型框架，支持对编码器，目标任务等进行任意的组合，从而复现已有的预训练模型，或在已有的预训练模型上进一步改进。基于UER训练了不同性质的预训练模型（不同语料、编码器、目标任务），构成了中文预训练模型仓库，适用于不同的场景。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;129. 中文自然语言处理向量合集&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ChineseEmbedding"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包括字向量,拼音向量,词向量,词性向量,依存关系向量.共5种类型的向量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;130. 基于金融-司法领域(兼有闲聊性质)的聊天机器人&lt;/strong&gt; &lt;a href="https://github.com/charlesXu86/Chatbot_CN"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中的主要模块有信息抽取、NLU、NLG、知识图谱等，并且利用Django整合了前端展示,目前已经封装了nlp和kg的restful接口&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;131. g2pC：基于上下文的汉语读音自动标记模块&lt;/strong&gt; &lt;a href="https://github.com/Kyubyong/g2pC"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;132. Zincbase 知识图谱构建工具包&lt;/strong&gt; &lt;a href="https://github.com/tomgrek/zincbase"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;133. 诗歌质量评价/细粒度情感诗歌语料库&lt;/strong&gt; &lt;a href="https://github.com/THUNLP-AIPoet/Datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;134. 快速转化「中文数字」和「阿拉伯数字」&lt;/strong&gt; &lt;a href="https://github.com/HaveTwoBrush/cn2an"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中文、阿拉伯数字互转&lt;/li&gt;
&lt;li&gt;中文与阿拉伯数字混合的情况，在开发中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;135. 百度知道问答语料库&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/MiningZhiDaoQACorpus"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;超过580万的问题，938万的答案，5800个分类标签。基于该问答语料库，可支持多种应用，如闲聊问答，逻辑挖掘&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;136. 基于知识图谱的问答系统&lt;/strong&gt; &lt;a href="https://github.com/WenRichard/KBQA-BERT"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT做命名实体识别和句子相似度，分为online和outline模式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;137. jieba_fast 加速版的jieba&lt;/strong&gt; &lt;a href="https://github.com/deepcs233/jieba_fast"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用cpython重写了jieba分词库中计算DAG和HMM中的vitrebi函数，速度得到大幅提升&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;138. 正则表达式教程&lt;/strong&gt; &lt;a href="https://github.com/ziishaned/learn-regex/blob/master/translations/README-cn.md"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;139. 中文阅读理解数据集&lt;/strong&gt; &lt;a href="https://github.com/ymcui/Chinese-RC-Datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;140. 基于BERT等最新语言模型的抽取式摘要提取&lt;/strong&gt; &lt;a href="https://github.com/Hellisotherpeople/CX_DB8"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;141. Python利用深度学习进行文本摘要的综合指南&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/gDZyTbM1nw3fbEnU--y3nQ" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;142. 知识图谱深度学习相关资料整理&lt;/strong&gt; &lt;a href="https://github.com/lihanghang/Knowledge-Graph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;深度学习与自然语言处理、知识图谱、对话系统。包括知识获取、知识库构建、知识库应用三大技术研究与应用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;143. 维基大规模平行文本语料&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;85种语言、1620种语言对、135M对照句&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;144. StanfordNLP 0.2.0：纯Python版自然语言处理包&lt;/strong&gt; &lt;a href="https://stanfordnlp.github.io/stanfordnlp/" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;145. NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具&lt;/strong&gt; &lt;a href="https://github.com/Tencent/NeuralNLP-NeuralClassifier"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;146. 端到端的封闭域对话系统&lt;/strong&gt; &lt;a href="https://github.com/cdqa-suite/cdQA"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;147. 中文命名实体识别：NeuroNER vs. BertNER&lt;/strong&gt; &lt;a href="https://github.com/EOA-AILab/NER-Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;148. 新闻事件线索抽取&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/ImportantEventExtractor"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An exploration for Eventline (important news Rank organized by pulic time)，针对某一事件话题下的新闻报道集合，通过使用docrank算法，对新闻报道进行重要性识别，并通过新闻报道时间挑选出时间线上重要新闻&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;149. 2019年百度的三元组抽取比赛，“科学空间队”源码(第7名)&lt;/strong&gt; &lt;a href="https://github.com/bojone/kg-2019"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;150. 基于依存句法的开放域文本知识三元组抽取和知识库构建&lt;/strong&gt; &lt;a href="https://github.com/lemonhu/open-entity-relation-extraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;151. 中文的GPT2训练代码&lt;/strong&gt; &lt;a href="https://github.com/Morizeyao/GPT2-Chinese"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;152. ML-NLP - 机器学习(Machine Learning)、NLP面试中常考到的知识点和代码实现&lt;/strong&gt; &lt;a href="https://github.com/NLP-LOVE/ML-NLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;153. nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查&lt;/strong&gt; &lt;a href="https://github.com/kidden/nlp4han"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;154. XLM：Facebook的跨语言预训练语言模型&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/XLM"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;155. 用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取&lt;/strong&gt; &lt;a href="https://github.com/sakuranew/BERT-AttributeExtraction"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;156. 中文自然语言处理相关的开放任务，数据集, 以及当前最佳结果&lt;/strong&gt; &lt;a href="https://github.com/didi/ChineseNLP"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;157. CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统&lt;/strong&gt; &lt;a href="https://github.com/WiseDoge/CoupletAI"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;158. 抽象知识图谱，目前规模50万，支持名词性实体、状态性描述、事件性动作进行抽象&lt;/strong&gt; &lt;a href="https://github.com/liuhuanyong/AbstractKnowledgeGraph"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;159. MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目&lt;/strong&gt; &lt;a href="%E7%99%BE%E5%BA%A6%E7%9F%A5%E9%81%93%E9%97%AE%E7%AD%94%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%8C%E5%8C%85%E6%8B%AC%E8%B6%85%E8%BF%87580%E4%B8%87%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E6%AF%8F%E4%B8%AA%E9%97%AE%E9%A2%98%E5%B8%A6%E6%9C%89%E9%97%AE%E9%A2%98%E6%A0%87%E7%AD%BE%E3%80%82%E5%9F%BA%E4%BA%8E%E8%AF%A5%E9%97%AE%E7%AD%94%E8%AF%AD%E6%96%99%E5%BA%93%EF%BC%8C%E5%8F%AF%E6%94%AF%E6%8C%81%E5%A4%9A%E7%A7%8D%E5%BA%94%E7%94%A8%EF%BC%8C%E5%A6%82%E9%80%BB%E8%BE%91%E6%8C%96%E6%8E%98"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;160. brat rapid annotation tool: 序列标注工具&lt;/strong&gt; &lt;a href="http://brat.nlplab.org/index.html" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;161. 大规模中文知识图谱数据：：1.4亿实体&lt;/strong&gt; &lt;a href="https://github.com/ownthink/KnowledgeGraphData"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;162. 数据增强在机器翻译及其他nlp任务中的应用及效果&lt;/strong&gt; &lt;a href="https://mp.weixin.qq.com/s/_aVwSWuYho_7MUT0LuFgVA" rel="nofollow"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;163. allennlp阅读理解:支持多种数据和模型&lt;/strong&gt; &lt;a href="https://github.com/allenai/allennlp-reading-comprehension"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;164. PDF表格数据提取工具&lt;/strong&gt; &lt;a href="https://github.com/camelot-dev/camelot"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;165. Graphbrain：AI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断&lt;/strong&gt; &lt;a href="https://github.com/graphbrain/graphbrain"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;166. 简历自动筛选系统&lt;/strong&gt; &lt;a href="https://github.com/JAIJANYANI/Automated-Resume-Screening-System"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;167. 基于命名实体识别的简历自动摘要&lt;/strong&gt; &lt;a href="https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;168. 中文语言理解测评基准，包括代表性的数据集&amp;amp;基准模型&amp;amp;语料库&amp;amp;排行榜&lt;/strong&gt; &lt;a href="https://github.com/brightmart/ChineseGLUE"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;169. 树洞 OCR 文字识别&lt;/strong&gt; &lt;a href="https://github.com/AnyListen/tools-ocr"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个c++ OCR &lt;a href="https://github.com/myhub/tr"&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;170. 从包含表格的扫描图片中识别表格和文字&lt;/strong&gt; &lt;a href="https://github.com/bitdata/ocrtable"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;171. 语声迁移&lt;/strong&gt; &lt;a href="https://github.com/fighting41love/become-yukarin"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;172. Python口语自然语言处理工具集(英文)&lt;/strong&gt; &lt;a href="https://github.com/gooofy/py-nltools"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;173. similarity：相似度计算工具包，java编写&lt;/strong&gt; &lt;a href="https://github.com/shibing624/similarity"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用于词语、短语、句子、词法分析、情感分析、语义分析等相关的相似度计算&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;174. 海量中文预训练ALBERT模型&lt;/strong&gt; &lt;a href="https://github.com/brightmart/albert_zh"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;175. Transformers 2.0&lt;/strong&gt; &lt;a href="https://github.com/huggingface/transformers"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持TensorFlow 2.0 和 PyTorch 的自然语言处理预训练语言模型(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) 8种架构/33种预训练模型/102种语言&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;176. 基于大规模音频数据集Audioset的音频增强&lt;/strong&gt; &lt;a href="https://github.com/AppleHolic/audioset_augmentor"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;177. Poplar：网页版自然语言标注工具&lt;/strong&gt; &lt;a href="https://github.com/synyi/poplar"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;178. 图片文字去除，可用于漫画翻译&lt;/strong&gt; &lt;a href="https://github.com/yu45020/Text_Segmentation_Image_Inpainting"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;179. 186种语言的数字叫法库&lt;/strong&gt; &lt;a href="https://github.com/google/UniNum"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;180. Amazon发布基于知识的人-人开放领域对话数据集&lt;/strong&gt; &lt;a href="https://github.com/alexa/alexa-prize-topical-chat-dataset/"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;181. 中文文本纠错模块代码&lt;/strong&gt; &lt;a href="https://github.com/zedom1/error-detection"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;182. 繁简体转换&lt;/strong&gt; &lt;a href="https://github.com/berniey/hanziconv"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;183. Python实现的多种文本可读性评价指标&lt;/strong&gt; &lt;a href="https://github.com/cdimascio/py-readability-metrics"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;184. 类似于人名/地名/组织机构名的命名体识别数据集&lt;/strong&gt; &lt;a href="https://github.com/LG-1/video_music_book_datasets"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;185. 东南大学《知识图谱》研究生课程(资料)&lt;/strong&gt; &lt;a href="https://github.com/npubird/KnowledgeGraphCourse"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fighting41love</author><guid isPermaLink="false">https://github.com/fighting41love/funNLP</guid><pubDate>Tue, 05 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>