<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, This week</title><link>https://github.com/trending/jupyter-notebook?since=weekly</link><description>The top repositories on GitHub for jupyter-notebook, measured weekly</description><pubDate>Fri, 31 Jan 2020 01:07:34 GMT</pubDate><lastBuildDate>Fri, 31 Jan 2020 01:07:34 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>jantic/DeOldify #1 in Jupyter Notebook, This week</title><link>https://github.com/jantic/DeOldify</link><description>&lt;p&gt;&lt;i&gt;A Deep Learning based project for colorizing and restoring old images (and video!)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deoldify" class="anchor" aria-hidden="true" href="#deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeOldify&lt;/h1&gt;
&lt;p&gt;Image &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; |
Video &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt; For those of you who are looking for a quick and easy way to run the open source version of DeOldify, for free, try this!  I love this implementation: &lt;a href="https://deepai.org/machine-learning-model/colorizer" rel="nofollow"&gt;DeOldify Image Colorization on DeepAI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instructions on how to use the Colabs above have been kindly provided in video tutorial form by Old Ireland in Colour's John Breslin.  It's great! Click video image below to watch.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=VaEl0faDw38" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9d812131195cc524d5fe03696fdc284208bedbde/687474703a2f2f696d672e796f75747562652e636f6d2f76692f5661456c306661447733382f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/VaEl0faDw38/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Get more updates on &lt;a href="https://twitter.com/citnaj" rel="nofollow"&gt;Twitter &lt;img src="resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg" width="16" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about-deoldify"&gt;About DeOldify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#example-videos"&gt;Example Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#example-images"&gt;Example Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stuff-that-should-probably-be-in-a-paper"&gt;Stuff That Should Probably Be In A Paper&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#how-to-achieve-stable-video"&gt;How to Achieve Stable Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-is-nogan"&gt;What is NoGAN?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#why-three-models"&gt;Why Three Models?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-technical-details"&gt;Technical Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#this-project-going-forward"&gt;Going Forward&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started-yourself"&gt;Getting Started Yourself&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#easiest-approach"&gt;Easiest Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#your-own-machine-not-as-easy"&gt;Your Own Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pretrained-weights"&gt;Pretrained Weights&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about-deoldify" class="anchor" aria-hidden="true" href="#about-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About DeOldify&lt;/h2&gt;
&lt;p&gt;Simply put, the mission of this project is to colorize and restore old images and film footage.
We'll get into the details in a bit, but first let's see some pretty pictures and videos!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-new-and-exciting-stuff-in-deoldify" class="anchor" aria-hidden="true" href="#new-and-exciting-stuff-in-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New and Exciting Stuff in DeOldify&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Glitches and artifacts are almost entirely eliminated&lt;/li&gt;
&lt;li&gt;Better skin (less zombies)&lt;/li&gt;
&lt;li&gt;More highly detailed and photorealistic renders&lt;/li&gt;
&lt;li&gt;Much less "blue bias"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt; - it actually looks good!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NoGAN&lt;/strong&gt; - a new and weird but highly effective way to do GAN training for image to image.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-example-videos" class="anchor" aria-hidden="true" href="#example-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Videos&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;  Click images to watch&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facebook-f8-demo" class="anchor" aria-hidden="true" href="#facebook-f8-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facebook F8 Demo&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=l3UXXid04Ys" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95e149f839667ddcd87e0a1970e3870f6a61c24a/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6c335558586964303459732f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/l3UXXid04Ys/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-silent-movie-examples" class="anchor" aria-hidden="true" href="#silent-movie-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Silent Movie Examples&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=EXn-n2iqEjI" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24d210457f7e8b57ef701788f013f2f72d2eda1c/687474703a2f2f696d672e796f75747562652e636f6d2f76692f45586e2d6e326971456a492f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-example-images" class="anchor" aria-hidden="true" href="#example-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Images&lt;/h2&gt;
&lt;p&gt;"Migrant Mother" by Dorothea Lange (1936)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067" alt="Migrant Mother" data-canonical-src="https://i.imgur.com/Bt0vnke.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Woman relaxing in her livingroom in Sweden (1920)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067" alt="Sweden Living Room" data-canonical-src="https://i.imgur.com/158d0oU.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Toffs and Toughs" by Jimmy Sime (1937)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067" alt="Class Divide" data-canonical-src="https://i.imgur.com/VYuav4I.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanksgiving Maskers (1911)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067" alt="Thanksgiving Maskers" data-canonical-src="https://i.imgur.com/n8qVJ5c.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Glen Echo Madame Careta Gypsy Camp in Maryland (1925)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067" alt="Gypsy Camp" data-canonical-src="https://i.imgur.com/1oYrJRI.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Mr. and Mrs. Lemuel Smith and their younger children in their farm house, Carroll County, Georgia." (1941)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067" alt="Georgia Farmhouse" data-canonical-src="https://i.imgur.com/I2j8ynm.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Building the Golden Gate Bridge" (est 1937)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067" alt="Golden Gate Bridge" data-canonical-src="https://i.imgur.com/6SbFjfq.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;  What you might be wondering is while this render looks cool, are the colors accurate? The original photo certainly makes it look like the towers of the bridge could be white. We looked into this and it turns out the answer is no - the towers were already covered in red primer by this time. So that's something to keep in mind- historical accuracy remains a huge challenge!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;"Terrasse de café, Paris" (1925)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067" alt="Cafe Paris" data-canonical-src="https://i.imgur.com/WprQwP5.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Norwegian Bride (est late 1890s)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067" alt="Norwegian Bride" data-canonical-src="https://i.imgur.com/MmtvrZm.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zitkála-Šá (Lakota: Red Bird), also known as Gertrude Simmons Bonnin (1898)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067" alt="Native Woman" data-canonical-src="https://i.imgur.com/zIGM043.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chinese Opium Smokers (1880)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067" alt="Opium Real" data-canonical-src="https://i.imgur.com/lVGq8Vq.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stuff-that-should-probably-be-in-a-paper" class="anchor" aria-hidden="true" href="#stuff-that-should-probably-be-in-a-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stuff That Should Probably Be In A Paper&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-achieve-stable-video" class="anchor" aria-hidden="true" href="#how-to-achieve-stable-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Achieve Stable Video&lt;/h3&gt;
&lt;p&gt;NoGAN training is crucial to getting the kind of stable and colorful images seen in this iteration of DeOldify. NoGAN training combines the benefits of GAN training (wonderful colorization) while eliminating the nasty side effects (like flickering objects in video). Believe it or not, video is rendered using isolated image generation without any sort of temporal modeling tacked on. The process performs 30-60 minutes of the GAN portion of "NoGAN" training, using 1% to 3% of imagenet data once.  Then, as with still image colorization, we "DeOldify" individual frames before rebuilding the video.&lt;/p&gt;
&lt;p&gt;In addition to improved video stability, there is an interesting thing going on here worth mentioning. It turns out the models I run, even different ones and with different training structures, keep arriving at more or less the same solution.  That's even the case for the colorization of things you may think would be arbitrary and unknowable, like the color of clothing, cars, and even special effects (as seen in "Metropolis").&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966" alt="Metropolis Special FX" data-canonical-src="https://thumbs.gfycat.com/HeavyLoneBlowfish-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My best guess is that the models are learning some interesting rules about how to colorize based on subtle cues present in the black and white images that I certainly wouldn't expect to exist.  This result leads to nicely deterministic and consistent results, and that means you don't have track model colorization decisions because they're not arbitrary.  Additionally, they seem remarkably robust so that even in moving scenes the renders are very consistent.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966" alt="Moving Scene Example" data-canonical-src="https://thumbs.gfycat.com/FamiliarJubilantAsp-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Other ways to stabilize video add up as well. First, generally speaking rendering at a higher resolution (higher render_factor) will increase stability of colorization decisions.  This stands to reason because the model has higher fidelity image information to work with and will have a greater chance of making the "right" decision consistently.  Closely related to this is the use of resnet101 instead of resnet34 as the backbone of the generator- objects are detected more consistently and correctly with this. This is especially important for getting good, consistent skin rendering.  It can be particularly visually jarring if you wind up with "zombie hands", for example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966" alt="Zombie Hand Example" data-canonical-src="https://thumbs.gfycat.com/ThriftyInferiorIsabellinewheatear-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Additionally, gaussian noise augmentation during training appears to help but at this point the conclusions as to just how much are bit more tenuous (I just haven't formally measured this yet).  This is loosely based on work done in style transfer video, described here:  &lt;a href="https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42" rel="nofollow"&gt;https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Special thanks go to Rani Horev for his contributions in implementing this noise augmentation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-is-nogan" class="anchor" aria-hidden="true" href="#what-is-nogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is NoGAN?&lt;/h3&gt;
&lt;p&gt;This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model. It provides the benefits of GAN training while spending minimal time doing direct GAN training.  Instead, most of the training time is spent pretraining the generator and critic separately with more straight-forward, fast and reliable conventional methods.  A key insight here is that those more "conventional" methods generally get you most of the results you need, and that GANs can be used to close the gap on realism. During the very short amount of actual GAN training the generator not only gets the full realistic colorization capabilities that used to take days of progressively resized GAN training, but it also doesn't accrue nearly as much of the artifacts and other ugly baggage of GANs. In fact, you can pretty much eliminate glitches and artifacts almost entirely depending on your approach. As far as I know this is a new technique. And it's incredibly effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original DeOldify Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966" alt="Before Flicker" data-canonical-src="https://thumbs.gfycat.com/CoordinatedVeneratedHogget-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NoGAN-Based DeOldify Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966" alt="After Flicker" data-canonical-src="https://thumbs.gfycat.com/OilyBlackArctichare-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The steps are as follows: First train the generator in a conventional way by itself with just the feature loss. Next, generate images from that, and train the critic on distinguishing between those outputs and real images as a basic binary classifier. Finally, train the generator and critic together in a GAN setting (starting right at the target size of 192px in this case).  Now for the weird part:  All the useful GAN training here only takes place within a very small window of time.  There's an inflection point where it appears the critic has transferred everything it can that is useful to the generator. Past this point, image quality oscillates between the best that you can get at the inflection point, or bad in a predictable way (orangish skin, overly red lips, etc).  There appears to be no productive training after the inflection point.  And this point lies within training on just 1% to 3% of the Imagenet Data!  That amounts to about 30-60 minutes of training at 192px.&lt;/p&gt;
&lt;p&gt;The hard part is finding this inflection point.  So far, I've accomplished this by making a whole bunch of model save checkpoints (every 0.1% of data iterated on) and then just looking for the point where images look great before they go totally bonkers with orange skin (always the first thing to go). Additionally, generator rendering starts immediately getting glitchy and inconsistent at this point, which is no good particularly for video. What I'd really like to figure out is what the tell-tale sign of the inflection point is that can be easily automated as an early stopping point.  Unfortunately, nothing definitive is jumping out at me yet.  For one, it's happening in the middle of training loss decreasing- not when it flattens out, which would seem more reasonable on the surface.&lt;/p&gt;
&lt;p&gt;Another key thing about NoGAN training is you can repeat pretraining the critic on generated images after the initial GAN training, then repeat the GAN training itself in the same fashion.  This is how I was able to get extra colorful results with the "artistic" model.  But this does come at a cost currently- the output of the generator becomes increasingly inconsistent and you have to experiment with render resolution (render_factor) to get the best result.  But the renders are still glitch free and way more consistent than I was ever able to achieve with the original DeOldify model. You can do about five of these repeat cycles, give or take, before you get diminishing returns, as far as I can tell.&lt;/p&gt;
&lt;p&gt;Keep in mind- I haven't been entirely rigorous in figuring out what all is going on in NoGAN- I'll save that for a paper. That means there's a good chance I'm wrong about something.  But I think it's definitely worth putting out there now because I'm finding it very useful- it's solving basically much of my remaining problems I had in DeOldify.&lt;/p&gt;
&lt;p&gt;This builds upon a technique developed in collaboration with Jeremy Howard and Sylvain Gugger for Fast.AI's Lesson 7 in version 3 of Practical Deep Learning for Coders Part I. The particular lesson notebook can be found here: &lt;a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb"&gt;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-why-three-models" class="anchor" aria-hidden="true" href="#why-three-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Three Models?&lt;/h2&gt;
&lt;p&gt;There are now three models to choose from in DeOldify. Each of these has key strengths and weaknesses, and so have different use cases.  Video is for video of course.  But stable and artistic are both for images, and sometimes one will do images better than the other.&lt;/p&gt;
&lt;p&gt;More details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Artistic&lt;/strong&gt; - This model achieves the highest quality results in image coloration, in terms of interesting details and vibrance. The most notable drawback however is that it's a bit of a pain to fiddle around with to get the best results (you have to adjust the rendering resolution or render_factor to achieve this).  Additionally, the model does not do as well as stable in a few key common scenarios- nature scenes and portraits.  The model uses a resnet34 backbone on a UNet with an emphasis on depth of layers on the decoder side.  This model was trained with 5 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 32% of Imagenet data trained once (12.5 hours of direct GAN training).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stable&lt;/strong&gt; - This model achieves the best results with landscapes and portraits. Notably, it produces less "zombies"- where faces or limbs stay gray rather than being colored in properly.  It generally has less weird miscolorations than artistic, but it's also less colorful in general.  This model uses a resnet101 backbone on a UNet with an emphasis on width of layers on the decoder side.  This model was trained with 3 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 7% of Imagenet data trained once (3 hours of direct GAN training).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt; - This model is optimized for smooth, consistent and flicker-free video.  This would definitely be the least colorful of the three models, but it's honestly not too far off from "stable". The model is the same as "stable" in terms of architecture, but differs in training.  It's trained for a mere 2.2% of Imagenet data once at 192px, using only the initial generator/critic pretrain/GAN NoGAN training (1 hour of direct GAN training).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because the training of the artistic and stable models was done before the "inflection point" of NoGAN training described in "What is NoGAN???" was discovered,  I believe this amount of training on them can be knocked down considerably. As far as I can tell, the models were stopped at "good points" that were well beyond where productive training was taking place.  I'll be looking into this in the future.&lt;/p&gt;
&lt;p&gt;Ideally, eventually these three models will be consolidated into one that has all these good desirable unified.  I think there's a path there, but it's going to require more work!  So for now, the most practical solution appears to be to maintain multiple models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-technical-details" class="anchor" aria-hidden="true" href="#the-technical-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Technical Details&lt;/h2&gt;
&lt;p&gt;This is a deep learning based model.  More specifically, what I've done is combined the following approaches:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-self-attention-generative-adversarial-network" class="anchor" aria-hidden="true" href="#self-attention-generative-adversarial-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://arxiv.org/abs/1805.08318" rel="nofollow"&gt;Self-Attention Generative Adversarial Network&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Except the generator is a &lt;strong&gt;pretrained U-Net&lt;/strong&gt;, and I've just modified it to have the spectral normalization and self-attention.  It's a pretty straightforward translation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-two-time-scale-update-rule" class="anchor" aria-hidden="true" href="#two-time-scale-update-rule"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://arxiv.org/abs/1706.08500" rel="nofollow"&gt;Two Time-Scale Update Rule&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is also very straightforward – it's just one to one generator/critic iterations and higher critic learning rate.
This is modified to incorporate a "threshold" critic loss that makes sure that the critic is "caught up" before moving on to generator training.
This is particularly useful for the "NoGAN" method described below.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nogan" class="anchor" aria-hidden="true" href="#nogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NoGAN&lt;/h3&gt;
&lt;p&gt;There's no paper here! This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model.
The gist is that you get the benefits of GAN training while spending minimal time doing direct GAN training.
More details are in the &lt;a href="#what-is-nogan"&gt;What is NoGAN?&lt;/a&gt; section (it's a doozy).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generator-loss" class="anchor" aria-hidden="true" href="#generator-loss"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generator Loss&lt;/h3&gt;
&lt;p&gt;Loss during NoGAN learning is two parts:  One is a basic Perceptual Loss (or Feature Loss) based on VGG16 – this just biases the generator model to replicate the input image.
The second is the loss score from the critic.  For the curious – Perceptual Loss isn't sufficient by itself to produce good results.
It tends to just encourage a bunch of brown/green/blue – you know, cheating to the test, basically, which neural networks are really good at doing!
Key thing to realize here is that GANs essentially are learning the loss function for you – which is really one big step closer to toward the ideal that we're shooting for in machine learning.
And of course you generally get much better results when you get the machine to learn something you were previously hand coding.
That's certainly the case here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Of note:&lt;/strong&gt;  There's no longer any "Progressive Growing of GANs" type training going on here.  It's just not needed in lieu of the superior results obtained by the "NoGAN" technique described above.&lt;/p&gt;
&lt;p&gt;The beauty of this model is that it should be generally useful for all sorts of image modification, and it should do it quite well.
What you're seeing above are the results of the colorization model, but that's just one component in a pipeline that I'm developing with the exact same approach.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-this-project-going-forward" class="anchor" aria-hidden="true" href="#this-project-going-forward"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;This Project, Going Forward&lt;/h2&gt;
&lt;p&gt;So that's the gist of this project – I'm looking to make old photos and film look reeeeaaally good with GANs, and more importantly, make the project &lt;em&gt;useful&lt;/em&gt;.
In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.
I'll try to make this as user-friendly as possible, but I'm sure there's going to be hiccups along the way.&lt;/p&gt;
&lt;p&gt;Oh and I swear I'll document the code properly...eventually.  Admittedly I'm &lt;em&gt;one of those&lt;/em&gt; people who believes in "self documenting code" (LOL).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-yourself" class="anchor" aria-hidden="true" href="#getting-started-yourself"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started Yourself&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-easiest-approach" class="anchor" aria-hidden="true" href="#easiest-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Easiest Approach&lt;/h3&gt;
&lt;p&gt;The easiest way to get started is to go straight to the Colab notebooks:&lt;/p&gt;
&lt;p&gt;Image &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
| Video &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Special thanks to Matt Robinson and María Benavente for their image Colab notebook contributions, and Robert Bell for the video Colab notebook work!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-your-own-machine-not-as-easy" class="anchor" aria-hidden="true" href="#your-own-machine-not-as-easy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Your Own Machine (not as easy)&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-hardware-and-operating-system-requirements" class="anchor" aria-hidden="true" href="#hardware-and-operating-system-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hardware and Operating System Requirements&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(Training Only) BEEFY Graphics card&lt;/strong&gt;.  I'd really like to have more memory than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.  The Generators and Critic are ridiculously large.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Colorization Alone) A decent graphics card&lt;/strong&gt;. Approximately 4GB+ memory video cards should be sufficient.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linux (or maybe Windows 10)&lt;/strong&gt;  I'm using Ubuntu 16.04, but nothing about this precludes Windows 10 support as far as I know.  I just haven't tested it and am not going to make it a priority for now.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-easy-install" class="anchor" aria-hidden="true" href="#easy-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Easy Install&lt;/h4&gt;
&lt;p&gt;You should now be able to do a simple install with Anaconda. Here are the steps:&lt;/p&gt;
&lt;p&gt;Open the command line and navigate to the root folder you wish to install.  Then type the following commands&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd DeOldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;conda env create -f environment.yml&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then start running with these commands:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;source activate deoldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;jupyter lab&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From there you can start running the notebooks in Jupyter Lab, via the url they provide you in the console.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can also now do "conda activate deoldify" if you have the latest version of conda and in fact that's now recommended. But a lot of people don't have that yet so I'm not going to make it the default instruction here yet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-note-on-test_images-folder" class="anchor" aria-hidden="true" href="#note-on-test_images-folder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note on test_images Folder&lt;/h4&gt;
&lt;p&gt;The images in the &lt;code&gt;test_images&lt;/code&gt; folder have been removed because they were using Git LFS and that costs a lot of money when GitHub actually charges for bandwidth on a popular open source project (they had a billing bug for while that was recently fixed).  The notebooks that use them (the image test ones) still point to images in that directory that I (Jason) have personally and I'd like to keep it that way because, after all, I'm by far the primary and most active developer.  But they won't work for you.  Still, those notebooks are a convenient template for making your own tests if you're so inclined.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-typical-training" class="anchor" aria-hidden="true" href="#typical-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typical training&lt;/h4&gt;
&lt;p&gt;The notebook &lt;code&gt;ColorizeTrainingWandb&lt;/code&gt; has been created to log and monitor results through &lt;a href="https://www.wandb.com/" rel="nofollow"&gt;Weights &amp;amp; Biases&lt;/a&gt;. You can find a description of typical training by consulting &lt;a href="https://app.wandb.ai/borisd13/DeOldify/reports?view=borisd13%2FDeOldify" rel="nofollow"&gt;W&amp;amp;B Report&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-docker-for-jupyter" class="anchor" aria-hidden="true" href="#docker-for-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker for Jupyter&lt;/h3&gt;
&lt;p&gt;You can build and run the docker using the following process:&lt;/p&gt;
&lt;p&gt;Cloning&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd DeOldify &amp;amp;&amp;amp; docker build -t deoldify_jupyter -f Dockerfile .&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;echo "http://$(curl ifconfig.io):8888" &amp;amp;&amp;amp; nvidia-docker run --ipc=host --env NOTEBOOK_PASSWORD="pass123" -p 8888:8888 -it deoldify_jupyter&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-docker-for-api" class="anchor" aria-hidden="true" href="#docker-for-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker for API&lt;/h3&gt;
&lt;p&gt;You can build and run the docker using the following process:&lt;/p&gt;
&lt;p&gt;Cloning&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd DeOldify &amp;amp;&amp;amp; docker build -t deoldify_api -f Dockerfile-api .&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;echo "http://$(curl ifconfig.io):5000" &amp;amp;&amp;amp; nvidia-docker run --ipc=host -p 5000:5000 -d deoldify_api&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Calling the API for image processing&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: image/png" -H "Content-Type: application/json" -d "{\"source_url\":\"http://www.afrikanheritage.com/wp-content/uploads/2015/08/slave-family-P.jpeg\", \"render_factor\":35}" --output colorized_image.png&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Calling the API for video processing&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: application/octet-stream" -H "Content-Type: application/json" -d "{\"source_url\":\"https://v.redd.it/d1ku57kvuf421/HLSPlaylist.m3u8\", \"render_factor\":35}" --output colorized_video.mp4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don't have Nvidia Docker, &lt;a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#installing-version-20"&gt;here&lt;/a&gt; is the installation guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-installation-details" class="anchor" aria-hidden="true" href="#installation-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Details&lt;/h3&gt;
&lt;p&gt;This project is built around the wonderful Fast.AI library.  Prereqs, in summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.AI 1.0.51&lt;/strong&gt; (and its dependencies).  If you use any higher version you'll see grid artifacts in rendering and tensorboard will malfunction. So yeah...don't do that.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 1.0.1&lt;/strong&gt; Not the latest version of PyTorch- that will not play nicely with the version of FastAI above.  Note however that the conda install of FastAI 1.0.51 grabs the latest PyTorch, which doesn't work.  This is patched over by our own conda install but fyi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; &lt;code&gt;conda install -c conda-forge jupyterlab&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensorboard&lt;/strong&gt; (i.e. install Tensorflow) and &lt;strong&gt;TensorboardX&lt;/strong&gt; (&lt;a href="https://github.com/lanpa/tensorboardX"&gt;https://github.com/lanpa/tensorboardX&lt;/a&gt;).  I guess you don't &lt;em&gt;have&lt;/em&gt; to but man, life is so much better with it.  FastAI now comes with built in support for this- you just  need to install the prereqs: &lt;code&gt;conda install -c anaconda tensorflow-gpu&lt;/code&gt; and &lt;code&gt;pip install tensorboardX&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageNet&lt;/strong&gt; – Only if you're training, of course. It has proven to be a great dataset for my purposes.  &lt;a href="http://www.image-net.org/download-images" rel="nofollow"&gt;http://www.image-net.org/download-images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h2&gt;
&lt;p&gt;To start right away on your own machine with your own images or videos without training the models yourself, you'll need to download the "Completed Generator Weights" listed below and drop them in the /models/ folder.&lt;/p&gt;
&lt;p&gt;The colorization inference notebooks should be able to guide you from here. The notebooks to use are named ImageColorizerArtistic.ipynb, ImageColorizerStable.ipynb, and VideoColorizer.ipynb.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-completed-generator-weights" class="anchor" aria-hidden="true" href="#completed-generator-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Completed Generator Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/336vn9y4qwyg9yz/ColorizeVideo_gen.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-completed-critic-weights" class="anchor" aria-hidden="true" href="#completed-critic-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Completed Critic Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/8g5txfzt2fw8mf5/ColorizeArtistic_crit.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/7a8u20e7xdu1dtd/ColorizeStable_crit.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/0401djgo1dfxdzt/ColorizeVideo_crit.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pretrain-only-generator-weights" class="anchor" aria-hidden="true" href="#pretrain-only-generator-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrain Only Generator Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/9zexurvrve141n9/ColorizeArtistic_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/mdnuo1563bb8nh4/ColorizeStable_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/avzixh1ujf86e8x/ColorizeVideo_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pretrain-only-critic-weights" class="anchor" aria-hidden="true" href="#pretrain-only-critic-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrain Only Critic Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/lakxe8akzjgjnmh/ColorizeArtistic_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/b3wka56iyv1fvdc/ColorizeStable_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/j7og84cbhpa94gs/ColorizeVideo_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-want-the-old-deoldify" class="anchor" aria-hidden="true" href="#want-the-old-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want the Old DeOldify?&lt;/h2&gt;
&lt;p&gt;We suspect some of you are going to want access to the original DeOldify model for various reasons.  We have that archived here:  &lt;a href="https://github.com/dana-kelley/DeOldify"&gt;https://github.com/dana-kelley/DeOldify&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-want-more" class="anchor" aria-hidden="true" href="#want-more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want More?&lt;/h2&gt;
&lt;p&gt;Follow &lt;a href="https://twitter.com/search?q=%23Deoldify" rel="nofollow"&gt;#DeOldify&lt;/a&gt; or &lt;a href="https://twitter.com/citnaj" rel="nofollow"&gt;Jason Antic&lt;/a&gt; on Twitter.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;All code in this repository is under the MIT license as specified by the LICENSE file.&lt;/p&gt;
&lt;p&gt;The model weights listed in this readme under the "Pretrained Weights" section are trained by ourselves and are released under the MIT license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jantic</author><guid isPermaLink="false">https://github.com/jantic/DeOldify</guid><pubDate>Fri, 31 Jan 2020 00:01:00 GMT</pubDate></item><item><title>fivethirtyeight/data #2 in Jupyter Notebook, This week</title><link>https://github.com/fivethirtyeight/data</link><description>&lt;p&gt;&lt;i&gt;Data and code behind the articles and graphics at FiveThirtyEight&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;See &lt;a href="https://data.fivethirtyeight.com/" rel="nofollow"&gt;https://data.fivethirtyeight.com/&lt;/a&gt; for a list of the data and code we've published.&lt;/p&gt;
&lt;p&gt;Unless otherwise noted, our data sets are available under the &lt;a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;, and the code is available under the &lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;MIT License&lt;/a&gt;. If you find this information useful, please &lt;a href="mailto:data@fivethirtyeight.com"&gt;let us know&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fivethirtyeight</author><guid isPermaLink="false">https://github.com/fivethirtyeight/data</guid><pubDate>Fri, 31 Jan 2020 00:02:00 GMT</pubDate></item><item><title>CoreyMSchafer/code_snippets #3 in Jupyter Notebook, This week</title><link>https://github.com/CoreyMSchafer/code_snippets</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-code_snippets" class="anchor" aria-hidden="true" href="#code_snippets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;code_snippets&lt;/h1&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CoreyMSchafer</author><guid isPermaLink="false">https://github.com/CoreyMSchafer/code_snippets</guid><pubDate>Fri, 31 Jan 2020 00:03:00 GMT</pubDate></item><item><title>lmoroney/dlaicourse #4 in Jupyter Notebook, This week</title><link>https://github.com/lmoroney/dlaicourse</link><description>&lt;p&gt;&lt;i&gt;Notebooks for learning deep learning&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This repo does not have a README.&lt;/i&gt;&lt;/p&gt;</description><author>lmoroney</author><guid isPermaLink="false">https://github.com/lmoroney/dlaicourse</guid><pubDate>Fri, 31 Jan 2020 00:04:00 GMT</pubDate></item><item><title>slundberg/shap #5 in Jupyter Notebook, This week</title><link>https://github.com/slundberg/shap</link><description>&lt;p&gt;&lt;i&gt;A game theoretic approach to explain the output of any machine learning model.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.png"&gt;&lt;img src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.png" width="800" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/slundberg/shap" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/19de279a6f67f8eea3f52ccecc779c3e1aff55e7/68747470733a2f2f7472617669732d63692e6f72672f736c756e64626572672f736861702e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/slundberg/shap.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://mybinder.org/v2/gh/slundberg/shap/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see &lt;a href="#citations"&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;Shap can be installed from either &lt;a href="https://pypi.org/project/shap" rel="nofollow"&gt;PyPI&lt;/a&gt; or &lt;a href="https://anaconda.org/conda-forge/shap" rel="nofollow"&gt;conda-forge&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;pip install shap
&lt;i&gt;or&lt;/i&gt;
conda install -c conda-forge shap
&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models" class="anchor" aria-hidden="true" href="#tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tree ensemble example with TreeExplainer (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt;
&lt;p&gt;While SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our &lt;a href="https://rdcu.be/b0z70" rel="nofollow"&gt;Nature MI paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; xgboost
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load JS visualization code to notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train XGBoost model&lt;/span&gt;
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.boston()
model &lt;span class="pl-k"&gt;=&lt;/span&gt; xgboost.train({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.01&lt;/span&gt;}, xgboost.DMatrix(X, &lt;span class="pl-v"&gt;label&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;y), &lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain the model's predictions using SHAP&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.TreeExplainer(model)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue (these force plots are introduced in our &lt;a href="https://rdcu.be/baVbR" rel="nofollow"&gt;Nature BME paper&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the training set predictions&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions &lt;code&gt;dependence_plot&lt;/code&gt; automatically selects another feature for coloring. In this case coloring by RAD (index of accessibility to radial highways) highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; create a dependence plot to show the effect of a single feature across the whole dataset&lt;/span&gt;
shap.dependence_plot(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;RM&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png"&gt;&lt;img width="544" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; summarize the effects of all the features&lt;/span&gt;
shap.summary_plot(shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;shap.summary_plot(shap_values, X, &lt;span class="pl-v"&gt;plot_type&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;bar&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png"&gt;&lt;img width="470" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-deepexplainer-tensorflowkeras-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-deepexplainer-tensorflowkeras-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt;
&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href="https://arxiv.org/abs/1704.02685" rel="nofollow"&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&lt;/span&gt;

&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; select a set of background examples to take an expectation over&lt;/span&gt;
background &lt;span class="pl-k"&gt;=&lt;/span&gt; x_train[np.random.choice(x_train.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;replace&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain predictions of the model on four images&lt;/span&gt;
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.DeepExplainer(model, background)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...or pass tensors directly&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&lt;/span&gt;
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the feature attributions&lt;/span&gt;
shap.image_plot(shap_values, &lt;span class="pl-k"&gt;-&lt;/span&gt;x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png"&gt;&lt;img width="820" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt;
&lt;p&gt;Expected gradients combines ideas from &lt;a href="https://arxiv.org/abs/1703.01365" rel="nofollow"&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href="https://arxiv.org/abs/1706.03825" rel="nofollow"&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;VGG16&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; preprocess_input
&lt;span class="pl-k"&gt;import&lt;/span&gt; keras.backend &lt;span class="pl-k"&gt;as&lt;/span&gt; K
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; json
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load pre-trained model and choose two images to explain&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; VGG16(&lt;span class="pl-v"&gt;weights&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;imagenet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;include_top&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.imagenet50()
to_explain &lt;span class="pl-k"&gt;=&lt;/span&gt; X[[&lt;span class="pl-c1"&gt;39&lt;/span&gt;,&lt;span class="pl-c1"&gt;41&lt;/span&gt;]]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load the ImageNet class names&lt;/span&gt;
url &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
fname &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.cache(url)
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(fname) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
    class_names &lt;span class="pl-k"&gt;=&lt;/span&gt; json.load(f)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain how the input to the 7th layer of the model explains the top two classes&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;map2layer&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;layer&lt;/span&gt;):
    feed_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;dict&lt;/span&gt;(&lt;span class="pl-c1"&gt;zip&lt;/span&gt;([model.layers[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].input], [preprocess_input(x.copy())]))
    &lt;span class="pl-k"&gt;return&lt;/span&gt; K.get_session().run(model.layers[layer].input, feed_dict)
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.GradientExplainer(
    (model.layers[&lt;span class="pl-c1"&gt;7&lt;/span&gt;].input, model.layers[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;].output),
    map2layer(X, &lt;span class="pl-c1"&gt;7&lt;/span&gt;),
    &lt;span class="pl-v"&gt;local_smoothing&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; std dev of smoothing noise&lt;/span&gt;
)
shap_values,indexes &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(map2layer(to_explain, &lt;span class="pl-c1"&gt;7&lt;/span&gt;), &lt;span class="pl-v"&gt;ranked_outputs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get the names for the classes&lt;/span&gt;
index_names &lt;span class="pl-k"&gt;=&lt;/span&gt; np.vectorize(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: class_names[&lt;span class="pl-c1"&gt;str&lt;/span&gt;(x)][&lt;span class="pl-c1"&gt;1&lt;/span&gt;])(indexes)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the explanations&lt;/span&gt;
shap.image_plot(shap_values, to_explain, index_names)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png"&gt;&lt;img width="500" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-agnostic-example-with-kernelexplainer-explains-any-function" class="anchor" aria-hidden="true" href="#model-agnostic-example-with-kernelexplainer-explains-any-function"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt;
&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sklearn
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class="pl-k"&gt;import&lt;/span&gt; train_test_split

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; print the JS visualization code to the notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train a SVM classifier&lt;/span&gt;
X_train,X_test,Y_train,Y_test &lt;span class="pl-k"&gt;=&lt;/span&gt; train_test_split(&lt;span class="pl-k"&gt;*&lt;/span&gt;shap.datasets.iris(), &lt;span class="pl-v"&gt;test_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-v"&gt;random_state&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
svm &lt;span class="pl-k"&gt;=&lt;/span&gt; sklearn.svm.SVC(&lt;span class="pl-v"&gt;kernel&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rbf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;probability&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
svm.fit(X_train, Y_train)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; use Kernel SHAP to explain test set predictions&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.KernelExplainer(svm.predict_proba, X_train, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X_test, &lt;span class="pl-v"&gt;nsamples&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of the first instance&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X_test.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png"&gt;&lt;img width="810" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of all instances&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], X_test, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png"&gt;&lt;img width="813" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-shap-interaction-values" class="anchor" aria-hidden="true" href="#shap-interaction-values"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SHAP Interaction Values&lt;/h2&gt;
&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sample-notebooks" class="anchor" aria-hidden="true" href="#sample-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample notebooks&lt;/h2&gt;
&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-treeexplainer" class="anchor" aria-hidden="true" href="#treeexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TreeExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html" rel="nofollow"&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html" rel="nofollow"&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deepexplainer" class="anchor" aria-hidden="true" href="#deepexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html" rel="nofollow"&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html" rel="nofollow"&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-gradientexplainer" class="anchor" aria-hidden="true" href="#gradientexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GradientExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html" rel="nofollow"&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-linearexplainer" class="anchor" aria-hidden="true" href="#linearexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LinearExplainer&lt;/h3&gt;
&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covaraince matrix. LinearExplainer supports both of these options.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html" rel="nofollow"&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kernelexplainer" class="anchor" aria-hidden="true" href="#kernelexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KernelExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes not assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html" rel="nofollow"&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation-notebooks" class="anchor" aria-hidden="true" href="#documentation-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation notebooks&lt;/h2&gt;
&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/decision_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-methods-unified-by-shap" class="anchor" aria-hidden="true" href="#methods-unified-by-shap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Methods Unified by SHAP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems." Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015): e0130140.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. "Analysis of regression in game theory approach." Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href="http://blog.datadive.net/interpreting-random-forests/" rel="nofollow"&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citations&lt;/h2&gt;
&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href="https://suinlee.cs.washington.edu" rel="nofollow"&gt;Su-In Lee's lab&lt;/a&gt; at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For general use of SHAP you can read/cite our &lt;a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions" rel="nofollow"&gt;NeurIPS paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For TreeExplainer you can read/cite our &lt;a href="https://www.nature.com/articles/s42256-019-0138-9" rel="nofollow"&gt;Nature Machine Intelligence paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/tree_explainer.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;; &lt;a href="https://rdcu.be/b0z70" rel="nofollow"&gt;free access&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;; &lt;a href="https://rdcu.be/baVbR" rel="nofollow"&gt;free access&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31"&gt;&lt;img height="1" width="1" src="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31" data-canonical-src="https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>slundberg</author><guid isPermaLink="false">https://github.com/slundberg/shap</guid><pubDate>Fri, 31 Jan 2020 00:05:00 GMT</pubDate></item><item><title>udacity/deep-learning-v2-pytorch #6 in Jupyter Notebook, This week</title><link>https://github.com/udacity/deep-learning-v2-pytorch</link><description>&lt;p&gt;&lt;i&gt;Projects and exercises for the latest Deep Learning ND program https://www.udacity.com/course/deep-learning-nanodegree--nd101&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-pytorch" class="anchor" aria-hidden="true" href="#deep-learning-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning (PyTorch)&lt;/h1&gt;
&lt;p&gt;This repository contains material related to Udacity's &lt;a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="nofollow"&gt;Deep Learning Nanodegree program&lt;/a&gt;. It consists of a bunch of tutorial notebooks for various deep learning topics. In most cases, the notebooks lead you through implementing models such as convolutional networks, recurrent networks, and GANs. There are other topics covered such as weight initialization and batch normalization.&lt;/p&gt;
&lt;p&gt;There are also notebooks used as projects for the Nanodegree program. In the program itself, the projects are reviewed by real people (Udacity reviewers), but the starting code is available here, as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table Of Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-neural-networks" class="anchor" aria-hidden="true" href="#introduction-to-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks"&gt;Introduction to Neural Networks&lt;/a&gt;: Learn how to implement gradient descent and apply it to predicting patterns in student admissions data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-analysis-network"&gt;Sentiment Analysis with NumPy&lt;/a&gt;: &lt;a href="http://iamtrask.github.io/" rel="nofollow"&gt;Andrew Trask&lt;/a&gt; leads you through building a sentiment analysis model, predicting if some text is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch"&gt;Introduction to PyTorch&lt;/a&gt;: Learn how to build neural networks in PyTorch and use pre-trained networks for state-of-the-art image classifiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks"&gt;Convolutional Neural Networks&lt;/a&gt;: Visualize the output of layers that make up a CNN. Learn how to define and train a CNN for classifying &lt;a href="https://en.wikipedia.org/wiki/MNIST_database" rel="nofollow"&gt;MNIST data&lt;/a&gt;, a handwritten digit database that is notorious in the fields of machine and deep learning. Also, define and train a CNN for classifying images in the &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"&gt;CIFAR10 dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/transfer-learning"&gt;Transfer Learning&lt;/a&gt;. In practice, most people don't train their own networks on huge datasets; they use &lt;strong&gt;pre-trained&lt;/strong&gt; networks such as VGGnet. Here you'll use VGGnet to help classify images of flowers without training an end-to-end network from scratch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/weight-initialization"&gt;Weight Initialization&lt;/a&gt;: Explore how initializing network weights affects performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder"&gt;Autoencoders&lt;/a&gt;: Build models for image compression and de-noising, using feedforward and convolutional networks in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/style-transfer"&gt;Style Transfer&lt;/a&gt;: Extract style and content features from images, using a pre-trained network. Implement style transfer according to the paper, &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="nofollow"&gt;Image Style Transfer Using Convolutional Neural Networks&lt;/a&gt; by Gatys et. al. Define appropriate losses for iteratively creating a target, style-transferred image of your own design!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/recurrent-neural-networks"&gt;Intro to Recurrent Networks (Time series &amp;amp; Character-level RNN)&lt;/a&gt;: Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text; learn how to implement these in PyTorch for a variety of tasks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"&gt;Embeddings (Word2Vec)&lt;/a&gt;: Implement the Word2Vec model to find semantic representations of words for use in natural language processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn"&gt;Sentiment Analysis RNN&lt;/a&gt;: Implement a recurrent neural network that can predict if the text of a moview review is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/attention"&gt;Attention&lt;/a&gt;: Implement attention and apply it to annotation vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/gan-mnist"&gt;Generative Adversarial Network on MNIST&lt;/a&gt;: Train a simple generative adversarial network on the MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/batch-norm"&gt;Batch Normalization&lt;/a&gt;: Learn how to improve training rates and network stability with batch normalizations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/dcgan-svhn"&gt;Deep Convolutional GAN (DCGAN)&lt;/a&gt;: Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/cycle-gan"&gt;CycleGAN&lt;/a&gt;: Implement a CycleGAN that is designed to learn from unpaired and unlabeled data; use trained generators to transform images from summer to winter and vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deploying-a-model-with-aws-sagemaker" class="anchor" aria-hidden="true" href="#deploying-a-model-with-aws-sagemaker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploying a Model (with AWS SageMaker)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/sagemaker-deployment"&gt;All exercise and project notebooks&lt;/a&gt; for the lessons on model deployment can be found in the linked, Github repo. Learn to deploy pre-trained models using AWS SageMaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-bikesharing"&gt;Predicting Bike-Sharing Patterns&lt;/a&gt;: Implement a neural network in NumPy to predict bike rentals.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification"&gt;Dog Breed Classifier&lt;/a&gt;: Build a convolutional neural network with PyTorch to classify any image (even an image of a face) as a specific dog breed.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-tv-script-generation"&gt;TV Script Generation&lt;/a&gt;: Train a recurrent neural network to generate scripts in the style of dialogue from Seinfeld.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-face-generation"&gt;Face Generation&lt;/a&gt;: Use a DCGAN on the CelebA dataset to generate images of new and realistic human faces.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-elective-material" class="anchor" aria-hidden="true" href="#elective-material"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Elective Material&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/tensorflow/intro-to-tensorflow"&gt;Intro to TensorFlow&lt;/a&gt;: Starting building neural networks with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/keras"&gt;Keras&lt;/a&gt;: Learn to build neural networks and convolutional neural networks with Keras.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-configure-and-manage-your-environment-with-anaconda" class="anchor" aria-hidden="true" href="#configure-and-manage-your-environment-with-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure and Manage Your Environment with Anaconda&lt;/h2&gt;
&lt;p&gt;Per the Anaconda &lt;a href="http://conda.pydata.org/docs" rel="nofollow"&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conda is an open source package management system and environment management system
for installing multiple versions of software packages and their dependencies and
switching easily between them. It works on Linux, OS X and Windows, and was created
for Python programs but can package and distribute any software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Using Anaconda consists of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt; on your computer, by selecting the latest Python version for your operating system. If you already have &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;miniconda&lt;/code&gt; installed, you should be able to skip this step and move on to step 2.&lt;/li&gt;
&lt;li&gt;Create and activate * a new &lt;code&gt;conda&lt;/code&gt; &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;environment&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;* Each time you wish to work on any exercises, activate your &lt;code&gt;conda&lt;/code&gt; environment!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-1-installation" class="anchor" aria-hidden="true" href="#1-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the latest version of &lt;code&gt;miniconda&lt;/code&gt; that matches your system.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Linux&lt;/th&gt;
&lt;th&gt;Mac&lt;/th&gt;
&lt;th&gt;Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe" rel="nofollow"&gt;64-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh" rel="nofollow"&gt;32-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe" rel="nofollow"&gt;32-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt; on your machine. Detailed instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-create-and-activate-the-environment" class="anchor" aria-hidden="true" href="#2-create-and-activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Create and Activate the Environment&lt;/h2&gt;
&lt;p&gt;For Windows users, these following commands need to be executed from the &lt;strong&gt;Anaconda prompt&lt;/strong&gt; as opposed to a Windows terminal window. For Mac, a normal terminal window will work.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-git-and-version-control" class="anchor" aria-hidden="true" href="#git-and-version-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git and version control&lt;/h4&gt;
&lt;p&gt;These instructions also assume you have &lt;code&gt;git&lt;/code&gt; installed for working with Github from a terminal window, but if you do not, you can download that first with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you'd like to learn more about version control and using &lt;code&gt;git&lt;/code&gt; from the command line, take a look at our &lt;a href="https://www.udacity.com/course/version-control-with-git--ud123" rel="nofollow"&gt;free course: Version Control with Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, we're ready to create our local environment!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository, and navigate to the downloaded folder. This may take a minute or two to clone due to the included image data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/udacity/deep-learning-v2-pytorch.git
cd deep-learning-v2-pytorch
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Create (and activate) a new environment, named &lt;code&gt;deep-learning&lt;/code&gt; with Python 3.6. If prompted to proceed with the install &lt;code&gt;(Proceed [y]/n)&lt;/code&gt; type y.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create -n deep-learning python=3.6
source activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create --name deep-learning python=3.6
activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point your command line should look something like: &lt;code&gt;(deep-learning) &amp;lt;User&amp;gt;:deep-learning-v2-pytorch &amp;lt;user&amp;gt;$&lt;/code&gt;. The &lt;code&gt;(deep-learning)&lt;/code&gt; indicates that your environment has been activated, and you can proceed with further package installations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install PyTorch and torchvision; this should install the latest version of PyTorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch -c pytorch
pip install torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install a few required pip packages, which are specified in the requirements text file (including OpenCV).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="7"&gt;
&lt;li&gt;That's it!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now most of the &lt;code&gt;deep-learning&lt;/code&gt; libraries are available to you. Very occasionally, you will see a repository with an addition requirements file, which exists should you want to use TensorFlow and Keras, for example. In this case, you're encouraged to install another library to your existing environment, or create a new environment for a specific project.&lt;/p&gt;
&lt;p&gt;Now, assuming your &lt;code&gt;deep-learning&lt;/code&gt; environment is still activated, you can navigate to the main repo and start looking at the notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd
cd deep-learning-v2-pytorch
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit the environment when you have completed your work session, simply close the terminal window.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>udacity</author><guid isPermaLink="false">https://github.com/udacity/deep-learning-v2-pytorch</guid><pubDate>Fri, 31 Jan 2020 00:06:00 GMT</pubDate></item><item><title>ageron/handson-ml2 #7 in Jupyter Notebook, This week</title><link>https://github.com/ageron/handson-ml2</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in the second edition of my O'Reilly book &lt;a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067" title="book" width="150" data-canonical-src="https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href="https://github.com/ageron/handson-ml"&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-play-with-these-notebooks-online-without-having-to-install-anything" class="anchor" aria-hidden="true" href="#want-to-play-with-these-notebooks-online-without-having-to-install-anything"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt;
&lt;p&gt;Use any of the following services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: open this repository in &lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;Colaboratory&lt;/a&gt;:
&lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e69988217d15707bdd8b6b27f1d7d53a0dd00af7/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f696d672f636f6c61625f66617669636f6e2e69636f" width="90" data-canonical-src="https://colab.research.google.com/img/colab_favicon.ico" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;Binder&lt;/a&gt;:
&lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/69ea8abed4df43bca4c671b965aeffef2c4f897a/68747470733a2f2f6d61747468696173627573736f6e6e6965722e636f6d2f706f7374732f696d672f62696e6465725f6c6f676f5f313238783132382e706e67" width="90" data-canonical-src="https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: Most of the time, Binder starts up quickly and works great, but when handson-ml2 is updated, Binder creates a new environment from scratch, and this can take quite some time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;Deepnote&lt;/a&gt;:
&lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3fae03be31b768100aa2a800d2cc3b6650c6cd48/68747470733a2f2f7777772e646565706e6f74652e636f6d2f7374617469632f696c6c757374726174696f6e2e706e67" width="150" data-canonical-src="https://www.deepnote.com/static/illustration.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-just-want-to-quickly-look-at-some-notebooks-without-executing-any-code" class="anchor" aria-hidden="true" href="#just-want-to-quickly-look-at-some-notebooks-without-executing-any-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt;
&lt;p&gt;Browse this repository using &lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;:
&lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/079030b4c39b76eafa0c6c3a5bd18112aafe42dd/68747470733a2f2f6a7570797465722e6f72672f6173736574732f6e61765f6c6f676f2e737667" width="150" data-canonical-src="https://jupyter.org/assets/nav_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;a href="index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math equations are not always displayed correctly.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-run-this-project-using-a-docker-image" class="anchor" aria-hidden="true" href="#want-to-run-this-project-using-a-docker-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to run this project using a Docker image?&lt;/h3&gt;
&lt;p&gt;Read the &lt;a href="https://github.com/ageron/handson-ml2/tree/master/docker"&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-install-this-project-on-your-own-machine" class="anchor" aria-hidden="true" href="#want-to-install-this-project-on-your-own-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to install this project on your own machine?&lt;/h3&gt;
&lt;p&gt;Start by installing &lt;a href="https://www.anaconda.com/distribution/" rel="nofollow"&gt;Anaconda&lt;/a&gt; (or &lt;a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow"&gt;Miniconda&lt;/a&gt;), &lt;a href="https://git-scm.com/downloads" rel="nofollow"&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href="https://www.nvidia.com/Download/index.aspx" rel="nofollow"&gt;GPU driver&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git
$ cd handson-ml2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to use a GPU, then edit &lt;code&gt;environment.yml&lt;/code&gt; (or &lt;code&gt;environment-windows.yml&lt;/code&gt; on Windows) and replace &lt;code&gt;tensorflow=2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-gpu=2.0.0&lt;/code&gt;. Also replace &lt;code&gt;tensorflow-serving-api==2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-serving-api-gpu==2.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml # or environment-windows.yml on Windows
$ conda activate tf2
$ python -m ipykernel install --user --name=python3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then if you're on Windows, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, start Jupyter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need further instructions, read the &lt;a href="INSTALL.md"&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park who helped on some of the exercise solutions, and to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml2</guid><pubDate>Fri, 31 Jan 2020 00:07:00 GMT</pubDate></item><item><title>mml-book/mml-book.github.io #8 in Jupyter Notebook, This week</title><link>https://github.com/mml-book/mml-book.github.io</link><description>&lt;p&gt;&lt;i&gt;Companion webpage to the book "Mathematics For Machine Learning"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mml-bookgithubio" class="anchor" aria-hidden="true" href="#mml-bookgithubio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mml-book.github.io&lt;/h1&gt;
&lt;p&gt;Companion webpage to the book "Mathematics For Machine Learning"&lt;/p&gt;
&lt;p&gt;&lt;a href="https://mml-book.com" rel="nofollow"&gt;https://mml-book.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyright 2020 by Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. To be published by Cambridge University Press.&lt;/p&gt;
&lt;p&gt;We are in the process of writing a book on Mathematics for Machine Learning that motivates people to learn mathematical concepts. The book is not intended to cover advanced machine learning techniques because there are already plenty of books doing this. Instead, we aim to provide the necessary mathematical skills to read those other books.&lt;/p&gt;
&lt;p&gt;We split the book into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mathematical foundations&lt;/li&gt;
&lt;li&gt;Example machine learning algorithms that use the mathematical foundations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We aim to keep this book reasonably short, so we cannot cover everything. We will also provide exercises for part 1 and jupyter notebooks for part 2 of the book.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mml-book</author><guid isPermaLink="false">https://github.com/mml-book/mml-book.github.io</guid><pubDate>Fri, 31 Jan 2020 00:08:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #9 in Jupyter Notebook, This week</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Fri, 31 Jan 2020 00:09:00 GMT</pubDate></item><item><title>ageron/handson-ml #10 in Jupyter Notebook, This week</title><link>https://github.com/ageron/handson-ml</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in my O'Reilly book &lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8e10a44b0ddbb9530cc27d877f06db68d9fa1c7d/687474703a2f2f616b616d6169636f766572732e6f7265696c6c792e636f6d2f696d616765732f393738313439313936323238322f6361742e676966" alt="book" data-canonical-src="http://akamaicovers.oreilly.com/images/9781491962282/cat.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simply open the &lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter&lt;/a&gt; notebooks you are interested in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using &lt;a href="http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;note: &lt;a href="https://github.com/ageron/handson-ml/blob/master/index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math formulas are not displayed correctly,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;by cloning this repository and running Jupyter locally. This option lets you play around with the code. In this case, follow the installation instructions below,&lt;/li&gt;
&lt;li&gt;or by running the notebooks in &lt;a href="https://beta.deepnote.com" rel="nofollow"&gt;Deepnote&lt;/a&gt;. This allows you to play around with the code online in your browser. For example, here's a link to the first chapter: &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb" rel="nofollow"&gt;&lt;img height="22" src="https://camo.githubusercontent.com/c3b9bd12a99f8de3301018192105256209bcf800/68747470733a2f2f626574612e646565706e6f74652e636f6d2f627574746f6e732f6c61756e63682d696e2d646565706e6f74652e737667" data-canonical-src="https://beta.deepnote.com/buttons/launch-in-deepnote.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;First, you will need to install &lt;a href="https://git-scm.com/" rel="nofollow"&gt;git&lt;/a&gt;, if you don't have it already.&lt;/p&gt;
&lt;p&gt;Next, clone this repository by opening a terminal and typing the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd $HOME  # or any other development directory you prefer
$ git clone https://github.com/ageron/handson-ml.git
$ cd handson-ml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not want to install git, you can instead download &lt;a href="https://github.com/ageron/handson-ml/archive/master.zip"&gt;master.zip&lt;/a&gt;, unzip it, rename the resulting directory to &lt;code&gt;handson-ml&lt;/code&gt; and move it to your development directory.&lt;/p&gt;
&lt;p&gt;If you want to go through chapter 16 on Reinforcement Learning, you will need to &lt;a href="https://gym.openai.com/docs" rel="nofollow"&gt;install OpenAI gym&lt;/a&gt; and its dependencies for Atari simulations.&lt;/p&gt;
&lt;p&gt;If you are familiar with Python and you know how to install Python libraries, go ahead and install the libraries listed in &lt;code&gt;requirements.txt&lt;/code&gt; and jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section. If you need detailed instructions, please read on.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python--required-libraries" class="anchor" aria-hidden="true" href="#python--required-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python &amp;amp; Required Libraries&lt;/h2&gt;
&lt;p&gt;Of course, you obviously need Python. Python 3 is already preinstalled on many systems nowadays. You can check which version you have by typing the following command (you may need to replace &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 --version  # for Python 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any Python 3 version should be fine, preferably 3.5 or above. If you don't have Python 3, I recommend installing it. To do so, you have several options: on Windows or MacOSX, you can just download it from &lt;a href="https://www.python.org/downloads/" rel="nofollow"&gt;python.org&lt;/a&gt;. On MacOSX, you can alternatively use &lt;a href="https://www.macports.org/" rel="nofollow"&gt;MacPorts&lt;/a&gt; or &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt;. If you are using Python 3.6 on MacOSX, you need to run the following command to install the &lt;code&gt;certifi&lt;/code&gt; package of certificates because Python 3.6 on MacOSX has no certificates to validate SSL connections (see this &lt;a href="https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error" rel="nofollow"&gt;StackOverflow question&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ /Applications/Python\ 3.6/Install\ Certificates.command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Linux, unless you know what you are doing, you should use your system's packaging system. For example, on Debian or Ubuntu, type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get update
$ sudo apt-get install python3 python3-pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to download and install &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt;. This is a package that includes both Python and many scientific libraries. You should prefer the Python 3 version.&lt;/p&gt;
&lt;p&gt;If you choose to use Anaconda, read the next section, or else jump to the &lt;a href="#using-pip"&gt;Using pip&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-anaconda" class="anchor" aria-hidden="true" href="#using-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Anaconda&lt;/h2&gt;
&lt;p&gt;Once you have &lt;a href="https://docs.anaconda.com/anaconda/install/" rel="nofollow"&gt;installed Anaconda&lt;/a&gt; (or Miniconda), you can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give you a conda environment named &lt;code&gt;mlbook&lt;/code&gt;, ready to use! Just activate it and you will have everything setup
for you:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda activate mlbook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You are all set! Next, jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-pip" class="anchor" aria-hidden="true" href="#using-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using pip&lt;/h2&gt;
&lt;p&gt;If you are not using Anaconda, you need to install several scientific Python libraries that are necessary for this project, in particular NumPy, Matplotlib, Pandas, Jupyter and TensorFlow (and a few others). For this, you can either use Python's integrated packaging system, pip, or you may prefer to use your system's own packaging system (if available, e.g. on Linux, or on MacOSX when using MacPorts or Homebrew). The advantage of using pip is that it is easy to create multiple isolated Python environments with different libraries and different library versions (e.g. one environment for each project). The advantage of using your system's packaging system is that there is less risk of having conflicts between your Python libraries and your system's other packages. Since I have many projects with different library requirements, I prefer to use pip with isolated environments. Moreover, the pip packages are usually the most recent ones available, while Anaconda and system packages often lag behind a bit.&lt;/p&gt;
&lt;p&gt;These are the commands you need to type in a terminal if you want to use pip to install the required libraries. Note: in all the following commands, if you chose to use Python 2 rather than Python 3, you must replace &lt;code&gt;pip3&lt;/code&gt; with &lt;code&gt;pip&lt;/code&gt;, and &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First you need to make sure you have the latest version of pip installed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;--user&lt;/code&gt; option will install the latest version of pip only for the current user. If you prefer to install it system wide (i.e. for all users), you must have administrator rights (e.g. use &lt;code&gt;sudo python3&lt;/code&gt; instead of &lt;code&gt;python3&lt;/code&gt; on Linux), and you should remove the &lt;code&gt;--user&lt;/code&gt; option. The same is true of the command below that uses the &lt;code&gt;--user&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Next, you can optionally create an isolated environment. This is recommended as it makes it possible to have a different environment for each project (e.g. one for this project), with potentially very different libraries, and different versions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade virtualenv
$ python3 -m virtualenv -p `which python3` env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a new directory called &lt;code&gt;env&lt;/code&gt; in the current directory, containing an isolated Python environment based on Python 3. If you installed multiple versions of Python 3 on your system, you can replace &lt;code&gt;`which python3`&lt;/code&gt; with the path to the Python executable you prefer to use.&lt;/p&gt;
&lt;p&gt;Now you must activate this environment. You will need to run this command every time you want to use this environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ source ./env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Windows, the command is slightly different:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ .\env\Scripts\activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, use pip to install the required python packages. If you are not using virtualenv, you should add the &lt;code&gt;--user&lt;/code&gt; option (alternatively you could install the libraries system-wide, but this will probably require administrator rights, e.g. using &lt;code&gt;sudo pip3&lt;/code&gt; instead of &lt;code&gt;pip3&lt;/code&gt; on Linux).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! You're all set, you just need to start Jupyter now.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-jupyter" class="anchor" aria-hidden="true" href="#starting-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting Jupyter&lt;/h2&gt;
&lt;p&gt;Okay! You can now start Jupyter, simply type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should open up your browser, and you should see Jupyter's tree view, with the contents of the current directory. If your browser does not open automatically, visit &lt;a href="http://127.0.0.1:8888/tree" rel="nofollow"&gt;127.0.0.1:8888&lt;/a&gt;. Click on &lt;code&gt;index.ipynb&lt;/code&gt; to get started!&lt;/p&gt;
&lt;p&gt;Congrats! You are ready to learn Machine Learning, hands on!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h1&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml</guid><pubDate>Fri, 31 Jan 2020 00:10:00 GMT</pubDate></item><item><title>jeffheaton/t81_558_deep_learning #11 in Jupyter Notebook, This week</title><link>https://github.com/jeffheaton/t81_558_deep_learning</link><description>&lt;p&gt;&lt;i&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-t81-558applications-of-deep-neural-networks" class="anchor" aria-hidden="true" href="#t81-558applications-of-deep-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.wustl.edu" rel="nofollow"&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instructor: &lt;a href="https://sites.wustl.edu/jeffheaton/" rel="nofollow"&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href="https://github.com/jeffheaton"&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Section 2. Spring 2020, Monday, 2:30 PM - 5:20 PM Online &amp;amp; Cupples I / 215&lt;/li&gt;
&lt;li&gt;Section 1. Spring 2020, Monday, 6:00 PM - 9:00 PM Online &amp;amp; Cupples I / 215&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-course-description" class="anchor" aria-hidden="true" href="#course-description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course Description&lt;/h1&gt;
&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-objectives" class="anchor" aria-hidden="true" href="#objectives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Objectives&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt;
&lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt;
&lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus&lt;/h1&gt;
&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments.  &lt;a href="https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/pdf/t81_558_spring2020_syllabus.pdf" rel="nofollow"&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Module&lt;/th&gt;
&lt;th&gt;Content&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_01_1_overview.ipynb"&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 01/13/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (first meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_02_1_python_pandas.ipynb"&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 01/27/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;	Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class1.ipynb"&gt;Module 1 Assignment&lt;/a&gt; Due: 01/28/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_03_1_neural_net.ipynb"&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 02/03/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class2.ipynb"&gt;Module 2: Assignment&lt;/a&gt; due: 02/04/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_04_1_feature_encode.ipynb"&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 02/10/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class3.ipynb"&gt;Module 3 Assignment&lt;/a&gt; due: 02/11/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_05_1_reg_ridge_lasso.ipynb"&gt;Module 5&lt;/a&gt;&lt;br&gt;Week of 02/17/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class4.ipynb"&gt;Module 4 Assignment&lt;/a&gt; due: 02/18/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_06_1_python_images.ipynb"&gt;Module 6&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 02/24/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&lt;ul&gt;	Part 6.1: Image Processing in Python&lt;li&gt;Part 6.2: Keras Neural Networks for MINST and Fashion MINST&lt;/li&gt;&lt;li&gt;Part 6.3: Implementing a ResNet in Keras&lt;/li&gt;&lt;li&gt;Part 6.4: Computer Vision with OpenCV&lt;/li&gt;&lt;li&gt;Part 6.5: Recognizing Multiple Images with Darknet&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class5.ipynb"&gt;Module 5 Assignment&lt;/a&gt; due: 02/25/2020&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (2nd Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_07_1_gan_intro.ipynb"&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 03/02/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 7: GAN&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&lt;li&gt;Part 7.2: Implementing a GAN in Keras&lt;/li&gt;&lt;li&gt;Part 7.3: Face Generation with StyleGAN and Python&lt;/li&gt;&lt;li&gt;Part 7.4: GANS for Semi-Supervised Learning in Keras&lt;/li&gt;&lt;li&gt;Part 7.5: An Overview of GAN Research&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class6.ipynb"&gt;Module 6 Assignment&lt;/a&gt; due: 03/03/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_08_1_kaggle_intro.ipynb"&gt;Module 8&lt;/a&gt;&lt;br&gt;Week of 03/16/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&lt;li&gt;Part 8.5: Current Semester's Kaggle&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class7.ipynb"&gt;Module 7 Assignment&lt;/a&gt; due: 03/17/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_09_1_keras_transfer.ipynb"&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 03/23/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&lt;li&gt;Part 9.2: Popular Pretrained Neural Networks for Keras. &lt;/li&gt;&lt;li&gt;Part 9.3: Transfer Learning for Computer Vision and Keras&lt;/li&gt;&lt;li&gt;Part 9.4: Transfer Learning for Languages and Keras&lt;/li&gt;&lt;li&gt;Part 9.5: Transfer Learning for Keras Feature Engineering&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class8.ipynb"&gt;Module 8 Assignment&lt;/a&gt; due: 03/24/2020&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (3rd Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_10_1_timeseries.ipynb"&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 03/30/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, TensorFlow and Keras&lt;/li&gt;&lt;li&gt;Part 10.2: Programming LSTM with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.3: Image Captioning with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.4: Temporal CNN in Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.5: Predicting the Stock Market with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class9.ipynb"&gt;Module 9 Assignment&lt;/a&gt; due: 03/31/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_11_01_spacy.ipynb"&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 04/06/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 11.1: Getting Started with Spacy in Python&lt;/li&gt;&lt;li&gt;Part 11.2: Word2Vec and Text Classification&lt;/li&gt;&lt;li&gt;Part 11.3: Natural Language Processing with Spacy and Keras&lt;/li&gt;&lt;li&gt;Part 11.4: What are Embedding Layers in Keras&lt;/li&gt;&lt;li&gt;Part 11.5: Learning English from Scratch with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class10.ipynb"&gt;Module 10 Assignment&lt;/a&gt; due: 04/07/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_12_01_ai_gym.ipynb"&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 04/13/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Kaggle Assignment due: 04/13/2020 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&lt;li&gt;Part 12.5: How Alpha Zero used Reinforcement Learning to Master Chess&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_13_01_flask.ipynb"&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 04/20/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 13.1: Deploying a Model to AWS&lt;/li&gt;&lt;li&gt;Part 13.2: Flask and Deep Learning Web Services&lt;/li&gt;&lt;li&gt;Part 13.3: AI at the Edge: Using Keras on a Mobile Device&lt;/li&gt;&lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&lt;li&gt;Part 13.5: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (4th Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_14_01_automl.ipynb"&gt;Module 14&lt;/a&gt;&lt;br&gt;Week of 04/27/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 14: Other Neural Network Techniques&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 14.1: What is AutoML&lt;/li&gt;&lt;li&gt;Part 14.2: Using Denoising AutoEncoders in Keras&lt;/li&gt;&lt;li&gt;Part 14.3: Training an Intrusion Detection System with KDD99&lt;/li&gt;&lt;li&gt;Part 14.4: Anomaly Detection in Keras&lt;/li&gt;&lt;li&gt;Part 14.5: New Technology in Deep Learning&lt;/li&gt;&lt;li&gt;Final Project due 05/04/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://data.heatonresearch.com/data/t81-558/index.html" rel="nofollow"&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jeffheaton</author><guid isPermaLink="false">https://github.com/jeffheaton/t81_558_deep_learning</guid><pubDate>Fri, 31 Jan 2020 00:11:00 GMT</pubDate></item><item><title>fastai/fastai #12 in Jupyter Notebook, This week</title><link>https://github.com/fastai/fastai</link><description>&lt;p&gt;&lt;i&gt;The fastai deep learning library, plus lessons and tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/fastdotai/fastai/_build/latest?definitionId=1" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a1b1234cce0c844f75224d1df07b4f236f78aee7/68747470733a2f2f6465762e617a7572652e636f6d2f66617374646f7461692f6661737461692f5f617069732f6275696c642f7374617475732f6661737461692e666173746169" alt="Build Status" data-canonical-src="https://dev.azure.com/fastdotai/fastai/_apis/build/status/fastai.fastai" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/26f7b20369ea7a096cfb30bdf0d14bc6ceda0275/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6661737461692e737667" alt="pypi fastai version" data-canonical-src="https://img.shields.io/pypi/v/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c20a6e61cb1c612b644253db1e3c1f7972d7f0e/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f762f6661737461692f6661737461692e737667" alt="Conda fastai version" data-canonical-src="https://img.shields.io/conda/v/fastai/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/39f258e6563b60eef6a88589cd194d3a85033747/68747470733a2f2f616e61636f6e64612e6f72672f6661737461692f6661737461692f6261646765732f706c6174666f726d732e737667" alt="Anaconda-Server Badge" data-canonical-src="https://anaconda.org/fastai/fastai/badges/platforms.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6dc643192dbbbd8edda826d1be289ba06ef2b57f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6661737461692e737667" alt="fastai python compatibility" data-canonical-src="https://img.shields.io/pypi/pyversions/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f600abfa75b593d49643c1710ed42865373f9d75/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6661737461692e737667" alt="fastai license" data-canonical-src="https://img.shields.io/pypi/l/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-fastai" class="anchor" aria-hidden="true" href="#fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;fastai&lt;/h1&gt;
&lt;p&gt;The fastai library simplifies training fast and accurate neural nets using modern best practices. See the &lt;a href="https://docs.fast.ai" rel="nofollow"&gt;fastai website&lt;/a&gt; to get started. The library is based on research into deep learning best practices undertaken at &lt;a href="http://www.fast.ai" rel="nofollow"&gt;fast.ai&lt;/a&gt;, and includes "out of the box" support for &lt;a href="https://docs.fast.ai/vision.html#vision" rel="nofollow"&gt;&lt;code&gt;vision&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/text.html#text" rel="nofollow"&gt;&lt;code&gt;text&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/tabular.html#tabular" rel="nofollow"&gt;&lt;code&gt;tabular&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://docs.fast.ai/collab.html#collab" rel="nofollow"&gt;&lt;code&gt;collab&lt;/code&gt;&lt;/a&gt; (collaborative filtering) models. For brief examples, see the &lt;a href="https://github.com/fastai/fastai/tree/master/examples"&gt;examples&lt;/a&gt; folder; detailed examples are provided in the full &lt;a href="https://docs.fast.ai/" rel="nofollow"&gt;documentation&lt;/a&gt;. For instance, here's how to train an MNIST model using &lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;resnet18&lt;/a&gt; (from the &lt;a href="https://github.com/fastai/fastai/blob/master/examples/vision.ipynb"&gt;vision example&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; fastai.vision &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;
path &lt;span class="pl-k"&gt;=&lt;/span&gt; untar_data(&lt;span class="pl-c1"&gt;MNIST_PATH&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; image_data_from_folder(path)
learn &lt;span class="pl-k"&gt;=&lt;/span&gt; cnn_learner(data, models.resnet18, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;accuracy)
learn.fit(&lt;span class="pl-c1"&gt;1&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-note-for-coursefastai-students" class="anchor" aria-hidden="true" href="#note-for-coursefastai-students"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note for &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; students&lt;/h2&gt;
&lt;p&gt;This document is written for &lt;code&gt;fastai v1&lt;/code&gt;, which we use for the current version the &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; deep learning courses. If you're following along with a course at &lt;a href="http://course18.fast.ai" rel="nofollow"&gt;course18.fast.ai&lt;/a&gt; (i.e. the machine learning course, which isn't updated for v1) you need to use &lt;code&gt;fastai 0.7&lt;/code&gt;;  please follow the installation instructions &lt;a href="https://forums.fast.ai/t/fastai-v0-install-issues-thread/24652" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; &lt;em&gt;fastai v1 currently supports Linux only, and requires &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; or later. Windows support is at an experimental stage: it should work fine but it's much slower and less well tested. Since Macs don't currently have good Nvidia GPU support, we do not currently prioritize Mac development.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fastai-1.x&lt;/code&gt; can be installed with either &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt; package managers and also from source. At the moment you can't just run &lt;em&gt;install&lt;/em&gt;, since you first need to get the correct &lt;code&gt;pytorch&lt;/code&gt; version installed - thus to get &lt;code&gt;fastai-1.x&lt;/code&gt; installed choose one of the installation recipes below using your favorite python package manager. Note that &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; are the minimal version requirements.&lt;/p&gt;
&lt;p&gt;It's highly recommended you install &lt;code&gt;fastai&lt;/code&gt; and its dependencies in a virtual environment (&lt;a href="https://conda.io/docs/user-guide/tasks/manage-environments.html" rel="nofollow"&gt;&lt;code&gt;conda&lt;/code&gt;&lt;/a&gt; or others), so that you don't interfere with system-wide python packages. It's not that you must, but if you experience problems with any dependency packages, please consider using a fresh virtual environment just for &lt;code&gt;fastai&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Starting with pytorch-1.x you no longer need to install a special pytorch-cpu version. Instead use the normal pytorch and it works with and without GPU. But &lt;a href="https://docs.fast.ai/install.html#cpu-build" rel="nofollow"&gt;you can install the cpu build too&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you experience installation problems, please read about &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#installation-issues"&gt;installation issues&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are planning on using &lt;code&gt;fastai&lt;/code&gt; in the jupyter notebook environment, make sure to also install the corresponding &lt;a href="https://docs.fast.ai/install.html#jupyter-notebook-dependencies" rel="nofollow"&gt;packages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More advanced installation issues, such as installing only partial dependencies are covered in a dedicated &lt;a href="https://docs.fast.ai/install.html" rel="nofollow"&gt;installation doc&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-conda-install" class="anchor" aria-hidden="true" href="#conda-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch -c fastai fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will install the &lt;code&gt;pytorch&lt;/code&gt; build with the latest &lt;code&gt;cudatoolkit&lt;/code&gt; version. If you need a higher or lower &lt;code&gt;CUDA XX&lt;/code&gt; build (e.g. CUDA 9.0), following the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install the desired &lt;code&gt;pytorch&lt;/code&gt; build.&lt;/p&gt;
&lt;p&gt;Note that JPEG decoding can be a bottleneck, particularly if you have a fast GPU. You can optionally install an optimized JPEG decoder as follows (Linux):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda uninstall --force jpeg libtiff -y
conda install -c conda-forge libjpeg-turbo pillow==6.0.0
CC=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cc -mavx2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; pip install --no-cache-dir -U --force-reinstall --no-binary :all: --compile pillow-simd&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you only care about faster JPEG decompression, it can be &lt;code&gt;pillow&lt;/code&gt; or &lt;code&gt;pillow-simd&lt;/code&gt; in the last command above, the latter speeds up other image processing operations. For the full story see &lt;a href="https://docs.fast.ai/performance.html#faster-image-processing" rel="nofollow"&gt;Pillow-SIMD&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pypi-install" class="anchor" aria-hidden="true" href="#pypi-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyPI Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By default pip will install the latest &lt;code&gt;pytorch&lt;/code&gt; with the latest &lt;code&gt;cudatoolkit&lt;/code&gt;. If your hardware doesn't support the latest &lt;code&gt;cudatoolkit&lt;/code&gt;, follow the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install a &lt;code&gt;pytorch&lt;/code&gt; build that fits your hardware.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-bug-fix-install" class="anchor" aria-hidden="true" href="#bug-fix-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bug Fix Install&lt;/h3&gt;
&lt;p&gt;If a bug fix was made in git and you can't wait till a new release is made, you can install the bleeding edge version of &lt;code&gt;fastai&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/fastai/fastai.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-developer-install" class="anchor" aria-hidden="true" href="#developer-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developer Install&lt;/h3&gt;
&lt;p&gt;The following instructions will result in a &lt;a href="https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs" rel="nofollow"&gt;pip editable install&lt;/a&gt;, so that you can &lt;code&gt;git pull&lt;/code&gt; at any time and your environment will automatically get the updates:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/fastai/fastai
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; fastai
tools/run-after-git-clone
pip install -e &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.[dev]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, you can test that the build works by starting the jupyter notebook:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and executing an example notebook. For example load &lt;code&gt;examples/tabular.ipynb&lt;/code&gt; and run it.&lt;/p&gt;
&lt;p&gt;Please refer to &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; and &lt;a href="https://docs.fast.ai/dev/develop.html" rel="nofollow"&gt;Notes For Developers&lt;/a&gt; for more details on how to contribute to the &lt;code&gt;fastai&lt;/code&gt; project.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-building-from-source" class="anchor" aria-hidden="true" href="#building-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building From Source&lt;/h3&gt;
&lt;p&gt;If for any reason you can't use the prepackaged packages and have to build from source, this section is for you.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To build &lt;code&gt;pytorch&lt;/code&gt; from source follow the &lt;a href="https://github.com/pytorch/pytorch#from-source"&gt;complete instructions&lt;/a&gt;. Remember to first install CUDA, CuDNN, and other required libraries as suggested - everything will be very slow without those libraries built into &lt;code&gt;pytorch&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you will also need to build &lt;code&gt;torchvision&lt;/code&gt; from source:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/pytorch/vision
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; vision
python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When both &lt;code&gt;pytorch&lt;/code&gt; and &lt;code&gt;torchvision&lt;/code&gt; are installed, first test that you can load each of these libraries:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;import torch
import torchvision&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to validate that they were installed correctly&lt;/p&gt;
&lt;p&gt;Finally, proceed with &lt;code&gt;fastai&lt;/code&gt; installation as normal, either through prepackaged pip or conda builds or installing from source ("the developer install") as explained in the sections above.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-installation-issues" class="anchor" aria-hidden="true" href="#installation-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Issues&lt;/h2&gt;
&lt;p&gt;If the installation process fails, first make sure &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#is-my-system-supported"&gt;your system is supported&lt;/a&gt;. And if the problem is still not addressed, please refer to the &lt;a href="https://docs.fast.ai/troubleshoot.html" rel="nofollow"&gt;troubleshooting document&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you encounter installation problems with conda, make sure you have the latest &lt;code&gt;conda&lt;/code&gt; client (&lt;code&gt;conda install&lt;/code&gt; will do an update too):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install conda&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-is-my-system-supported" class="anchor" aria-hidden="true" href="#is-my-system-supported"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is My System Supported?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Python: You need to have python 3.6 or higher&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CPU or GPU&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;pytorch&lt;/code&gt; binary package comes with its own CUDA, CuDNN, NCCL, MKL, and other libraries so you don't have to install system-wide NVIDIA's CUDA and related libraries if you don't need them for something else. If you have them installed already it doesn't matter which NVIDIA's CUDA version library you have installed system-wide. Your system could have CUDA 9.0 libraries, and you can still use &lt;code&gt;pytorch&lt;/code&gt; build with CUDA 10.0 libraries without any problem, since the &lt;code&gt;pytorch&lt;/code&gt; binary package is self-contained.&lt;/p&gt;
&lt;p&gt;The only requirement is that you have installed and configured the NVIDIA driver correctly. Usually you can test that by running &lt;code&gt;nvidia-smi&lt;/code&gt;. While it's possible that this application is not available on your system, it's very likely that if it doesn't work, then you don't have your NVIDIA drivers configured properly. And remember that a reboot is always required after installing NVIDIA drivers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Operating System:&lt;/p&gt;
&lt;p&gt;Since fastai-1.0 relies on pytorch-1.0, you need to be able to install pytorch-1.0 first.&lt;/p&gt;
&lt;p&gt;As of this moment pytorch.org's 1.0 version supports:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Platform&lt;/th&gt;
&lt;th&gt;GPU&lt;/th&gt;
&lt;th&gt;CPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;linux&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mac&lt;/td&gt;
&lt;td&gt;source&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;windows&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Legend: &lt;code&gt;binary&lt;/code&gt; = can be installed directly, &lt;code&gt;source&lt;/code&gt; = needs to be built from source.&lt;/p&gt;
&lt;p&gt;If there is no &lt;code&gt;pytorch&lt;/code&gt; preview conda or pip package available for your system, you may still be able to &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;build it from source&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How do you know which pytorch cuda version build to choose?&lt;/p&gt;
&lt;p&gt;It depends on the version of the installed NVIDIA driver. Here are the requirements for CUDA versions supported by pre-built &lt;code&gt;pytorch&lt;/code&gt; releases:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CUDA Toolkit&lt;/th&gt;
&lt;th&gt;NVIDIA (Linux x86_64)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 10.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 410.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 9.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 384.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 8.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 367.48&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So if your NVIDIA driver is less than 384, then you can only use CUDA 8.0. Of course, you can upgrade your drivers to more recent ones if your card supports it.&lt;/p&gt;
&lt;p&gt;You can find a complete table with all variations &lt;a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you use NVIDIA driver 410+, you most likely want to install the &lt;code&gt;cudatoolkit=10.0&lt;/code&gt; pytorch variant, via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=10.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or if you need a lower version, use one of:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=8.0
conda install -c pytorch pytorch cudatoolkit=9.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For other options refer to the complete list of &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;the available pytorch variants&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;In order to update your environment, simply install &lt;code&gt;fastai&lt;/code&gt; in exactly the same way you did the initial installation.&lt;/p&gt;
&lt;p&gt;Top level files &lt;code&gt;environment.yml&lt;/code&gt; and &lt;code&gt;environment-cpu.yml&lt;/code&gt; belong to the old fastai (0.7). &lt;code&gt;conda env update&lt;/code&gt; is no longer the way to update your &lt;code&gt;fastai-1.x&lt;/code&gt; environment. These files remain because the fastai course-v2 video instructions rely on this setup. Eventually, once fastai course-v3 p1 and p2 will be completed, they will probably be moved to where they belong - under &lt;code&gt;old/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to &lt;code&gt;fastai&lt;/code&gt;, be sure to review the &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;. This project adheres to fastai's &lt;a href="https://github.com/fastai/fastai/blob/master/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/p&gt;
&lt;p&gt;We use GitHub issues for tracking requests and bugs, so please see &lt;a href="https://forums.fast.ai/" rel="nofollow"&gt;fastai forum&lt;/a&gt; for general questions and discussion.&lt;/p&gt;
&lt;p&gt;The fastai project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;A detailed history of changes can be found &lt;a href="https://github.com/fastai/fastai/blob/master/CHANGES.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h2&gt;
&lt;p&gt;Copyright 2017 onwards, fast.ai, Inc. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project's files except in compliance with the License. A copy of the License is provided in the LICENSE file in this repository.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/fastai</guid><pubDate>Fri, 31 Jan 2020 00:12:00 GMT</pubDate></item><item><title>jakevdp/PythonDataScienceHandbook #13 in Jupyter Notebook, This week</title><link>https://github.com/jakevdp/PythonDataScienceHandbook</link><description>&lt;p&gt;&lt;i&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-data-science-handbook" class="anchor" aria-hidden="true" href="#python-data-science-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Data Science Handbook&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository contains the entire &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="notebooks/figures/PDSH-cover.png"&gt;&lt;img src="notebooks/figures/PDSH-cover.png" alt="cover image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-this-book" class="anchor" aria-hidden="true" href="#how-to-use-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use this Book&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Read the book in its entirety online at &lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/" rel="nofollow"&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the code using the Jupyter notebooks available in this repository's &lt;a href="notebooks"&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch executable versions of these notebooks using &lt;a href="http://colab.research.google.com" rel="nofollow"&gt;Google Colab&lt;/a&gt;: &lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href="https://beta.mybinder.org/" rel="nofollow"&gt;binder&lt;/a&gt;: &lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Buy the printed book through &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;O'Reilly Media&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt;
&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href="http://ipython.org" rel="nofollow"&gt;IPython&lt;/a&gt;, &lt;a href="http://numpy.org" rel="nofollow"&gt;NumPy&lt;/a&gt;, &lt;a href="http://pandas.pydata.org" rel="nofollow"&gt;Pandas&lt;/a&gt;, &lt;a href="http://matplotlib.org" rel="nofollow"&gt;Matplotlib&lt;/a&gt;, &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;Scikit-Learn&lt;/a&gt;, and related packages.
Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project,
&lt;a href="https://github.com/jakevdp/WhirlwindTourOfPython"&gt;A Whirlwind Tour of Python&lt;/a&gt;: it's a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt;
&lt;p&gt;See &lt;a href="http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-software" class="anchor" aria-hidden="true" href="#software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Software&lt;/h2&gt;
&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt;
&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href="requirements.txt"&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use).
To install the requirements using &lt;a href="http://conda.pydata.org" rel="nofollow"&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can read more about using conda environments in the &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h3&gt;
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href="LICENSE-CODE"&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-text" class="anchor" aria-hidden="true" href="#text"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text&lt;/h3&gt;
&lt;p&gt;The text content of the book is released under the &lt;a href="LICENSE-TEXT"&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode" rel="nofollow"&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jakevdp</author><guid isPermaLink="false">https://github.com/jakevdp/PythonDataScienceHandbook</guid><pubDate>Fri, 31 Jan 2020 00:13:00 GMT</pubDate></item><item><title>tensorflow/probability #14 in Jupyter Notebook, This week</title><link>https://github.com/tensorflow/probability</link><description>&lt;p&gt;&lt;i&gt;Probabilistic reasoning and statistical analysis in TensorFlow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-probability" class="anchor" aria-hidden="true" href="#tensorflow-probability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Probability&lt;/h1&gt;
&lt;p&gt;TensorFlow Probability is a library for probabilistic reasoning and statistical
analysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow
Probability provides integration of probabilistic methods with deep networks,
gradient-based inference via automatic differentiation, and scalability to
large datasets and models via hardware acceleration (e.g., GPUs) and distributed
computation.&lt;/p&gt;
&lt;p&gt;Our probabilistic machine learning tools are structured as follows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Layer 0: TensorFlow.&lt;/strong&gt; Numerical operations. In particular, the LinearOperator
class enables matrix-free implementations that can exploit special structure
(diagonal, low-rank, etc.) for efficient computation. It is built and maintained
by the TensorFlow Probability team and is now part of
&lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/linalg"&gt;&lt;code&gt;tf.linalg&lt;/code&gt;&lt;/a&gt;
in core TF.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Layer 1: Statistical Building Blocks&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distributions (&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/distributions"&gt;&lt;code&gt;tfp.distributions&lt;/code&gt;&lt;/a&gt;):
A large collection of probability
distributions and related statistics with batch and
&lt;a href="https://docs.scipy.org/doc/numpy-1.14.0/user/basics.broadcasting.html" rel="nofollow"&gt;broadcasting&lt;/a&gt;
semantics. See the
&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb"&gt;Distributions Tutorial&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Bijectors (&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/bijectors"&gt;&lt;code&gt;tfp.bijectors&lt;/code&gt;&lt;/a&gt;):
Reversible and composable transformations of random variables. Bijectors
provide a rich class of transformed distributions, from classical examples
like the
&lt;a href="https://en.wikipedia.org/wiki/Log-normal_distribution" rel="nofollow"&gt;log-normal distribution&lt;/a&gt;
to sophisticated deep learning models such as
&lt;a href="https://arxiv.org/abs/1705.07057" rel="nofollow"&gt;masked autoregressive flows&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Layer 2: Model Building&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Joint Distributions (e.g., &lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/distributions/joint_distribution_sequential.py"&gt;&lt;code&gt;tfp.distributions.JointDistributionSequential&lt;/code&gt;&lt;/a&gt;):
Joint distributions over one or more possibly-interdependent distributions.
For an introduction to modeling with TFP's &lt;code&gt;JointDistribution&lt;/code&gt;s, check out
&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Modeling_with_JointDistribution.ipynb"&gt;this colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Probabilistic Layers (&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/layers"&gt;&lt;code&gt;tfp.layers&lt;/code&gt;&lt;/a&gt;):
Neural network layers with uncertainty over the functions they represent,
extending TensorFlow Layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Layer 3: Probabilistic Inference&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Markov chain Monte Carlo (&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/mcmc"&gt;&lt;code&gt;tfp.mcmc&lt;/code&gt;&lt;/a&gt;):
Algorithms for approximating integrals via sampling. Includes
&lt;a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo" rel="nofollow"&gt;Hamiltonian Monte Carlo&lt;/a&gt;,
random-walk Metropolis-Hastings, and the ability to build custom transition
kernels.&lt;/li&gt;
&lt;li&gt;Variational Inference (&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/vi"&gt;&lt;code&gt;tfp.vi&lt;/code&gt;&lt;/a&gt;):
Algorithms for approximating integrals via optimization.&lt;/li&gt;
&lt;li&gt;Optimizers (&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/optimizer"&gt;&lt;code&gt;tfp.optimizer&lt;/code&gt;&lt;/a&gt;):
Stochastic optimization methods, extending TensorFlow Optimizers. Includes
&lt;a href="http://www.icml-2011.org/papers/398_icmlpaper.pdf" rel="nofollow"&gt;Stochastic Gradient Langevin Dynamics&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Monte Carlo (&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/monte_carlo"&gt;&lt;code&gt;tfp.monte_carlo&lt;/code&gt;&lt;/a&gt;):
Tools for computing Monte Carlo expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TensorFlow Probability is under active development. Interfaces may change at any
time.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;See &lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/"&gt;&lt;code&gt;tensorflow_probability/examples/&lt;/code&gt;&lt;/a&gt;
for end-to-end examples. It includes tutorial notebooks such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Linear_Mixed_Effects_Models.ipynb"&gt;Linear Mixed Effects Models&lt;/a&gt;.
A hierarchical linear model for sharing statistical strength across examples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Eight_Schools.ipynb"&gt;Eight Schools&lt;/a&gt;.
A hierarchical normal model for exchangeable treatment effects.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/HLM_TFP_R_Stan.ipynb"&gt;Hierarchical Linear Models&lt;/a&gt;.
Hierarchical linear models compared among TensorFlow Probability, R, and Stan.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb"&gt;Bayesian Gaussian Mixture Models&lt;/a&gt;.
Clustering with a probabilistic generative model.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_PCA.ipynb"&gt;Probabilistic Principal Components Analysis&lt;/a&gt;.
Dimensionality reduction with latent variables.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Copula.ipynb"&gt;Gaussian Copulas&lt;/a&gt;.
Probability distributions for capturing dependence across random variables.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb"&gt;TensorFlow Distributions: A Gentle Introduction&lt;/a&gt;.
Introduction to TensorFlow Distributions.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb"&gt;Understanding TensorFlow Distributions Shapes&lt;/a&gt;.
How to distinguish between samples, batches, and events for arbitrarily shaped
probabilistic computations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Probability_Case_Study_Covariance_Estimation.ipynb"&gt;TensorFlow Probability Case Study: Covariance Estimation&lt;/a&gt;.
A user's case study in applying TensorFlow Probability to estimate covariances.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It also includes example scripts such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/vae.py"&gt;Variational Autoencoders&lt;/a&gt;.
Representation learning with a latent code and variational inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/vq_vae.py"&gt;Vector-Quantized Autoencoder&lt;/a&gt;.
Discrete representation learning with vector quantization.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/disentangled_vae.py"&gt;Disentangled Sequential Variational Autoencoder&lt;/a&gt;
Disentangled representation learning over sequences with variational inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/grammar_vae.py"&gt;Grammar Variational Autoencoder&lt;/a&gt;.
Representation learning over productions in a context-free grammar.&lt;/li&gt;
&lt;li&gt;Latent Dirichlet Allocation
(&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/latent_dirichlet_allocation_distributions.py"&gt;Distributions version&lt;/a&gt;,
Mixed membership modeling for capturing topics in a document.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/deep_exponential_family.py"&gt;Deep Exponential Family&lt;/a&gt;.
A deep, sparse generative model for discovering a hierarchy of topics.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/bayesian_neural_network.py"&gt;Bayesian Neural Networks&lt;/a&gt;.
Neural networks with uncertainty over their weights.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/tree/master/tensorflow_probability/examples/logistic_regression.py"&gt;Bayesian Logistic Regression&lt;/a&gt;.
Bayesian inference for binary classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-stable-builds" class="anchor" aria-hidden="true" href="#stable-builds"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stable Builds&lt;/h3&gt;
&lt;p&gt;To install the latest version, run the following:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Notes:&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; - We recommend that users move towards using TensorFlow 2.x as soon as&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;   possible. Until the TF2 stable package is released (due in Sep. 2019),&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;   the best way to use TFP with TF2 is to use nightly TFP and TF2 packages:&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;     - Nightly TFP: [tfp-nightly](http://pypi.python.org/pypi/tfp-nightly)&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;     - Nightly TF2: [tf-nightly-2.0-preview](http://pypi.python.org/pypi/tf-nightly-2.0-preview)&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;   Once the TF2 stable release comes out, TFP will issue its 0.8.0 release,&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;   which will be tested and stable against TF 2.0.0.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; - You need the latest version of `pip` in order to get the latest version of&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;   `tf-nightly-2.0-preview`.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; - For GPU TF, use `tf-nightly-2.0-preview-gpu`.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; - The `--upgrade` flag ensures you'll get the latest version.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; - The `--user` flag ensures the packages are installed to your user directory&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;   rather than the system directory.&lt;/span&gt;
python -m pip install pip --upgrade --user
python -m pip install tf-nightly-2.0-preview tfp-nightly --upgrade --user
TFVERSION=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;python -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;import tensorflow; print(tensorflow.__version__)&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you have an older pip, you might get this older version of&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; tf-nightly-2.0-preview, so check to be sure.&lt;/span&gt;
[[ &lt;span class="pl-smi"&gt;$TFVERSION&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;2.0.0-dev20190731&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; ]] &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
  &lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&amp;amp;2&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Failed to install the most recent TF. Found: &lt;span class="pl-smi"&gt;${TFVERSION}&lt;/span&gt;.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;TensorFlow Probability depends on a recent stable release of
&lt;a href="https://www.tensorflow.org/install" rel="nofollow"&gt;TensorFlow&lt;/a&gt; (pip package &lt;code&gt;tensorflow&lt;/code&gt;). See
the &lt;a href="https://github.com/tensorflow/probability/releases"&gt;TFP release notes&lt;/a&gt; for
details about dependencies between TensorFlow and TensorFlow Probability.&lt;/p&gt;
&lt;p&gt;Note: Since TensorFlow is &lt;em&gt;not&lt;/em&gt; included as a dependency of the TensorFlow
Probability package (in &lt;code&gt;setup.py&lt;/code&gt;), you must explicitly install the TensorFlow
package (&lt;code&gt;tensorflow&lt;/code&gt; or &lt;code&gt;tensorflow-gpu&lt;/code&gt;). This allows us to maintain one
package instead of separate packages for CPU and GPU-enabled TensorFlow.&lt;/p&gt;
&lt;p&gt;To force a Python 3-specific install, replace &lt;code&gt;pip&lt;/code&gt; with &lt;code&gt;pip3&lt;/code&gt; in the above
commands. For additional installation help, guidance installing prerequisites,
and (optionally) setting up virtual environments, see the &lt;a href="https://www.tensorflow.org/install" rel="nofollow"&gt;TensorFlow
installation guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nightly-builds" class="anchor" aria-hidden="true" href="#nightly-builds"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Nightly Builds&lt;/h3&gt;
&lt;p&gt;There are also nightly builds of TensorFlow Probability under the pip package
&lt;code&gt;tfp-nightly&lt;/code&gt;, which depends on one of &lt;code&gt;tf-nightly&lt;/code&gt;, &lt;code&gt;tf-nightly-gpu&lt;/code&gt;,
&lt;code&gt;tf-nightly-2.0-preview&lt;/code&gt; or &lt;code&gt;tf-nightly-gpu-2.0-preview&lt;/code&gt;. Nightly builds include
newer features, but may be less stable than the versioned releases. Docs are
periodically refreshed &lt;a href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/g3doc/api_docs/python/tfp.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-installing-from-source" class="anchor" aria-hidden="true" href="#installing-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing from Source&lt;/h3&gt;
&lt;p&gt;You can also install from source. This requires the &lt;a href="https://bazel.build/" rel="nofollow"&gt;Bazel&lt;/a&gt; build system.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; sudo apt-get install bazel git python-pip  # Ubuntu; others, see above links.&lt;/span&gt;
git clone https://github.com/tensorflow/probability.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; probability
bazel build --copt=-O3 --copt=-march=native :pip_pkg
PKGDIR=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;mktemp -d&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;
./bazel-bin/pip_pkg &lt;span class="pl-smi"&gt;$PKGDIR&lt;/span&gt;
pip install --user --upgrade &lt;span class="pl-smi"&gt;$PKGDIR&lt;/span&gt;/&lt;span class="pl-k"&gt;*&lt;/span&gt;.whl&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-community" class="anchor" aria-hidden="true" href="#community"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community&lt;/h2&gt;
&lt;p&gt;As part of TensorFlow, we're committed to fostering an open and welcoming
environment.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/tagged/tensorflow" rel="nofollow"&gt;Stack Overflow&lt;/a&gt;: Ask
or answer technical questions.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/probability/issues"&gt;GitHub&lt;/a&gt;: Report bugs or
make feature requests.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.tensorflow.org/" rel="nofollow"&gt;TensorFlow Blog&lt;/a&gt;: Stay up to date on content
from the TensorFlow team and best articles from the community.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://youtube.com/tensorflow/" rel="nofollow"&gt;Youtube Channel&lt;/a&gt;: Follow TensorFlow shows.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!forum/tfprobability" rel="nofollow"&gt;tfprobability@tensorflow.org&lt;/a&gt;:
Open mailing list for discussion and questions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the &lt;a href="https://www.tensorflow.org/community/" rel="nofollow"&gt;TensorFlow Community&lt;/a&gt; page for
more details. Check out our latest publicity here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=BjUkL8DFH5Q" rel="nofollow"&gt;Coffee with a Googler: Probabilistic Machine Learning in TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245" rel="nofollow"&gt;Introducing TensorFlow Probability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We're eager to collaborate with you! See &lt;a href="CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;
for a guide on how to contribute. This project adheres to TensorFlow's
&lt;a href="CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you are expected to
uphold this code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;If you use TensorFlow Probability in a paper, please cite:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;TensorFlow Distributions.&lt;/em&gt; Joshua V. Dillon, Ian Langmore, Dustin Tran,
Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt
Hoffman, Rif A. Saurous.
&lt;a href="https://arxiv.org/abs/1711.10604" rel="nofollow"&gt;arXiv preprint arXiv:1711.10604, 2017&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(We're aware there's a lot more to TensorFlow Probability than Distributions, but the Distributions paper lays out our vision and is a fine thing to cite for now.)&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/probability</guid><pubDate>Fri, 31 Jan 2020 00:14:00 GMT</pubDate></item><item><title>rasbt/python-machine-learning-book-3rd-edition #15 in Jupyter Notebook, This week</title><link>https://github.com/rasbt/python-machine-learning-book-3rd-edition</link><description>&lt;p&gt;&lt;i&gt;The "Python Machine Learning (3rd edition)" book code repository&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python-machine-learning-3rd-ed-code-repository" class="anchor" aria-hidden="true" href="#python-machine-learning-3rd-ed-code-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Machine Learning (3rd Ed.) Code Repository&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.6" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/Code%20License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code repositories for the 1st and 2nd edition are available at&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/rasbt/python-machine-learning-book"&gt;https://github.com/rasbt/python-machine-learning-book&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition"&gt;https://github.com/rasbt/python-machine-learning-book-2nd-edition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Python Machine Learning, 3rd Ed.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;to be published December 12th, 2019&lt;/p&gt;
&lt;p&gt;Paperback: 770 pages&lt;br&gt;
Publisher: Packt Publishing&lt;br&gt;
Language: English&lt;/p&gt;
&lt;p&gt;ISBN-10: 1789955750&lt;br&gt;
ISBN-13: 978-1789955750&lt;br&gt;
Kindle ASIN: B07VBLX2W7&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/" rel="nofollow"&gt;&lt;img src="./.other/cover_1.jpg" width="248" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/" rel="nofollow"&gt;Amazon Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.packtpub.com/data/python-machine-learning-third-edition" rel="nofollow"&gt;Packt Page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents-and-code-notebooks" class="anchor" aria-hidden="true" href="#table-of-contents-and-code-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents and Code Notebooks&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Helpful installation and setup instructions can be found in the &lt;a href="ch01/README.md"&gt;README.md file of Chapter 1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note that these are just the code examples accompanying the book, which we uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Machine Learning - Giving Computers the Ability to Learn from Data [&lt;a href="ch01"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Training Machine Learning Algorithms for Classification [&lt;a href="ch02"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Tour of Machine Learning Classifiers Using Scikit-Learn [&lt;a href="ch03"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Building Good Training Sets – Data Pre-Processing [&lt;a href="ch04"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Compressing Data via Dimensionality Reduction [&lt;a href="ch05"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Best Practices for Model Evaluation and Hyperparameter Optimization [&lt;a href="ch06"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Combining Different Models for Ensemble Learning [&lt;a href="ch07"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Applying Machine Learning to Sentiment Analysis [&lt;a href="ch08"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Embedding a Machine Learning Model into a Web Application [&lt;a href="ch09"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting Continuous Target Variables with Regression Analysis [&lt;a href="ch10"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Working with Unlabeled Data – Clustering Analysis [&lt;a href="ch11"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Implementing a Multi-layer Artificial Neural Network from Scratch [&lt;a href="ch12"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Parallelizing Neural Network Training with TensorFlow [&lt;a href="ch13"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Going Deeper: The Mechanics of TensorFlow [&lt;a href="ch14"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Classifying Images with Deep Convolutional Neural Networks [&lt;a href="ch15"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Modeling Sequential Data Using Recurrent Neural Networks [&lt;a href="ch16"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Adversarial Networks for Synthesizing New Data [&lt;a href="ch17"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Reinforcement Learning for Decision Making in Complex Environments [&lt;a href="ch18"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;Raschka, Sebastian, and Vahid Mirjalili. &lt;em&gt;Python Machine Learning, 3rd Ed&lt;/em&gt;. Packt Publishing, 2019.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{RaschkaMirjalili2019,  
address = {Birmingham, UK},  
author = {Raschka, Sebastian and Mirjalili, Vahid},  
edition = {3},  
isbn = {978-1789955750},   
publisher = {Packt Publishing},  
title = {{Python Machine Learning, 3rd Ed.}},  
year = {2019}  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/python-machine-learning-book-3rd-edition</guid><pubDate>Fri, 31 Jan 2020 00:15:00 GMT</pubDate></item><item><title>Azure/Azure-Sentinel #16 in Jupyter Notebook, This week</title><link>https://github.com/Azure/Azure-Sentinel</link><description>&lt;p&gt;&lt;i&gt;Cloud-native SIEM for intelligent security analytics for your entire enterprise.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-azure-sentinel" class="anchor" aria-hidden="true" href="#azure-sentinel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Azure Sentinel&lt;/h1&gt;
&lt;p&gt;Welcome to the Azure Sentinel repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks and playbooks to help you get ramped up with Azure Sentinel and provide you security content to secure your environment and hunt for threats. You can also submit to &lt;a href="https://github.com/Azure/Azure-Sentinel/issues"&gt;issues&lt;/a&gt; for any samples or resources you would like to see here as you onboard to Azure Sentinel. For questions and feedback, please contact &lt;a href="AzureSentinel@microsoft.com"&gt;AzureSentinel@microsoft.com&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://help.github.com/en#dotcom"&gt;Getting started with GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://go.microsoft.com/fwlink/?linkid=2073774&amp;amp;clcid=0x409" rel="nofollow"&gt;Azure Sentinel documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aka.ms/azuresentinelcommunity" rel="nofollow"&gt;Azure Sentinel Techcommunity&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aka.ms/azuresentineluservoice" rel="nofollow"&gt;Azure Sentinel UserVoice&lt;/a&gt;
Use this to request features to our product.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aka.ms/securitywebinars" rel="nofollow"&gt;Security Community Webinars&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit &lt;a href="https://cla.microsoft.com" rel="nofollow"&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt;
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;.
For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow"&gt;Code of Conduct FAQ&lt;/a&gt; or
contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;
&lt;p&gt;For information on how to contribute, refer to the &lt;a href="https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries"&gt;"how to contribute"&lt;/a&gt; guide on the project's &lt;a href="https://github.com/Azure/Azure-Sentinel/wiki"&gt;wiki&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Azure</author><guid isPermaLink="false">https://github.com/Azure/Azure-Sentinel</guid><pubDate>Fri, 31 Jan 2020 00:16:00 GMT</pubDate></item><item><title>codebasics/py #17 in Jupyter Notebook, This week</title><link>https://github.com/codebasics/py</link><description>&lt;p&gt;&lt;i&gt;Repository to store sample python programs for python learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-py" class="anchor" aria-hidden="true" href="#py"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;py&lt;/h1&gt;
&lt;p&gt;Repository to store sample python programs for python learning
Youtube channel &lt;a href="https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ" rel="nofollow"&gt;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>codebasics</author><guid isPermaLink="false">https://github.com/codebasics/py</guid><pubDate>Fri, 31 Jan 2020 00:17:00 GMT</pubDate></item><item><title>aimacode/aima-python #18 in Jupyter Notebook, This week</title><link>https://github.com/aimacode/aima-python</link><description>&lt;p&gt;&lt;i&gt;Python implementation of algorithms from Russell And Norvig's "Artificial Intelligence - A Modern Approach"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
  &lt;a href="http://aima.cs.berkeley.edu/" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/aimacode/aima-python/master/images/aima_logo.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;
&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-aima-python--" class="anchor" aria-hidden="true" href="#aima-python--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;aima-python&lt;/code&gt; &lt;a href="https://travis-ci.org/aimacode/aima-python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/01177a7ca5a96309e258148841f9c5c82ff80993/68747470733a2f2f7472617669732d63692e6f72672f61696d61636f64652f61696d612d707974686f6e2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/aimacode/aima-python.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="http://mybinder.org/repo/aimacode/aima-python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Python code for the book &lt;em&gt;&lt;a href="http://aima.cs.berkeley.edu" rel="nofollow"&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt;.&lt;/em&gt; You can use this in conjunction with a course on AI, or for study on your own. We're looking for &lt;a href="https://github.com/aimacode/aima-python/blob/master/CONTRIBUTING.md"&gt;solid contributors&lt;/a&gt; to help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-structure-of-the-project" class="anchor" aria-hidden="true" href="#structure-of-the-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Structure of the Project&lt;/h2&gt;
&lt;p&gt;When complete, this project will have Python implementations for all the pseudocode algorithms in the book, as well as tests and examples of use. For each major topic, such as &lt;code&gt;nlp&lt;/code&gt; (natural language processing), we provide the following  files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nlp.py&lt;/code&gt;: Implementations of all the pseudocode algorithms, and necessary support functions/classes/data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tests/test_nlp.py&lt;/code&gt;: A lightweight test suite, using &lt;code&gt;assert&lt;/code&gt; statements, designed for use with &lt;a href="http://pytest.org/latest/" rel="nofollow"&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/a&gt;, but also usable on their own.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp.ipynb&lt;/code&gt;: A Jupyter (IPython) notebook that explains and gives examples of how to use the code.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp_apps.ipynb&lt;/code&gt;: A Jupyter notebook that gives example applications of the code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-python-34-and-up" class="anchor" aria-hidden="true" href="#python-34-and-up"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 3.4 and up&lt;/h2&gt;
&lt;p&gt;This code requires Python 3.4 or later, and does not run in Python 2. You can &lt;a href="https://www.python.org/downloads" rel="nofollow"&gt;install Python&lt;/a&gt; or use a browser-based Python interpreter such as &lt;a href="https://repl.it/languages/python3" rel="nofollow"&gt;repl.it&lt;/a&gt;.
You can run the code in an IDE, or from the command line with &lt;code&gt;python -i filename.py&lt;/code&gt; where the &lt;code&gt;-i&lt;/code&gt; option puts you in an interactive loop where you can run Python functions. All notebooks are available in a &lt;a href="http://mybinder.org/repo/aimacode/aima-python" rel="nofollow"&gt;binder environment&lt;/a&gt;. Alternatively, visit &lt;a href="http://jupyter.org/" rel="nofollow"&gt;jupyter.org&lt;/a&gt; for instructions on setting up your own Jupyter notebook environment.&lt;/p&gt;
&lt;p&gt;There is a sibling &lt;a href="https://github.com/rajatjain1997/aima-docker"&gt;aima-docker&lt;/a&gt; project that shows you how to use docker containers to run more complex problems in more complex software environments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation-guide" class="anchor" aria-hidden="true" href="#installation-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Guide&lt;/h2&gt;
&lt;p&gt;To download the repository:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git clone https://github.com/aimacode/aima-python.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then you need to install the basic dependencies to run the project on your system:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd aima-python
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to fetch the datasets from the &lt;a href="https://github.com/aimacode/aima-data"&gt;&lt;code&gt;aima-data&lt;/code&gt;&lt;/a&gt; repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git submodule init
git submodule update
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait for the datasets to download, it may take a while. Once they are downloaded, you need to install &lt;code&gt;pytest&lt;/code&gt;, so that you can run the test suite:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install pytest&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then to run the tests:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And you are good to go!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-index-of-algorithms" class="anchor" aria-hidden="true" href="#index-of-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index of Algorithms&lt;/h1&gt;
&lt;p&gt;Here is a table of algorithms, the figure, name of the algorithm in the book and in the repository, and the file where they are implemented in the repository. This chart was made for the third edition of the book and is being updated for the upcoming fourth edition. Empty implementations are a good place for contributors to look for an issue. The &lt;a href="https://github.com/aimacode/aima-pseudocode"&gt;aima-pseudocode&lt;/a&gt; project describes all the algorithms from the book. An asterisk next to the file name denotes the algorithm is not fully implemented. Another great place for contributors to start is by adding tests and writing on the notebooks. You can see which algorithms have tests and notebook sections below. If the algorithm you want to work on is covered, don't worry! You can still add more tests and provide some examples of use in the notebook!&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Name (in 3&lt;sup&gt;rd&lt;/sup&gt; edition)&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Tests&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Notebook&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;td align="left"&gt;Random-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;RandomVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;td align="left"&gt;Model-Based-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ModelBasedVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.1&lt;/td&gt;
&lt;td align="left"&gt;Environment&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Environment&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.1&lt;/td&gt;
&lt;td align="left"&gt;Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Agent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.3&lt;/td&gt;
&lt;td align="left"&gt;Table-Driven-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;TableDrivenVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.7&lt;/td&gt;
&lt;td align="left"&gt;Table-Driven-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;TableDrivenAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.8&lt;/td&gt;
&lt;td align="left"&gt;Reflex-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ReflexVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.10&lt;/td&gt;
&lt;td align="left"&gt;Simple-Reflex-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;SimpleReflexAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.12&lt;/td&gt;
&lt;td align="left"&gt;Model-Based-Reflex-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ReflexAgentWithState&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;Problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Problem&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;Node&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Node&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;Queue&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Queue&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/utils.py"&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;No Need&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.1&lt;/td&gt;
&lt;td align="left"&gt;Simple-Problem-Solving-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;SimpleProblemSolvingAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.2&lt;/td&gt;
&lt;td align="left"&gt;Romania&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;romania&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.7&lt;/td&gt;
&lt;td align="left"&gt;Tree-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;depth/breadth_first_tree_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.7&lt;/td&gt;
&lt;td align="left"&gt;Graph-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;depth/breadth_first_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.11&lt;/td&gt;
&lt;td align="left"&gt;Breadth-First-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;breadth_first_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.14&lt;/td&gt;
&lt;td align="left"&gt;Uniform-Cost-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;uniform_cost_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.17&lt;/td&gt;
&lt;td align="left"&gt;Depth-Limited-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;depth_limited_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.18&lt;/td&gt;
&lt;td align="left"&gt;Iterative-Deepening-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;iterative_deepening_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.22&lt;/td&gt;
&lt;td align="left"&gt;Best-First-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;best_first_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.24&lt;/td&gt;
&lt;td align="left"&gt;A*-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;astar_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.26&lt;/td&gt;
&lt;td align="left"&gt;Recursive-Best-First-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;recursive_best_first_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.2&lt;/td&gt;
&lt;td align="left"&gt;Hill-Climbing&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;hill_climbing&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.5&lt;/td&gt;
&lt;td align="left"&gt;Simulated-Annealing&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;simulated_annealing&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.8&lt;/td&gt;
&lt;td align="left"&gt;Genetic-Algorithm&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;genetic_algorithm&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.11&lt;/td&gt;
&lt;td align="left"&gt;And-Or-Graph-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;and_or_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.21&lt;/td&gt;
&lt;td align="left"&gt;Online-DFS-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;online_dfs_agent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.24&lt;/td&gt;
&lt;td align="left"&gt;LRTA*-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;LRTAStarAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;5.3&lt;/td&gt;
&lt;td align="left"&gt;Minimax-Decision&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;minimax_decision&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/games.py"&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;5.7&lt;/td&gt;
&lt;td align="left"&gt;Alpha-Beta-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;alphabeta_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/games.py"&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6&lt;/td&gt;
&lt;td align="left"&gt;CSP&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;CSP&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.3&lt;/td&gt;
&lt;td align="left"&gt;AC-3&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;AC3&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.5&lt;/td&gt;
&lt;td align="left"&gt;Backtracking-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;backtracking_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.8&lt;/td&gt;
&lt;td align="left"&gt;Min-Conflicts&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;min_conflicts&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.11&lt;/td&gt;
&lt;td align="left"&gt;Tree-CSP-Solver&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;tree_csp_solver&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7&lt;/td&gt;
&lt;td align="left"&gt;KB&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;KB&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.1&lt;/td&gt;
&lt;td align="left"&gt;KB-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;KB_AgentProgram&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.7&lt;/td&gt;
&lt;td align="left"&gt;Propositional Logic Sentence&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Expr&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/utils.py"&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.10&lt;/td&gt;
&lt;td align="left"&gt;TT-Entails&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;tt_entails&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.12&lt;/td&gt;
&lt;td align="left"&gt;PL-Resolution&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;pl_resolution&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.14&lt;/td&gt;
&lt;td align="left"&gt;Convert to CNF&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;to_cnf&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.15&lt;/td&gt;
&lt;td align="left"&gt;PL-FC-Entails?&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;pl_fc_entails&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.17&lt;/td&gt;
&lt;td align="left"&gt;DPLL-Satisfiable?&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;dpll_satisfiable&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.18&lt;/td&gt;
&lt;td align="left"&gt;WalkSAT&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;WalkSAT&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.20&lt;/td&gt;
&lt;td align="left"&gt;Hybrid-Wumpus-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;HybridWumpusAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.22&lt;/td&gt;
&lt;td align="left"&gt;SATPlan&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;SAT_plan&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9&lt;/td&gt;
&lt;td align="left"&gt;Subst&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;subst&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9.1&lt;/td&gt;
&lt;td align="left"&gt;Unify&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;unify&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9.3&lt;/td&gt;
&lt;td align="left"&gt;FOL-FC-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;fol_fc_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9.6&lt;/td&gt;
&lt;td align="left"&gt;FOL-BC-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;fol_bc_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.1&lt;/td&gt;
&lt;td align="left"&gt;Air-Cargo-problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;air_cargo&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.2&lt;/td&gt;
&lt;td align="left"&gt;Spare-Tire-Problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;spare_tire&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.3&lt;/td&gt;
&lt;td align="left"&gt;Three-Block-Tower&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;three_block_tower&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.7&lt;/td&gt;
&lt;td align="left"&gt;Cake-Problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;have_cake_and_eat_cake_too&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.9&lt;/td&gt;
&lt;td align="left"&gt;Graphplan&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;GraphPlan&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.13&lt;/td&gt;
&lt;td align="left"&gt;Partial-Order-Planner&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;PartialOrderPlanner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.1&lt;/td&gt;
&lt;td align="left"&gt;Job-Shop-Problem-With-Resources&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;job_shop_problem&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.5&lt;/td&gt;
&lt;td align="left"&gt;Hierarchical-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;hierarchical_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.8&lt;/td&gt;
&lt;td align="left"&gt;Angelic-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;angelic_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.10&lt;/td&gt;
&lt;td align="left"&gt;Doubles-tennis&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;double_tennis_problem&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;13&lt;/td&gt;
&lt;td align="left"&gt;Discrete Probability Distribution&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ProbDist&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;13.1&lt;/td&gt;
&lt;td align="left"&gt;DT-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;DTAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.9&lt;/td&gt;
&lt;td align="left"&gt;Enumeration-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;enumeration_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.11&lt;/td&gt;
&lt;td align="left"&gt;Elimination-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;elimination_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.13&lt;/td&gt;
&lt;td align="left"&gt;Prior-Sample&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;prior_sample&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.14&lt;/td&gt;
&lt;td align="left"&gt;Rejection-Sampling&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;rejection_sampling&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.15&lt;/td&gt;
&lt;td align="left"&gt;Likelihood-Weighting&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;likelihood_weighting&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.16&lt;/td&gt;
&lt;td align="left"&gt;Gibbs-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;gibbs_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15.4&lt;/td&gt;
&lt;td align="left"&gt;Forward-Backward&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;forward_backward&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15.6&lt;/td&gt;
&lt;td align="left"&gt;Fixed-Lag-Smoothing&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;fixed_lag_smoothing&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15.17&lt;/td&gt;
&lt;td align="left"&gt;Particle-Filtering&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;particle_filtering&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;16.9&lt;/td&gt;
&lt;td align="left"&gt;Information-Gathering-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;InformationGatheringAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.4&lt;/td&gt;
&lt;td align="left"&gt;Value-Iteration&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;value_iteration&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.7&lt;/td&gt;
&lt;td align="left"&gt;Policy-Iteration&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;policy_iteration&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.9&lt;/td&gt;
&lt;td align="left"&gt;POMDP-Value-Iteration&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;pomdp_value_iteration&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.5&lt;/td&gt;
&lt;td align="left"&gt;Decision-Tree-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;DecisionTreeLearner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.8&lt;/td&gt;
&lt;td align="left"&gt;Cross-Validation&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;cross_validation&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.11&lt;/td&gt;
&lt;td align="left"&gt;Decision-List-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;DecisionListLearner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.24&lt;/td&gt;
&lt;td align="left"&gt;Back-Prop-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;BackPropagationLearner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.34&lt;/td&gt;
&lt;td align="left"&gt;AdaBoost&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;AdaBoost&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.2&lt;/td&gt;
&lt;td align="left"&gt;Current-Best-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;current_best_learning&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.3&lt;/td&gt;
&lt;td align="left"&gt;Version-Space-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;version_space_learning&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.8&lt;/td&gt;
&lt;td align="left"&gt;Minimal-Consistent-Det&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;minimal_consistent_det&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.12&lt;/td&gt;
&lt;td align="left"&gt;FOIL&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;FOIL_container&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;21.2&lt;/td&gt;
&lt;td align="left"&gt;Passive-ADP-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;PassiveADPAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/rl.py"&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;21.4&lt;/td&gt;
&lt;td align="left"&gt;Passive-TD-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;PassiveTDAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/rl.py"&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;21.8&lt;/td&gt;
&lt;td align="left"&gt;Q-Learning-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;QLearningAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/rl.py"&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;22.1&lt;/td&gt;
&lt;td align="left"&gt;HITS&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;HITS&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/nlp.py"&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;23&lt;/td&gt;
&lt;td align="left"&gt;Chart-Parse&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Chart&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/nlp.py"&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;23.5&lt;/td&gt;
&lt;td align="left"&gt;CYK-Parse&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;CYK_parse&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/nlp.py"&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;25.9&lt;/td&gt;
&lt;td align="left"&gt;Monte-Carlo-Localization&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;monte_carlo_localization&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-index-of-data-structures" class="anchor" aria-hidden="true" href="#index-of-data-structures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index of data structures&lt;/h1&gt;
&lt;p&gt;Here is a table of the implemented data structures, the figure, name of the implementation in the repository, and the file where they are implemented.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.2&lt;/td&gt;
&lt;td align="left"&gt;romania_map&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.9&lt;/td&gt;
&lt;td align="left"&gt;vacumm_world&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.23&lt;/td&gt;
&lt;td align="left"&gt;one_dim_state_space&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.1&lt;/td&gt;
&lt;td align="left"&gt;australia_map&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.13&lt;/td&gt;
&lt;td align="left"&gt;wumpus_world_inference&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.16&lt;/td&gt;
&lt;td align="left"&gt;horn_clauses_KB&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.1&lt;/td&gt;
&lt;td align="left"&gt;sequential_decision_environment&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.2&lt;/td&gt;
&lt;td align="left"&gt;waiting_decision_tree&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;Many thanks for contributions over the years. I got bug reports, corrected code, and other support from Darius Bacon, Phil Ruggera, Peng Shao, Amit Patil, Ted Nienstedt, Jim Martin, Ben Catanzariti, and others. Now that the project is on GitHub, you can see the &lt;a href="https://github.com/aimacode/aima-python/graphs/contributors"&gt;contributors&lt;/a&gt; who are doing a great job of actively improving the project. Many thanks to all contributors, especially &lt;a href="https://github.com/darius"&gt;@darius&lt;/a&gt;, &lt;a href="https://github.com/SnShine"&gt;@SnShine&lt;/a&gt;, &lt;a href="https://github.com/reachtarunhere"&gt;@reachtarunhere&lt;/a&gt;, &lt;a href="https://github.com/antmarakis"&gt;@antmarakis&lt;/a&gt;, &lt;a href="https://github.com/Chipe1"&gt;@Chipe1&lt;/a&gt;, &lt;a href="https://github.com/ad71"&gt;@ad71&lt;/a&gt; and &lt;a href="https://github.com/MariannaSpyrakou"&gt;@MariannaSpyrakou&lt;/a&gt;.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>aimacode</author><guid isPermaLink="false">https://github.com/aimacode/aima-python</guid><pubDate>Fri, 31 Jan 2020 00:18:00 GMT</pubDate></item><item><title>awslabs/amazon-sagemaker-examples #19 in Jupyter Notebook, This week</title><link>https://github.com/awslabs/amazon-sagemaker-examples</link><description>&lt;p&gt;&lt;i&gt;Example notebooks that show how to apply machine learning, deep learning and reinforcement learning in Amazon SageMaker&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-amazon-sagemaker-examples" class="anchor" aria-hidden="true" href="#amazon-sagemaker-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Examples&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks that show how to apply machine learning and deep learning in &lt;a href="https://aws.amazon.com/sagemaker" rel="nofollow"&gt;Amazon SageMaker&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-ground-truth-labeling-jobs" class="anchor" aria-hidden="true" href="#introduction-to-ground-truth-labeling-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification"&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/ground_truth_object_detection_tutorial"&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output"&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/object_detection_augmented_manifest_training"&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an "augmented manifest" and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/annotation_consolidation"&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-applying-machine-learning" class="anchor" aria-hidden="true" href="#introduction-to-applying-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Applying Machine Learning&lt;/h3&gt;
&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; predicts potential customers that are most likely to convert based on customer and aggregate level metrics, using Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives.  This uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/linear_time_series_forecast"&gt;Time-series Forecasting&lt;/a&gt; generates a forecast for topline product demand using Amazon SageMaker's Linear Learner algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/breast_cancer_prediction"&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker's Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/ensemble_modeling"&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/video_game_sales"&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/gluon_recommender_system"&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/fair_linear_learner"&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans"&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/object2vec_document_embedding"&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sagemaker-automatic-model-tuning" class="anchor" aria-hidden="true" href="#sagemaker-automatic-model-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SageMaker Automatic Model Tuning&lt;/h3&gt;
&lt;p&gt;These examples introduce SageMaker's hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/xgboost_direct_marketing"&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fits for the &lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; task.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/tensorflow_mnist"&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/mxnet_mnist"&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/keras_bring_your_own"&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/r_bring_your_own"&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/analyze_results"&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-amazon-algorithms" class="anchor" aria-hidden="true" href="#introduction-to-amazon-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Amazon Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker's custom developed algorithms.  Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/1P_kmeans_highlevel"&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker.  It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/factorization_machines_mnist"&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker's implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/linear_learner_mnist"&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ntm_synthetic"&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/pca_mnist"&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/seq2seq_translation_en-de"&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that's built on top of &lt;a href="https://github.com/awslabs/sockeye"&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet.  Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation.  This notebook shows translation from English to German text.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker's Image Classification algorithm.  This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_abalone"&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html" rel="nofollow"&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_mnist"&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/deepar_synthetic"&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/blazingtext_word2vec_text8"&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker's fast and scalable BlazingText implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_pascalvoc_coco"&gt;Object Detection&lt;/a&gt; illustrates how to train an object detector using the Amazon SageMaker Object Detection algorithm with different input formats (RecordIO and image).  It uses the Pascal VOC dataset. A third notebook is provided to demonstrate the use of incremental training.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_birds"&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_movie_recommendation"&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification"&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_sentence_similarity"&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ipinsights_login"&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc"&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentaion masks and probability of segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-rl" class="anchor" aria-hidden="true" href="#amazon-sagemaker-rl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker RL&lt;/h3&gt;
&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_cartpole_coach"&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel's RL Coach.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_deepracer_robomaker_coach_gazebo"&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_hvac_coach_energyplus"&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_knapsack_coach_custom"&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_mountain_car_coach_gymEnv"&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_network_compression_ray_custom"&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_objecttracker_robomaker_coach_gazebo"&gt;Turtlebot Tracker&lt;/a&gt; This notebook demonstrates object tracking using AWS Robomaker and RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_portfolio_management_coach_customEnv"&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_predictive_autoscaling_coach_customEnv"&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_ray"&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_stable_baselines"&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach"&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_tic_tac_toe_coach_customEnv"&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-scientific-details-of-algorithms" class="anchor" aria-hidden="true" href="#scientific-details-of-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scientific Details of Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/streaming_median"&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker's spectral decomposition approach to LDA.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/linear_learner_class_weights_loss_functions"&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-debugger" class="anchor" aria-hidden="true" href="#amazon-sagemaker-debugger"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Debugger&lt;/h3&gt;
&lt;p&gt;These examples provide and introduction to SageMaker Debugger which allows debugging and monitoring capabilities for training of machine learning and deep learning algorithms. Note that although these notebooks focus on a specific framework, the same approach works with all the frameworks that Amazon SageMaker Debugger supports. The notebooks below are listed in the order in which we recommend you review them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_builtin_rule/"&gt;Using a built-in rule with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_keras_custom_rule/"&gt;Using a custom rule with TensorFlow Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mnist_tensor_analysis/"&gt;Interactive tensor analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mnist_tensor_plot/"&gt;Visualizing Debugging Tensors of MXNet training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mxnet_realtime_analysis/"&gt;Real-time analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/xgboost_builtin_rules/"&gt;Using a built in rule with XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/xgboost_realtime_analysis/"&gt;Real-time analysis in notebook with XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mxnet_spot_training/"&gt;Using SageMaker Debugger with Managed Spot Training and MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_action_on_rule/"&gt;Reacting to CloudWatch Events from Rules to take an action based on status with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/pytorch_custom_container/"&gt;Using SageMaker Debugger with a custom PyTorch container&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-advanced-amazon-sagemaker-functionality" class="anchor" aria-hidden="true" href="#advanced-amazon-sagemaker-functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt;
&lt;p&gt;These examples that showcase unique functionality available in Amazon SageMaker.  They cover a broad range of topics and will utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="advanced_functionality/data_distribution_types"&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances.  This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/handling_kms_encrypted_data"&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/parquet_to_recordio_protobuf"&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href="https://parquet.apache.org/" rel="nofollow"&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/working_with_redshift_data"&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/xgboost_bring_your_own_model"&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/kmeans_bring_your_own_model"&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that's been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/install_r_kernel"&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/scikit_bring_your_own"&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/mxnet_mnist_byom"&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/tensorflow_iris_byom"&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_xgboost_abalone"&gt;Inference Pipeline with SparkML and XGBoost&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and XGBoost for training on the Abalone dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_blazingtext_dbpedia"&gt;Inference Pipeline with SparkML and BlazingText&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and BlazingText for training on the DBPedia dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/search"&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_bring_your_own"&gt;Host Multiple Models with Your Own Algorithm&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint with your own custom algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_xgboost_home_value"&gt;Host Multiple Models with XGBoost&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled XGBoost container.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_sklearn_home_value"&gt;Host Multiple Models with SKLearn&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled SKLearn container.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-neo-compilation-jobs" class="anchor" aria-hidden="true" href="#amazon-sagemaker-neo-compilation-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide you an introduction to how to use Neo to optimizes deep learning model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; Adapts form &lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;image classification&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;mxnet mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/pytorch_torchvision"&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;tensorflow mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; Adapts form &lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;xgboost customer churn&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-procesing" class="anchor" aria-hidden="true" href="#amazon-sagemaker-procesing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Procesing&lt;/h3&gt;
&lt;p&gt;These examples show you how to use SageMaker Processing jobs to run data processing workloads.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_processing/scikit_learn_data_processing_and_model_evaluation"&gt;Scikit-Learn Data Processing and Model Evaluation&lt;/a&gt; shows how to use SageMaker Processing and the Scikit-Learn container to run data preprocessing and model evaluation workloads.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_processing/feature_transformation_with_sagemaker_processing"&gt;Feature transformation with Amazon SageMaker Processing and SparkML&lt;/a&gt; shows how to use SageMaker Processing to run data processing workloads using SparkML prior to training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk" class="anchor" aria-hidden="true" href="#amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-deep-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-deep-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you to write idiomatic TensorFlow or MXNet and then train or host in pre-built containers using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_cifar10"&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_mnist"&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_sentiment_analysis"&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_iris"&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_cifar10"&gt;CIFAR-10 with MXNet Gluon&lt;/a&gt; trains a ResNet-34  image classification model using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_mnist"&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet's symbolic syntax&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_sentiment"&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_layers"&gt;TensorFlow Neural Networks with Layers&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow layers&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_keras"&gt;TensorFlow Networks with Keras&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow and Keras&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators"&gt;Introduction to Estimators in TensorFlow&lt;/a&gt; trains a DNN classifier estimator on the Iris dataset using TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_resnet_cifar10_with_tensorboard"&gt;TensorFlow and TensorBoard&lt;/a&gt; trains a ResNet image classification model on CIFAR-10 using TensorFlow and showcases how to track results using TensorBoard&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; trains a simple convolutional neural network on MNIST using TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-machine-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-machine-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone"&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_inference_pipeline"&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-using-amazon-sagemaker-with-apache-spark" class="anchor" aria-hidden="true" href="#using-amazon-sagemaker-with-apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt;
&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href="https://github.com/aws/sagemaker-spark"&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-spark/pyspark_mnist"&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aws-marketplace" class="anchor" aria-hidden="true" href="#aws-marketplace"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS Marketplace&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;This example shows you how to package a model-package/algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/creating_marketplace_products"&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use algorithms and model packages from AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_algorithms"&gt;Using Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/amazon_demo_product"&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/automl"&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_model_packages"&gt;Using Model Packages&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/amazon_demo_product"&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/auto_insurance"&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/improving_industrial_workplace_safety"&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_data"&gt;Using Data&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_data/using_data_from_aws_data_exchange_to_predict_product_popularity"&gt;Using data and algorithm from AWS Marketplace for training a model&lt;/a&gt; provides a detailed walkthrough on how to use data from AWS Marketplace for training a model that predicts popularity of a bath product.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-under-development" class="anchor" aria-hidden="true" href="#under-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Under Development&lt;/h3&gt;
&lt;p&gt;These Amazon SageMaker examples fully illustrate a concept, but may require some additional configuration on the users part to complete.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What do I need in order to get started?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The quickest setup to run example notebooks includes:
&lt;ul&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html" rel="nofollow"&gt;AWS account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Proper &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html" rel="nofollow"&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html" rel="nofollow"&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html" rel="nofollow"&gt;S3 bucket&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Will these examples work outside of Amazon SageMaker Notebook Instances?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;How do I contribute my own example notebook?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources.  Please bear with us in the short-term if pull requests take longer than expected or are closed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>awslabs</author><guid isPermaLink="false">https://github.com/awslabs/amazon-sagemaker-examples</guid><pubDate>Fri, 31 Jan 2020 00:19:00 GMT</pubDate></item><item><title>dennybritz/reinforcement-learning #20 in Jupyter Notebook, This week</title><link>https://github.com/dennybritz/reinforcement-learning</link><description>&lt;p&gt;&lt;i&gt;Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h3&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h3&gt;
&lt;p&gt;This repository provides code, exercises and solutions for popular Reinforcement Learning algorithms. These are meant to serve as a learning tool to complement the theoretical materials from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf" rel="nofollow"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="nofollow"&gt;David Silver's Reinforcement Learning Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each folder in corresponds to one or more chapters of the above textbook and/or course. In addition to exercises and solution, each folder also contains a list of learning goals, a brief concept summary, and links to the relevant readings.&lt;/p&gt;
&lt;p&gt;All code is written in Python 3 and uses RL environments from &lt;a href="https://gym.openai.com/" rel="nofollow"&gt;OpenAI Gym&lt;/a&gt;. Advanced techniques use &lt;a href="https://www.tensorflow.org/" rel="nofollow"&gt;Tensorflow&lt;/a&gt; for neural network implementations.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="Introduction/"&gt;Introduction to RL problems &amp;amp; OpenAI Gym&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MDP/"&gt;MDPs and Bellman Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DP/"&gt;Dynamic Programming: Model-Based RL, Policy Iteration and Value Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/"&gt;Monte Carlo Model-Free Prediction &amp;amp; Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="TD/"&gt;Temporal Difference Model-Free Prediction &amp;amp; Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="FA/"&gt;Function Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DQN/"&gt;Deep Q Learning&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/"&gt;Policy Gradient Methods&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;Learning and Planning (WIP)&lt;/li&gt;
&lt;li&gt;Exploration and Exploitation (WIP)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-list-of-implemented-algorithms" class="anchor" aria-hidden="true" href="#list-of-implemented-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of Implemented Algorithms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="DP/Policy%20Evaluation%20Solution.ipynb"&gt;Dynamic Programming Policy Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DP/Policy%20Iteration%20Solution.ipynb"&gt;Dynamic Programming Policy Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DP/Value%20Iteration%20Solution.ipynb"&gt;Dynamic Programming Value Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/MC%20Prediction%20Solution.ipynb"&gt;Monte Carlo Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/MC%20Control%20with%20Epsilon-Greedy%20Policies%20Solution.ipynb"&gt;Monte Carlo Control with Epsilon-Greedy Policies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb"&gt;Monte Carlo Off-Policy Control with Importance Sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="TD/SARSA%20Solution.ipynb"&gt;SARSA (On Policy TD Learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="TD/Q-Learning%20Solution.ipynb"&gt;Q-Learning (Off Policy TD Learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="FA/Q-Learning%20with%20Value%20Function%20Approximation%20Solution.ipynb"&gt;Q-Learning with Linear Function Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DQN/Deep%20Q%20Learning%20Solution.ipynb"&gt;Deep Q-Learning for Atari Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DQN/Double%20DQN%20Solution.ipynb"&gt;Double Deep-Q Learning for Atari Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Q-Learning with Prioritized Experience Replay (WIP)&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb"&gt;Policy Gradient: REINFORCE with Baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/CliffWalk%20Actor%20Critic%20Solution.ipynb"&gt;Policy Gradient: Actor Critic with Baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb"&gt;Policy Gradient: Actor Critic with Baseline for Continuous Action Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deterministic Policy Gradients for Continuous Action Spaces (WIP)&lt;/li&gt;
&lt;li&gt;Deep Deterministic Policy Gradients (DDPG) (WIP)&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/a3c"&gt;Asynchronous Advantage Actor Critic (A3C)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h3&gt;
&lt;p&gt;Textbooks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf" rel="nofollow"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Classes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="nofollow"&gt;David Silver's Reinforcement Learning Course (UCL, 2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rll.berkeley.edu/deeprlcourse/" rel="nofollow"&gt;CS294 - Deep Reinforcement Learning (Berkeley, Fall 2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udacity.com/course/reinforcement-learning--ud600" rel="nofollow"&gt;CS 8803 - Reinforcement Learning (Georgia Tech)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/" rel="nofollow"&gt;CS885 - Reinforcement Learning (UWaterloo), Spring 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="nofollow"&gt;CS294-112 - Deep Reinforcement Learning (UC Berkeley)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Talks/Tutorials:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/" rel="nofollow"&gt;Introduction to Reinforcement Learning (Joelle Pineau @ Deep Learning Summer School 2016)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/" rel="nofollow"&gt;Deep Reinforcement Learning (Pieter Abbeel @ Deep Learning Summer School 2016)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://techtalks.tv/talks/deep-reinforcement-learning/62360/" rel="nofollow"&gt;Deep Reinforcement Learning ICML 2016 Tutorial (David Silver)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=ggqnxyjaKe4" rel="nofollow"&gt;Tutorial: Introduction to Reinforcement Learning with Function Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLjKEIQlKCTZYN3CYBlj8r58SbNorobqcp" rel="nofollow"&gt;John Schulman - Deep Reinforcement Learning (4 Lectures)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf" rel="nofollow"&gt;Deep Reinforcement Learning Slides @ NIPS 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spinningup.openai.com/en/latest/user/introduction.html" rel="nofollow"&gt;OpenAI Spinning Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs" rel="nofollow"&gt;Advanced Deep Learning &amp;amp; Reinforcement Learning (UCL 2018, DeepMind)&lt;/a&gt;
-&lt;a href="https://sites.google.com/view/deep-rl-bootcamp/lectures" rel="nofollow"&gt;Deep RL Bootcamp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other Projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/carpedm20/deep-rl-tensorflow"&gt;carpedm20/deep-rl-tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/matthiasplappert/keras-rl"&gt;matthiasplappert/keras-rl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Selected Papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.readcube.com/articles/10.1038/nature14236" rel="nofollow"&gt;Human-Level Control through Deep Reinforcement Learning (2015-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1509.06461" rel="nofollow"&gt;Deep Reinforcement Learning with Double Q-learning (2015-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1509.02971" rel="nofollow"&gt;Continuous control with deep reinforcement learning (2015-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1511.05952" rel="nofollow"&gt;Prioritized Experience Replay (2015-11)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1511.06581" rel="nofollow"&gt;Dueling Network Architectures for Deep Reinforcement Learning (2015-11)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1602.01783" rel="nofollow"&gt;Asynchronous Methods for Deep Reinforcement Learning (2016-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1603.01121" rel="nofollow"&gt;Deep Reinforcement Learning from Self-Play in Imperfect-Information Games (2016-03)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf" rel="nofollow"&gt;Mastering the game of Go with deep neural networks and tree search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dennybritz</author><guid isPermaLink="false">https://github.com/dennybritz/reinforcement-learning</guid><pubDate>Fri, 31 Jan 2020 00:20:00 GMT</pubDate></item><item><title>google-research/google-research #21 in Jupyter Notebook, This week</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Fri, 31 Jan 2020 00:21:00 GMT</pubDate></item><item><title>norvig/pytudes #22 in Jupyter Notebook, This week</title><link>https://github.com/norvig/pytudes</link><description>&lt;p&gt;&lt;i&gt;Python programs to practice or demonstrate skills.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytudes" class="anchor" aria-hidden="true" href="#pytudes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pytudes&lt;/h1&gt;
&lt;p&gt;"An &lt;em&gt;étude&lt;/em&gt; (a French word meaning &lt;em&gt;study&lt;/em&gt;) is an instrumental musical composition, usually short, of considerable difficulty, and designed to provide practice material for perfecting a particular musical skill." — &lt;a href="https://en.wikipedia.org/wiki/%C3%89tude" rel="nofollow"&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This project contains &lt;strong&gt;pytudes&lt;/strong&gt;—Python programs, usually short, for perfecting programming skills.&lt;/p&gt;
&lt;p&gt;Some are in Jupyter (IPython) notebooks, some in &lt;code&gt;.py&lt;/code&gt; files. You can view the files here on github, or click the &lt;strong&gt;NB&lt;/strong&gt; link to view them on &lt;a href="http://nbviewer.jupyter.org/" rel="nofollow"&gt;nbviewer&lt;/a&gt; (which sometimes works better). If you want to &lt;em&gt;run&lt;/em&gt; the notebooks, not just view them, you can clone the project, or run all the notebooks online by clicking this button: &lt;a href="https://mybinder.org/v2/gh/norvig/pytudes/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;, or click the &lt;strong&gt;DN&lt;/strong&gt; link to run each individual notebook on   &lt;a href="https://beta.deepnote.org/" rel="nofollow"&gt;deepnote&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-index-of-jupyter-ipython-notebooks" class="anchor" aria-hidden="true" href="#index-of-jupyter-ipython-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index of Jupyter (IPython) Notebooks&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Programming Examples&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Advent-2018.ipynb"&gt;Advent of Code 2018&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Advent-2018.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FAdvent-2018.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Puzzle site with a coding puzzle each day for Advent 2018 .&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Advent%202017.ipynb"&gt;Advent of Code 2017&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Advent%202017.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FAdvent%25202017.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Puzzle site with a coding puzzle each day for Advent 2017.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Advent%20of%20Code.ipynb"&gt;Advent of Code 2016&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Advent%20of%20Code.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FAdvent%2520of%2520Code.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Puzzle site with a coding puzzle each day for Advent 2016&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Project%20Euler%20Utils.ipynb"&gt;Project Euler Utilities&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Project%20Euler%20Utils.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FProject%2520Euler%2520Utils.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;My utility functions for the Project Euler problems, including &lt;code&gt;Primes&lt;/code&gt; and &lt;code&gt;Factors&lt;/code&gt;.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/PropositionalLogic.ipynb"&gt;Translating English Sentences into Propositional Logic Statements&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/PropositionalLogic.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FPropositionalLogic.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Automatically converting informal English sentences into formal Propositional Logic.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Beal.ipynb"&gt;Beal's Conjecture Revisited&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Beal.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FBeal.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A search for counterexamples to Beal's Conjecture&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/WWW.ipynb"&gt;WWW: Who Will Win (NBA Title)?&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/WWW.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FWWW.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Computing the probability of winning the NBA title, for my home town Warriors, or any other team.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Pickleball.ipynb"&gt;Pickleball Tournament&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Pickleball.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FPickleball.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Scheduling a doubles tournament fairly and efficiently.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Dice%20Baseball.ipynb"&gt;Dice Baseball&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Dice%2520Baseball.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FDice%2520Baseball.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Simulating baseball games.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Life.ipynb"&gt;Conway's Game of Life&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Life.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FLife.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;The cellular automata zero-player game.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Sierpinski.ipynb"&gt;A Chaos Game with Triangles&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Sierpinski.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FSierpinski.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A surprising appearance of the Sierpinski triangle in a random walk between vertexes.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Maze.ipynb"&gt;Generating Mazes&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Maze.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FMaze.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Make a maze by generating a random tree superimposed on a grid.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/TwelveBalls.ipynb"&gt;Weighing Twelve Balls&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/TwelveBalls.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FTwelveBalls.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A puzzle where you are given some billiard balls and a balance scale, and asked to find the one ball that is heavier or lighter, in a limited number of weighings.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Cant-Stop.ipynb"&gt;Can't Stop&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Cant-Stop.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FCant-Stop.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Optimal play in a dice board game.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Bike%20Speed%20versus%20Grade.ipynb"&gt;Bike Speed Versus Grade&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Bike%20Speed%20versus%20Grade.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FBike%2520Speed%2520versus%2520Grade.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;How fast can I bike as the route gets steeper?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Orderable%20Cards.ipynb"&gt;Properly Ordered Card Hands&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Orderable%20Cards.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FOrderable%2520Cards.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Can you get your hand of cards into a nice order with just one move?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Electoral%20Votes.ipynb"&gt;Tracking Trump: Electoral Votes&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Electoral%20Votes.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FElectoral%20Votes.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;How many electoral votes would Trump get if he wins the state where he has positive net approval?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Logic and Number Puzzles&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Cheryl.ipynb"&gt;When is Cheryl's Birthday?&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Cheryl.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FCheryl.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Solving the "Cheryl's Birthday" logic puzzle.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Cheryl-and-Eve.ipynb"&gt;When Cheryl Met Eve: A Birthday Story&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Cheryl-and-Eve.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FCheryl-and-Eve.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Inventing new puzzles in the Style of Cheryl's Birthday.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/NightKing.ipynb"&gt;How Many Soldiers Do You Need to Beat the Night King?&lt;/a&gt; &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/NightKing.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FNightKing.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Investigasting a battle between the army of the dead and the army of the living.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Coin%20Flip.ipynb"&gt;The Devil and the Coin Flip Game&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Coin%20Flip.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FCoin%2520Flip.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;How to beat the Devil at his own game.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Mean%20Misanthrope%20Density.ipynb"&gt;The Puzzle of the Misanthropic Neighbors&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Mean%20Misanthrope%20Density.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FMean%2520Misanthrope%2520Density.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;How crowded will this neighborhood be, if nobody wants to live next door to anyone else?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Countdown.ipynb"&gt;Four 4s, Five 5s, and Countdown to 2016&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Countdown.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FCountdown.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Solving the equation 10 _ 9 _ 8 _ 7 _ 6 _ 5 _ 4 _ 3 _ 2 _ 1 = 2016. From an Alex Bellos puzzle.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Sicherman%20Dice.ipynb"&gt;Sicherman Dice&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Sicherman%20Dice.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FSicherman%2520Dice.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Find a pair of dice that is like a regular pair of dice, only different.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Golomb-Puzzle.ipynb"&gt;Sol Golomb's Rectangle Puzzle&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Golomb-Puzzle.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FGolomb-Puzzle.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A Puzzle involving placing rectangles of different sizes inside a square.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Cryptarithmetic.ipynb"&gt;Cryptarithmetic&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Cryptarithmetic.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FCryptarithmetic.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Substitute digits for letters and make NUM + BER = PLAY.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Riddler%20Battle%20Royale.ipynb"&gt;The Riddler: Battle Royale&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Riddler%20Battle%20Royale.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FRiddler%2520Battle%2520Royale.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A puzzle involving allocating your troops and going up against an opponent.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Word Games&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/SpellingBee.ipynb"&gt;Spelling Bee&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/SpellingBee.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FSpellingBee.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Find the highest-scoring board for the NY Times Spelling Bee puzzle.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Portmantout.ipynb"&gt;Portmantout Words&lt;/a&gt; &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Portmantout.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FPortmantout.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Find a long word that squishes together a bunch of words.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/xkcd-Name-Dominoes.ipynb"&gt;xkcd 1970: Name Dominoes&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/xkcd-Name-Dominoes.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2Fxkcd-Name-Dominoes.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Lay out dominoes legally; the dominoes have people names, not numbers.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Ghost.ipynb"&gt;Ghost&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Ghost.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FGhost.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;The word game Ghost (add letters, try to avoid making a word).&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/pal3.ipynb"&gt;World's Longest Palindrome&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/pal3.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2Fpal3.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Searching for a long Panama-style palindrome, this time letter-by-letter.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Scrabble.ipynb"&gt;Refactoring a Crossword Game Program&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Scrabble.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FScrabble.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Refactoring the Scrabble / Word with Friends game from Udacity 212.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/xkcd1313.ipynb"&gt;xkcd 1313: Regex Golf&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/xkcd1313.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2Fxkcd1313.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Find the smallest regular expression; inspired by Randall Monroe.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/xkcd1313-part2.ipynb"&gt;xkcd 1313: Regex Golf (Part 2: Infinite Problems)&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/xkcd1313-part2.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2Fxkcd1313-part2.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Regex Golf: better, faster, funner. With Stefan Pochmann.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Fred%20Buns.ipynb"&gt;Let's Code About Bike Locks&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Fred%20Buns.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FFred%2520Buns.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A tale of a bicycle combination lock that uses letters instead of digits. Inspired by Bike Snob NYC.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Gesture%20Typing.ipynb"&gt;Gesture Typing&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Gesture%20Typing.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FGesture%2520Typing.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;What word has the longest path on a gesture-typing smartphone keyboard?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/How%20to%20Do%20Things%20with%20Words.ipynb"&gt;How to Do Things with Words, or Statistical Natural Language Processing in Python&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/How%20to%20Do%20Things%20with%20Words.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FHow%2520to%2520Do%2520Things%2520with%2520Words.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Spelling Correction, Secret Codes, Word Segmentation, and more: grab your bag of words.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Math Concepts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Probability.ipynb"&gt;A Concrete Introduction to Probability&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Probability.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FProbability.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Code and examples of the basic principles of Probability Theory.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/ProbabilityParadox.ipynb"&gt;Probability, Paradox, and the Reasonable Person Principle&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/ProbabilityParadox.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FProbabilityParadox.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Some classic paradoxes in Probability Theory, and how to think about disagreements.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Differentiation.ipynb"&gt;Symbolic Algebra, Simplification, and Differentiation&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Differentiation.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FDifferentiation.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A computer algebra system that  manipulates expressions, including symbolic differentiation.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Economics.ipynb"&gt;Economics Simulation&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Economics.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FEconomics.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A simulation of a simple economic game.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/How%20To%20Count%20Things.ipynb"&gt;How to Count Things&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/How%20To%20Count%20Things.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FHow%2520To%2520Count%2520Things.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Combinatorial math: how to count how many things there are, when there are a lot of them.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Socks.ipynb"&gt;Pairing Socks&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Socks.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FSocks.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;What is the probability that you will be able to pair up socks as you randomly pull them out of the dryer?&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Euler's%20Conjecture.ipynb"&gt;Euler's Sum of Powers Conjecture&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Euler's%20Conjecture.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FEuler's%2520Conjecture.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Solving a 200-year-old puzzle by finding integers that satisfy a&lt;sup&gt;5&lt;/sup&gt; + b&lt;sup&gt;5&lt;/sup&gt; + c&lt;sup&gt;5&lt;/sup&gt; + d&lt;sup&gt;5&lt;/sup&gt; = e&lt;sup&gt;5&lt;/sup&gt;.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Computer Science Algorithms and Concepts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/BASIC.ipynb"&gt;BASIC Interpreter&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/BASIC.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FBASIC.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;How to write an interpreter for the BASIC programming language.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Snobol.ipynb"&gt;Bad Grade, Good Experience&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Snobol.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FSnobol.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;As a student, did you ever get a bad grade on a programming assignment? (Snobol, Concordance)&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/Convex%20Hull.ipynb"&gt;The Convex Hull Problem&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/Convex%20Hull.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FConvex%2520Hull.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;A classic Computer Science Algorithm.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="ipynb/TSP.ipynb"&gt;The Traveling Salesperson Problem&lt;/a&gt;    &lt;strong&gt;&lt;a href="http:/nbviewer.jupyter.org/github/norvig/pytudes/blob/master/ipynb/TSP.ipynb"&gt;NB&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://beta.deepnote.org/launch?template=python_3.6&amp;amp;url=https%3A%2F%2Fgithub.com%2Fnorvig%2Fpytudes%2Fblob%2Fmaster%2Fipynb%2FTSP.ipynb" rel="nofollow"&gt;DN&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Another of the classics.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-index-of-python-files" class="anchor" aria-hidden="true" href="#index-of-python-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index of Python Files&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/SET.py"&gt;SET.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Analyze the card game &lt;a href="http://www.setgame.com/set" rel="nofollow"&gt;SET&lt;/a&gt;.&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/SET.html" rel="nofollow"&gt;SET.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/beal.py"&gt;beal.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Search for counterexamples to Beal's Conjecture&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/beal.html" rel="nofollow"&gt;beal.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/docex.py"&gt;docex.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;A framework for running unit tests, similar to &lt;code&gt;doctest&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/ibol.py"&gt;ibol.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;An Exercise in Species Barcoding&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/ibol.html" rel="nofollow"&gt;ibol.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/lettercount.py"&gt;lettercount.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Convert Google Ngram Counts to Letter Counts&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/mayzner.html" rel="nofollow"&gt;mayzner.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/lis.py"&gt;lis.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Lisp Interpreter written in Python&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/lispy.html" rel="nofollow"&gt;lispy.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/lispy.py"&gt;lispy.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Even Better Lisp Interpreter written in Python&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/lispy2.html" rel="nofollow"&gt;lispy2.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/lispytest.py"&gt;lispytest.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Tests for Lisp Interpreters&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/pal.py"&gt;pal.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Find long palindromes&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/palindrome.html" rel="nofollow"&gt;palindrome.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/pal2.py"&gt;pal2.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Find longer palindromes&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/palindrome.html" rel="nofollow"&gt;palindrome.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/pal3.py"&gt;pal3.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Find even longer palindromes&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/palindrome.html" rel="nofollow"&gt;palindrome.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/py2html.py"&gt;py2html.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Pretty-printer to format Python files as html&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/spell.py"&gt;spell.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Spelling corrector&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/spell-correct.html" rel="nofollow"&gt;spell-correct.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/sudoku.py"&gt;sudoku.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Program to solve sudoku puzzles&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/sudoku.html" rel="nofollow"&gt;sudoku.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/testaccum.py"&gt;testaccum.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Tests for my failed Python &lt;code&gt;accumulation display&lt;/code&gt; proposal&lt;/td&gt;
&lt;td&gt;&lt;a href="http://norvig.com/pyacc.html" rel="nofollow"&gt;pyacc.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/norvig/pytudes/blob/master/py/yaptu.py"&gt;yaptu.py&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Yet Another Python Templating Utility&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-etudes-for-programmers" class="anchor" aria-hidden="true" href="#etudes-for-programmers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Etudes for Programmers&lt;/h1&gt;
&lt;p&gt;I got the idea for the "etudes" part of the name from
this &lt;a href="https://books.google.com/books/about/Etudes_for_programmers.html?id=u89WAAAAMAAJ" rel="nofollow"&gt;1978 book&lt;/a&gt;
by &lt;a href="http://demin.ws/blog/english/2012/08/25/interview-with-charles-wetherell/" rel="nofollow"&gt;Charles Wetherell&lt;/a&gt;
that was very influential to me when I was first learning to program.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3f08a9e787adadcb78fa75a25bbae4c435c7f84a/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f35315a6e5a48323964764c2e5f53583339345f424f312c3230342c3230332c3230305f2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/3f08a9e787adadcb78fa75a25bbae4c435c7f84a/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f35315a6e5a48323964764c2e5f53583339345f424f312c3230342c3230332c3230305f2e6a7067" alt="" data-canonical-src="https://images-na.ssl-images-amazon.com/images/I/51ZnZH29dvL._SX394_BO1,204,203,200_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>norvig</author><guid isPermaLink="false">https://github.com/norvig/pytudes</guid><pubDate>Fri, 31 Jan 2020 00:22:00 GMT</pubDate></item><item><title>facebookresearch/ClassyVision #23 in Jupyter Notebook, This week</title><link>https://github.com/facebookresearch/ClassyVision</link><description>&lt;p&gt;&lt;i&gt;An end-to-end PyTorch framework for image and video classification&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="website/static/img/cv-logo.png"&gt;&lt;img width="70%" src="website/static/img/cv-logo.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/facebookresearch/ClassyVision/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/890acbdcb87868b382af9a4b1fac507b9659d9bf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="GitHub license" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/facebookresearch/ClassyVision" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/b23b82f8a5b9266ee5b145d48c51aa7868cc2c61/68747470733a2f2f636972636c6563692e636f6d2f67682f66616365626f6f6b72657365617263682f436c61737379566973696f6e2e7376673f7374796c653d73766726636972636c652d746f6b656e3d66656561666130353766386433663663306331356466643734646238646435393664393638346338" alt="CircleCI" data-canonical-src="https://circleci.com/gh/facebookresearch/ClassyVision.svg?style=svg&amp;amp;circle-token=feeafa057f8d3f6c0c15dfd74db8dd596d9684c8" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://github.com/facebookresearch/ClassyVision/blob/master/CONTRIBUTING.md"&gt;&lt;img src="https://camo.githubusercontent.com/d4e0f63e9613ee474a7dfdc23c240b9795712c96/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d627269676874677265656e2e737667" alt="PRs Welcome" data-canonical-src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="http://classyvision.ai" rel="nofollow"&gt;Classy Vision&lt;/a&gt; is a new end-to-end, PyTorch-based framework for large-scale training of state-of-the-art image and video classification models. Previous computer vision (CV) libraries have been focused on providing components for users to build their own frameworks for their research. While this approach offers flexibility for researchers, in production settings it leads to duplicative efforts, and requires users to migrate research between frameworks and to relearn the minutiae of efficient distributed training and data loading. Our PyTorch-based CV framework offers a better solution for training at scale and for deploying to production. It offers several notable advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ease of use. The library features a modular, flexible design that allows anyone to train machine learning models on top of PyTorch using very simple abstractions. The system also has out-of-the-box integration with Amazon Web Services (AWS), facilitating research at scale and making it simple to move between research and production.&lt;/li&gt;
&lt;li&gt;High performance. Researchers can use the framework to train Resnet50 on ImageNet in as little as 15 minutes, for example.&lt;/li&gt;
&lt;li&gt;Demonstrated success in training at scale. We’ve used it to replicate the state-of-the-art results from the paper &lt;a href="https://arxiv.org/abs/1805.00932" rel="nofollow"&gt;Exploring the Limits of Weakly Supervised Pretraining&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Integration with PyTorch Hub. AI researchers and engineers can download and fine-tune the best publically available ImageNet models with just a few lines of code.&lt;/li&gt;
&lt;li&gt;Elastic training. We have also added experimental integration with &lt;a href="https://github.com/pytorch/elastic"&gt;PyTorch Elastic&lt;/a&gt;, which allows distributed training jobs to adjust as available resources in the cluster changes. It also makes distributed training robust to transient hardware failures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Classy Vision is beta software. The project is under active development and our APIs are subject to change in future releases.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-installation-requirements" class="anchor" aria-hidden="true" href="#installation-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Requirements&lt;/h4&gt;
&lt;p&gt;Make sure you have an up-to-date installation of PyTorch (1.4), Python (3.6) and torchvision (0.5). If you want to use GPUs, then a CUDA installation (10.1) is also required.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-installing-the-latest-stable-release" class="anchor" aria-hidden="true" href="#installing-the-latest-stable-release"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing the latest stable release&lt;/h4&gt;
&lt;p&gt;To install Classy Vision via pip:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install classy_vision&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To install Classy Vision via conda:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c conda-forge classy_vision&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-manual-install-of-latest-commit-on-master" class="anchor" aria-hidden="true" href="#manual-install-of-latest-commit-on-master"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual install of latest commit on master&lt;/h4&gt;
&lt;p&gt;Alternatively you can do a manual install.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/facebookresearch/ClassyVision.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; ClassyVision
pip install &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
&lt;p&gt;Classy Vision aims to support a variety of projects to be built and open sourced on top of the core library. We provide utilities for setting up a project in a standard format with some simple generated examples to get started with. To start a new project:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;classy-project my-project
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; my-project&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We even include a simple, synthetic, training example to show how to use Classy Vision:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt; ./classy_train.py --config configs/template_config.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Voila! A few seconds later your first training run using our classification task should be done. Check out the results in the output folder:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;ls output_&lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;timestamp&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;/checkpoints/
checkpoint.torch model_phase-0_end.torch model_phase-1_end.torch model_phase-2_end.torch model_phase-3_end.torch&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;checkpoint.torch&lt;/code&gt; is the latest model (in this case, same as &lt;code&gt;model_phase-3_end.torch&lt;/code&gt;), a checkpoint is saved at the end of each phase.&lt;/p&gt;
&lt;p&gt;For more details / tutorials see the documentation section below.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Please see our &lt;a href="https://classyvision.ai/tutorials/" rel="nofollow"&gt;tutorials&lt;/a&gt; to learn how to get started on Classy Vision and customize your training runs. Full documentation is available &lt;a href="https://classyvision.ai/api/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-join-the-classy-vision-community" class="anchor" aria-hidden="true" href="#join-the-classy-vision-community"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Join the Classy Vision community&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Website: &lt;a href="https://classyvision.ai" rel="nofollow"&gt;https://classyvision.ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/tagged/classy-vision" rel="nofollow"&gt;Stack overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Slack: &lt;a href="https://join.slack.com/t/classyvision/shared_invite/enQtODczNTEyOTUyNTY0LTc4YTc3NThhMzhiNGNjZTkzY2RkYjZiNDc1ZDcyZGYxY2Q0M2M5YjAyYjA4OGQ2M2FlNDk4YzBlNWRjOTg3ZTE" rel="nofollow"&gt;invite link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Classy Vision is MIT licensed, as found in the LICENSE file.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citing-classy-vision" class="anchor" aria-hidden="true" href="#citing-classy-vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing Classy Vision&lt;/h2&gt;
&lt;p&gt;If you use Classy Vision in your work, please use the following BibTeX entry:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{adcock2019classy,
  title={Classy Vision},
  author={{Adcock}, A. and {Reis}, V. and {Singh}, M. and {Yan}, Z. and {van der Maaten} L., and {Zhang}, K. and {Motwani}, S. and {Guerin}, J. and {Goyal}, N. and {Misra}, I. and {Gustafson}, L. and {Changhan}, C. and {Goyal}, P.},
  howpublished = {\url{https://github.com/facebookresearch/ClassyVision}},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>facebookresearch</author><guid isPermaLink="false">https://github.com/facebookresearch/ClassyVision</guid><pubDate>Fri, 31 Jan 2020 00:23:00 GMT</pubDate></item><item><title>empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks #24 in Jupyter Notebook, This week</title><link>https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks</link><description>&lt;p&gt;&lt;i&gt;A series of Python Jupyter notebooks that help you better understand "The Elements of Statistical Learning" book&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-the-elements-of-statistical-learning-notebooks" class="anchor" aria-hidden="true" href="#the-elements-of-statistical-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;"The Elements of Statistical Learning" Notebooks&lt;/h1&gt;
&lt;p&gt;Reproducing examples from the "The Elements of Statistical Learning" by Trevor Hastie, Robert Tibshirani and Jerome Friedman with Python and its popular libraries:
&lt;strong&gt;numpy&lt;/strong&gt;, &lt;strong&gt;math&lt;/strong&gt;, &lt;strong&gt;scipy&lt;/strong&gt;, &lt;strong&gt;sklearn&lt;/strong&gt;, &lt;strong&gt;pandas&lt;/strong&gt;, &lt;strong&gt;tensorflow&lt;/strong&gt;, &lt;strong&gt;statsmodels&lt;/strong&gt;, &lt;strong&gt;sympy&lt;/strong&gt;, &lt;strong&gt;catboost&lt;/strong&gt;, &lt;strong&gt;pyearth&lt;/strong&gt;, &lt;strong&gt;mlxtend&lt;/strong&gt;, &lt;strong&gt;cvxpy&lt;/strong&gt;. Almost all plotting is done using &lt;strong&gt;matplotlib&lt;/strong&gt;, sometimes using &lt;strong&gt;seaborn&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;The documented Jupyter Notebooks are in the &lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/tree/master/examples"&gt;examples&lt;/a&gt; folder:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesmixtureipynb" class="anchor" aria-hidden="true" href="#examplesmixtureipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Mixture.ipynb"&gt;examples/Mixture.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Classifying the points from a mixture of "gaussians" using linear regression, nearest-neighbor, logistic regression with natural cubic splines basis expansion, neural networks, support vector machines, flexible discriminant analysis over MARS regression, mixture discriminant analysis, k-Means clustering, Gaussian mixture model and random forests.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/mixture.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/mixture.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesprostate-canceripynb" class="anchor" aria-hidden="true" href="#examplesprostate-canceripynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Prostate%20Cancer.ipynb"&gt;examples/Prostate Cancer.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Predicting prostate specific antigen using ordinary least squares, ridge/lasso regularized linear regression, principal components regression, partial least squares and best subset regression. Model parameters are selected by K-folds cross-validation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/cancer.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/cancer.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplessouth-african-heart-diseaseipynb" class="anchor" aria-hidden="true" href="#examplessouth-african-heart-diseaseipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/South%20African%20Heart%20Disease.ipynb"&gt;examples/South African Heart Disease.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Understanding the risk factors using logistic regression, L1 regularized logistic regression, natural cubic splines basis expansion for nonlinearities, thin-plate spline for mutual dependency, local logistic regression, kernel density estimation and gaussian mixture models.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/chd.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/chd.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesvowelipynb" class="anchor" aria-hidden="true" href="#examplesvowelipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Vowel.ipynb"&gt;examples/Vowel.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Vowel speech recognition using regression of an indicator matrix, linear/quadratic/regularized/reduced-rank discriminant analysis and logistic regression.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/vowel.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/vowel.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesbone-mineral-densityipynb" class="anchor" aria-hidden="true" href="#examplesbone-mineral-densityipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Bone%20Mineral%20Density.ipynb"&gt;examples/Bone Mineral Density.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Comparing patterns of bone mineral density relative change for men and women using smoothing splines.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/bone.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/bone.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesair-pollution-dataipynb" class="anchor" aria-hidden="true" href="#examplesair-pollution-dataipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Air%20Pollution.ipynb"&gt;examples/Air Pollution Data.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing Los Angeles pollution data using smoothing splines.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/ozone_vs_pressure_gradient.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/ozone_vs_pressure_gradient.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesphoneme-recognitionipynb" class="anchor" aria-hidden="true" href="#examplesphoneme-recognitionipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Phoneme%20Recognition.ipynb"&gt;examples/Phoneme Recognition.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Phonemes speech recognition using reduced flexibility logistic regression.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/phoneme.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/phoneme.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesgalaxyipynb" class="anchor" aria-hidden="true" href="#examplesgalaxyipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Galaxy.ipynb"&gt;examples/Galaxy.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing radial velocity of galaxy NGC7531 using local regression in multidimentional space.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/galaxy.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/galaxy.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesozoneipynb" class="anchor" aria-hidden="true" href="#examplesozoneipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Ozone.ipynb"&gt;examples/Ozone.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing the factors influencing ozone concentration using local regression and trellis plot.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/ozone.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/ozone.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesspamipynb" class="anchor" aria-hidden="true" href="#examplesspamipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Spam.ipynb"&gt;examples/Spam.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Detecting email spam using logistic regression, generalized additive logistic model, decision tree, multivariate adaptive regression splines, boosting and random forest.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/spam.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/spam.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplescalifornia-housingipynb" class="anchor" aria-hidden="true" href="#examplescalifornia-housingipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/California%20Housing.ipynb"&gt;examples/California Housing.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing the factors influencing California houses prices using boosting over decision trees and partial dependance plots.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/california.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/california.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesdemographicsipynb" class="anchor" aria-hidden="true" href="#examplesdemographicsipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Demographics.ipynb"&gt;examples/Demographics.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Predicting shopping mall customers occupation, and hence identifying demographic variables that discriminate between different occupational categories using boosting and market basket analysis.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/demographics.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/demographics.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-exampleszip-codeipynb" class="anchor" aria-hidden="true" href="#exampleszip-codeipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/ZIP%20Code.ipynb"&gt;examples/ZIP Code.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Recognizing small hand-drawn digits using LeCun's Net-1 - Net-5 neural networks.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/zip1.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/zip1.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Analysing of the number three variation in ZIP codes using principal component and archetypal analysis.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/zip2.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/zip2.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-exampleshuman-tumor-microarray-dataipynb" class="anchor" aria-hidden="true" href="#exampleshuman-tumor-microarray-dataipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Human%20Tumor%20Microarray%20Data.ipynb"&gt;examples/Human Tumor Microarray Data.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing microarray data using K-means clustring and hierarchical clustering.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/tumor.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/tumor.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplescountry-dissimilaritiesipynb" class="anchor" aria-hidden="true" href="#examplescountry-dissimilaritiesipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Country%20Dissimilarities.ipynb"&gt;examples/Country Dissimilarities.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing country dissimilarities using K-medoids clustering and multidimensional scaling.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/country.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/country.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplessignatureipynb" class="anchor" aria-hidden="true" href="#examplessignatureipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Signature.ipynb"&gt;examples/Signature.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing signature shapes using Procrustes transformation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/signature.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/signature.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-exampleswaveformipynb" class="anchor" aria-hidden="true" href="#exampleswaveformipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Waveform.ipynb"&gt;examples/Waveform.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Recognizing wave classes using linear, quadratic, flexible (over MARS regression), mixture discriminant analysis and decision trees.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/waveform.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/waveform.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesprotein-flow-cytometryipynb" class="anchor" aria-hidden="true" href="#examplesprotein-flow-cytometryipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Protein%20Flow%20Cytometry.ipynb"&gt;examples/Protein Flow-Cytometry.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing protein flow-cytometry data using graphical-lasso undirected graphical model for continuous variables.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/cytometry.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/cytometry.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplessrbct-microarrayipynb" class="anchor" aria-hidden="true" href="#examplessrbct-microarrayipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/SRBCT%20Microarray.ipynb"&gt;examples/SRBCT Microarray.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing microarray data of 2308 genes and selecting the most significant genes for cancer classification using nearest shrunken centroids.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/srbct.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/srbct.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examples14-cancer-microarrayipynb" class="anchor" aria-hidden="true" href="#examples14-cancer-microarrayipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/14%20Cancer.ipynb"&gt;examples/14 Cancer Microarray.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Analysing microarray data of 16,063 genes gathered by Ramaswamy et al. (2001) and selecting the most significant genes for cancer classification using nearest shrunken centroids, L2-penalized discriminant analysis, support vector classifier, k-nearest neighbors, L2-penalized multinominal, L1-penalized multinominal and elastic-net penalized multinominal. It is a difficult classification problem with p&amp;gt;&amp;gt;N (only 144 training observations).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesskin-of-the-orangeipynb" class="anchor" aria-hidden="true" href="#examplesskin-of-the-orangeipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Skin%20of%20the%20Orange.ipynb"&gt;examples/Skin of the Orange.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Solving a synthetic classification problem using Support Vector Machines and multivariate adaptive regression splines to show the influence of additional noise features.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examplesradiation-sensitivityipynb" class="anchor" aria-hidden="true" href="#examplesradiation-sensitivityipynb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Radiation%20Sensitivity.ipynb"&gt;examples/Radiation Sensitivity.ipynb&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Assessing the significance of 12,625 genes from microarray study of radiation sensitivity using Benjamini-Hochberg method and the significane analysis of microarrays (SAM) approach.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/images/radiation.png"&gt;&lt;img src="https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/raw/master/images/radiation.png" alt="alt" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>empathy87</author><guid isPermaLink="false">https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks</guid><pubDate>Fri, 31 Jan 2020 00:24:00 GMT</pubDate></item><item><title>rlabbe/Kalman-and-Bayesian-Filters-in-Python #25 in Jupyter Notebook, This week</title><link>https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</link><description>&lt;p&gt;&lt;i&gt;Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs.  Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kalman-and-bayesian-filters-in-python" class="anchor" aria-hidden="true" href="#kalman-and-bayesian-filters-in-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"&gt;Kalman and Bayesian Filters in Python&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Introductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Juptyer Notebook so that you can run and modify the code in your browser. What better way to learn?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"Kalman and Bayesian Filters in Python" looks amazing! ... your book is just what I needed&lt;/strong&gt; - Allen Downey, Professor and O'Reilly author.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.&lt;/strong&gt; - Sam Rodkey, SpaceX&lt;/p&gt;
&lt;p&gt;Start reading online now by clicking the binder or Azure badge below:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://notebooks.azure.com/import/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c33d8af3d101ffcd6ea73a8d02290b8d829ac52/68747470733a2f2f6e6f7465626f6f6b732e617a7572652e636f6d2f6c61756e63682e706e67" data-canonical-src="https://notebooks.azure.com/launch.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif"&gt;&lt;img src="https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-are-kalman-and-bayesian-filters" class="anchor" aria-hidden="true" href="#what-are-kalman-and-bayesian-filters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What are Kalman and Bayesian Filters?&lt;/h2&gt;
&lt;p&gt;Sensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.&lt;/p&gt;
&lt;p&gt;In simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, and the computer vision algorithms create very noisy and unreliable results.&lt;/p&gt;
&lt;p&gt;This book teaches you how to solve these sorts of filtering problems. I use many different algorithms, but they are all based on Bayesian probability. In simple terms Bayesian probability determines what is likely to be true based on past information.&lt;/p&gt;
&lt;p&gt;If I asked you the heading of my car at this moment you would have no idea. You'd proffer a number between 1∘∘ and 360∘∘ degrees, and have a 1 in 360 chance of being right. Now suppose I told you that 2 seconds ago its heading was 243∘∘. In 2 seconds my car could not turn very far so you could make a far more accurate prediction. You are using past information to more accurately infer information about the present or future.&lt;/p&gt;
&lt;p&gt;The world is also noisy. That prediction helps you make a better estimate, but it also subject to noise. I may have just braked for a dog or swerved around a pothole. Strong winds and ice on the road are external influences on the path of my car. In control literature we call this noise though you may not think of it that way.&lt;/p&gt;
&lt;p&gt;There is more to Bayesian probability, but you have the main idea. Knowledge is uncertain, and we alter our beliefs based on the strength of the evidence. Kalman and Bayesian filters blend our noisy and limited knowledge of how a system behaves with the noisy and limited sensor readings to produce the best possible estimate of the state of the system. Our principle is to never discard information.&lt;/p&gt;
&lt;p&gt;Say we are tracking an object and a sensor reports that it suddenly changed direction. Did it really turn, or is the data noisy? It depends. If this is a jet fighter we'd be very inclined to believe the report of a sudden maneuver. If it is a freight train on a straight track we would discount it. We'd further modify our belief depending on how accurate the sensor is. Our beliefs depend on the past and on our knowledge of the system we are tracking and on the characteristics of the sensors.&lt;/p&gt;
&lt;p&gt;The Kalman filter was invented by Rudolf Emil Kálmán to solve this sort of problem in a mathematically optimal way. Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains. There are Kalman filters in aircraft, on submarines, and on cruise missiles. Wall street uses them to track the market. They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments. Chemical plants use them to control and monitor reactions. They are used to perform medical imaging and to remove noise from cardiac signals. If it involves a sensor and/or time-series data, a Kalman filter or a close relative to the Kalman filter is usually involved.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h2&gt;
&lt;p&gt;The motivation for this book came out of my desire for a gentle introduction to Kalman filtering. I'm a software engineer that spent almost two decades in the avionics field, and so I have always been 'bumping elbows' with the Kalman filter, but never implemented one myself. As I moved into solving tracking problems with computer vision the need became urgent. There are classic textbooks in the field, such as Grewal and Andrew's excellent &lt;em&gt;Kalman Filtering&lt;/em&gt;. But sitting down and trying to read many of these books is a dismal experience if you do not have the required background. Typically the first few chapters fly through several years of undergraduate math, blithely referring you to textbooks on topics such as Itō calculus, and present an entire semester's worth of statistics in a few brief paragraphs. They are good texts for an upper undergraduate course, and an invaluable reference to researchers and professionals, but the going is truly difficult for the more casual reader. Symbology is introduced without explanation, different texts use different terms and variables for the same concept, and the books are almost devoid of examples or worked problems. I often found myself able to parse the words and comprehend the mathematics of a definition, but had no idea as to what real world phenomena they describe. "But what does that &lt;em&gt;mean?&lt;/em&gt;" was my repeated thought.&lt;/p&gt;
&lt;p&gt;However, as I began to finally understand the Kalman filter I realized the underlying concepts are quite straightforward. A few simple probability rules, some intuition about how we integrate disparate knowledge to explain events in our everyday life and the core concepts of the Kalman filter are accessible. Kalman filters have a reputation for difficulty, but shorn of much of the formal terminology the beauty of the subject and of their math became clear to me, and I fell in love with the topic.&lt;/p&gt;
&lt;p&gt;As I began to understand the math and theory more difficulties present themselves. A book or paper's author makes some statement of fact and presents a graph as proof.  Unfortunately, why the statement is true is not clear to me, nor is the method for making that plot obvious. Or maybe I wonder "is this true if R=0?"  Or the author provides pseudocode at such a high level that the implementation is not obvious. Some books offer Matlab code, but I do not have a license to that expensive package. Finally, many books end each chapter with many useful exercises. Exercises which you need to understand if you want to implement Kalman filters for yourself, but exercises with no answers. If you are using the book in a classroom, perhaps this is okay, but it is terrible for the independent reader. I loathe that an author withholds information from me, presumably to avoid 'cheating' by the student in the classroom.&lt;/p&gt;
&lt;p&gt;From my point of view none of this necessary. Certainly if you are designing a Kalman filter for a aircraft or missile you must thoroughly master of all of the mathematics and topics in a typical Kalman filter textbook. I just want to track an image on a screen, or write some code for an Arduino project. I want to know how the plots in the book are made, and chose different parameters than the author chose. I want to run simulations. I want to inject more noise in the signal and see how a filter performs. There are thousands of opportunities for using Kalman filters in everyday code, and yet this fairly straightforward topic is the provenance of rocket scientists and academics.&lt;/p&gt;
&lt;p&gt;I wrote this book to address all of those needs. This is not the book for you if you program navigation computers for Boeing or design radars for Raytheon. Go get an advanced degree at Georgia Tech, UW, or the like, because you'll need it. This book is for the hobbiest, the curious, and the working engineer that needs to filter or smooth data.&lt;/p&gt;
&lt;p&gt;This book is interactive. While you can read it online as static content, I urge you to use it as intended. It is written using Jupyter Notebook, which allows me to combine text, math, Python, and Python output in one place. Every plot, every piece of data in this book is generated from Python that is available to you right inside the notebook. Want to double the value of a parameter? Click on the Python cell, change the parameter's value, and click 'Run'. A new plot or printed output will appear in the book.&lt;/p&gt;
&lt;p&gt;This book has exercises, but it also has the answers. I trust you. If you just need an answer, go ahead and read the answer. If you want to internalize this knowledge, try to implement the exercise before you read the answer.&lt;/p&gt;
&lt;p&gt;This book has supporting libraries for computing statistics, plotting various things related to filters, and for the various filters that we cover. This does require a strong caveat; most of the code is written for didactic purposes. It is rare that I chose the most efficient solution (which often obscures the intent of the code), and in the first parts of the book I did not concern myself with numerical stability. This is important to understand - Kalman filters in aircraft are carefully designed and implemented to be numerically stable; the naive implementation is not stable in many cases. If you are serious about Kalman filters this book will not be the last book you need. My intention is to introduce you to the concepts and mathematics, and to get you to the point where the textbooks are approachable.&lt;/p&gt;
&lt;p&gt;Finally, this book is free. The cost for the books required to learn Kalman filtering is somewhat prohibitive even for a Silicon Valley engineer like myself; I cannot believe they are within the reach of someone in a depressed economy, or a financially struggling student. I have gained so much from free software like Python, and free books like those from Allen B. Downey &lt;a href="http://www.greenteapress.com/" rel="nofollow"&gt;here&lt;/a&gt;. It's time to repay that. So, the book is free, it is hosted on free servers, and it uses only free and open software such as IPython and mathjax to create the book.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reading-online" class="anchor" aria-hidden="true" href="#reading-online"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reading Online&lt;/h2&gt;
&lt;p&gt;The book is written as a collection of Jupyter Notebooks, an interactive, browser based system that allows you to combine text, Python, and math into your browser. There are multiple ways to read these online, listed below.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-binder" class="anchor" aria-hidden="true" href="#binder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;binder&lt;/h3&gt;
&lt;p&gt;binder serves interactive notebooks online, so you can run the code and change the code within your browser without downloading the book or installing Jupyter.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nbviewer" class="anchor" aria-hidden="true" href="#nbviewer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;nbviewer&lt;/h3&gt;
&lt;p&gt;The website &lt;a href="http://nbviewer.org" rel="nofollow"&gt;http://nbviewer.org&lt;/a&gt; provides an Jupyter Notebook server that renders notebooks stored at github (or elsewhere). The rendering is done in real time when you load the book. You may use &lt;a href="http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb" rel="nofollow"&gt;&lt;em&gt;this nbviewer link&lt;/em&gt;&lt;/a&gt; to access my book via nbviewer. If you read my book today, and then I make a change tomorrow, when you go back tomorrow you will see that change. Notebooks are rendered statically - you can read them, but not modify or run the code.&lt;/p&gt;
&lt;p&gt;nbviewer seems to lag the checked in version by a few days, so you might not be reading the most recent content.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-github" class="anchor" aria-hidden="true" href="#github"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GitHub&lt;/h3&gt;
&lt;p&gt;GitHub is able to render the notebooks directly. The quickest way to view a notebook is to just click on them above. However, it renders the math incorrectly, and I cannot recommend using it if you are doing more than just dipping into the book.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pdf-version" class="anchor" aria-hidden="true" href="#pdf-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PDF Version&lt;/h2&gt;
&lt;p&gt;A PDF version of the book is available &lt;a href="https://drive.google.com/open?id=0By_SW19c1BfhSVFzNHc0SjduNzg" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The PDF will usually lag behind what is in github as I don't update it for every minor check in.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-downloading-and-running-the-book" class="anchor" aria-hidden="true" href="#downloading-and-running-the-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading and Running the Book&lt;/h2&gt;
&lt;p&gt;However, this book is intended to be interactive and I recommend using it in that form. It's a little more effort to set up, but worth it. If you install IPython and some supporting libraries on your computer and then clone this book you will be able to run all of the code in the book yourself. You can perform experiments, see how filters react to different data, see how different filters react to the same data, and so on. I find this sort of immediate feedback both vital and invigorating. You do not have to wonder "what happens if". Try it and see!&lt;/p&gt;
&lt;p&gt;The book and supporting software can be downloaded from GitHub by running this command on  the command line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python.git
pip install filterpy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instructions for installation of the IPython ecosystem can be found in the Installation appendix, found &lt;a href="http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/Appendix-A-Installation.ipynb" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once the software is installed you can navigate to the installation directory and run Juptyer notebook with the command line instruction&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open a browser window showing the contents of the base directory. The book is organized into chapters. To read Chapter 2, click on the link for chapter 2. This will cause the browser to open that subdirectory. In each subdirectory there will be one or more IPython Notebooks (all notebooks have a .ipynb file extension). The chapter contents are in the notebook with the same name as the chapter name. There are sometimes supporting notebooks for doing things like generating animations that are displayed in the chapter. These are not intended to be read by the end user, but of course if you are curious as to how an animation is made go ahead and take a look.&lt;/p&gt;
&lt;p&gt;This is admittedly a somewhat cumbersome interface to a book; I am following in the footsteps of several other projects that are somewhat repurposing Jupyter Notebook to generate entire books. I feel the slight annoyances have a huge payoff - instead of having to download a separate code base and run it in an IDE while you try to read a book, all of the code and text is in one place. If you want to alter the code, you may do so and immediately see the effects of your change. If you find a bug, you can make a fix, and push it back to my repository so that everyone in the world benefits. And, of course, you will never encounter a problem I face all the time with traditional books - the book and the code are out of sync with each other, and you are left scratching your head as to which source to trust.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-companion-software" class="anchor" aria-hidden="true" href="#companion-software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Companion Software&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://pypi.python.org/pypi/filterpy" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2514efa585c6a295744d54c87effb293c6ffee51/687474703a2f2f696d672e736869656c64732e696f2f707970692f762f66696c74657270792e737667" alt="Latest Version" data-canonical-src="http://img.shields.io/pypi/v/filterpy.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I wrote an open source Bayesian filtering Python library called &lt;strong&gt;FilterPy&lt;/strong&gt;. I have made the project available on PyPi, the Python Package Index.  To install from PyPi, at the command line issue the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install filterpy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not have pip, you may follow the instructions here: &lt;a href="https://pip.pypa.io/en/latest/installing.html" rel="nofollow"&gt;https://pip.pypa.io/en/latest/installing.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All of the filters used in this book as well as others not in this book are implemented in my Python library FilterPy, available &lt;a href="https://github.com/rlabbe/filterpy"&gt;here&lt;/a&gt;. You do not need to download or install this to read the book, but you will likely want to use this library to write your own filters. It includes Kalman filters, Fading Memory filters, H infinity filters, Extended and Unscented filters, least square filters, and many more.  It also includes helper routines that simplify the designing the matrices used by some of the filters, and other code such as Kalman based smoothers.&lt;/p&gt;
&lt;p&gt;FilterPy is hosted github at (&lt;a href="https://github.com/rlabbe/filterpy"&gt;https://github.com/rlabbe/filterpy&lt;/a&gt;).  If you want the bleading edge release you will want to grab a copy from github, and follow your Python installation's instructions for adding it to the Python search path. This might expose you to some instability since you might not get a tested release, but as a benefit you will also get all of the test scripts used to test the library. You can examine these scripts to see many examples of writing and running filters while not in the Jupyter Notebook environment.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-alternative-way-of-running-the-book-in-conda-environment" class="anchor" aria-hidden="true" href="#alternative-way-of-running-the-book-in-conda-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternative Way of Running the Book in Conda environment&lt;/h2&gt;
&lt;p&gt;If you have conda or miniconda installed, you can create environment by&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda env update -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source activate kf_bf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source deactivate kf_bf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to activate and deactivate the environment.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-issues-or-questions" class="anchor" aria-hidden="true" href="#issues-or-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues or Questions&lt;/h2&gt;
&lt;p&gt;If you have comments, you can write an issue at GitHub so that everyone can read it along with my response. Please don't view it as a way to report bugs only. Alternatively I've created a gitter room for more informal discussion. &lt;a href="https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/005cfe27b7c4520ac0d6b607d6a7e33f5ad4eb6e/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Kalman and Bayesian Filters in Python&lt;/span&gt; by &lt;a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"&gt;Roger R. Labbe&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All software in this book, software that supports this book (such as in the the code directory) or used in the generation of the book (in the pdf directory) that is contained in this repository is licensed under the following MIT license:&lt;/p&gt;
&lt;p&gt;The MIT License (MIT)&lt;/p&gt;
&lt;p&gt;Copyright (c) 2015 Roger R. Labbe Jr&lt;/p&gt;
&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt;
&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt;
&lt;p&gt;THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.TION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;rlabbejr at gmail.com&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rlabbe</author><guid isPermaLink="false">https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</guid><pubDate>Fri, 31 Jan 2020 00:25:00 GMT</pubDate></item></channel></rss>