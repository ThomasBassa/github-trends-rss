<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, This week</title><link>https://github.com/trending/jupyter-notebook?since=weekly</link><description>The top repositories on GitHub for jupyter-notebook, measured weekly</description><pubDate>Mon, 18 Nov 2019 01:06:53 GMT</pubDate><lastBuildDate>Mon, 18 Nov 2019 01:06:53 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>fengdu78/lihang-code #1 in Jupyter Notebook, This week</title><link>https://github.com/fengdu78/lihang-code</link><description>&lt;p&gt;&lt;i&gt;ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹çš„ä»£ç å®ç°&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;ä»£ç ç›®å½•&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ç¬¬1ç«  ç»Ÿè®¡å­¦ä¹ æ–¹æ³•æ¦‚è®º&lt;/p&gt;
&lt;p&gt;ç¬¬2ç«  æ„ŸçŸ¥æœº&lt;/p&gt;
&lt;p&gt;ç¬¬3ç«  kè¿‘é‚»æ³•&lt;/p&gt;
&lt;p&gt;ç¬¬4ç«  æœ´ç´ è´å¶æ–¯&lt;/p&gt;
&lt;p&gt;ç¬¬5ç«  å†³ç­–æ ‘&lt;/p&gt;
&lt;p&gt;ç¬¬6ç«  é€»è¾‘æ–¯è°›å›å½’&lt;/p&gt;
&lt;p&gt;ç¬¬7ç«  æ”¯æŒå‘é‡æœº&lt;/p&gt;
&lt;p&gt;ç¬¬8ç«  æå‡æ–¹æ³•&lt;/p&gt;
&lt;p&gt;ç¬¬9ç«  EMç®—æ³•åŠå…¶æ¨å¹¿&lt;/p&gt;
&lt;p&gt;ç¬¬10ç«  éšé©¬å°”å¯å¤«æ¨¡å‹&lt;/p&gt;
&lt;p&gt;ç¬¬11ç«  æ¡ä»¶éšæœºåœº&lt;/p&gt;
&lt;p&gt;ç¬¬12ç«  ç›‘ç£å­¦ä¹ æ–¹æ³•æ€»ç»“&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;å‚è€ƒï¼š
&lt;a href="https://github.com/wzyonggege/statistical-learning-method"&gt;https://github.com/wzyonggege/statistical-learning-method&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm"&gt;https://github.com/WenDesi/lihang_book_algorithm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/tudaodiaozhale" rel="nofollow"&gt;https://blog.csdn.net/tudaodiaozhale&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ä»£ç æ•´ç†å’Œä¿®æ”¹ï¼šæœºå™¨å­¦ä¹ åˆå­¦è€…&lt;/p&gt;
&lt;p&gt;å¾®ä¿¡å…¬ä¼—å·ï¼šæœºå™¨å­¦ä¹ åˆå­¦è€… &lt;a target="_blank" rel="noopener noreferrer" href="images/gongzhong.jpg"&gt;&lt;img src="images/gongzhong.jpg" alt="gongzhong" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;çŸ¥è¯†æ˜Ÿçƒï¼šé»„åšçš„æœºå™¨å­¦ä¹ åœˆå­&lt;a target="_blank" rel="noopener noreferrer" href="images/zhishixingqiu1.jpg"&gt;&lt;img src="images/zhishixingqiu1.jpg" alt="xingqiu" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/people/fengdu78" rel="nofollow"&gt;çŸ¥ä¹&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fengdu78</author><guid isPermaLink="false">https://github.com/fengdu78/lihang-code</guid><pubDate>Mon, 18 Nov 2019 00:01:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #2 in Jupyter Notebook, This week</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Mon, 18 Nov 2019 00:02:00 GMT</pubDate></item><item><title>dragen1860/TensorFlow-2.x-Tutorials #3 in Jupyter Notebook, This week</title><link>https://github.com/dragen1860/TensorFlow-2.x-Tutorials</link><description>&lt;p&gt;&lt;i&gt;TensorFlow 2.x version's  Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT examples, etc. TF 2.0ç‰ˆå…¥é—¨å®ä¾‹ä»£ç ï¼Œå®æˆ˜æ•™ç¨‹ã€‚&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-20-tutorials" class="anchor" aria-hidden="true" href="#tensorflow-20-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 Tutorials&lt;/h1&gt;
&lt;p&gt;Our repo. is the &lt;strong&gt;Winner&lt;/strong&gt; of &lt;a href="https://devpost.com/software/tensorflow-2-0-tutorials" rel="nofollow"&gt;âš¡#PoweredByTF 2.0 Challenge!&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="res/tensorflow-2.0.gif"&gt;&lt;img src="res/tensorflow-2.0.gif" width="250" align="middle" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Timeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oct. 1, 2019: TensorFlow 2.0 Stable!&lt;/li&gt;
&lt;li&gt;Aug. 24, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow 2.0 rc0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jun. 8, 2019: &lt;a href="https://twitter.com/fchollet/status/1134583289384120320" rel="nofollow"&gt;TensorFlow 2.0 Beta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mar. 7, 2019: &lt;a href="https://www.tensorflow.org/alpha" rel="nofollow"&gt;Tensorflow 2.0 Alpha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jan. 11, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow r2.0 preview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Aug. 14, 2018: &lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A" rel="nofollow"&gt;TensorFlow 2.0 is coming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;make sure you are using python 3.x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;GPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install &lt;code&gt;CUDA 10.0&lt;/code&gt;(or after) and &lt;code&gt;cudnn&lt;/code&gt; by yourself. and set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; up.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow&lt;span class="pl-k"&gt;-&lt;/span&gt;gpu  &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Test installation:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;In [&lt;span class="pl-c1"&gt;2&lt;/span&gt;]: &lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow  &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

In [&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: tf.&lt;span class="pl-c1"&gt;__version__&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;2.0.0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
In [&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: tf.test.is_gpu_available()
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
totalMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;95GiB&lt;/span&gt; freeMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;00GiB&lt;/span&gt;
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: &lt;span class="pl-c1"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-é…å¥—tf2è§†é¢‘æ•™ç¨‹" class="anchor" aria-hidden="true" href="#é…å¥—tf2è§†é¢‘æ•™ç¨‹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;é…å¥—TF2è§†é¢‘æ•™ç¨‹&lt;/h1&gt;
&lt;p align="center"&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/cover.png" width="400" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/TF_QR_163.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt; 
&lt;p&gt;TensorFlow 2.0çš„è§†é¢‘æ•™ç¨‹é“¾æ¥ï¼š&lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;æ·±åº¦å­¦ä¹ ä¸TensorFlow 2å®æˆ˜&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;çˆ±å¯å¯-çˆ±ç”Ÿæ´» å‹æƒ…æ¨è &lt;a target="_blank" rel="noopener noreferrer" href="res/weibo.jpg"&gt;&lt;img src="res/weibo.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-includes" class="anchor" aria-hidden="true" href="#includes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Includes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow 2.0 Overview&lt;/li&gt;
&lt;li&gt;TensorFlow 2.0 Basic Usage&lt;/li&gt;
&lt;li&gt;Linear Regression&lt;/li&gt;
&lt;li&gt;MNIST, FashionMNIST&lt;/li&gt;
&lt;li&gt;CIFAR10&lt;/li&gt;
&lt;li&gt;Fully Connected Layer&lt;/li&gt;
&lt;li&gt;VGG16&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;ResNet18&lt;/li&gt;
&lt;li&gt;Naive RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;ColorBot&lt;/li&gt;
&lt;li&gt;Auto-Encoders&lt;/li&gt;
&lt;li&gt;Variational Auto-Encoders&lt;/li&gt;
&lt;li&gt;DCGAN&lt;/li&gt;
&lt;li&gt;CycleGAN&lt;/li&gt;
&lt;li&gt;WGAN&lt;/li&gt;
&lt;li&gt;Pixel2Pixel&lt;/li&gt;
&lt;li&gt;Faster RCNN&lt;/li&gt;
&lt;li&gt;A2C&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;GCN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to submit a &lt;strong&gt;PR&lt;/strong&gt; request to make this repo. more complete!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-refered-repos" class="anchor" aria-hidden="true" href="#refered-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Refered Repos.&lt;/h1&gt;
&lt;p&gt;Our work is not built from scratch. Great appreciation to these open worksï¼&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/madalinabuzau/tensorflow-eager-tutorials"&gt;https://github.com/madalinabuzau/tensorflow-eager-tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/herbiebradley/CycleGAN-Tensorflow"&gt;https://github.com/herbiebradley/CycleGAN-Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/moono/tf-eager-on-GAN"&gt;https://github.com/moono/tf-eager-on-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Viredery/tf-eager-fasterrcnn"&gt;https://github.com/Viredery/tf-eager-fasterrcnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/github/gitignore/blob/master/Python.gitignore"&gt;https://github.com/github/gitignore/blob/master/Python.gitignore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dragen1860</author><guid isPermaLink="false">https://github.com/dragen1860/TensorFlow-2.x-Tutorials</guid><pubDate>Mon, 18 Nov 2019 00:03:00 GMT</pubDate></item><item><title>practicalAI/practicalAI #4 in Jupyter Notebook, This week</title><link>https://github.com/practicalAI/practicalAI</link><description>&lt;p&gt;&lt;i&gt;ğŸ“š A practical approach to machine learning. &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a href="https://practicalai.me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/369fcfdcd241a0324ffaee51715926a56c049302/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f70726163746963616c41492f6c6f676f2e706e67" width="200" data-canonical-src="https://practicalai.me/static/img/practicalAI/logo.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;A &lt;i&gt;&lt;b&gt;practical&lt;/b&gt;&lt;/i&gt; approach to machine learning.&lt;/p&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;
&lt;img src="https://camo.githubusercontent.com/c1b6c20adc52e06a1c58218665169097a63bd549/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726163746963616c41492f70726163746963616c41492e7376673f7374796c653d736f6369616c266c6162656c3d53746172" data-canonical-src="https://img.shields.io/github/stars/practicalAI/practicalAI.svg?style=social&amp;amp;label=Star" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/19c0cf9ba93aa446aa855a0203c46ee39841cba9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d2d3565626130302e7376673f6c6162656c3d4c696e6b6564496e266c6f676f3d6c696e6b6564696e267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://twitter.com/GokuMohandas" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/fd3346389d2255e7c7aa4395d6afb74e7a1df552/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f476f6b754d6f68616e6461732e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;sub&gt;Created by
&lt;a href="https://goku.me" rel="nofollow"&gt;Goku Mohandas&lt;/a&gt; and
&lt;a href="https://github.com/GokuMohandas/practicalAI/graphs/contributors"&gt;
contributors
&lt;/a&gt;
&lt;/sub&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-notebooks" class="anchor" aria-hidden="true" href="#notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notebooks&lt;/h2&gt;
&lt;ul&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="earth_americas" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30e.png"&gt;ğŸŒ&lt;/g-emoji&gt; â†’ &lt;a href="https://practicalai.me" rel="nofollow"&gt;https://practicalai.me&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        All of these notebooks are in &lt;a href="https://tensorflow.org" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt; but you can find old PyTorch notebooks in the &lt;a href="https://github.com/practicalAI/practicalAI/tree/4ad626098aca25db5628fe67895e738d5a5c2c2a"&gt;v0.1&lt;/a&gt; release.
    &lt;/li&gt;
    &lt;li&gt;
        If you prefer Jupyter Notebooks or want to add/fix content, check out the &lt;a href="https://github.com/practicalAI/practicalAI/tree/master/notebooks"&gt;notebooks&lt;/a&gt; directory.
    &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;
        &lt;h4 align="center"&gt;&lt;a id="user-content-basic-ml" class="anchor" aria-hidden="true" href="#basic-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic ML&lt;/h4&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Basics&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Machine Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Deep Learning&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn Python basics with notebooks.&lt;/li&gt;
            &lt;li&gt;Use data science libraries like &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/" rel="nofollow"&gt;Pandas&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Implement basic ML models in &lt;a href="https://www.tensorflow.org/overview/" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Create deep learning models for improved performance.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/00_Notebooks.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="notebook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d3.png"&gt;ğŸ““&lt;/g-emoji&gt; Notebooks&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;ğŸ“ˆ&lt;/g-emoji&gt; Linear Regression&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="mag_right" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50e.png"&gt;ğŸ”&lt;/g-emoji&gt; Data &amp;amp; Models&lt;/td&gt;
        &lt;td&gt;ï¸&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;ğŸ–¼&lt;/g-emoji&gt; Convolutional Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/01_Python.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt; Python&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;ğŸ“Š&lt;/g-emoji&gt; Logistic Regression&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="hammer_and_wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png"&gt;ğŸ› &lt;/g-emoji&gt; Utilities&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crown" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f451.png"&gt;ğŸ‘‘&lt;/g-emoji&gt; Embeddings&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/02_NumPy.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;ğŸ”¢&lt;/g-emoji&gt; NumPy&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;ï¸&lt;g-emoji class="g-emoji" alias="control_knobs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f39b.png"&gt;ğŸ›&lt;/g-emoji&gt; Multilayer Perceptrons&lt;/td&gt;
        &lt;td&gt;ï¸&lt;g-emoji class="g-emoji" alias="scissors" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2702.png"&gt;âœ‚ï¸&lt;/g-emoji&gt; Preprocessing&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;ğŸ“—&lt;/g-emoji&gt; Recurrent Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/03_Pandas.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="panda_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png"&gt;ğŸ¼&lt;/g-emoji&gt; Pandas&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-production-ml" class="anchor" aria-hidden="true" href="#production-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Production ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Local&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Applications&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Scale&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Setup your local environment for ML.&lt;/li&gt;
            &lt;li&gt;Wrap your ML in RESTful APIs using &lt;a href="http://flask.pocoo.org/" rel="nofollow"&gt;Flask&lt;/a&gt; to create applications.&lt;/li&gt;
            &lt;li&gt;Standardize and scale your ML applications with &lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt; and &lt;a href="https://kubernetes.io/" rel="nofollow"&gt;Kubernetes&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Deploy simple and scalable ML workflows using &lt;a href="https://www.kubeflow.org/" rel="nofollow"&gt;Kubeflow&lt;/a&gt;.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;ğŸ’»&lt;/g-emoji&gt; Local Setup&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="evergreen_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f332.png"&gt;ğŸŒ²&lt;/g-emoji&gt; Logging&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="whale" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f433.png"&gt;ğŸ³&lt;/g-emoji&gt; Docker&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="handshake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f91d.png"&gt;ğŸ¤&lt;/g-emoji&gt; Distributed Training&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt; ML Scripts&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="funeral_urn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png"&gt;âš±ï¸&lt;/g-emoji&gt; Flask Applications&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ship" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a2.png"&gt;ğŸš¢&lt;/g-emoji&gt; Kubernetes&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="battery" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50b.png"&gt;ğŸ”‹&lt;/g-emoji&gt; Databases&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png"&gt;âœ…&lt;/g-emoji&gt; Unit Tests&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ocean" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png"&gt;ğŸŒŠ&lt;/g-emoji&gt; Kubeflow&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="closed_lock_with_key" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f510.png"&gt;ğŸ”&lt;/g-emoji&gt; Authentication&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-advanced-ml" class="anchor" aria-hidden="true" href="#advanced-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;General&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Sequential&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Popular&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Dive into architectural and interpretable advancements in neural networks.&lt;/li&gt;
            &lt;li&gt;Implement state-of-the-art NLP techniques.&lt;/li&gt;
            &lt;li&gt;Learn about popular deep learning algorithms used for generation, time-series, etc.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;ğŸ§ Attention&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bee" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41d.png"&gt;ğŸ&lt;/g-emoji&gt; Transformers&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="performing_arts" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ad.png"&gt;ğŸ­&lt;/g-emoji&gt; Generative Adversarial Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crystal_ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52e.png"&gt;ğŸ”®&lt;/g-emoji&gt; Autoencoders&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="racing_car" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ce.png"&gt;ğŸï¸&lt;/g-emoji&gt; Highway Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="japanese_ogre" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f479.png"&gt;ğŸ‘¹&lt;/g-emoji&gt; BERT, GPT2, XLNet&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="8ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b1.png"&gt;ğŸ±&lt;/g-emoji&gt; Bayesian Deep Learning&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="spider" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f577.png"&gt;ğŸ•·ï¸&lt;/g-emoji&gt; Graph Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="droplet" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a7.png"&gt;ğŸ’§&lt;/g-emoji&gt; Residual Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="clock9" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f558.png"&gt;ğŸ•˜&lt;/g-emoji&gt; Temporal CNNs&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="cherries" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f352.png"&gt;ğŸ’&lt;/g-emoji&gt; Reinforcement Learning&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-topics" class="anchor" aria-hidden="true" href="#topics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Topics&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Computer Vision&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Natural Language&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Unsupervised Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn how to use deep learning for computer vision tasks.&lt;/li&gt;
            &lt;li&gt;Implement techniques for natural language tasks.&lt;/li&gt;
            &lt;li&gt;Derive insights from unlabeled data using unsupervised learning.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="camera_flash" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f8.png"&gt;ğŸ“¸&lt;/g-emoji&gt; Image Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;ğŸ“–&lt;/g-emoji&gt; Text classification&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dango" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f361.png"&gt;ğŸ¡&lt;/g-emoji&gt; Clustering&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="alarm_clock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f0.png"&gt;â°&lt;/g-emoji&gt; Time-series Analysis&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;ğŸ–¼ï¸&lt;/g-emoji&gt; Image Segmentation&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;ğŸ’¬&lt;/g-emoji&gt; Named Entity Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="houses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3d8.png"&gt;ğŸ˜ï¸&lt;/g-emoji&gt; Topic Modeling&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="shopping_cart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6d2.png"&gt;ğŸ›’&lt;/g-emoji&gt; Recommendation Systems&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="art" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a8.png"&gt;ğŸ¨&lt;/g-emoji&gt; Image Generation&lt;/td&gt;
        &lt;td&gt;ğŸ§  Knowledge Graphs&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png"&gt;ğŸ¯&lt;/g-emoji&gt; One-shot Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="card_file_box" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5c3.png"&gt;ğŸ—ƒï¸&lt;/g-emoji&gt; Interpretability&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://practicalai.me/#newsletter" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="mailbox_with_mail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ec.png"&gt;ğŸ“¬&lt;/g-emoji&gt; Newsletter&lt;/a&gt; - Subscribe to get updates on new content&lt;/p&gt;
&lt;div&gt;
&lt;a href="https://twitter.com/practicalai_me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6729390835283dc393cb7f500d840be48cdac6c6/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f74772e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/tw.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.youtube.com/channel/UCgW4K2UDK21kHIzxpjNos7Q" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/786df4e8cd9d5421583bf063cdcf9e3d06d14de5/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f7974622e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/ytb.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;&lt;img src="https://camo.githubusercontent.com/082d52273e2945cb5126868e9cc44d4c91ffe912/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f677468622e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/gthb.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/045b09864f16b18059d20d17bfaf906b26ca3aae/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f6c6e6b646e2e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/lnkdn.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>practicalAI</author><guid isPermaLink="false">https://github.com/practicalAI/practicalAI</guid><pubDate>Mon, 18 Nov 2019 00:04:00 GMT</pubDate></item><item><title>jackfrued/Python-100-Days #5 in Jupyter Notebook, This week</title><link>https://github.com/jackfrued/Python-100-Days</link><description>&lt;p&gt;&lt;i&gt;Python - 100å¤©ä»æ–°æ‰‹åˆ°å¤§å¸ˆ&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python---100å¤©ä»æ–°æ‰‹åˆ°å¤§å¸ˆ" class="anchor" aria-hidden="true" href="#python---100å¤©ä»æ–°æ‰‹åˆ°å¤§å¸ˆ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python - 100å¤©ä»æ–°æ‰‹åˆ°å¤§å¸ˆ&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;ä½œè€…ï¼šéª†æ˜Š&lt;/p&gt;
&lt;p&gt;æœ€è¿‘æœ‰å¾ˆå¤šæƒ³å­¦ä¹ Pythonçš„å°ä¼™ä¼´é™†é™†ç»­ç»­åŠ å…¥æˆ‘ä»¬çš„äº¤æµç¾¤ï¼Œç›®å‰æˆ‘ä»¬çš„äº¤æµç¾¤äººæ•°å·²ç»è¶…è¿‡ä¸€ä¸‡äººã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰“é€ ä¸€ä¸ªä¼˜è´¨çš„Pythonäº¤æµç¤¾åŒºï¼Œä¸€æ–¹é¢ä¸ºæƒ³å­¦ä¹ Pythonçš„åˆå­¦è€…æ‰«å¹³å…¥é—¨è¿‡ç¨‹ä¸­çš„é‡é‡éšœç¢ï¼›å¦ä¸€æ–¹ä¸ºæ–°å…¥è¡Œçš„å¼€å‘è€…æä¾›é—®é“çš„é€”å¾„ï¼Œå¸®åŠ©ä»–ä»¬è¿…é€Ÿæˆé•¿ä¸ºä¼˜ç§€çš„èŒä¸šäººï¼›æ­¤å¤–ï¼Œæœ‰ç»éªŒçš„å¼€å‘è€…å¯ä»¥åˆ©ç”¨è¿™ä¸ªå¹³å°æŠŠè‡ªå·±çš„å·¥ä½œç»éªŒæ— å¿åˆ†äº«æˆ–æœ‰å¿æä¾›å‡ºæ¥ï¼Œè®©å¤§å®¶éƒ½èƒ½å¤Ÿå¾—åˆ°èŒä¸šæŠ€èƒ½ä»¥åŠç»¼åˆç´ è´¨çš„å…¨é¢æå‡ã€‚ä¹‹å‰çš„å…¬å¼€è¯¾å’Œçº¿ä¸‹æŠ€æœ¯äº¤æµæ´»åŠ¨å› ä¸ºå·¥ä½œçš„å…³ç³»è’åºŸäº†ä¸€æ®µæ—¶é—´äº†ï¼Œä½†æ˜¯å„ä½å°ä¼™ä¼´ä»ç„¶æ´»è·ƒåœ¨äº¤æµç¾¤å¹¶ä¸€å¦‚æ—¢å¾€çš„æ”¯æŒæˆ‘ä»¬ï¼Œåœ¨æ­¤å‘å¤§å®¶è¡¨ç¤ºæ„Ÿè°¢ã€‚è¿‘æœŸå¼€å§‹æŒç»­æ›´æ–°å‰15å¤©å’Œæœ€å10å¤©çš„å†…å®¹ï¼Œå‰15å¤©æ˜¯å†™ç»™åˆå­¦è€…çš„ï¼Œæˆ‘å¸Œæœ›æŠŠä¸Šæ‰‹çš„éš¾åº¦è¿›ä¸€æ­¥é™ä½ï¼Œä¾‹å­ç¨‹åºæ›´åŠ ç®€å•æ¸…æ™°ï¼›æœ€å10å¤©æ˜¯Pythoné¡¹ç›®å®æˆ˜å’Œé¢è¯•ç›¸å…³çš„ä¸œè¥¿ï¼Œæˆ‘å¸Œæœ›å†…å®¹æ›´è¯¦å®å’Œå®Œæ•´ï¼Œå°¤å…¶æ˜¯ç¬¬100å¤©çš„é¢è¯•é¢˜éƒ¨åˆ†ï¼›åˆ›ä½œä¸æ˜“ï¼Œæ„Ÿè°¢å¤§å®¶çš„æ‰“èµæ”¯æŒï¼Œè¿™äº›é’±ä¸ä¼šç”¨äºè´­ä¹°å’–å•¡è€Œæ˜¯é€šè¿‡è…¾è®¯å…¬ç›Šå¹³å°æèµ ç»™éœ€è¦å¸®åŠ©çš„äººï¼ˆ&lt;a href="./%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.md"&gt;ç‚¹å‡»&lt;/a&gt;äº†è§£æèµ æƒ…å†µï¼‰ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-qq-group.png"&gt;&lt;img src="./res/python-qq-group.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pythonåº”ç”¨é¢†åŸŸå’Œå°±ä¸šå½¢åŠ¿åˆ†æ" class="anchor" aria-hidden="true" href="#pythonåº”ç”¨é¢†åŸŸå’Œå°±ä¸šå½¢åŠ¿åˆ†æ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pythonåº”ç”¨é¢†åŸŸå’Œå°±ä¸šå½¢åŠ¿åˆ†æ&lt;/h3&gt;
&lt;p&gt;ç®€å•çš„è¯´ï¼ŒPythonæ˜¯ä¸€ä¸ªâ€œä¼˜é›…â€ã€â€œæ˜ç¡®â€ã€â€œç®€å•â€çš„ç¼–ç¨‹è¯­è¨€ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å­¦ä¹ æ›²çº¿ä½ï¼Œéä¸“ä¸šäººå£«ä¹Ÿèƒ½ä¸Šæ‰‹&lt;/li&gt;
&lt;li&gt;å¼€æºç³»ç»Ÿï¼Œæ‹¥æœ‰å¼ºå¤§çš„ç”Ÿæ€åœˆ&lt;/li&gt;
&lt;li&gt;è§£é‡Šå‹è¯­è¨€ï¼Œå®Œç¾çš„å¹³å°å¯ç§»æ¤æ€§&lt;/li&gt;
&lt;li&gt;æ”¯æŒé¢å‘å¯¹è±¡å’Œå‡½æ•°å¼ç¼–ç¨‹&lt;/li&gt;
&lt;li&gt;èƒ½å¤Ÿé€šè¿‡è°ƒç”¨C/C++ä»£ç æ‰©å±•åŠŸèƒ½&lt;/li&gt;
&lt;li&gt;ä»£ç è§„èŒƒç¨‹åº¦é«˜ï¼Œå¯è¯»æ€§å¼º&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç›®å‰å‡ ä¸ªæ¯”è¾ƒæµè¡Œçš„é¢†åŸŸï¼ŒPythonéƒ½æœ‰ç”¨æ­¦ä¹‹åœ°ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;äº‘åŸºç¡€è®¾æ–½ - Python / Java / Go&lt;/li&gt;
&lt;li&gt;DevOps - Python / Shell / Ruby / Go&lt;/li&gt;
&lt;li&gt;ç½‘ç»œçˆ¬è™« - Python / PHP / C++&lt;/li&gt;
&lt;li&gt;æ•°æ®åˆ†ææŒ–æ˜ - Python / R / Scala / Matlab&lt;/li&gt;
&lt;li&gt;æœºå™¨å­¦ä¹  - Python / R / Java / Lisp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ä½œä¸ºä¸€åPythonå¼€å‘è€…ï¼Œä¸»è¦çš„å°±ä¸šé¢†åŸŸåŒ…æ‹¬ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PythonæœåŠ¡å™¨åå°å¼€å‘ / æ¸¸æˆæœåŠ¡å™¨å¼€å‘ / æ•°æ®æ¥å£å¼€å‘å·¥ç¨‹å¸ˆ&lt;/li&gt;
&lt;li&gt;Pythonè‡ªåŠ¨åŒ–è¿ç»´å·¥ç¨‹å¸ˆ&lt;/li&gt;
&lt;li&gt;Pythonæ•°æ®åˆ†æ / æ•°æ®å¯è§†åŒ– / å¤§æ•°æ®å·¥ç¨‹å¸ˆ&lt;/li&gt;
&lt;li&gt;Pythonçˆ¬è™«å·¥ç¨‹å¸ˆ&lt;/li&gt;
&lt;li&gt;PythonèŠå¤©æœºå™¨äººå¼€å‘ / å›¾åƒè¯†åˆ«å’Œè§†è§‰ç®—æ³• / æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ä¸‹å›¾æ˜¾ç¤ºäº†ä¸»è¦åŸå¸‚Pythonæ‹›è˜éœ€æ±‚é‡åŠè–ªèµ„å¾…é‡æ’è¡Œæ¦œï¼ˆæˆªæ­¢åˆ°2018å¹´5æœˆï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-top-10.png"&gt;&lt;img src="./res/python-top-10.png" alt="Pythonæ‹›è˜éœ€æ±‚åŠè–ªèµ„å¾…é‡Top 10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-bj-salary.png"&gt;&lt;img src="./res/python-bj-salary.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-salary-chengdu.png"&gt;&lt;img src="./res/python-salary-chengdu.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ç»™åˆå­¦è€…çš„å‡ ä¸ªå»ºè®®ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make English as your working language.&lt;/li&gt;
&lt;li&gt;Practice makes perfect.&lt;/li&gt;
&lt;li&gt;All experience comes from mistakes.&lt;/li&gt;
&lt;li&gt;Don't be one of the leeches.&lt;/li&gt;
&lt;li&gt;Either stand out or kicked out.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day0115---pythonè¯­è¨€åŸºç¡€" class="anchor" aria-hidden="true" href="#day0115---pythonè¯­è¨€åŸºç¡€"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01~15 - &lt;a href="./Day01-15"&gt;Pythonè¯­è¨€åŸºç¡€&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day01---åˆè¯†python" class="anchor" aria-hidden="true" href="#day01---åˆè¯†python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01 - &lt;a href="./Day01-15/01.%E5%88%9D%E8%AF%86Python.md"&gt;åˆè¯†Python&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pythonç®€ä»‹ - Pythonçš„å†å² / Pythonçš„ä¼˜ç¼ºç‚¹ / Pythonçš„åº”ç”¨é¢†åŸŸ&lt;/li&gt;
&lt;li&gt;æ­å»ºç¼–ç¨‹ç¯å¢ƒ - Windowsç¯å¢ƒ / Linuxç¯å¢ƒ / MacOSç¯å¢ƒ&lt;/li&gt;
&lt;li&gt;ä»ç»ˆç«¯è¿è¡ŒPythonç¨‹åº - Hello, world / printå‡½æ•° / è¿è¡Œç¨‹åº&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨IDLE - äº¤äº’å¼ç¯å¢ƒ(REPL) / ç¼–å†™å¤šè¡Œä»£ç  / è¿è¡Œç¨‹åº / é€€å‡ºIDLE&lt;/li&gt;
&lt;li&gt;æ³¨é‡Š - æ³¨é‡Šçš„ä½œç”¨ / å•è¡Œæ³¨é‡Š / å¤šè¡Œæ³¨é‡Š&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day02---è¯­è¨€å…ƒç´ " class="anchor" aria-hidden="true" href="#day02---è¯­è¨€å…ƒç´ "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day02 - &lt;a href="./Day01-15/02.%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0.md"&gt;è¯­è¨€å…ƒç´ &lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ç¨‹åºå’Œè¿›åˆ¶ - æŒ‡ä»¤å’Œç¨‹åº / å†¯è¯ºä¾æ›¼æœº / äºŒè¿›åˆ¶å’Œåè¿›åˆ¶ / å…«è¿›åˆ¶å’Œåå…­è¿›åˆ¶&lt;/li&gt;
&lt;li&gt;å˜é‡å’Œç±»å‹ - å˜é‡çš„å‘½å / å˜é‡çš„ä½¿ç”¨ / inputå‡½æ•° / æ£€æŸ¥å˜é‡ç±»å‹ / ç±»å‹è½¬æ¢&lt;/li&gt;
&lt;li&gt;æ•°å­—å’Œå­—ç¬¦ä¸² - æ•´æ•° / æµ®ç‚¹æ•° / å¤æ•° / å­—ç¬¦ä¸² / å­—ç¬¦ä¸²åŸºæœ¬æ“ä½œ / å­—ç¬¦ç¼–ç &lt;/li&gt;
&lt;li&gt;è¿ç®—ç¬¦ - æ•°å­¦è¿ç®—ç¬¦ / èµ‹å€¼è¿ç®—ç¬¦ / æ¯”è¾ƒè¿ç®—ç¬¦ / é€»è¾‘è¿ç®—ç¬¦ / èº«ä»½è¿ç®—ç¬¦ / è¿ç®—ç¬¦çš„ä¼˜å…ˆçº§&lt;/li&gt;
&lt;li&gt;åº”ç”¨æ¡ˆä¾‹ - åæ°æ¸©åº¦è½¬æ¢æˆæ‘„æ°æ¸©åº¦ / è¾“å…¥åœ†çš„åŠå¾„è®¡ç®—å‘¨é•¿å’Œé¢ç§¯ / è¾“å…¥å¹´ä»½åˆ¤æ–­æ˜¯å¦æ˜¯é—°å¹´&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day03---åˆ†æ”¯ç»“æ„" class="anchor" aria-hidden="true" href="#day03---åˆ†æ”¯ç»“æ„"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day03 - &lt;a href="./Day01-15/03.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md"&gt;åˆ†æ”¯ç»“æ„&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;åˆ†æ”¯ç»“æ„çš„åº”ç”¨åœºæ™¯ - æ¡ä»¶ / ç¼©è¿› / ä»£ç å— / æµç¨‹å›¾&lt;/li&gt;
&lt;li&gt;ifè¯­å¥ - ç®€å•çš„if / if-elseç»“æ„ / if-elif-elseç»“æ„ / åµŒå¥—çš„if&lt;/li&gt;
&lt;li&gt;åº”ç”¨æ¡ˆä¾‹ - ç”¨æˆ·èº«ä»½éªŒè¯ / è‹±åˆ¶å•ä½ä¸å…¬åˆ¶å•ä½äº’æ¢ / æ·éª°å­å†³å®šåšä»€ä¹ˆ / ç™¾åˆ†åˆ¶æˆç»©è½¬ç­‰çº§åˆ¶ / åˆ†æ®µå‡½æ•°æ±‚å€¼ / è¾“å…¥ä¸‰æ¡è¾¹çš„é•¿åº¦å¦‚æœèƒ½æ„æˆä¸‰è§’å½¢å°±è®¡ç®—å‘¨é•¿å’Œé¢ç§¯&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day04---å¾ªç¯ç»“æ„" class="anchor" aria-hidden="true" href="#day04---å¾ªç¯ç»“æ„"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day04 - &lt;a href="./Day01-15/04.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md"&gt;å¾ªç¯ç»“æ„&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å¾ªç¯ç»“æ„çš„åº”ç”¨åœºæ™¯ - æ¡ä»¶ / ç¼©è¿› / ä»£ç å— / æµç¨‹å›¾&lt;/li&gt;
&lt;li&gt;whileå¾ªç¯ - åŸºæœ¬ç»“æ„ / breakè¯­å¥ / continueè¯­å¥&lt;/li&gt;
&lt;li&gt;forå¾ªç¯ - åŸºæœ¬ç»“æ„ / rangeç±»å‹ / å¾ªç¯ä¸­çš„åˆ†æ”¯ç»“æ„ / åµŒå¥—çš„å¾ªç¯ / æå‰ç»“æŸç¨‹åº&lt;/li&gt;
&lt;li&gt;åº”ç”¨æ¡ˆä¾‹ - 1~100æ±‚å’Œ / åˆ¤æ–­ç´ æ•° / çŒœæ•°å­—æ¸¸æˆ / æ‰“å°ä¹ä¹è¡¨ / æ‰“å°ä¸‰è§’å½¢å›¾æ¡ˆ / çŒ´å­åƒæ¡ƒ / ç™¾é’±ç™¾é¸¡&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day05---æ„é€ ç¨‹åºé€»è¾‘" class="anchor" aria-hidden="true" href="#day05---æ„é€ ç¨‹åºé€»è¾‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day05 - &lt;a href="./Day01-15/05.%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91.md"&gt;æ„é€ ç¨‹åºé€»è¾‘&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ç»å…¸æ¡ˆä¾‹ï¼šæ°´ä»™èŠ±æ•° / ç™¾é’±ç™¾é¸¡ / CrapsèµŒåšæ¸¸æˆ&lt;/li&gt;
&lt;li&gt;ç»ƒä¹ é¢˜ç›®ï¼šæ–æ³¢é‚£å¥‘æ•°åˆ— / å®Œç¾æ•° / ç´ æ•°&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day06---å‡½æ•°å’Œæ¨¡å—çš„ä½¿ç”¨" class="anchor" aria-hidden="true" href="#day06---å‡½æ•°å’Œæ¨¡å—çš„ä½¿ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day06 - &lt;a href="./Day01-15/06.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;å‡½æ•°å’Œæ¨¡å—çš„ä½¿ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å‡½æ•°çš„ä½œç”¨ - ä»£ç çš„åå‘³é“ / ç”¨å‡½æ•°å°è£…åŠŸèƒ½æ¨¡å—&lt;/li&gt;
&lt;li&gt;å®šä¹‰å‡½æ•° - defè¯­å¥ / å‡½æ•°å / å‚æ•°åˆ—è¡¨ / returnè¯­å¥ / è°ƒç”¨è‡ªå®šä¹‰å‡½æ•°&lt;/li&gt;
&lt;li&gt;è°ƒç”¨å‡½æ•° - Pythonå†…ç½®å‡½æ•° /  å¯¼å…¥æ¨¡å—å’Œå‡½æ•°&lt;/li&gt;
&lt;li&gt;å‡½æ•°çš„å‚æ•° - é»˜è®¤å‚æ•° / å¯å˜å‚æ•° / å…³é”®å­—å‚æ•° / å‘½åå…³é”®å­—å‚æ•°&lt;/li&gt;
&lt;li&gt;å‡½æ•°çš„è¿”å›å€¼ - æ²¡æœ‰è¿”å›å€¼  / è¿”å›å•ä¸ªå€¼ / è¿”å›å¤šä¸ªå€¼&lt;/li&gt;
&lt;li&gt;ä½œç”¨åŸŸé—®é¢˜ - å±€éƒ¨ä½œç”¨åŸŸ / åµŒå¥—ä½œç”¨åŸŸ / å…¨å±€ä½œç”¨åŸŸ / å†…ç½®ä½œç”¨åŸŸ / å’Œä½œç”¨åŸŸç›¸å…³çš„å…³é”®å­—&lt;/li&gt;
&lt;li&gt;ç”¨æ¨¡å—ç®¡ç†å‡½æ•° - æ¨¡å—çš„æ¦‚å¿µ / ç”¨è‡ªå®šä¹‰æ¨¡å—ç®¡ç†å‡½æ•° / å‘½åå†²çªçš„æ—¶å€™ä¼šæ€æ ·ï¼ˆåŒä¸€ä¸ªæ¨¡å—å’Œä¸åŒçš„æ¨¡å—ï¼‰&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day07---å­—ç¬¦ä¸²å’Œå¸¸ç”¨æ•°æ®ç»“æ„" class="anchor" aria-hidden="true" href="#day07---å­—ç¬¦ä¸²å’Œå¸¸ç”¨æ•°æ®ç»“æ„"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day07 - &lt;a href="./Day01-15/07.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md"&gt;å­—ç¬¦ä¸²å’Œå¸¸ç”¨æ•°æ®ç»“æ„&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å­—ç¬¦ä¸²çš„ä½¿ç”¨ - è®¡ç®—é•¿åº¦ / ä¸‹æ ‡è¿ç®— / åˆ‡ç‰‡ / å¸¸ç”¨æ–¹æ³•&lt;/li&gt;
&lt;li&gt;åˆ—è¡¨åŸºæœ¬ç”¨æ³• - å®šä¹‰åˆ—è¡¨ / ç”¨ä¸‹è¡¨è®¿é—®å…ƒç´  / ä¸‹æ ‡è¶Šç•Œ / æ·»åŠ å…ƒç´  / åˆ é™¤å…ƒç´  / ä¿®æ”¹å…ƒç´  / åˆ‡ç‰‡ / å¾ªç¯éå†&lt;/li&gt;
&lt;li&gt;åˆ—è¡¨å¸¸ç”¨æ“ä½œ - è¿æ¥ / å¤åˆ¶(å¤åˆ¶å…ƒç´ å’Œå¤åˆ¶æ•°ç»„) / é•¿åº¦ / æ’åº / å€’è½¬ / æŸ¥æ‰¾&lt;/li&gt;
&lt;li&gt;ç”Ÿæˆåˆ—è¡¨ - ä½¿ç”¨rangeåˆ›å»ºæ•°å­—åˆ—è¡¨ / ç”Ÿæˆè¡¨è¾¾å¼ / ç”Ÿæˆå™¨&lt;/li&gt;
&lt;li&gt;å…ƒç»„çš„ä½¿ç”¨ - å®šä¹‰å…ƒç»„ / ä½¿ç”¨å…ƒç»„ä¸­çš„å€¼ / ä¿®æ”¹å…ƒç»„å˜é‡ / å…ƒç»„å’Œåˆ—è¡¨è½¬æ¢&lt;/li&gt;
&lt;li&gt;é›†åˆåŸºæœ¬ç”¨æ³• - é›†åˆå’Œåˆ—è¡¨çš„åŒºåˆ« /  åˆ›å»ºé›†åˆ / æ·»åŠ å…ƒç´  / åˆ é™¤å…ƒç´  /  æ¸…ç©º&lt;/li&gt;
&lt;li&gt;é›†åˆå¸¸ç”¨æ“ä½œ - äº¤é›† / å¹¶é›† / å·®é›† / å¯¹ç§°å·® / å­é›† / è¶…é›†&lt;/li&gt;
&lt;li&gt;å­—å…¸çš„åŸºæœ¬ç”¨æ³• - å­—å…¸çš„ç‰¹ç‚¹ / åˆ›å»ºå­—å…¸ / æ·»åŠ å…ƒç´  / åˆ é™¤å…ƒç´  / å–å€¼ / æ¸…ç©º&lt;/li&gt;
&lt;li&gt;å­—å…¸å¸¸ç”¨æ“ä½œ - keys()æ–¹æ³• / values()æ–¹æ³• / items()æ–¹æ³• / setdefault()æ–¹æ³•&lt;/li&gt;
&lt;li&gt;åŸºç¡€ç»ƒä¹  - è·‘é©¬ç¯æ•ˆæœ / åˆ—è¡¨æ‰¾æœ€å¤§å…ƒç´  / ç»Ÿè®¡è€ƒè¯•æˆç»©çš„å¹³å‡åˆ† / Fibonacciæ•°åˆ— / æ¨è¾‰ä¸‰è§’&lt;/li&gt;
&lt;li&gt;ç»¼åˆæ¡ˆä¾‹ - åŒè‰²çƒé€‰å· / äº•å­—æ£‹&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day08---é¢å‘å¯¹è±¡ç¼–ç¨‹åŸºç¡€" class="anchor" aria-hidden="true" href="#day08---é¢å‘å¯¹è±¡ç¼–ç¨‹åŸºç¡€"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day08 - &lt;a href="./Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80.md"&gt;é¢å‘å¯¹è±¡ç¼–ç¨‹åŸºç¡€&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ç±»å’Œå¯¹è±¡ - ä»€ä¹ˆæ˜¯ç±» / ä»€ä¹ˆæ˜¯å¯¹è±¡ / é¢å‘å¯¹è±¡å…¶ä»–ç›¸å…³æ¦‚å¿µ&lt;/li&gt;
&lt;li&gt;å®šä¹‰ç±» - åŸºæœ¬ç»“æ„ / å±æ€§å’Œæ–¹æ³• / æ„é€ å™¨ / ææ„å™¨ / __str__æ–¹æ³•&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨å¯¹è±¡ - åˆ›å»ºå¯¹è±¡ / ç»™å¯¹è±¡å‘æ¶ˆæ¯&lt;/li&gt;
&lt;li&gt;é¢å‘å¯¹è±¡çš„å››å¤§æ”¯æŸ± - æŠ½è±¡ / å°è£… / ç»§æ‰¿ / å¤šæ€&lt;/li&gt;
&lt;li&gt;åŸºç¡€ç»ƒä¹  - å®šä¹‰å­¦ç”Ÿç±» / å®šä¹‰æ—¶é’Ÿç±» / å®šä¹‰å›¾å½¢ç±» / å®šä¹‰æ±½è½¦ç±»&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day09---é¢å‘å¯¹è±¡è¿›é˜¶" class="anchor" aria-hidden="true" href="#day09---é¢å‘å¯¹è±¡è¿›é˜¶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day09 - &lt;a href="./Day01-15/09.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6.md"&gt;é¢å‘å¯¹è±¡è¿›é˜¶&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å±æ€§ - ç±»å±æ€§ / å®ä¾‹å±æ€§ / å±æ€§è®¿é—®å™¨ / å±æ€§ä¿®æ”¹å™¨ / å±æ€§åˆ é™¤å™¨ / ä½¿ç”¨__slots__&lt;/li&gt;
&lt;li&gt;ç±»ä¸­çš„æ–¹æ³• - å®ä¾‹æ–¹æ³• / ç±»æ–¹æ³• / é™æ€æ–¹æ³•&lt;/li&gt;
&lt;li&gt;è¿ç®—ç¬¦é‡è½½ - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__&lt;/li&gt;
&lt;li&gt;ç±»(çš„å¯¹è±¡)ä¹‹é—´çš„å…³ç³» - å…³è” / ç»§æ‰¿ / ä¾èµ–&lt;/li&gt;
&lt;li&gt;ç»§æ‰¿å’Œå¤šæ€ - ä»€ä¹ˆæ˜¯ç»§æ‰¿ / ç»§æ‰¿çš„è¯­æ³• / è°ƒç”¨çˆ¶ç±»æ–¹æ³• / æ–¹æ³•é‡å†™ / ç±»å‹åˆ¤å®š / å¤šé‡ç»§æ‰¿ / è±å½¢ç»§æ‰¿(é’»çŸ³ç»§æ‰¿)å’ŒC3ç®—æ³•&lt;/li&gt;
&lt;li&gt;ç»¼åˆæ¡ˆä¾‹ - å·¥èµ„ç»“ç®—ç³»ç»Ÿ / å›¾ä¹¦è‡ªåŠ¨æŠ˜æ‰£ç³»ç»Ÿ / è‡ªå®šä¹‰åˆ†æ•°ç±»&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day10---å›¾å½¢ç”¨æˆ·ç•Œé¢å’Œæ¸¸æˆå¼€å‘" class="anchor" aria-hidden="true" href="#day10---å›¾å½¢ç”¨æˆ·ç•Œé¢å’Œæ¸¸æˆå¼€å‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day10 - &lt;a href="./Day01-15/10.%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91.md"&gt;å›¾å½¢ç”¨æˆ·ç•Œé¢å’Œæ¸¸æˆå¼€å‘&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨tkinterå¼€å‘GUIç¨‹åº&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨pygameä¸‰æ–¹åº“å¼€å‘æ¸¸æˆåº”ç”¨&lt;/li&gt;
&lt;li&gt;â€œå¤§çƒåƒå°çƒâ€æ¸¸æˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day11---æ–‡ä»¶å’Œå¼‚å¸¸" class="anchor" aria-hidden="true" href="#day11---æ–‡ä»¶å’Œå¼‚å¸¸"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day11 - &lt;a href="./Day01-15/11.%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8.md"&gt;æ–‡ä»¶å’Œå¼‚å¸¸&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;è¯»æ–‡ä»¶ - è¯»å–æ•´ä¸ªæ–‡ä»¶ / é€è¡Œè¯»å– / æ–‡ä»¶è·¯å¾„&lt;/li&gt;
&lt;li&gt;å†™æ–‡ä»¶ - è¦†ç›–å†™å…¥ / è¿½åŠ å†™å…¥ / æ–‡æœ¬æ–‡ä»¶ / äºŒè¿›åˆ¶æ–‡ä»¶&lt;/li&gt;
&lt;li&gt;å¼‚å¸¸å¤„ç† - å¼‚å¸¸æœºåˆ¶çš„é‡è¦æ€§ / try-exceptä»£ç å— / elseä»£ç å— / finallyä»£ç å— / å†…ç½®å¼‚å¸¸ç±»å‹ / å¼‚å¸¸æ ˆ / raiseè¯­å¥&lt;/li&gt;
&lt;li&gt;æ•°æ®æŒä¹…åŒ– - CSVæ–‡ä»¶æ¦‚è¿° / csvæ¨¡å—çš„åº”ç”¨ / JSONæ•°æ®æ ¼å¼ / jsonæ¨¡å—çš„åº”ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day12---å­—ç¬¦ä¸²å’Œæ­£åˆ™è¡¨è¾¾å¼" class="anchor" aria-hidden="true" href="#day12---å­—ç¬¦ä¸²å’Œæ­£åˆ™è¡¨è¾¾å¼"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day12 - &lt;a href="./Day01-15/12.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.md"&gt;å­—ç¬¦ä¸²å’Œæ­£åˆ™è¡¨è¾¾å¼&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å­—ç¬¦ä¸²é«˜çº§æ“ä½œ - è½¬ä¹‰å­—ç¬¦ / åŸå§‹å­—ç¬¦ä¸² / å¤šè¡Œå­—ç¬¦ä¸² / inå’Œ not inè¿ç®—ç¬¦ / iså¼€å¤´çš„æ–¹æ³• / joinå’Œsplitæ–¹æ³• / stripç›¸å…³æ–¹æ³• / pyperclipæ¨¡å— / ä¸å˜å­—ç¬¦ä¸²å’Œå¯å˜å­—ç¬¦ä¸² / StringIOçš„ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;æ­£åˆ™è¡¨è¾¾å¼å…¥é—¨ - æ­£åˆ™è¡¨è¾¾å¼çš„ä½œç”¨ / å…ƒå­—ç¬¦ / è½¬ä¹‰ / é‡è¯ / åˆ†ç»„ / é›¶å®½æ–­è¨€ /è´ªå©ªåŒ¹é…ä¸æƒ°æ€§åŒ¹é…æ‡’æƒ° / ä½¿ç”¨reæ¨¡å—å®ç°æ­£åˆ™è¡¨è¾¾å¼æ“ä½œï¼ˆåŒ¹é…ã€æœç´¢ã€æ›¿æ¢ã€æ•è·ï¼‰&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ - reæ¨¡å— / compileå‡½æ•° / groupå’Œgroupsæ–¹æ³• / matchæ–¹æ³• / searchæ–¹æ³• / findallå’Œfinditeræ–¹æ³• / subå’Œsubnæ–¹æ³• / splitæ–¹æ³•&lt;/li&gt;
&lt;li&gt;åº”ç”¨æ¡ˆä¾‹ - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼éªŒè¯è¾“å…¥çš„å­—ç¬¦ä¸²&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day13---è¿›ç¨‹å’Œçº¿ç¨‹" class="anchor" aria-hidden="true" href="#day13---è¿›ç¨‹å’Œçº¿ç¨‹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day13 - &lt;a href="./Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B.md"&gt;è¿›ç¨‹å’Œçº¿ç¨‹&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;è¿›ç¨‹å’Œçº¿ç¨‹çš„æ¦‚å¿µ - ä»€ä¹ˆæ˜¯è¿›ç¨‹ / ä»€ä¹ˆæ˜¯çº¿ç¨‹ / å¤šçº¿ç¨‹çš„åº”ç”¨åœºæ™¯&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨è¿›ç¨‹ - forkå‡½æ•° / multiprocessingæ¨¡å— / è¿›ç¨‹æ±  / è¿›ç¨‹é—´é€šä¿¡&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨çº¿ç¨‹ - threadæ¨¡å— / threadingæ¨¡å— / Threadç±» / Lockç±» / Conditionç±» / çº¿ç¨‹æ± &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day14---ç½‘ç»œç¼–ç¨‹å…¥é—¨å’Œç½‘ç»œåº”ç”¨å¼€å‘" class="anchor" aria-hidden="true" href="#day14---ç½‘ç»œç¼–ç¨‹å…¥é—¨å’Œç½‘ç»œåº”ç”¨å¼€å‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day14 - &lt;a href="./Day01-15/14.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91.md"&gt;ç½‘ç»œç¼–ç¨‹å…¥é—¨å’Œç½‘ç»œåº”ç”¨å¼€å‘&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;è®¡ç®—æœºç½‘ç»œåŸºç¡€ - è®¡ç®—æœºç½‘ç»œå‘å±•å² / â€œTCP-IPâ€æ¨¡å‹ / IPåœ°å€ / ç«¯å£ / åè®® / å…¶ä»–ç›¸å…³æ¦‚å¿µ&lt;/li&gt;
&lt;li&gt;ç½‘ç»œåº”ç”¨æ¨¡å¼ - â€œå®¢æˆ·ç«¯-æœåŠ¡å™¨â€æ¨¡å¼ / â€œæµè§ˆå™¨-æœåŠ¡å™¨â€æ¨¡å¼&lt;/li&gt;
&lt;li&gt;åŸºäºHTTPåè®®è®¿é—®ç½‘ç»œèµ„æº - ç½‘ç»œAPIæ¦‚è¿° / è®¿é—®URL / requestsæ¨¡å— / è§£æJSONæ ¼å¼æ•°æ®&lt;/li&gt;
&lt;li&gt;Pythonç½‘ç»œç¼–ç¨‹ - å¥—æ¥å­—çš„æ¦‚å¿µ / socketæ¨¡å— /  socketå‡½æ•° / åˆ›å»ºTCPæœåŠ¡å™¨ / åˆ›å»ºTCPå®¢æˆ·ç«¯ / åˆ›å»ºUDPæœåŠ¡å™¨ / åˆ›å»ºUDPå®¢æˆ·ç«¯ / SocketServeræ¨¡å—&lt;/li&gt;
&lt;li&gt;ç”µå­é‚®ä»¶ - SMTPåè®® / POP3åè®® / IMAPåè®® / smtplibæ¨¡å— / poplibæ¨¡å— / imaplibæ¨¡å—&lt;/li&gt;
&lt;li&gt;çŸ­ä¿¡æœåŠ¡ - è°ƒç”¨çŸ­ä¿¡æœåŠ¡ç½‘å…³&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day15---å›¾åƒå’Œæ–‡æ¡£å¤„ç†" class="anchor" aria-hidden="true" href="#day15---å›¾åƒå’Œæ–‡æ¡£å¤„ç†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day15 - &lt;a href="./Day01-15/15.%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86.md"&gt;å›¾åƒå’Œæ–‡æ¡£å¤„ç†&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨Pillowå¤„ç†å›¾ç‰‡ - å›¾ç‰‡è¯»å†™ / å›¾ç‰‡åˆæˆ / å‡ ä½•å˜æ¢ / è‰²å½©è½¬æ¢ / æ»¤é•œæ•ˆæœ&lt;/li&gt;
&lt;li&gt;è¯»å†™Wordæ–‡æ¡£ - æ–‡æœ¬å†…å®¹çš„å¤„ç† / æ®µè½ / é¡µçœ‰å’Œé¡µè„š / æ ·å¼çš„å¤„ç†&lt;/li&gt;
&lt;li&gt;è¯»å†™Excelæ–‡ä»¶ - xlrdæ¨¡å— / xlwtæ¨¡å—&lt;/li&gt;
&lt;li&gt;ç”ŸæˆPDFæ–‡ä»¶ - pypdf2æ¨¡å— / reportlabæ¨¡å—&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day16day20---pythonè¯­è¨€è¿›é˜¶-" class="anchor" aria-hidden="true" href="#day16day20---pythonè¯­è¨€è¿›é˜¶-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day16~Day20 - &lt;a href="./Day16-20/16-20.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md"&gt;Pythonè¯­è¨€è¿›é˜¶ &lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;å¸¸ç”¨æ•°æ®ç»“æ„&lt;/li&gt;
&lt;li&gt;å‡½æ•°çš„é«˜çº§ç”¨æ³• - â€œä¸€ç­‰å…¬æ°‘â€ / é«˜é˜¶å‡½æ•° / Lambdaå‡½æ•° / ä½œç”¨åŸŸå’Œé—­åŒ… / è£…é¥°å™¨&lt;/li&gt;
&lt;li&gt;é¢å‘å¯¹è±¡é«˜çº§çŸ¥è¯† - â€œä¸‰å¤§æ”¯æŸ±â€ / ç±»ä¸ç±»ä¹‹é—´çš„å…³ç³» / åƒåœ¾å›æ”¶ / é­”æœ¯å±æ€§å’Œæ–¹æ³• / æ··å…¥ / å…ƒç±» / é¢å‘å¯¹è±¡è®¾è®¡åŸåˆ™ / GoFè®¾è®¡æ¨¡å¼&lt;/li&gt;
&lt;li&gt;è¿­ä»£å™¨å’Œç”Ÿæˆå™¨ - ç›¸å…³é­”æœ¯æ–¹æ³• / åˆ›å»ºç”Ÿæˆå™¨çš„ä¸¤ç§æ–¹å¼ /&lt;/li&gt;
&lt;li&gt;å¹¶å‘å’Œå¼‚æ­¥ç¼–ç¨‹ - å¤šçº¿ç¨‹ / å¤šè¿›ç¨‹ / å¼‚æ­¥IO / asyncå’Œawait&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day2130---webå‰ç«¯å…¥é—¨" class="anchor" aria-hidden="true" href="#day2130---webå‰ç«¯å…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day21~30 - &lt;a href="./Day21-30/21-30.Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0.md"&gt;Webå‰ç«¯å…¥é—¨&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨HTMLæ ‡ç­¾æ‰¿è½½é¡µé¢å†…å®¹&lt;/li&gt;
&lt;li&gt;ç”¨CSSæ¸²æŸ“é¡µé¢&lt;/li&gt;
&lt;li&gt;ç”¨JavaScriptå¤„ç†äº¤äº’å¼è¡Œä¸º&lt;/li&gt;
&lt;li&gt;jQueryå…¥é—¨å’Œæé«˜&lt;/li&gt;
&lt;li&gt;Vue.jså…¥é—¨&lt;/li&gt;
&lt;li&gt;Elementçš„ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;Bootstrapçš„ä½¿ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3135---ç©è½¬linuxæ“ä½œç³»ç»Ÿ" class="anchor" aria-hidden="true" href="#day3135---ç©è½¬linuxæ“ä½œç³»ç»Ÿ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day31~35 - &lt;a href="./Day31-35/31-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md"&gt;ç©è½¬Linuxæ“ä½œç³»ç»Ÿ&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;æ“ä½œç³»ç»Ÿå‘å±•å²å’ŒLinuxæ¦‚è¿°&lt;/li&gt;
&lt;li&gt;LinuxåŸºç¡€å‘½ä»¤&lt;/li&gt;
&lt;li&gt;Linuxä¸­çš„å®ç”¨ç¨‹åº&lt;/li&gt;
&lt;li&gt;Linuxçš„æ–‡ä»¶ç³»ç»Ÿ&lt;/li&gt;
&lt;li&gt;Vimç¼–è¾‘å™¨çš„åº”ç”¨&lt;/li&gt;
&lt;li&gt;ç¯å¢ƒå˜é‡å’ŒShellç¼–ç¨‹&lt;/li&gt;
&lt;li&gt;è½¯ä»¶çš„å®‰è£…å’ŒæœåŠ¡çš„é…ç½®&lt;/li&gt;
&lt;li&gt;ç½‘ç»œè®¿é—®å’Œç®¡ç†&lt;/li&gt;
&lt;li&gt;å…¶ä»–ç›¸å…³å†…å®¹&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3640---æ•°æ®åº“åŸºç¡€å’Œè¿›é˜¶" class="anchor" aria-hidden="true" href="#day3640---æ•°æ®åº“åŸºç¡€å’Œè¿›é˜¶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day36~40 - &lt;a href="./Day36-40"&gt;æ•°æ®åº“åŸºç¡€å’Œè¿›é˜¶&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./Day36-40/36-38.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL.md"&gt;å…³ç³»å‹æ•°æ®åº“MySQL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;å…³ç³»å‹æ•°æ®åº“æ¦‚è¿°&lt;/li&gt;
&lt;li&gt;MySQLçš„å®‰è£…å’Œä½¿ç”¨&lt;/li&gt;
&lt;li&gt;SQLçš„ä½¿ç”¨
&lt;ul&gt;
&lt;li&gt;DDL - æ•°æ®å®šä¹‰è¯­è¨€ - create / drop / alter&lt;/li&gt;
&lt;li&gt;DML - æ•°æ®æ“ä½œè¯­è¨€ - insert / delete / update / select&lt;/li&gt;
&lt;li&gt;DCL - æ•°æ®æ§åˆ¶è¯­è¨€ - grant / revoke&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç›¸å…³çŸ¥è¯†
&lt;ul&gt;
&lt;li&gt;èŒƒå¼ç†è®º - è®¾è®¡äºŒç»´è¡¨çš„æŒ‡å¯¼æ€æƒ³&lt;/li&gt;
&lt;li&gt;æ•°æ®å®Œæ•´æ€§&lt;/li&gt;
&lt;li&gt;æ•°æ®ä¸€è‡´æ€§&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;åœ¨Pythonä¸­æ“ä½œMySQL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="./Day36-40/39-40.NoSQL%E5%85%A5%E9%97%A8.md"&gt;NoSQLå…¥é—¨&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;NoSQLæ¦‚è¿°&lt;/li&gt;
&lt;li&gt;Redisæ¦‚è¿°&lt;/li&gt;
&lt;li&gt;Mongoæ¦‚è¿°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day4155---å®æˆ˜django" class="anchor" aria-hidden="true" href="#day4155---å®æˆ˜django"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41~55 - &lt;a href="./Day41-55"&gt;å®æˆ˜Django&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day41---å¿«é€Ÿä¸Šæ‰‹" class="anchor" aria-hidden="true" href="#day41---å¿«é€Ÿä¸Šæ‰‹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41 - &lt;a href="./Day41-55/41.%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md"&gt;å¿«é€Ÿä¸Šæ‰‹&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Webåº”ç”¨å·¥ä½œåŸç†å’ŒHTTPåè®®&lt;/li&gt;
&lt;li&gt;Djangoæ¡†æ¶æ¦‚è¿°&lt;/li&gt;
&lt;li&gt;5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨è§†å›¾æ¨¡æ¿&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day42---æ·±å…¥æ¨¡å‹" class="anchor" aria-hidden="true" href="#day42---æ·±å…¥æ¨¡å‹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day42 - &lt;a href="./Day41-55/42.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md"&gt;æ·±å…¥æ¨¡å‹&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å…³ç³»å‹æ•°æ®åº“é…ç½®&lt;/li&gt;
&lt;li&gt;ç®¡ç†åå°çš„ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨ORMå®Œæˆå¯¹æ¨¡å‹çš„CRUDæ“ä½œ&lt;/li&gt;
&lt;li&gt;Djangoæ¨¡å‹æœ€ä½³å®è·µ&lt;/li&gt;
&lt;li&gt;æ¨¡å‹å®šä¹‰å‚è€ƒ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day43---é™æ€èµ„æºå’Œajaxè¯·æ±‚" class="anchor" aria-hidden="true" href="#day43---é™æ€èµ„æºå’Œajaxè¯·æ±‚"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day43 - &lt;a href="./Day41-55/43.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md"&gt;é™æ€èµ„æºå’ŒAjaxè¯·æ±‚&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;åŠ è½½é™æ€èµ„æº&lt;/li&gt;
&lt;li&gt;ç”¨Ajaxè¯·æ±‚è·å–æ•°æ®&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day44---è¡¨å•çš„åº”ç”¨" class="anchor" aria-hidden="true" href="#day44---è¡¨å•çš„åº”ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day44 - &lt;a href="./Day41-55/44.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;è¡¨å•çš„åº”ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;è¡¨å•å’Œè¡¨å•æ§ä»¶&lt;/li&gt;
&lt;li&gt;è·¨ç«™è¯·æ±‚ä¼ªé€ å’ŒCSRFä»¤ç‰Œ&lt;/li&gt;
&lt;li&gt;Formå’ŒModelForm&lt;/li&gt;
&lt;li&gt;è¡¨å•éªŒè¯&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day45---cookieå’Œsession" class="anchor" aria-hidden="true" href="#day45---cookieå’Œsession"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day45 - &lt;a href="./Day41-55/45.Cookie%E5%92%8CSession.md"&gt;Cookieå’ŒSession&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å®ç°ç”¨æˆ·è·Ÿè¸ª&lt;/li&gt;
&lt;li&gt;cookieå’Œsessionçš„å…³ç³»&lt;/li&gt;
&lt;li&gt;Djangoæ¡†æ¶å¯¹sessionçš„æ”¯æŒ&lt;/li&gt;
&lt;li&gt;è§†å›¾å‡½æ•°ä¸­çš„cookieè¯»å†™æ“ä½œ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day46---æŠ¥è¡¨å’Œæ—¥å¿—" class="anchor" aria-hidden="true" href="#day46---æŠ¥è¡¨å’Œæ—¥å¿—"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day46 - &lt;a href="./Day41-55/46.%E6%8A%A5%E8%A1%A8%E5%92%8C%E6%97%A5%E5%BF%97.md"&gt;æŠ¥è¡¨å’Œæ—¥å¿—&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;é€šè¿‡HttpResponseä¿®æ”¹å“åº”å¤´&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨StreamingHttpResponseå¤„ç†å¤§æ–‡ä»¶&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨xlwtç”ŸæˆExcelæŠ¥è¡¨&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨reportlabç”ŸæˆPDFæŠ¥è¡¨&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨EChartsç”Ÿæˆå‰ç«¯å›¾è¡¨&lt;/li&gt;
&lt;li&gt;é…ç½®æ—¥å¿—å’ŒDjango-Debug-Toolbar&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day47---ä¸­é—´ä»¶çš„åº”ç”¨" class="anchor" aria-hidden="true" href="#day47---ä¸­é—´ä»¶çš„åº”ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day47 - &lt;a href="./Day41-55/47.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;ä¸­é—´ä»¶çš„åº”ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ä»€ä¹ˆæ˜¯ä¸­é—´ä»¶&lt;/li&gt;
&lt;li&gt;Djangoæ¡†æ¶å†…ç½®çš„ä¸­é—´ä»¶&lt;/li&gt;
&lt;li&gt;è‡ªå®šä¹‰ä¸­é—´ä»¶åŠå…¶åº”ç”¨åœºæ™¯&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day48---å‰åç«¯åˆ†ç¦»å¼€å‘å…¥é—¨" class="anchor" aria-hidden="true" href="#day48---å‰åç«¯åˆ†ç¦»å¼€å‘å…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day48 - &lt;a href="./Day41-55/48.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md"&gt;å‰åç«¯åˆ†ç¦»å¼€å‘å…¥é—¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;è¿”å›JSONæ ¼å¼çš„æ•°æ®&lt;/li&gt;
&lt;li&gt;ç”¨Vue.jsæ¸²æŸ“é¡µé¢&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day49---restfulæ¶æ„å’Œdrfå…¥é—¨" class="anchor" aria-hidden="true" href="#day49---restfulæ¶æ„å’Œdrfå…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day49 - &lt;a href="./Day41-55/49.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md"&gt;RESTfulæ¶æ„å’ŒDRFå…¥é—¨&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day50---restfulæ¶æ„å’Œdrfè¿›é˜¶" class="anchor" aria-hidden="true" href="#day50---restfulæ¶æ„å’Œdrfè¿›é˜¶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day50 - &lt;a href="./Day41-55/50.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md"&gt;RESTfulæ¶æ„å’ŒDRFè¿›é˜¶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day51---ä½¿ç”¨ç¼“å­˜" class="anchor" aria-hidden="true" href="#day51---ä½¿ç”¨ç¼“å­˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day51 - &lt;a href="./Day41-55/51.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md"&gt;ä½¿ç”¨ç¼“å­˜&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ç½‘ç«™ä¼˜åŒ–ç¬¬ä¸€å®šå¾‹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨Djangoé¡¹ç›®ä¸­ä½¿ç”¨Redisæä¾›ç¼“å­˜æœåŠ¡&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨è§†å›¾å‡½æ•°ä¸­è¯»å†™ç¼“å­˜&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½¿ç”¨è£…é¥°å™¨å®ç°é¡µé¢ç¼“å­˜&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¸ºæ•°æ®æ¥å£æä¾›ç¼“å­˜æœåŠ¡&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day52---æ–‡ä»¶ä¸Šä¼ å’Œå¯Œæ–‡æœ¬ç¼–è¾‘" class="anchor" aria-hidden="true" href="#day52---æ–‡ä»¶ä¸Šä¼ å’Œå¯Œæ–‡æœ¬ç¼–è¾‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day52 - &lt;a href="./Day41-55/52.%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8.md"&gt;æ–‡ä»¶ä¸Šä¼ å’Œå¯Œæ–‡æœ¬ç¼–è¾‘&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;æ–‡ä»¶ä¸Šä¼ è¡¨å•æ§ä»¶å’Œå›¾ç‰‡æ–‡ä»¶é¢„è§ˆ&lt;/li&gt;
&lt;li&gt;æœåŠ¡å™¨ç«¯å¦‚ä½•å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶&lt;/li&gt;
&lt;li&gt;å¯Œæ–‡æœ¬ç¼–è¾‘å™¨æ¦‚è¿°&lt;/li&gt;
&lt;li&gt;wangEditorçš„ä½¿ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day53---çŸ­ä¿¡å’Œé‚®ä»¶" class="anchor" aria-hidden="true" href="#day53---çŸ­ä¿¡å’Œé‚®ä»¶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day53 - &lt;a href="./Day41-55/53.%E7%9F%AD%E4%BF%A1%E5%92%8C%E9%82%AE%E4%BB%B6.md"&gt;çŸ­ä¿¡å’Œé‚®ä»¶&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å¸¸ç”¨çŸ­ä¿¡ç½‘å…³å¹³å°ä»‹ç»&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨èºä¸å¸½å‘é€çŸ­ä¿¡&lt;/li&gt;
&lt;li&gt;Djangoæ¡†æ¶å¯¹é‚®ä»¶æœåŠ¡çš„æ”¯æŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day54---å¼‚æ­¥ä»»åŠ¡å’Œå®šæ—¶ä»»åŠ¡" class="anchor" aria-hidden="true" href="#day54---å¼‚æ­¥ä»»åŠ¡å’Œå®šæ—¶ä»»åŠ¡"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day54 - &lt;a href="./Day41-55/54.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md"&gt;å¼‚æ­¥ä»»åŠ¡å’Œå®šæ—¶ä»»åŠ¡&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ç½‘ç«™ä¼˜åŒ–ç¬¬äºŒå®šå¾‹&lt;/li&gt;
&lt;li&gt;é…ç½®æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡&lt;/li&gt;
&lt;li&gt;åœ¨é¡¹ç›®ä¸­ä½¿ç”¨celeryå®ç°ä»»åŠ¡å¼‚æ­¥åŒ–&lt;/li&gt;
&lt;li&gt;åœ¨é¡¹ç›®ä¸­ä½¿ç”¨celeryå®ç°å®šæ—¶ä»»åŠ¡&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day55---å•å…ƒæµ‹è¯•å’Œé¡¹ç›®ä¸Šçº¿" class="anchor" aria-hidden="true" href="#day55---å•å…ƒæµ‹è¯•å’Œé¡¹ç›®ä¸Šçº¿"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day55 - &lt;a href="./Day41-55/55.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md"&gt;å•å…ƒæµ‹è¯•å’Œé¡¹ç›®ä¸Šçº¿&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pythonä¸­çš„å•å…ƒæµ‹è¯•&lt;/li&gt;
&lt;li&gt;Djangoæ¡†æ¶å¯¹å•å…ƒæµ‹è¯•çš„æ”¯æŒ&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ&lt;/li&gt;
&lt;li&gt;é…ç½®å’Œä½¿ç”¨uWSGI&lt;/li&gt;
&lt;li&gt;åŠ¨é™åˆ†ç¦»å’ŒNginxé…ç½®&lt;/li&gt;
&lt;li&gt;é…ç½®HTTPS&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day5660---å®æˆ˜flask" class="anchor" aria-hidden="true" href="#day5660---å®æˆ˜flask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56~60 - &lt;a href="./Day56-65"&gt;å®æˆ˜Flask&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day56---flaskå…¥é—¨" class="anchor" aria-hidden="true" href="#day56---flaskå…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56 - &lt;a href="./Day56-60/56.Flask%E5%85%A5%E9%97%A8.md"&gt;Flaskå…¥é—¨&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day57---æ¨¡æ¿çš„ä½¿ç”¨" class="anchor" aria-hidden="true" href="#day57---æ¨¡æ¿çš„ä½¿ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day57 - &lt;a href="./Day56-60/57.%E6%A8%A1%E6%9D%BF%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;æ¨¡æ¿çš„ä½¿ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day58---è¡¨å•çš„å¤„ç†" class="anchor" aria-hidden="true" href="#day58---è¡¨å•çš„å¤„ç†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day58 - &lt;a href="./Day56-60/58.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%A4%84%E7%90%86.md"&gt;è¡¨å•çš„å¤„ç†&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day59---æ•°æ®åº“æ“ä½œ" class="anchor" aria-hidden="true" href="#day59---æ•°æ®åº“æ“ä½œ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day59 - &lt;a href="./Day56-60/59.%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C.md"&gt;æ•°æ®åº“æ“ä½œ&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day60---é¡¹ç›®å®æˆ˜" class="anchor" aria-hidden="true" href="#day60---é¡¹ç›®å®æˆ˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day60 - &lt;a href="./Day56-60/60.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;é¡¹ç›®å®æˆ˜&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day6165---å®æˆ˜tornado" class="anchor" aria-hidden="true" href="#day6165---å®æˆ˜tornado"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61~65 - &lt;a href="./Day61-65"&gt;å®æˆ˜Tornado&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day61---é¢„å¤‡çŸ¥è¯†" class="anchor" aria-hidden="true" href="#day61---é¢„å¤‡çŸ¥è¯†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61 - &lt;a href="./Day61-65/61.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md"&gt;é¢„å¤‡çŸ¥è¯†&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å¹¶å‘ç¼–ç¨‹&lt;/li&gt;
&lt;li&gt;I/Oæ¨¡å¼å’Œäº‹ä»¶é©±åŠ¨&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day62---tornadoå…¥é—¨" class="anchor" aria-hidden="true" href="#day62---tornadoå…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day62 - &lt;a href="./Day61-65/62.Tornado%E5%85%A5%E9%97%A8.md"&gt;Tornadoå…¥é—¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tornadoæ¦‚è¿°&lt;/li&gt;
&lt;li&gt;5åˆ†é’Ÿä¸Šæ‰‹Tornado&lt;/li&gt;
&lt;li&gt;è·¯ç”±è§£æ&lt;/li&gt;
&lt;li&gt;è¯·æ±‚å¤„ç†å™¨&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day63---å¼‚æ­¥åŒ–" class="anchor" aria-hidden="true" href="#day63---å¼‚æ­¥åŒ–"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day63 - &lt;a href="./Day61-65/63.%E5%BC%82%E6%AD%A5%E5%8C%96.md"&gt;å¼‚æ­¥åŒ–&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;aiomysqlå’Œaioredisçš„ä½¿ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day64---websocketçš„åº”ç”¨" class="anchor" aria-hidden="true" href="#day64---websocketçš„åº”ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day64 - &lt;a href="./Day61-65/64.WebSocket%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;WebSocketçš„åº”ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebSocketç®€ä»‹&lt;/li&gt;
&lt;li&gt;WebSocketæœåŠ¡å™¨ç«¯ç¼–ç¨‹&lt;/li&gt;
&lt;li&gt;WebSocketå®¢æˆ·ç«¯ç¼–ç¨‹&lt;/li&gt;
&lt;li&gt;é¡¹ç›®ï¼šWebèŠå¤©å®¤&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day65---é¡¹ç›®å®æˆ˜" class="anchor" aria-hidden="true" href="#day65---é¡¹ç›®å®æˆ˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day65 - &lt;a href="./Day61-65/65.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;é¡¹ç›®å®æˆ˜&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å‰åç«¯åˆ†ç¦»å¼€å‘å’Œæ¥å£æ–‡æ¡£çš„æ’°å†™&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨Vue.jså®ç°å‰ç«¯æ¸²æŸ“&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨EChartså®ç°æŠ¥è¡¨åŠŸèƒ½&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨WebSocketå®ç°æ¨é€æœåŠ¡&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day6675---çˆ¬è™«å¼€å‘" class="anchor" aria-hidden="true" href="#day6675---çˆ¬è™«å¼€å‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66~75 - &lt;a href="./Day66-75"&gt;çˆ¬è™«å¼€å‘&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day66---ç½‘ç»œçˆ¬è™«å’Œç›¸å…³å·¥å…·" class="anchor" aria-hidden="true" href="#day66---ç½‘ç»œçˆ¬è™«å’Œç›¸å…³å·¥å…·"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66 - &lt;a href="./Day66-75/66.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7.md"&gt;ç½‘ç»œçˆ¬è™«å’Œç›¸å…³å·¥å…·&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ç½‘ç»œçˆ¬è™«çš„æ¦‚å¿µåŠå…¶åº”ç”¨é¢†åŸŸ&lt;/li&gt;
&lt;li&gt;ç½‘ç»œçˆ¬è™«çš„åˆæ³•æ€§æ¢è®¨&lt;/li&gt;
&lt;li&gt;å¼€å‘ç½‘ç»œçˆ¬è™«çš„ç›¸å…³å·¥å…·&lt;/li&gt;
&lt;li&gt;ä¸€ä¸ªçˆ¬è™«ç¨‹åºçš„æ„æˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day67---æ•°æ®é‡‡é›†å’Œè§£æ" class="anchor" aria-hidden="true" href="#day67---æ•°æ®é‡‡é›†å’Œè§£æ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day67 - &lt;a href="./Day66-75/67.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90.md"&gt;æ•°æ®é‡‡é›†å’Œè§£æ&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;æ•°æ®é‡‡é›†çš„æ ‡å‡†å’Œä¸‰æ–¹åº“&lt;/li&gt;
&lt;li&gt;é¡µé¢è§£æçš„ä¸‰ç§æ–¹å¼ï¼šæ­£åˆ™è¡¨è¾¾å¼è§£æ / XPathè§£æ / CSSé€‰æ‹©å™¨è§£æ&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day68---å­˜å‚¨æ•°æ®" class="anchor" aria-hidden="true" href="#day68---å­˜å‚¨æ•°æ®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day68 - &lt;a href="./Day66-75/68.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE.md"&gt;å­˜å‚¨æ•°æ®&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å¦‚ä½•å­˜å‚¨æµ·é‡æ•°æ®&lt;/li&gt;
&lt;li&gt;å®ç°æ•°æ®çš„ç¼“å­˜&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day69---å¹¶å‘ä¸‹è½½" class="anchor" aria-hidden="true" href="#day69---å¹¶å‘ä¸‹è½½"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day69 - &lt;a href="./Day66-75/69.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD.md"&gt;å¹¶å‘ä¸‹è½½&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹&lt;/li&gt;
&lt;li&gt;å¼‚æ­¥I/Oå’Œåç¨‹&lt;/li&gt;
&lt;li&gt;asyncå’Œawaitå…³é”®å­—çš„ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;ä¸‰æ–¹åº“aiohttpçš„åº”ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day70---è§£æåŠ¨æ€å†…å®¹" class="anchor" aria-hidden="true" href="#day70---è§£æåŠ¨æ€å†…å®¹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day70 - &lt;a href="./Day66-75/70.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md"&gt;è§£æåŠ¨æ€å†…å®¹&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JavaScripté€†å‘å·¥ç¨‹&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨Seleniumè·å–åŠ¨æ€å†…å®¹&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day71---è¡¨å•äº¤äº’å’ŒéªŒè¯ç å¤„ç†" class="anchor" aria-hidden="true" href="#day71---è¡¨å•äº¤äº’å’ŒéªŒè¯ç å¤„ç†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day71 - &lt;a href="./Day66-75/71.%E8%A1%A8%E5%8D%95%E4%BA%A4%E4%BA%92%E5%92%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E5%A4%84%E7%90%86.md"&gt;è¡¨å•äº¤äº’å’ŒéªŒè¯ç å¤„ç†&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;è‡ªåŠ¨æäº¤è¡¨å•&lt;/li&gt;
&lt;li&gt;Cookieæ± çš„åº”ç”¨&lt;/li&gt;
&lt;li&gt;éªŒè¯ç å¤„ç†&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day72---scrapyå…¥é—¨" class="anchor" aria-hidden="true" href="#day72---scrapyå…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day72 - &lt;a href="./Day66-75/72.Scrapy%E5%85%A5%E9%97%A8.md"&gt;Scrapyå…¥é—¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Scrapyçˆ¬è™«æ¡†æ¶æ¦‚è¿°&lt;/li&gt;
&lt;li&gt;å®‰è£…å’Œä½¿ç”¨Scrapy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day73---scrapyé«˜çº§åº”ç”¨" class="anchor" aria-hidden="true" href="#day73---scrapyé«˜çº§åº”ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day73 - &lt;a href="./Day66-75/73.Scrapy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md"&gt;Scrapyé«˜çº§åº”ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Spiderçš„ç”¨æ³•&lt;/li&gt;
&lt;li&gt;ä¸­é—´ä»¶çš„åº”ç”¨ï¼šä¸‹è½½ä¸­é—´ä»¶ / èœ˜è››ä¸­é—´ä»¶&lt;/li&gt;
&lt;li&gt;Scrapyå¯¹æ¥SeleniumæŠ“å–åŠ¨æ€å†…å®¹&lt;/li&gt;
&lt;li&gt;Scrapyéƒ¨ç½²åˆ°Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day74---scrapyåˆ†å¸ƒå¼å®ç°" class="anchor" aria-hidden="true" href="#day74---scrapyåˆ†å¸ƒå¼å®ç°"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day74 - &lt;a href="./Day66-75/74.Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0.md"&gt;Scrapyåˆ†å¸ƒå¼å®ç°&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;åˆ†å¸ƒå¼çˆ¬è™«çš„åŸç†&lt;/li&gt;
&lt;li&gt;Scrapyåˆ†å¸ƒå¼å®ç°&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨Scrapydå®ç°åˆ†å¸ƒå¼éƒ¨ç½²&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day75---çˆ¬è™«é¡¹ç›®å®æˆ˜" class="anchor" aria-hidden="true" href="#day75---çˆ¬è™«é¡¹ç›®å®æˆ˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day75 - &lt;a href="./Day66-75/75.%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;çˆ¬è™«é¡¹ç›®å®æˆ˜&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;çˆ¬å–æ‹›è˜ç½‘ç«™æ•°æ®&lt;/li&gt;
&lt;li&gt;çˆ¬å–æˆ¿åœ°äº§è¡Œä¸šæ•°æ®&lt;/li&gt;
&lt;li&gt;çˆ¬å–äºŒæ‰‹è½¦äº¤æ˜“å¹³å°æ•°æ®&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day7690---æ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ " class="anchor" aria-hidden="true" href="#day7690---æ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76~90 - &lt;a href="./Day76-90"&gt;æ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ &lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day76---æœºå™¨å­¦ä¹ åŸºç¡€" class="anchor" aria-hidden="true" href="#day76---æœºå™¨å­¦ä¹ åŸºç¡€"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76 - &lt;a href="./Day76-90/76.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md"&gt;æœºå™¨å­¦ä¹ åŸºç¡€&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day77---pandasçš„åº”ç”¨" class="anchor" aria-hidden="true" href="#day77---pandasçš„åº”ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day77 - &lt;a href="./Day76-90/77.Pandas%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;Pandasçš„åº”ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day78---numpyå’Œscipyçš„åº”ç”¨" class="anchor" aria-hidden="true" href="#day78---numpyå’Œscipyçš„åº”ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day78 - &lt;a href="./Day76-90/78.NumPy%E5%92%8CSciPy%E7%9A%84%E5%BA%94%E7%94%A8"&gt;NumPyå’ŒSciPyçš„åº”ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day79---matplotlibå’Œæ•°æ®å¯è§†åŒ–" class="anchor" aria-hidden="true" href="#day79---matplotlibå’Œæ•°æ®å¯è§†åŒ–"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day79 - &lt;a href="./Day76-90/79.Matplotlib%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"&gt;Matplotlibå’Œæ•°æ®å¯è§†åŒ–&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day80---kæœ€è¿‘é‚»knnåˆ†ç±»" class="anchor" aria-hidden="true" href="#day80---kæœ€è¿‘é‚»knnåˆ†ç±»"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day80 - &lt;a href="./Day76-90/80.k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB.md"&gt;kæœ€è¿‘é‚»(KNN)åˆ†ç±»&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day81---å†³ç­–æ ‘" class="anchor" aria-hidden="true" href="#day81---å†³ç­–æ ‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day81 - &lt;a href="./Day76-90/81.%E5%86%B3%E7%AD%96%E6%A0%91.md"&gt;å†³ç­–æ ‘&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day82---è´å¶æ–¯åˆ†ç±»" class="anchor" aria-hidden="true" href="#day82---è´å¶æ–¯åˆ†ç±»"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day82 - &lt;a href="./Day76-90/82.%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB.md"&gt;è´å¶æ–¯åˆ†ç±»&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day83---æ”¯æŒå‘é‡æœºsvm" class="anchor" aria-hidden="true" href="#day83---æ”¯æŒå‘é‡æœºsvm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day83 - &lt;a href="./Day76-90/83.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md"&gt;æ”¯æŒå‘é‡æœº(SVM)&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day84---k-å‡å€¼èšç±»" class="anchor" aria-hidden="true" href="#day84---k-å‡å€¼èšç±»"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day84 - &lt;a href="./Day76-90/84.K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB.md"&gt;K-å‡å€¼èšç±»&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day85---å›å½’åˆ†æ" class="anchor" aria-hidden="true" href="#day85---å›å½’åˆ†æ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day85 - &lt;a href="./Day76-90/85.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90.md"&gt;å›å½’åˆ†æ&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day86---å¤§æ•°æ®åˆ†æå…¥é—¨" class="anchor" aria-hidden="true" href="#day86---å¤§æ•°æ®åˆ†æå…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day86 - &lt;a href="./Day76-90/86.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8.md"&gt;å¤§æ•°æ®åˆ†æå…¥é—¨&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day87---å¤§æ•°æ®åˆ†æè¿›é˜¶" class="anchor" aria-hidden="true" href="#day87---å¤§æ•°æ®åˆ†æè¿›é˜¶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day87 - &lt;a href="./Day76-90/87.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BF%9B%E9%98%B6.md"&gt;å¤§æ•°æ®åˆ†æè¿›é˜¶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day88---tensorflowå…¥é—¨" class="anchor" aria-hidden="true" href="#day88---tensorflowå…¥é—¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day88 - &lt;a href="./Day76-90/88.Tensorflow%E5%85%A5%E9%97%A8.md"&gt;Tensorflowå…¥é—¨&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day89---tensorflowå®æˆ˜" class="anchor" aria-hidden="true" href="#day89---tensorflowå®æˆ˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day89 - &lt;a href="./Day76-90/89.Tensorflow%E5%AE%9E%E6%88%98.md"&gt;Tensorflowå®æˆ˜&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day90---æ¨èç³»ç»Ÿ" class="anchor" aria-hidden="true" href="#day90---æ¨èç³»ç»Ÿ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day90 - &lt;a href="./Day76-90/90.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.md"&gt;æ¨èç³»ç»Ÿ&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day91100---å›¢é˜Ÿé¡¹ç›®å¼€å‘" class="anchor" aria-hidden="true" href="#day91100---å›¢é˜Ÿé¡¹ç›®å¼€å‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day91~100 - &lt;a href="./Day91-100"&gt;å›¢é˜Ÿé¡¹ç›®å¼€å‘&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬91å¤©å›¢é˜Ÿé¡¹ç›®å¼€å‘çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ" class="anchor" aria-hidden="true" href="#ç¬¬91å¤©å›¢é˜Ÿé¡¹ç›®å¼€å‘çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬91å¤©ï¼š&lt;a href="./Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;å›¢é˜Ÿé¡¹ç›®å¼€å‘çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;è½¯ä»¶è¿‡ç¨‹æ¨¡å‹&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ç»å…¸è¿‡ç¨‹æ¨¡å‹ï¼ˆç€‘å¸ƒæ¨¡å‹ï¼‰&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯è¡Œæ€§åˆ†æï¼ˆç ”ç©¶åšè¿˜æ˜¯ä¸åšï¼‰ï¼Œè¾“å‡ºã€Šå¯è¡Œæ€§åˆ†ææŠ¥å‘Šã€‹ã€‚&lt;/li&gt;
&lt;li&gt;éœ€æ±‚åˆ†æï¼ˆç ”ç©¶åšä»€ä¹ˆï¼‰ï¼Œè¾“å‡ºã€Šéœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦ã€‹å’Œäº§å“ç•Œé¢åŸå‹å›¾ã€‚&lt;/li&gt;
&lt;li&gt;æ¦‚è¦è®¾è®¡å’Œè¯¦ç»†è®¾è®¡ï¼Œè¾“å‡ºæ¦‚å¿µæ¨¡å‹å›¾ã€ç‰©ç†æ¨¡å‹å›¾ã€ç±»å›¾ã€æ—¶åºå›¾ç­‰ã€‚&lt;/li&gt;
&lt;li&gt;ç¼–ç  / æµ‹è¯•ã€‚&lt;/li&gt;
&lt;li&gt;ä¸Šçº¿ / ç»´æŠ¤ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ•æ·å¼€å‘ï¼ˆScrumï¼‰- äº§å“æ‰€æœ‰è€…ã€Scrum Masterã€ç ”å‘äººå‘˜ - Sprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;äº§å“çš„Backlogï¼ˆç”¨æˆ·æ•…äº‹ã€äº§å“åŸå‹ï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;è®¡åˆ’ä¼šè®®ï¼ˆè¯„ä¼°å’Œé¢„ç®—ï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;æ—¥å¸¸å¼€å‘ï¼ˆç«™ç«‹ä¼šè®®ã€ç•ªèŒ„å·¥ä½œæ³•ã€ç»“å¯¹ç¼–ç¨‹ã€æµ‹è¯•å…ˆè¡Œã€ä»£ç é‡æ„â€¦â€¦ï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;ä¿®å¤bugï¼ˆé—®é¢˜æè¿°ã€é‡ç°æ­¥éª¤ã€æµ‹è¯•äººå‘˜ã€è¢«æŒ‡æ´¾äººï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;è¯„å®¡ä¼šè®®ï¼ˆShowcaseï¼‰ã€‚&lt;/li&gt;
&lt;li&gt;å›é¡¾ä¼šè®®ï¼ˆå½“å‰å‘¨æœŸåšå¾—å¥½å’Œä¸å¥½çš„åœ°æ–¹ï¼‰ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;è¡¥å……ï¼šæ•æ·è½¯ä»¶å¼€å‘å®£è¨€&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ä¸ªä½“å’Œäº’åŠ¨&lt;/strong&gt; é«˜äº æµç¨‹å’Œå·¥å…·&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;å·¥ä½œçš„è½¯ä»¶&lt;/strong&gt; é«˜äº è¯¦å°½çš„æ–‡æ¡£&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;å®¢æˆ·åˆä½œ&lt;/strong&gt; é«˜äº åˆåŒè°ˆåˆ¤&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;å“åº”å˜åŒ–&lt;/strong&gt; é«˜äº éµå¾ªè®¡åˆ’&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/agile-scrum-sprint-cycle.png"&gt;&lt;img src="./res/agile-scrum-sprint-cycle.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;è§’è‰²ï¼šäº§å“æ‰€æœ‰è€…ï¼ˆå†³å®šåšä»€ä¹ˆï¼Œèƒ½å¯¹éœ€æ±‚æ‹æ¿çš„äººï¼‰ã€å›¢é˜Ÿè´Ÿè´£äººï¼ˆè§£å†³å„ç§é—®é¢˜ï¼Œä¸“æ³¨å¦‚ä½•æ›´å¥½çš„å·¥ä½œï¼Œå±è”½å¤–éƒ¨å¯¹å¼€å‘å›¢é˜Ÿçš„å½±å“ï¼‰ã€å¼€å‘å›¢é˜Ÿï¼ˆé¡¹ç›®æ‰§è¡Œäººå‘˜ï¼Œå…·ä½“æŒ‡å¼€å‘äººå‘˜å’Œæµ‹è¯•äººå‘˜ï¼‰ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;å‡†å¤‡å·¥ä½œï¼šå•†ä¸šæ¡ˆä¾‹å’Œèµ„é‡‘ã€åˆåŒã€æ†§æ†¬ã€åˆå§‹äº§å“éœ€æ±‚ã€åˆå§‹å‘å¸ƒè®¡åˆ’ã€å…¥è‚¡ã€ç»„å»ºå›¢é˜Ÿã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;æ•æ·å›¢é˜Ÿé€šå¸¸äººæ•°ä¸º8-10äººã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;å·¥ä½œé‡ä¼°ç®—ï¼šå°†å¼€å‘ä»»åŠ¡é‡åŒ–ï¼ŒåŒ…æ‹¬åŸå‹ã€Logoè®¾è®¡ã€UIè®¾è®¡ã€å‰ç«¯å¼€å‘ç­‰ï¼Œå°½é‡æŠŠæ¯ä¸ªå·¥ä½œåˆ†è§£åˆ°æœ€å°ä»»åŠ¡é‡ï¼Œæœ€å°ä»»åŠ¡é‡æ ‡å‡†ä¸ºå·¥ä½œæ—¶é—´ä¸èƒ½è¶…è¿‡ä¸¤å¤©ï¼Œç„¶åä¼°ç®—æ€»ä½“é¡¹ç›®æ—¶é—´ã€‚æŠŠæ¯ä¸ªä»»åŠ¡éƒ½è´´åœ¨ç™½æ¿ä¸Šé¢ï¼Œç™½æ¿ä¸Šåˆ†ä¸‰éƒ¨åˆ†ï¼što doï¼ˆå¾…å®Œæˆï¼‰ã€in progressï¼ˆè¿›è¡Œä¸­ï¼‰å’Œdoneï¼ˆå·²å®Œæˆï¼‰ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;é¡¹ç›®å›¢é˜Ÿç»„å»º&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;å›¢é˜Ÿçš„æ„æˆå’Œè§’è‰²&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;è¯´æ˜ï¼šè°¢è°¢ä»˜ç¥¥è‹±å¥³å£«ç»˜åˆ¶äº†ä¸‹é¢è¿™å¼ ç²¾ç¾çš„å…¬å¸ç»„ç»‡æ¶æ„å›¾ã€‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/company_architecture.png"&gt;&lt;img src="./res/company_architecture.png" alt="company_architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç¼–ç¨‹è§„èŒƒå’Œä»£ç å®¡æŸ¥ï¼ˆflake8ã€pylintï¼‰&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/pylint.png"&gt;&lt;img src="./res/pylint.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pythonä¸­çš„ä¸€äº›â€œæƒ¯ä¾‹â€ï¼ˆè¯·å‚è€ƒ&lt;a href="Python%E6%83%AF%E4%BE%8B.md"&gt;ã€ŠPythonæƒ¯ä¾‹-å¦‚ä½•ç¼–å†™Pythonicçš„ä»£ç ã€‹&lt;/a&gt;ï¼‰&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å½±å“ä»£ç å¯è¯»æ€§çš„åŸå› ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä»£ç æ³¨é‡Šå¤ªå°‘æˆ–è€…æ²¡æœ‰æ³¨é‡Š&lt;/li&gt;
&lt;li&gt;ä»£ç ç ´åäº†è¯­è¨€çš„æœ€ä½³å®è·µ&lt;/li&gt;
&lt;li&gt;åæ¨¡å¼ç¼–ç¨‹ï¼ˆæ„å¤§åˆ©é¢ä»£ç ã€å¤åˆ¶-é»è´´ç¼–ç¨‹ã€è‡ªè´Ÿç¼–ç¨‹ã€â€¦â€¦ï¼‰&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å›¢é˜Ÿå¼€å‘å·¥å…·ä»‹ç»&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç‰ˆæœ¬æ§åˆ¶ï¼šGitã€Mercury&lt;/li&gt;
&lt;li&gt;ç¼ºé™·ç®¡ç†ï¼š&lt;a href="https://about.gitlab.com/" rel="nofollow"&gt;Gitlab&lt;/a&gt;ã€&lt;a href="http://www.redmine.org.cn/" rel="nofollow"&gt;Redmine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;æ•æ·é—­ç¯å·¥å…·ï¼š&lt;a href="https://www.zentao.net/" rel="nofollow"&gt;ç¦…é“&lt;/a&gt;ã€&lt;a href="https://www.atlassian.com/software/jira/features" rel="nofollow"&gt;JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;æŒç»­é›†æˆï¼š&lt;a href="https://jenkins.io/" rel="nofollow"&gt;Jenkins&lt;/a&gt;ã€&lt;a href="https://travis-ci.org/" rel="nofollow"&gt;Travis-CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;è¯·å‚è€ƒ&lt;a href="Day91-100/%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91.md"&gt;ã€Šå›¢é˜Ÿé¡¹ç›®å¼€å‘ã€‹&lt;/a&gt;ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-é¡¹ç›®é€‰é¢˜å’Œç†è§£ä¸šåŠ¡" class="anchor" aria-hidden="true" href="#é¡¹ç›®é€‰é¢˜å’Œç†è§£ä¸šåŠ¡"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;é¡¹ç›®é€‰é¢˜å’Œç†è§£ä¸šåŠ¡&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;é€‰é¢˜èŒƒå›´è®¾å®š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CMSï¼ˆç”¨æˆ·ç«¯ï¼‰ï¼šæ–°é—»èšåˆç½‘ç«™ã€é—®ç­”/åˆ†äº«ç¤¾åŒºã€å½±è¯„/ä¹¦è¯„ç½‘ç«™ç­‰ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MISï¼ˆç”¨æˆ·ç«¯+ç®¡ç†ç«¯ï¼‰ï¼šKMSã€KPIè€ƒæ ¸ç³»ç»Ÿã€HRSã€CRMç³»ç»Ÿã€ä¾›åº”é“¾ç³»ç»Ÿã€ä»“å‚¨ç®¡ç†ç³»ç»Ÿç­‰ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Appåå°ï¼ˆç®¡ç†ç«¯+æ•°æ®æ¥å£ï¼‰ï¼šäºŒæ‰‹äº¤æ˜“ç±»ã€æŠ¥åˆŠæ‚å¿—ç±»ã€å°ä¼—ç”µå•†ç±»ã€æ–°é—»èµ„è®¯ç±»ã€æ—…æ¸¸ç±»ã€ç¤¾äº¤ç±»ã€é˜…è¯»ç±»ç­‰ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å…¶ä»–ç±»å‹ï¼šè‡ªèº«è¡Œä¸šèƒŒæ™¯å’Œå·¥ä½œç»éªŒã€ä¸šåŠ¡å®¹æ˜“ç†è§£å’ŒæŠŠæ§ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;éœ€æ±‚ç†è§£ã€æ¨¡å—åˆ’åˆ†å’Œä»»åŠ¡åˆ†é…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;éœ€æ±‚ç†è§£ï¼šå¤´è„‘é£æš´å’Œç«å“åˆ†æã€‚&lt;/li&gt;
&lt;li&gt;æ¨¡å—åˆ’åˆ†ï¼šç”»æ€ç»´å¯¼å›¾ï¼ˆXMindï¼‰ï¼Œæ¯ä¸ªæ¨¡å—æ˜¯ä¸€ä¸ªæèŠ‚ç‚¹ï¼Œæ¯ä¸ªå…·ä½“çš„åŠŸèƒ½æ˜¯ä¸€ä¸ªå¶èŠ‚ç‚¹ï¼ˆç”¨åŠ¨è¯è¡¨è¿°ï¼‰ï¼Œéœ€è¦ç¡®ä¿æ¯ä¸ªå¶èŠ‚ç‚¹æ— æ³•å†ç”Ÿå‡ºæ–°èŠ‚ç‚¹ï¼Œç¡®å®šæ¯ä¸ªå¶å­èŠ‚ç‚¹çš„é‡è¦æ€§ã€ä¼˜å…ˆçº§å’Œå·¥ä½œé‡ã€‚&lt;/li&gt;
&lt;li&gt;ä»»åŠ¡åˆ†é…ï¼šç”±é¡¹ç›®è´Ÿè´£äººæ ¹æ®ä¸Šé¢çš„æŒ‡æ ‡ä¸ºæ¯ä¸ªå›¢é˜Ÿæˆå‘˜åˆ†é…ä»»åŠ¡ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/requirements_by_xmind.png"&gt;&lt;img src="./res/requirements_by_xmind.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åˆ¶å®šé¡¹ç›®è¿›åº¦è¡¨ï¼ˆæ¯æ—¥æ›´æ–°ï¼‰&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ¨¡å—&lt;/th&gt;
&lt;th&gt;åŠŸèƒ½&lt;/th&gt;
&lt;th&gt;äººå‘˜&lt;/th&gt;
&lt;th&gt;çŠ¶æ€&lt;/th&gt;
&lt;th&gt;å®Œæˆ&lt;/th&gt;
&lt;th&gt;å·¥æ—¶&lt;/th&gt;
&lt;th&gt;è®¡åˆ’å¼€å§‹&lt;/th&gt;
&lt;th&gt;å®é™…å¼€å§‹&lt;/th&gt;
&lt;th&gt;è®¡åˆ’ç»“æŸ&lt;/th&gt;
&lt;th&gt;å®é™…ç»“æŸ&lt;/th&gt;
&lt;th&gt;å¤‡æ³¨&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;è¯„è®º&lt;/td&gt;
&lt;td&gt;æ·»åŠ è¯„è®º&lt;/td&gt;
&lt;td&gt;ç‹å¤§é”¤&lt;/td&gt;
&lt;td&gt;æ­£åœ¨è¿›è¡Œ&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;åˆ é™¤è¯„è®º&lt;/td&gt;
&lt;td&gt;ç‹å¤§é”¤&lt;/td&gt;
&lt;td&gt;ç­‰å¾…&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;æŸ¥çœ‹è¯„è®º&lt;/td&gt;
&lt;td&gt;ç™½å…ƒèŠ³&lt;/td&gt;
&lt;td&gt;æ­£åœ¨è¿›è¡Œ&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;éœ€è¦è¿›è¡Œä»£ç å®¡æŸ¥&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;è¯„è®ºæŠ•ç¥¨&lt;/td&gt;
&lt;td&gt;ç™½å…ƒèŠ³&lt;/td&gt;
&lt;td&gt;ç­‰å¾…&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OOADå’Œæ•°æ®åº“è®¾è®¡&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;UMLï¼ˆç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼‰çš„ç±»å›¾&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/uml-class-diagram.png"&gt;&lt;img src="./res/uml-class-diagram.png" alt="uml" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;é€šè¿‡æ¨¡å‹åˆ›å»ºè¡¨ï¼ˆæ­£å‘å·¥ç¨‹ï¼‰&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py makemigrations app
python manage.py migrate&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½¿ç”¨PowerDesignerç»˜åˆ¶ç‰©ç†æ¨¡å‹å›¾ã€‚&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/power-designer-pdm.png"&gt;&lt;img src="./res/power-designer-pdm.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;é€šè¿‡æ•°æ®è¡¨åˆ›å»ºæ¨¡å‹ï¼ˆåå‘å·¥ç¨‹ï¼‰&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py inspectdb &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; app/models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬92å¤©ä½¿ç”¨dockeréƒ¨ç½²åº”ç”¨" class="anchor" aria-hidden="true" href="#ç¬¬92å¤©ä½¿ç”¨dockeréƒ¨ç½²åº”ç”¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬92å¤©ï¼š&lt;a href="./Day91-100/92.%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8.md"&gt;ä½¿ç”¨Dockeréƒ¨ç½²åº”ç”¨&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Dockerç®€ä»‹&lt;/li&gt;
&lt;li&gt;å®‰è£…Docker&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨Dockeråˆ›å»ºå®¹å™¨ï¼ˆNginxã€MySQLã€Redisã€Gitlabã€Jenkinsï¼‰&lt;/li&gt;
&lt;li&gt;æ„å»ºDockeré•œåƒï¼ˆDockerfileçš„ç¼–å†™å’Œç›¸å…³æŒ‡ä»¤ï¼‰&lt;/li&gt;
&lt;li&gt;å®¹å™¨ç¼–æ’ï¼ˆDocker-composeï¼‰&lt;/li&gt;
&lt;li&gt;é›†ç¾¤ç®¡ç†&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬93å¤©mysqlæ€§èƒ½ä¼˜åŒ–" class="anchor" aria-hidden="true" href="#ç¬¬93å¤©mysqlæ€§èƒ½ä¼˜åŒ–"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬93å¤©ï¼š&lt;a href="./Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md"&gt;MySQLæ€§èƒ½ä¼˜åŒ–&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬94å¤©ç½‘ç»œapiæ¥å£è®¾è®¡" class="anchor" aria-hidden="true" href="#ç¬¬94å¤©ç½‘ç»œapiæ¥å£è®¾è®¡"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬94å¤©ï¼š&lt;a href="./Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md"&gt;ç½‘ç»œAPIæ¥å£è®¾è®¡&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬95å¤©ä½¿ç”¨djangoå¼€å‘å•†ä¸šé¡¹ç›®day91-10095ä½¿ç”¨djangoå¼€å‘å•†ä¸šé¡¹ç›®md" class="anchor" aria-hidden="true" href="#ç¬¬95å¤©ä½¿ç”¨djangoå¼€å‘å•†ä¸šé¡¹ç›®day91-10095ä½¿ç”¨djangoå¼€å‘å•†ä¸šé¡¹ç›®md"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬95å¤©ï¼š[ä½¿ç”¨Djangoå¼€å‘å•†ä¸šé¡¹ç›®](./Day91-100/95.ä½¿ç”¨Djangoå¼€å‘å•†ä¸šé¡¹	ç›®.md)&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-é¡¹ç›®å¼€å‘ä¸­çš„å…¬å…±é—®é¢˜" class="anchor" aria-hidden="true" href="#é¡¹ç›®å¼€å‘ä¸­çš„å…¬å…±é—®é¢˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;é¡¹ç›®å¼€å‘ä¸­çš„å…¬å…±é—®é¢˜&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;æ•°æ®åº“çš„é…ç½®ï¼ˆå¤šæ•°æ®åº“ã€ä¸»ä»å¤åˆ¶ã€æ•°æ®åº“è·¯ç”±ï¼‰&lt;/li&gt;
&lt;li&gt;ç¼“å­˜çš„é…ç½®ï¼ˆåˆ†åŒºç¼“å­˜ã€é”®è®¾ç½®ã€è¶…æ—¶è®¾ç½®ã€ä¸»ä»å¤åˆ¶ã€æ•…éšœæ¢å¤ï¼ˆå“¨å…µï¼‰ï¼‰&lt;/li&gt;
&lt;li&gt;æ—¥å¿—çš„é…ç½®&lt;/li&gt;
&lt;li&gt;åˆ†æå’Œè°ƒè¯•ï¼ˆDjango-Debug-ToolBarï¼‰&lt;/li&gt;
&lt;li&gt;å¥½ç”¨çš„Pythonæ¨¡å—ï¼ˆæ—¥æœŸè®¡ç®—ã€å›¾åƒå¤„ç†ã€æ•°æ®åŠ å¯†ã€ä¸‰æ–¹APIï¼‰&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-rest-apiè®¾è®¡" class="anchor" aria-hidden="true" href="#rest-apiè®¾è®¡"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;REST APIè®¾è®¡&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;RESTfulæ¶æ„
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2011/09/restful.html" rel="nofollow"&gt;ç†è§£RESTfulæ¶æ„&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html" rel="nofollow"&gt;RESTful APIè®¾è®¡æŒ‡å—&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html" rel="nofollow"&gt;RESTful APIæœ€ä½³å®è·µ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;APIæ¥å£æ–‡æ¡£çš„æ’°å†™
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rap2.taobao.org/" rel="nofollow"&gt;RAP2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yapi.demo.qunar.com/" rel="nofollow"&gt;YAPI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.django-rest-framework.org/" rel="nofollow"&gt;django-REST-framework&lt;/a&gt;çš„åº”ç”¨&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-é¡¹ç›®ä¸­çš„é‡ç‚¹éš¾ç‚¹å‰–æ" class="anchor" aria-hidden="true" href="#é¡¹ç›®ä¸­çš„é‡ç‚¹éš¾ç‚¹å‰–æ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;é¡¹ç›®ä¸­çš„é‡ç‚¹éš¾ç‚¹å‰–æ&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ä½¿ç”¨ç¼“å­˜ç¼“è§£æ•°æ®åº“å‹åŠ› - Redis&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—åšè§£è€¦åˆå’Œå‰Šå³° - Celery + RabbitMQ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬96å¤©è½¯ä»¶æµ‹è¯•å’Œè‡ªåŠ¨åŒ–æµ‹è¯•" class="anchor" aria-hidden="true" href="#ç¬¬96å¤©è½¯ä»¶æµ‹è¯•å’Œè‡ªåŠ¨åŒ–æµ‹è¯•"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬96å¤©ï¼š&lt;a href="Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md"&gt;è½¯ä»¶æµ‹è¯•å’Œè‡ªåŠ¨åŒ–æµ‹è¯•&lt;/a&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-å•å…ƒæµ‹è¯•" class="anchor" aria-hidden="true" href="#å•å…ƒæµ‹è¯•"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;å•å…ƒæµ‹è¯•&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;æµ‹è¯•çš„ç§ç±»&lt;/li&gt;
&lt;li&gt;ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆunittestã€pytestã€nose2ã€toxã€ddtã€â€¦â€¦ï¼‰&lt;/li&gt;
&lt;li&gt;æµ‹è¯•è¦†ç›–ç‡ï¼ˆcoverageï¼‰&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-é¡¹ç›®éƒ¨ç½²" class="anchor" aria-hidden="true" href="#é¡¹ç›®éƒ¨ç½²"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;é¡¹ç›®éƒ¨ç½²&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;éƒ¨ç½²å‰çš„å‡†å¤‡å·¥ä½œ
&lt;ul&gt;
&lt;li&gt;å…³é”®è®¾ç½®ï¼ˆSECRET_KEY / DEBUG / ALLOWED_HOSTS / ç¼“å­˜ / æ•°æ®åº“ï¼‰&lt;/li&gt;
&lt;li&gt;HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE&lt;/li&gt;
&lt;li&gt;æ—¥å¿—ç›¸å…³é…ç½®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linuxå¸¸ç”¨å‘½ä»¤å›é¡¾&lt;/li&gt;
&lt;li&gt;Linuxå¸¸ç”¨æœåŠ¡çš„å®‰è£…å’Œé…ç½®&lt;/li&gt;
&lt;li&gt;uWSGI/Gunicornå’ŒNginxçš„ä½¿ç”¨
&lt;ul&gt;
&lt;li&gt;Gunicornå’ŒuWSGIçš„æ¯”è¾ƒ
&lt;ul&gt;
&lt;li&gt;å¯¹äºä¸éœ€è¦å¤§é‡å®šåˆ¶åŒ–çš„ç®€å•åº”ç”¨ç¨‹åºï¼ŒGunicornæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼ŒuWSGIçš„å­¦ä¹ æ›²çº¿æ¯”Gunicornè¦é™¡å³­å¾—å¤šï¼ŒGunicornçš„é»˜è®¤å‚æ•°å°±å·²ç»èƒ½å¤Ÿé€‚åº”å¤§å¤šæ•°åº”ç”¨ç¨‹åºã€‚&lt;/li&gt;
&lt;li&gt;uWSGIæ”¯æŒå¼‚æ„éƒ¨ç½²ã€‚&lt;/li&gt;
&lt;li&gt;ç”±äºNginxæœ¬èº«æ”¯æŒuWSGIï¼Œåœ¨çº¿ä¸Šä¸€èˆ¬éƒ½å°†Nginxå’ŒuWSGIæ†ç»‘åœ¨ä¸€èµ·éƒ¨ç½²ï¼Œè€Œä¸”uWSGIå±äºåŠŸèƒ½é½å…¨ä¸”é«˜åº¦å®šåˆ¶çš„WSGIä¸­é—´ä»¶ã€‚&lt;/li&gt;
&lt;li&gt;åœ¨æ€§èƒ½ä¸Šï¼ŒGunicornå’ŒuWSGIå…¶å®è¡¨ç°ç›¸å½“ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨è™šæ‹ŸåŒ–æŠ€æœ¯ï¼ˆDockerï¼‰éƒ¨ç½²æµ‹è¯•ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒ&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-æ€§èƒ½æµ‹è¯•" class="anchor" aria-hidden="true" href="#æ€§èƒ½æµ‹è¯•"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æ€§èƒ½æµ‹è¯•&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ABçš„ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;SQLslapçš„ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;sysbenchçš„ä½¿ç”¨&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-è‡ªåŠ¨åŒ–æµ‹è¯•" class="anchor" aria-hidden="true" href="#è‡ªåŠ¨åŒ–æµ‹è¯•"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;è‡ªåŠ¨åŒ–æµ‹è¯•&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ä½¿ç”¨Shellå’ŒPythonè¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨Seleniumå®ç°è‡ªåŠ¨åŒ–æµ‹è¯•
&lt;ul&gt;
&lt;li&gt;Selenium IDE&lt;/li&gt;
&lt;li&gt;Selenium WebDriver&lt;/li&gt;
&lt;li&gt;Selenium Remote Control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æµ‹è¯•å·¥å…·Robot Frameworkä»‹ç»&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬97å¤©ç”µå•†ç½‘ç«™æŠ€æœ¯è¦ç‚¹å‰–æ" class="anchor" aria-hidden="true" href="#ç¬¬97å¤©ç”µå•†ç½‘ç«™æŠ€æœ¯è¦ç‚¹å‰–æ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬97å¤©ï¼š&lt;a href="./Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md"&gt;ç”µå•†ç½‘ç«™æŠ€æœ¯è¦ç‚¹å‰–æ&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬98å¤©é¡¹ç›®éƒ¨ç½²ä¸Šçº¿å’Œæ€§èƒ½è°ƒä¼˜" class="anchor" aria-hidden="true" href="#ç¬¬98å¤©é¡¹ç›®éƒ¨ç½²ä¸Šçº¿å’Œæ€§èƒ½è°ƒä¼˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬98å¤©ï¼š&lt;a href="./Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md"&gt;é¡¹ç›®éƒ¨ç½²ä¸Šçº¿å’Œæ€§èƒ½è°ƒä¼˜&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;MySQLæ•°æ®åº“è°ƒä¼˜&lt;/li&gt;
&lt;li&gt;WebæœåŠ¡å™¨æ€§èƒ½ä¼˜åŒ–
&lt;ul&gt;
&lt;li&gt;Nginxè´Ÿè½½å‡è¡¡é…ç½®&lt;/li&gt;
&lt;li&gt;Keepalivedå®ç°é«˜å¯ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä»£ç æ€§èƒ½è°ƒä¼˜
&lt;ul&gt;
&lt;li&gt;å¤šçº¿ç¨‹&lt;/li&gt;
&lt;li&gt;å¼‚æ­¥åŒ–&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;é™æ€èµ„æºè®¿é—®ä¼˜åŒ–
&lt;ul&gt;
&lt;li&gt;äº‘å­˜å‚¨&lt;/li&gt;
&lt;li&gt;CDN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬99å¤©é¢è¯•ä¸­çš„å…¬å…±é—®é¢˜" class="anchor" aria-hidden="true" href="#ç¬¬99å¤©é¢è¯•ä¸­çš„å…¬å…±é—®é¢˜"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬99å¤©ï¼š&lt;a href="./Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md"&gt;é¢è¯•ä¸­çš„å…¬å…±é—®é¢˜&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-ç¬¬100å¤©pythoné¢è¯•é¢˜é›†" class="anchor" aria-hidden="true" href="#ç¬¬100å¤©pythoné¢è¯•é¢˜é›†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç¬¬100å¤©ï¼š&lt;a href="./Day91-100/100.Python%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86.md"&gt;Pythoné¢è¯•é¢˜é›†&lt;/a&gt;&lt;/h4&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jackfrued</author><guid isPermaLink="false">https://github.com/jackfrued/Python-100-Days</guid><pubDate>Mon, 18 Nov 2019 00:05:00 GMT</pubDate></item><item><title>fastai/fastai #6 in Jupyter Notebook, This week</title><link>https://github.com/fastai/fastai</link><description>&lt;p&gt;&lt;i&gt;The fastai deep learning library, plus lessons and tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/fastdotai/fastai/_build/latest?definitionId=1" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a1b1234cce0c844f75224d1df07b4f236f78aee7/68747470733a2f2f6465762e617a7572652e636f6d2f66617374646f7461692f6661737461692f5f617069732f6275696c642f7374617475732f6661737461692e666173746169" alt="Build Status" data-canonical-src="https://dev.azure.com/fastdotai/fastai/_apis/build/status/fastai.fastai" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/26f7b20369ea7a096cfb30bdf0d14bc6ceda0275/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6661737461692e737667" alt="pypi fastai version" data-canonical-src="https://img.shields.io/pypi/v/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c20a6e61cb1c612b644253db1e3c1f7972d7f0e/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f762f6661737461692f6661737461692e737667" alt="Conda fastai version" data-canonical-src="https://img.shields.io/conda/v/fastai/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/39f258e6563b60eef6a88589cd194d3a85033747/68747470733a2f2f616e61636f6e64612e6f72672f6661737461692f6661737461692f6261646765732f706c6174666f726d732e737667" alt="Anaconda-Server Badge" data-canonical-src="https://anaconda.org/fastai/fastai/badges/platforms.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6dc643192dbbbd8edda826d1be289ba06ef2b57f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6661737461692e737667" alt="fastai python compatibility" data-canonical-src="https://img.shields.io/pypi/pyversions/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f600abfa75b593d49643c1710ed42865373f9d75/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6661737461692e737667" alt="fastai license" data-canonical-src="https://img.shields.io/pypi/l/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-fastai" class="anchor" aria-hidden="true" href="#fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;fastai&lt;/h1&gt;
&lt;p&gt;The fastai library simplifies training fast and accurate neural nets using modern best practices. See the &lt;a href="https://docs.fast.ai" rel="nofollow"&gt;fastai website&lt;/a&gt; to get started. The library is based on research into deep learning best practices undertaken at &lt;a href="http://www.fast.ai" rel="nofollow"&gt;fast.ai&lt;/a&gt;, and includes "out of the box" support for &lt;a href="https://docs.fast.ai/vision.html#vision" rel="nofollow"&gt;&lt;code&gt;vision&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/text.html#text" rel="nofollow"&gt;&lt;code&gt;text&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/tabular.html#tabular" rel="nofollow"&gt;&lt;code&gt;tabular&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://docs.fast.ai/collab.html#collab" rel="nofollow"&gt;&lt;code&gt;collab&lt;/code&gt;&lt;/a&gt; (collaborative filtering) models. For brief examples, see the &lt;a href="https://github.com/fastai/fastai/tree/master/examples"&gt;examples&lt;/a&gt; folder; detailed examples are provided in the full &lt;a href="https://docs.fast.ai/" rel="nofollow"&gt;documentation&lt;/a&gt;. For instance, here's how to train an MNIST model using &lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;resnet18&lt;/a&gt; (from the &lt;a href="https://github.com/fastai/fastai/blob/master/examples/vision.ipynb"&gt;vision example&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; fastai.vision &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;
path &lt;span class="pl-k"&gt;=&lt;/span&gt; untar_data(&lt;span class="pl-c1"&gt;MNIST_PATH&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; image_data_from_folder(path)
learn &lt;span class="pl-k"&gt;=&lt;/span&gt; cnn_learner(data, models.resnet18, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;accuracy)
learn.fit(&lt;span class="pl-c1"&gt;1&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-note-for-coursefastai-students" class="anchor" aria-hidden="true" href="#note-for-coursefastai-students"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note for &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; students&lt;/h2&gt;
&lt;p&gt;This document is written for &lt;code&gt;fastai v1&lt;/code&gt;, which we use for the current version the &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; deep learning courses. If you're following along with a course at &lt;a href="http://course18.fast.ai" rel="nofollow"&gt;course18.fast.ai&lt;/a&gt; (i.e. the machine learning course, which isn't updated for v1) you need to use &lt;code&gt;fastai 0.7&lt;/code&gt;;  please follow the installation instructions &lt;a href="https://forums.fast.ai/t/fastai-v0-install-issues-thread/24652" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; &lt;em&gt;fastai v1 currently supports Linux only, and requires &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; or later. Windows support is at an experimental stage: it should work fine but it's much slower and less well tested. Since Macs don't currently have good Nvidia GPU support, we do not currently prioritize Mac development.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fastai-1.x&lt;/code&gt; can be installed with either &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt; package managers and also from source. At the moment you can't just run &lt;em&gt;install&lt;/em&gt;, since you first need to get the correct &lt;code&gt;pytorch&lt;/code&gt; version installed - thus to get &lt;code&gt;fastai-1.x&lt;/code&gt; installed choose one of the installation recipes below using your favorite python package manager. Note that &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; are the minimal version requirements.&lt;/p&gt;
&lt;p&gt;It's highly recommended you install &lt;code&gt;fastai&lt;/code&gt; and its dependencies in a virtual environment (&lt;a href="https://conda.io/docs/user-guide/tasks/manage-environments.html" rel="nofollow"&gt;&lt;code&gt;conda&lt;/code&gt;&lt;/a&gt; or others), so that you don't interfere with system-wide python packages. It's not that you must, but if you experience problems with any dependency packages, please consider using a fresh virtual environment just for &lt;code&gt;fastai&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Starting with pytorch-1.x you no longer need to install a special pytorch-cpu version. Instead use the normal pytorch and it works with and without GPU. But &lt;a href="https://docs.fast.ai/install.html#cpu-build" rel="nofollow"&gt;you can install the cpu build too&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you experience installation problems, please read about &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#installation-issues"&gt;installation issues&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are planning on using &lt;code&gt;fastai&lt;/code&gt; in the jupyter notebook environment, make sure to also install the corresponding &lt;a href="https://docs.fast.ai/install.html#jupyter-notebook-dependencies" rel="nofollow"&gt;packages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More advanced installation issues, such as installing only partial dependencies are covered in a dedicated &lt;a href="https://docs.fast.ai/install.html" rel="nofollow"&gt;installation doc&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-conda-install" class="anchor" aria-hidden="true" href="#conda-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch -c fastai fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will install the &lt;code&gt;pytorch&lt;/code&gt; build with the latest &lt;code&gt;cudatoolkit&lt;/code&gt; version. If you need a higher or lower &lt;code&gt;CUDA XX&lt;/code&gt; build (e.g. CUDA 9.0), following the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install the desired &lt;code&gt;pytorch&lt;/code&gt; build.&lt;/p&gt;
&lt;p&gt;Note that JPEG decoding can be a bottleneck, particularly if you have a fast GPU. You can optionally install an optimized JPEG decoder as follows (Linux):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda uninstall --force jpeg libtiff -y
conda install -c conda-forge libjpeg-turbo pillow==6.0.0
CC=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cc -mavx2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; pip install --no-cache-dir -U --force-reinstall --no-binary :all: --compile pillow-simd&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you only care about faster JPEG decompression, it can be &lt;code&gt;pillow&lt;/code&gt; or &lt;code&gt;pillow-simd&lt;/code&gt; in the last command above, the latter speeds up other image processing operations. For the full story see &lt;a href="https://docs.fast.ai/performance.html#faster-image-processing" rel="nofollow"&gt;Pillow-SIMD&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pypi-install" class="anchor" aria-hidden="true" href="#pypi-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyPI Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By default pip will install the latest &lt;code&gt;pytorch&lt;/code&gt; with the latest &lt;code&gt;cudatoolkit&lt;/code&gt;. If your hardware doesn't support the latest &lt;code&gt;cudatoolkit&lt;/code&gt;, follow the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install a &lt;code&gt;pytorch&lt;/code&gt; build that fits your hardware.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-bug-fix-install" class="anchor" aria-hidden="true" href="#bug-fix-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bug Fix Install&lt;/h3&gt;
&lt;p&gt;If a bug fix was made in git and you can't wait till a new release is made, you can install the bleeding edge version of &lt;code&gt;fastai&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/fastai/fastai.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-developer-install" class="anchor" aria-hidden="true" href="#developer-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developer Install&lt;/h3&gt;
&lt;p&gt;The following instructions will result in a &lt;a href="https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs" rel="nofollow"&gt;pip editable install&lt;/a&gt;, so that you can &lt;code&gt;git pull&lt;/code&gt; at any time and your environment will automatically get the updates:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/fastai/fastai
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; fastai
tools/run-after-git-clone
pip install -e &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.[dev]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, you can test that the build works by starting the jupyter notebook:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and executing an example notebook. For example load &lt;code&gt;examples/tabular.ipynb&lt;/code&gt; and run it.&lt;/p&gt;
&lt;p&gt;Please refer to &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; and &lt;a href="https://docs.fast.ai/dev/develop.html" rel="nofollow"&gt;Notes For Developers&lt;/a&gt; for more details on how to contribute to the &lt;code&gt;fastai&lt;/code&gt; project.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-building-from-source" class="anchor" aria-hidden="true" href="#building-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building From Source&lt;/h3&gt;
&lt;p&gt;If for any reason you can't use the prepackaged packages and have to build from source, this section is for you.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To build &lt;code&gt;pytorch&lt;/code&gt; from source follow the &lt;a href="https://github.com/pytorch/pytorch#from-source"&gt;complete instructions&lt;/a&gt;. Remember to first install CUDA, CuDNN, and other required libraries as suggested - everything will be very slow without those libraries built into &lt;code&gt;pytorch&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you will also need to build &lt;code&gt;torchvision&lt;/code&gt; from source:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/pytorch/vision
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; vision
python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When both &lt;code&gt;pytorch&lt;/code&gt; and &lt;code&gt;torchvision&lt;/code&gt; are installed, first test that you can load each of these libraries:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;import torch
import torchvision&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to validate that they were installed correctly&lt;/p&gt;
&lt;p&gt;Finally, proceed with &lt;code&gt;fastai&lt;/code&gt; installation as normal, either through prepackaged pip or conda builds or installing from source ("the developer install") as explained in the sections above.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-installation-issues" class="anchor" aria-hidden="true" href="#installation-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Issues&lt;/h2&gt;
&lt;p&gt;If the installation process fails, first make sure &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#is-my-system-supported"&gt;your system is supported&lt;/a&gt;. And if the problem is still not addressed, please refer to the &lt;a href="https://docs.fast.ai/troubleshoot.html" rel="nofollow"&gt;troubleshooting document&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you encounter installation problems with conda, make sure you have the latest &lt;code&gt;conda&lt;/code&gt; client (&lt;code&gt;conda install&lt;/code&gt; will do an update too):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install conda&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-is-my-system-supported" class="anchor" aria-hidden="true" href="#is-my-system-supported"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is My System Supported?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Python: You need to have python 3.6 or higher&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CPU or GPU&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;pytorch&lt;/code&gt; binary package comes with its own CUDA, CuDNN, NCCL, MKL, and other libraries so you don't have to install system-wide NVIDIA's CUDA and related libraries if you don't need them for something else. If you have them installed already it doesn't matter which NVIDIA's CUDA version library you have installed system-wide. Your system could have CUDA 9.0 libraries, and you can still use &lt;code&gt;pytorch&lt;/code&gt; build with CUDA 10.0 libraries without any problem, since the &lt;code&gt;pytorch&lt;/code&gt; binary package is self-contained.&lt;/p&gt;
&lt;p&gt;The only requirement is that you have installed and configured the NVIDIA driver correctly. Usually you can test that by running &lt;code&gt;nvidia-smi&lt;/code&gt;. While it's possible that this application is not available on your system, it's very likely that if it doesn't work, then you don't have your NVIDIA drivers configured properly. And remember that a reboot is always required after installing NVIDIA drivers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Operating System:&lt;/p&gt;
&lt;p&gt;Since fastai-1.0 relies on pytorch-1.0, you need to be able to install pytorch-1.0 first.&lt;/p&gt;
&lt;p&gt;As of this moment pytorch.org's 1.0 version supports:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Platform&lt;/th&gt;
&lt;th&gt;GPU&lt;/th&gt;
&lt;th&gt;CPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;linux&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mac&lt;/td&gt;
&lt;td&gt;source&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;windows&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Legend: &lt;code&gt;binary&lt;/code&gt; = can be installed directly, &lt;code&gt;source&lt;/code&gt; = needs to be built from source.&lt;/p&gt;
&lt;p&gt;If there is no &lt;code&gt;pytorch&lt;/code&gt; preview conda or pip package available for your system, you may still be able to &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;build it from source&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How do you know which pytorch cuda version build to choose?&lt;/p&gt;
&lt;p&gt;It depends on the version of the installed NVIDIA driver. Here are the requirements for CUDA versions supported by pre-built &lt;code&gt;pytorch&lt;/code&gt; releases:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CUDA Toolkit&lt;/th&gt;
&lt;th&gt;NVIDIA (Linux x86_64)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 10.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 410.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 9.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 384.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 8.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 367.48&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So if your NVIDIA driver is less than 384, then you can only use CUDA 8.0. Of course, you can upgrade your drivers to more recent ones if your card supports it.&lt;/p&gt;
&lt;p&gt;You can find a complete table with all variations &lt;a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you use NVIDIA driver 410+, you most likely want to install the &lt;code&gt;cudatoolkit=10.0&lt;/code&gt; pytorch variant, via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=10.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or if you need a lower version, use one of:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=8.0
conda install -c pytorch pytorch cudatoolkit=9.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For other options refer to the complete list of &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;the available pytorch variants&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;In order to update your environment, simply install &lt;code&gt;fastai&lt;/code&gt; in exactly the same way you did the initial installation.&lt;/p&gt;
&lt;p&gt;Top level files &lt;code&gt;environment.yml&lt;/code&gt; and &lt;code&gt;environment-cpu.yml&lt;/code&gt; belong to the old fastai (0.7). &lt;code&gt;conda env update&lt;/code&gt; is no longer the way to update your &lt;code&gt;fastai-1.x&lt;/code&gt; environment. These files remain because the fastai course-v2 video instructions rely on this setup. Eventually, once fastai course-v3 p1 and p2 will be completed, they will probably be moved to where they belong - under &lt;code&gt;old/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to &lt;code&gt;fastai&lt;/code&gt;, be sure to review the &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;. This project adheres to fastai's &lt;a href="https://github.com/fastai/fastai/blob/master/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/p&gt;
&lt;p&gt;We use GitHub issues for tracking requests and bugs, so please see &lt;a href="https://forums.fast.ai/" rel="nofollow"&gt;fastai forum&lt;/a&gt; for general questions and discussion.&lt;/p&gt;
&lt;p&gt;The fastai project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;A detailed history of changes can be found &lt;a href="https://github.com/fastai/fastai/blob/master/CHANGES.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h2&gt;
&lt;p&gt;Copyright 2017 onwards, fast.ai, Inc. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project's files except in compliance with the License. A copy of the License is provided in the LICENSE file in this repository.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/fastai</guid><pubDate>Mon, 18 Nov 2019 00:06:00 GMT</pubDate></item><item><title>lexfridman/mit-deep-learning #7 in Jupyter Notebook, This week</title><link>https://github.com/lexfridman/mit-deep-learning</link><description>&lt;p&gt;&lt;i&gt;Tutorials, assignments, and competitions for MIT Deep Learning related courses.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mit-deep-learning" class="anchor" aria-hidden="true" href="#mit-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MIT Deep Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://deeplearning.mit.edu/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/42d3d635cdb0e88dee786c6edc1f41e0902faa2b/68747470733a2f2f646565706c6561726e696e672e6d69742e6564752f66696c65732f696d616765732f6d69745f646565705f6c6561726e696e672e706e67" data-canonical-src="https://deeplearning.mit.edu/files/images/mit_deep_learning.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository is a collection of tutorials for &lt;a href="https://deeplearning.mit.edu/" rel="nofollow"&gt;MIT Deep Learning&lt;/a&gt; courses. More added as courses progress.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-deep-learning-basics" class="anchor" aria-hidden="true" href="#tutorial-deep-learning-basics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Deep Learning Basics&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb"&gt;&lt;img src="https://camo.githubusercontent.com/0d2d7920619e3df257f1c677431968ff491a5e80/68747470733a2f2f692e696d6775722e636f6d2f6a3446714275522e676966" data-canonical-src="https://i.imgur.com/j4FqBuR.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial accompanies the &lt;a href="https://www.youtube.com/watch?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&amp;amp;v=O5xeyoRL95U" rel="nofollow"&gt;lecture on Deep Learning Basics&lt;/a&gt;. It presents several concepts in deep learning, demonstrating the first two (feed forward and convolutional neural networks) and providing pointers to tutorials on the others. This is a good place to start.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]
[ &lt;a href="https://medium.com/tensorflow/mit-deep-learning-basics-introduction-and-overview-with-tensorflow-355bcd26baf0" rel="nofollow"&gt;Blog Post&lt;/a&gt; ]
[ &lt;a href="https://www.youtube.com/watch?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&amp;amp;v=O5xeyoRL95U" rel="nofollow"&gt;Lecture Video&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-driving-scene-segmentation" class="anchor" aria-hidden="true" href="#tutorial-driving-scene-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Driving Scene Segmentation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb"&gt;&lt;img src="images/thumb_driving_scene_segmentation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial demostrates semantic segmentation with a state-of-the-art model (DeepLab) on a sample video from the MIT Driving Scene Segmentation Dataset.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#tutorial-generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial: Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb"&gt;&lt;img src="images/thumb_mushroom_biggan.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial explores generative adversarial networks (GANs) starting with BigGAN, the state-of-the-art conditional GAN.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb"&gt;Jupyter Notebook&lt;/a&gt; ]
[ &lt;a href="https://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_gans/tutorial_gans.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deeptraffic-deep-reinforcement-learning-competition" class="anchor" aria-hidden="true" href="#deeptraffic-deep-reinforcement-learning-competition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepTraffic Deep Reinforcement Learning Competition&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://selfdrivingcars.mit.edu/deeptraffic" rel="nofollow"&gt;&lt;img src="images/thumb_deeptraffic.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;DeepTraffic is a deep reinforcement learning competition. The goal is to create a neural network that drives a vehicle (or multiple vehicles) as fast as possible through dense highway traffic.&lt;/p&gt;
&lt;p&gt;Links: [ &lt;a href="https://github.com/lexfridman/deeptraffic"&gt;GitHub&lt;/a&gt; ] [ &lt;a href="https://selfdrivingcars.mit.edu/deeptraffic" rel="nofollow"&gt;Website&lt;/a&gt; ] [ &lt;a href="https://arxiv.org/abs/1801.02805" rel="nofollow"&gt;Paper&lt;/a&gt; ]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-team" class="anchor" aria-hidden="true" href="#team"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Team&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://lexfridman.com" rel="nofollow"&gt;Lex Fridman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~liding/" rel="nofollow"&gt;Li Ding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~jterwill/" rel="nofollow"&gt;Jack Terwilliger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~glazermi/" rel="nofollow"&gt;Michael Glazer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~patsekin/" rel="nofollow"&gt;Aleksandr Patsekin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~aishni/" rel="nofollow"&gt;Aishni Parab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~aladawy/" rel="nofollow"&gt;Dina AlAdawy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mit.edu/~henris/" rel="nofollow"&gt;Henri Schmidt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lexfridman</author><guid isPermaLink="false">https://github.com/lexfridman/mit-deep-learning</guid><pubDate>Mon, 18 Nov 2019 00:07:00 GMT</pubDate></item><item><title>YunYang1994/TensorFlow2.0-Examples #8 in Jupyter Notebook, This week</title><link>https://github.com/YunYang1994/TensorFlow2.0-Examples</link><description>&lt;p&gt;&lt;i&gt;ğŸ™„ difficult algorithm, simple code.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2 align="center"&gt;&lt;a id="user-content-tensorflow20-examples" class="anchor" aria-hidden="true" href="#tensorflow20-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;ğŸ‰TensorFlow2.0-ExamplesğŸ‰!&lt;/code&gt;&lt;/h2&gt;
&lt;p align="center"&gt;"&lt;i&gt;Talk is cheap, show me the code.&lt;/i&gt;" 
  ----- Linus Torvalds&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master"&gt;
    &lt;img src="https://camo.githubusercontent.com/588410b32491be114a084c302282529d4759412a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4272616e63682d6d61737465722d677265656e2e7376673f6c6f6e6743616368653d74727565" alt="Branch" data-canonical-src="https://img.shields.io/badge/Branch-master-green.svg?longCache=true" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/stargazers"&gt;
    &lt;img src="https://camo.githubusercontent.com/9a6823f33eb16c4b8050789afcd2e248d043f8e7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f59756e59616e67313939342f54656e736f72466c6f77322e302d4578616d706c65732e7376673f6c6162656c3d5374617273267374796c653d736f6369616c" alt="Stars" data-canonical-src="https://img.shields.io/github/stars/YunYang1994/TensorFlow2.0-Examples.svg?label=Stars&amp;amp;style=social" style="max-width:100%;"&gt;
  &lt;/a&gt;
    &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/network/members"&gt;
    &lt;img src="https://camo.githubusercontent.com/0abf80c6579aab48b8fa89e521d75254b1ac1c58/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f59756e59616e67313939342f54656e736f72466c6f77322e302d4578616d706c65732e7376673f6c6162656c3d466f726b73267374796c653d736f6369616c" alt="Forks" data-canonical-src="https://img.shields.io/github/forks/YunYang1994/TensorFlow2.0-Examples.svg?label=Forks&amp;amp;style=social" style="max-width:100%;"&gt;
  &lt;/a&gt;
  
   &lt;a href="https://github.com/sindresorhus/awesome"&gt;
   &lt;img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  
   &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/LICENSE"&gt;
   &lt;img src="https://camo.githubusercontent.com/72b8fa08522b87c996b58d36be5132a346d434c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e7376673f6d61784167653d32353932303030" alt="Awesome" data-canonical-src="https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/LICENSE"&gt;
&lt;/a&gt;&lt;div align="center"&gt;&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/LICENSE"&gt;
  &lt;sub&gt;Created by
  &lt;/sub&gt;&lt;/a&gt;&lt;a href="https://github.com/YunYang1994"&gt;YunYang1994&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;p&gt;This tutorial was designed for easily diving into TensorFlow2.0.  it includes both notebooks and source codes with explanation. &lt;strong&gt;It will be continuously updated !&lt;/strong&gt; &lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;ğŸ&lt;/g-emoji&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-1---introduction" class="anchor" aria-hidden="true" href="#1---introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1 - Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hello World&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/helloworld.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/helloworld.py"&gt;code&lt;/a&gt;). Very simple example to learn how to print "hello world" using TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variable&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/variable.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/variable.py"&gt;code&lt;/a&gt;). Learn to use variable in tensorflow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Basical operation&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/basic_operations.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/basic_operations.py"&gt;code&lt;/a&gt;). A simple example that covers TensorFlow basic operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Activation&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/activation.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/activation.py"&gt;code&lt;/a&gt;). Start to know some activation functions in tensorflow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GradientTape&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/GradientTape.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/GradientTape.py"&gt;code&lt;/a&gt;). Introduce a key technique for automatic differentiation&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-2---basical-models" class="anchor" aria-hidden="true" href="#2---basical-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2 - Basical Models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/2-Basical_Models/Linear_Regression.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/Linear_Regression.py"&gt;code&lt;/a&gt;). Implement a Linear Regression with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/2-Basical_Models/Logistic_Regression.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/Logistic_Regression.py"&gt;code&lt;/a&gt;). Implement a Logistic Regression with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multilayer Perceptron Layer&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/2-Basical_Models/Multilayer_Perceptron.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/Multilayer_Perceptron.py"&gt;code&lt;/a&gt;). Implement Multi-Layer Perceptron Model with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNN&lt;/strong&gt; (&lt;a href="https://tensorflow.google.cn/tutorials/quickstart/advanced" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/CNN.py"&gt;code&lt;/a&gt;). Implement CNN Model with TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-3---neural-network-architecture" class="anchor" aria-hidden="true" href="#3---neural-network-architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3 - Neural Network Architecture&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VGG16&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/vgg16.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1409.1556.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). VGG16: Very Deep Convolutional Networks for Large-Scale Image Recognition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resnet&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/resnet.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1512.03385.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). Resnet: Deep Residual Learning for Image Recognition. &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AutoEncoder&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/autoencoder.py"&gt;code&lt;/a&gt;)(&lt;a href="http://www.cs.toronto.edu/~hinton/science.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). AutoEncoder: Reducing the Dimensionality of Data with Neural Networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68206851-b08d2580-0008-11ea-8b51-061e0cbead62.gif"&gt;&lt;img width="65%" src="https://user-images.githubusercontent.com/30433053/68206851-b08d2580-0008-11ea-8b51-061e0cbead62.gif" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FPN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/fpn.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1612.03144" rel="nofollow"&gt;paper&lt;/a&gt;). FPN: Feature Pyramid Networks for Object Detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-4---object-detection" class="anchor" aria-hidden="true" href="#4---object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4 - Object Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RPN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/RPN"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/RPN/rpn.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1703.06283.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). RPN:  a Region Proposal Network &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67913231-4e2ac400-fbc7-11e9-9995-94ed6f7181d4.png"&gt;&lt;img width="70%" src="https://user-images.githubusercontent.com/30433053/67913231-4e2ac400-fbc7-11e9-9995-94ed6f7181d4.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MTCNN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/MTCNN"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/MTCNN/mtcnn.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1604.02878" rel="nofollow"&gt;paper&lt;/a&gt;). MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks. &lt;em&gt;(Face detection and Alignment)&lt;/em&gt; &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68547531-7e6f2f80-041d-11ea-8cfb-0c5a22af0921.jpg"&gt;&lt;img width="40%" src="https://user-images.githubusercontent.com/30433053/68547531-7e6f2f80-041d-11ea-8cfb-0c5a22af0921.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;YOLOv3&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3/core/yolov3.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1804.02767.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). YOLOv3: An Incremental Improvement.&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67914531-656bb080-fbcb-11e9-9775-302a25faf747.png"&gt;&lt;img width="65%" src="https://user-images.githubusercontent.com/30433053/67914531-656bb080-fbcb-11e9-9775-302a25faf747.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/SSD"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/SSD/ssd.py"&gt;code&lt;/a&gt;)(&lt;a href="http://arxiv.org/abs/1512.02325" rel="nofollow"&gt;paper&lt;/a&gt;). SSD: Single Shot MultiBox Detector.&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt; ã€TO DOã€‘&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68290134-5f416c80-00c2-11ea-8cbc-d6010ced4efd.png"&gt;&lt;img width="56%" src="https://user-images.githubusercontent.com/30433053/68290134-5f416c80-00c2-11ea-8cbc-d6010ced4efd.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster R-CNN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/Faster-RCNN"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/Faster-RCNN/frcnn.py"&gt;code&lt;/a&gt;)(&lt;a href="http://arxiv.org/abs/1506.01497" rel="nofollow"&gt;paper&lt;/a&gt;). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt; ã€TO DOã€‘&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68546623-54187480-0413-11ea-9396-0a698c5a2580.png"&gt;&lt;img width="65%" src="https://user-images.githubusercontent.com/30433053/68546623-54187480-0413-11ea-9396-0a698c5a2580.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-5---image-segmentation" class="anchor" aria-hidden="true" href="#5---image-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5 - Image Segmentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FCN&lt;/strong&gt; (&lt;a href="5-Image_Segmentation/FCN"&gt;notebook&lt;/a&gt;) (&lt;a href="5-Image_Segmentation/FCN/fcn8s.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1411.4038" rel="nofollow"&gt;paper&lt;/a&gt;). FCN: Fully Convolutional Networks for Semantic Segmentation. &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67917411-e62eaa80-fbd3-11e9-9fe1-95550cf559d7.png"&gt;&lt;img width="60%" src="https://user-images.githubusercontent.com/30433053/67917411-e62eaa80-fbd3-11e9-9fe1-95550cf559d7.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unet&lt;/strong&gt; (&lt;a href="5-Image_Segmentation/Unet"&gt;notebook&lt;/a&gt;) (&lt;a href="5-Image_Segmentation/Unet/train.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1505.04597" rel="nofollow"&gt;paper&lt;/a&gt;). U-Net: Convolutional Networks for Biomedical Image Segmentation. &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;ğŸ”¥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67922238-2ba7a380-fbe5-11e9-96a0-55c6827bd024.png"&gt;&lt;img width="50%" src="https://user-images.githubusercontent.com/30433053/67922238-2ba7a380-fbe5-11e9-96a0-55c6827bd024.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-6---generative-adversarial-networks" class="anchor" aria-hidden="true" href="#6---generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6 - Generative Adversarial Networks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DCGAN&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/6-Generative_Adversarial_Networks/dcgan.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="6-Generative_Adversarial_Networks/dcgan.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1511.06434.pdf" rel="nofollow"&gt;paper&lt;/a&gt;).  Deep Convolutional Generative Adversarial Network.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pix2Pix&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/6-Generative_Adversarial_Networks/Pix2Pix.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="6-Generative_Adversarial_Networks/Pix2Pix.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1611.07004.pdf" rel="nofollow"&gt;paper&lt;/a&gt;).  Image-to-Image Translation with Conditional Adversarial Networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-7---reinforcement-learning" class="anchor" aria-hidden="true" href="#7---reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;7 - Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DQN&lt;/strong&gt; (&lt;a href="6-Reinforcement_Learning/YOLOV2.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="6-Reinforcement_Learning/YOLOV2.py"&gt;code&lt;/a&gt;). deep Q-network (DQN).&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>YunYang1994</author><guid isPermaLink="false">https://github.com/YunYang1994/TensorFlow2.0-Examples</guid><pubDate>Mon, 18 Nov 2019 00:08:00 GMT</pubDate></item><item><title>Azure/MachineLearningNotebooks #9 in Jupyter Notebook, This week</title><link>https://github.com/Azure/MachineLearningNotebooks</link><description>&lt;p&gt;&lt;i&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning | Microsoft&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-azure-machine-learning-service-example-notebooks" class="anchor" aria-hidden="true" href="#azure-machine-learning-service-example-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Azure Machine Learning service example notebooks&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks demonstrating the &lt;a href="https://azure.microsoft.com/en-us/services/machine-learning-service/" rel="nofollow"&gt;Azure Machine Learning&lt;/a&gt; Python SDK which allows you to build, train, deploy and manage machine learning solutions using Azure.  The AML SDK allows you the choice of using local or cloud compute resources, while managing and maintaining the complete data science workflow from the cloud.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/machine-learning/service/media/concept-azure-machine-learning-architecture/workflow.png"&gt;&lt;img src="https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/machine-learning/service/media/concept-azure-machine-learning-architecture/workflow.png" alt="Azure ML Workflow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-installation" class="anchor" aria-hidden="true" href="#quick-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick installation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install azureml-sdk&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Read more detailed instructions on &lt;a href="./NBSETUP.md"&gt;how to set up your environment&lt;/a&gt; using Azure Notebook service, your own Jupyter notebook server, or Docker.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-navigate-and-use-the-example-notebooks" class="anchor" aria-hidden="true" href="#how-to-navigate-and-use-the-example-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to navigate and use the example notebooks?&lt;/h2&gt;
&lt;p&gt;If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, you should always run the &lt;a href="./configuration.ipynb"&gt;Configuration&lt;/a&gt; notebook first when setting up a notebook library on a new machine or in a new environment. It configures your notebook library to connect to an Azure Machine Learning workspace, and sets up your workspace and compute to be used by many of the other examples.
This &lt;a href=".index.md"&gt;index&lt;/a&gt; should assist in navigating the Azure Machine Learning notebook samples and encourage efficient retrieval of topics and content.&lt;/p&gt;
&lt;p&gt;If you want to...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;...try out and explore Azure ML, start with image classification tutorials: &lt;a href="./tutorials/img-classification-part1-training.ipynb"&gt;Part 1 (Training)&lt;/a&gt; and &lt;a href="./tutorials/img-classification-part2-deploy.ipynb"&gt;Part 2 (Deployment)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...learn about experimentation and tracking run history, first &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;train within Notebook&lt;/a&gt;, then try &lt;a href="./how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"&gt;training on remote VM&lt;/a&gt; and &lt;a href="./how-to-use-azureml/training/logging-api/logging-api.ipynb"&gt;using logging APIs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...train deep learning models at scale, first learn about &lt;a href="./how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"&gt;Machine Learning Compute&lt;/a&gt;, and then try &lt;a href="./how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"&gt;distributed hyperparameter tuning&lt;/a&gt; and &lt;a href="./how-to-use-azureml/training-with-deep-learning/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"&gt;distributed training&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...deploy models as a realtime scoring service, first learn the basics by &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;training within Notebook and deploying to Azure Container Instance&lt;/a&gt;, then learn how to &lt;a href="./how-to-use-azureml/deployment/register-model-create-image-deploy-service/register-model-create-image-deploy-service.ipynb"&gt;register and manage models, and create Docker images&lt;/a&gt;, and &lt;a href="./how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"&gt;production deploy models on Azure Kubernetes Cluster&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...deploy models as a batch scoring service, first &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;train a model within Notebook&lt;/a&gt;, learn how to &lt;a href="./how-to-use-azureml/deployment/register-model-create-image-deploy-service/register-model-create-image-deploy-service.ipynb"&gt;register and manage models&lt;/a&gt;, then &lt;a href="./how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"&gt;create Machine Learning Compute for scoring compute&lt;/a&gt;, and &lt;a href="https://aka.ms/pl-batch-scoring" rel="nofollow"&gt;use Machine Learning Pipelines to deploy your model&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...monitor your deployed models, learn about using &lt;a href="./how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"&gt;App Insights&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;The &lt;a href="./tutorials"&gt;Tutorials&lt;/a&gt; folder contains notebooks for the tutorials described in the &lt;a href="https://aka.ms/aml-docs" rel="nofollow"&gt;Azure Machine Learning documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-azure-ml" class="anchor" aria-hidden="true" href="#how-to-use-azure-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use Azure ML&lt;/h2&gt;
&lt;p&gt;The &lt;a href="./how-to-use-azureml"&gt;How to use Azure ML&lt;/a&gt; folder contains specific examples demonstrating the features of the Azure Machine Learning SDK&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/training"&gt;Training&lt;/a&gt; - Examples of how to build models using Azure ML's logging and execution capabilities on local and remote compute targets&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/training-with-deep-learning"&gt;Training with Deep Learning&lt;/a&gt; - Examples demonstrating how to build deep learning models using estimators and parameter sweeps&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/manage-azureml-service"&gt;Manage Azure ML Service&lt;/a&gt; - Examples how to perform tasks, such as authenticate against Azure ML service in different ways.&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/automated-machine-learning"&gt;Automated Machine Learning&lt;/a&gt; - Examples using Automated Machine Learning to automatically generate optimal machine learning pipelines and models&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/machine-learning-pipelines"&gt;Machine Learning Pipelines&lt;/a&gt; - Examples showing how to create and use reusable pipelines for training and batch scoring&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/deployment"&gt;Deployment&lt;/a&gt; - Examples showing how to deploy and manage machine learning models and solutions&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/azure-databricks"&gt;Azure Databricks&lt;/a&gt; - Examples showing how to use Azure ML with Azure Databricks&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/monitor-models"&gt;Monitor Models&lt;/a&gt; - Examples showing how to enable model monitoring services such as DataDrift&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Quickstarts, end-to-end tutorials, and how-tos on the &lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/" rel="nofollow"&gt;official documentation site for Azure Machine Learning service&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py" rel="nofollow"&gt;Python SDK reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure ML Data Prep SDK &lt;a href="https://aka.ms/data-prep-sdk" rel="nofollow"&gt;overview&lt;/a&gt;, &lt;a href="https://aka.ms/aml-data-prep-apiref" rel="nofollow"&gt;Python SDK reference&lt;/a&gt;, and &lt;a href="https://aka.ms/aml-data-prep-notebooks" rel="nofollow"&gt;tutorials and how-tos&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-community-repository" class="anchor" aria-hidden="true" href="#community-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community Repository&lt;/h2&gt;
&lt;p&gt;Visit this &lt;a href="https://github.com/microsoft/MLOps/tree/master/examples"&gt;community repository&lt;/a&gt; to find useful end-to-end sample notebooks. Also, please follow these &lt;a href="https://github.com/microsoft/MLOps/blob/master/contributing.md"&gt;contribution guidelines&lt;/a&gt; when contributing to this repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects-using-azure-machine-learning" class="anchor" aria-hidden="true" href="#projects-using-azure-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects using Azure Machine Learning&lt;/h2&gt;
&lt;p&gt;Visit following repos to see projects contributed by Azure ML users:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Azure/AMLSamples"&gt;AMLSamples&lt;/a&gt; Number of end-to-end examples, including face recognition, predictive maintenance, customer churn and sentiment analysis.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/nlp"&gt;Learn about Natural Language Processing best practices using Azure Machine Learning service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Microsoft/AzureML-BERT"&gt;Pre-Train BERT models using Azure Machine Learning service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/amynic/azureml-sdk-fashion"&gt;Fashion MNIST with Azure ML SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/katiehouse3/microsoft-azure-ml-notebooks"&gt;UMass Amherst Student Samples&lt;/a&gt; - A number of end-to-end machine learning notebooks, including machine translation, image classification, and customer churn, created by students in the 696DS course at UMass Amherst.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-datatelemetry" class="anchor" aria-hidden="true" href="#datatelemetry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data/Telemetry&lt;/h2&gt;
&lt;p&gt;This repository collects usage data and sends it to Mircosoft to help improve our products and services. Read Microsoft's &lt;a href="https://privacy.microsoft.com/en-US/privacystatement" rel="nofollow"&gt;privacy statement to learn more&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To opt out of tracking, please go to the raw markdown or .ipynb files and remove the following line of code:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/README.png)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This URL will be slightly different depending on the file.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c46ad7e1ce8c1c4ffaf085f08619fe9c1b16eb50/68747470733a2f2f506978656c53657276657232303139303432333131343233382e617a75726577656273697465732e6e65742f6170692f696d7072657373696f6e732f4d616368696e654c6561726e696e674e6f7465626f6f6b732f524541444d452e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c46ad7e1ce8c1c4ffaf085f08619fe9c1b16eb50/68747470733a2f2f506978656c53657276657232303139303432333131343233382e617a75726577656273697465732e6e65742f6170692f696d7072657373696f6e732f4d616368696e654c6561726e696e674e6f7465626f6f6b732f524541444d452e706e67" alt="Impressions" data-canonical-src="https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/README.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Azure</author><guid isPermaLink="false">https://github.com/Azure/MachineLearningNotebooks</guid><pubDate>Mon, 18 Nov 2019 00:09:00 GMT</pubDate></item><item><title>dotnet/try #10 in Jupyter Notebook, This week</title><link>https://github.com/dotnet/try</link><description>&lt;p&gt;&lt;i&gt;Try .NET provides developers and content authors with tools to create interactive experiences.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-try-net-" class="anchor" aria-hidden="true" href="#try-net-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try .NET &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/56708992-deee8780-66ec-11e9-9991-eb85abb1d10a.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/56708992-deee8780-66ec-11e9-9991-eb85abb1d10a.png" width="80px" alt="dotnet bot in space" align="right" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;|| &lt;a href="#contribution-guidelines"&gt;&lt;strong&gt;Contribution Guidelines&lt;/strong&gt;&lt;/a&gt; || &lt;a href="#table-of-contents"&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/a&gt; || &lt;a href="#customers--partners"&gt;&lt;strong&gt;Customers &amp;amp; Partners&lt;/strong&gt;&lt;/a&gt; ||&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/845bd5c6c705e84b68997ab32b7a4437ed366fb7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5472795f2e4e45542d456e61626c65642d3530313037382e737667"&gt;&lt;img src="https://camo.githubusercontent.com/845bd5c6c705e84b68997ab32b7a4437ed366fb7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5472795f2e4e45542d456e61626c65642d3530313037382e737667" alt="Try_.NET Enabled" data-canonical-src="https://img.shields.io/badge/Try_.NET-Enabled-501078.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dev.azure.com/dnceng/public/_build/latest?definitionId=495&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/93b7ca6217cf06f12405c38f80e27b9578f0ee18/68747470733a2f2f6465762e617a7572652e636f6d2f646e63656e672f7075626c69632f5f617069732f6275696c642f7374617475732f646f746e65742f7472792f7472792d63693f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/dnceng/public/_apis/build/status/dotnet/try/try-ci?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-welcome-to-the-try-net-repo" class="anchor" aria-hidden="true" href="#welcome-to-the-try-net-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to the Try .NET repo.&lt;/h2&gt;
&lt;p&gt;Try .NET provides developers and content authors with tools to create interactive experiences.&lt;/p&gt;
&lt;p&gt;There are different flavors of  Try .NET interactive experiences, from the web experience powered by Blazor (as seen on &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/tutorials/intro-to-csharp/hello-world" rel="nofollow"&gt;Microsoft docs&lt;/a&gt;), to interactive documentation for .NET Core with the &lt;a href="DotNetTry.md"&gt;dotnet try global tool&lt;/a&gt;, to &lt;a href="Notebook.md"&gt;.NET Jupyter Notebooks&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-flavors-of-try-net" class="anchor" aria-hidden="true" href="#flavors-of-try-net"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Flavors of Try .NET&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-try-net-online" class="anchor" aria-hidden="true" href="#try-net-online"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try .NET Online&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/57144765-c850cc00-6d8f-11e9-982d-50d2b6dc3591.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/57144765-c850cc00-6d8f-11e9-982d-50d2b6dc3591.gif" width="60%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-interactive-net-core-documentation-with-the-dotnet-try-global-tool" class="anchor" aria-hidden="true" href="#interactive-net-core-documentation-with-the-dotnet-try-global-tool"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interactive .NET Core documentation with the &lt;code&gt;dotnet try&lt;/code&gt; global tool&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/57158389-47a2c780-6db1-11e9-96ad-8c6e9ab52853.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/57158389-47a2c780-6db1-11e9-96ad-8c6e9ab52853.png" width="52%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jupyter-notebooks-with-net" class="anchor" aria-hidden="true" href="#jupyter-notebooks-with-net"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebooks with .NET&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/67889988-3b13e780-fb26-11e9-91a1-48d5972b5df2.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/67889988-3b13e780-fb26-11e9-91a1-48d5972b5df2.png" width="60%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/2546640/67912370-1b99b080-fb60-11e9-9839-0058d02488cf.png"&gt;&lt;img src="https://user-images.githubusercontent.com/2546640/67912370-1b99b080-fb60-11e9-9839-0058d02488cf.png" width="52%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="DotNetTry.md"&gt;Getting Started with Try .NET&lt;/a&gt;: Interactive documentation generator for .NET Core.&lt;/li&gt;
&lt;li&gt;&lt;a href="Notebook.md"&gt;Getting started with .NET Jupyter Notebooks&lt;/a&gt; : Write and run .NET code in Jupyter Notebooks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution Guidelines&lt;/h2&gt;
&lt;p&gt;As we are still in the early stages of our development, we are unable to take any feature PRs at the moment, but we intend to do so in the future.
If you find an issue or have a feature suggestion, please open an &lt;a href="https://github.com/dotnet/try/issues/new/choose"&gt;issue&lt;/a&gt;. And if you have any feature suggestions, please submit them using the "community suggestions" label.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-customers--partners" class="anchor" aria-hidden="true" href="#customers--partners"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Customers &amp;amp; Partners&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Microsoft Docs&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://azure.microsoft.com/en-us/services/synapse-analytics/" rel="nofollow"&gt;Azure Synapse Analytics &lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;Azure HDInsight (HDI)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Microsoft Docs uses &lt;a href="https://docs.microsoft.com/en-us/dotnet/csharp/tutorials/intro-to-csharp/hello-world" rel="nofollow"&gt;Try .NET&lt;/a&gt; to create interactive documentation. Users can run and edit .NET code in the browser.&lt;/td&gt;
&lt;td align="center"&gt;Azure Synapse Analytics uses the .NET kernel to write and run quick ad-hoc queries in addition to developing complete, end-to-end big data scenarios, such as reading in data, transforming it, and visualizing it&lt;/td&gt;
&lt;td align="center"&gt;You can launch Jupyter notebooks from your HDInsight cluster to run big data queries against the compute resources in that cluster.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dotnet</author><guid isPermaLink="false">https://github.com/dotnet/try</guid><pubDate>Mon, 18 Nov 2019 00:10:00 GMT</pubDate></item><item><title>NVIDIA/tacotron2 #11 in Jupyter Notebook, This week</title><link>https://github.com/NVIDIA/tacotron2</link><description>&lt;p&gt;&lt;i&gt;Tacotron 2 - PyTorch implementation with faster-than-realtime inference&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tacotron-2-without-wavenet" class="anchor" aria-hidden="true" href="#tacotron-2-without-wavenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tacotron 2 (without wavenet)&lt;/h1&gt;
&lt;p&gt;PyTorch implementation of &lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;Natural TTS Synthesis By Conditioning
Wavenet On Mel Spectrogram Predictions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This implementation includes &lt;strong&gt;distributed&lt;/strong&gt; and &lt;strong&gt;automatic mixed precision&lt;/strong&gt; support
and uses the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJSpeech dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Distributed and Automatic Mixed Precision support relies on NVIDIA's &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt; and &lt;a href="https://github.com/NVIDIA/apex/tree/master/apex/amp"&gt;AMP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit our &lt;a href="https://nv-adlr.github.io/WaveGlow" rel="nofollow"&gt;website&lt;/a&gt; for audio samples using our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; and
&lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; models.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="tensorboard.png"&gt;&lt;img src="tensorboard.png" alt="Alignment, Predicted Mel Spectrogram, Target Mel Spectrogram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-requisites" class="anchor" aria-hidden="true" href="#pre-requisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-requisites&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;NVIDIA GPU + CUDA cuDNN&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download and extract the &lt;a href="https://keithito.com/LJ-Speech-Dataset/" rel="nofollow"&gt;LJ Speech dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clone this repo: &lt;code&gt;git clone https://github.com/NVIDIA/tacotron2.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;CD into this repo: &lt;code&gt;cd tacotron2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Initialize submodule: &lt;code&gt;git submodule init; git submodule update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update .wav paths: &lt;code&gt;sed -i -- 's,DUMMY,ljs_dataset_folder/wavs,g' filelists/*.txt&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, set &lt;code&gt;load_mel_from_disk=True&lt;/code&gt; in &lt;code&gt;hparams.py&lt;/code&gt; and update mel-spectrogram paths&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/pytorch/pytorch#installation"&gt;PyTorch 1.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install &lt;a href="https://github.com/nvidia/apex"&gt;Apex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install python requirements or build docker image
&lt;ul&gt;
&lt;li&gt;Install python requirements: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;(OPTIONAL) &lt;code&gt;tensorboard --logdir=outdir/logdir&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-training-using-a-pre-trained-model" class="anchor" aria-hidden="true" href="#training-using-a-pre-trained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training using a pre-trained model&lt;/h2&gt;
&lt;p&gt;Training using a pre-trained model can lead to faster convergence&lt;br&gt;
By default, the dataset dependent text embedding layers are &lt;a href="https://github.com/NVIDIA/tacotron2/blob/master/hparams.py#L22"&gt;ignored&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;python train.py --output_directory=outdir --log_directory=logdir -c tacotron2_statedict.pt --warm_start&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-distributed-and-automatic-mixed-precision-training" class="anchor" aria-hidden="true" href="#multi-gpu-distributed-and-automatic-mixed-precision-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU (distributed) and Automatic Mixed Precision Training&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;python -m multiproc train.py --output_directory=outdir --log_directory=logdir --hparams=distributed_run=True,fp16_run=True&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-inference-demo" class="anchor" aria-hidden="true" href="#inference-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference demo&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA/view?usp=sharing" rel="nofollow"&gt;Tacotron 2&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;Download our published &lt;a href="https://drive.google.com/file/d/1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx/view?usp=sharing" rel="nofollow"&gt;WaveGlow&lt;/a&gt; model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;jupyter notebook --ip=127.0.0.1 --port=31337&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Load inference.ipynb&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;N.b.  When performing Mel-Spectrogram to Audio synthesis, make sure Tacotron 2
and the Mel decoder were trained on the same mel-spectrogram representation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-repos" class="anchor" aria-hidden="true" href="#related-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related repos&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/WaveGlow"&gt;WaveGlow&lt;/a&gt; Faster than real time Flow-based
Generative Network for Speech Synthesis&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/nv-wavenet/"&gt;nv-wavenet&lt;/a&gt; Faster than real time
WaveNet.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This implementation uses code from the following repos: &lt;a href="https://github.com/keithito/tacotron/"&gt;Keith
Ito&lt;/a&gt;, &lt;a href="https://github.com/pseeth/pytorch-stft"&gt;Prem
Seetharaman&lt;/a&gt; as described in our code.&lt;/p&gt;
&lt;p&gt;We are inspired by &lt;a href="https://github.com/r9y9/tacotron_pytorch"&gt;Ryuchi Yamamoto's&lt;/a&gt;
Tacotron PyTorch implementation.&lt;/p&gt;
&lt;p&gt;We are thankful to the Tacotron 2 paper authors, specially Jonathan Shen, Yuxuan
Wang and Zongheng Yang.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/tacotron2</guid><pubDate>Mon, 18 Nov 2019 00:11:00 GMT</pubDate></item><item><title>ultralytics/yolov3 #12 in Jupyter Notebook, This week</title><link>https://github.com/ultralytics/yolov3</link><description>&lt;p&gt;&lt;i&gt;YOLOv3 in PyTorch &gt; ONNX &gt; CoreML &gt; iOS&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td align="center"&gt;
    &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/c7f01c9051691f7f4c6239349b6b55cb5a0871c9/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f756c7472616c79746963732f6c6f676f2f6c6f676f6e616d65313030302e706e67" width="160" data-canonical-src="https://storage.googleapis.com/ultralytics/logo/logoname1000.png" style="max-width:100%;"&gt;&lt;/a&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
          &lt;a href="https://itunes.apple.com/app/id1452689527" rel="nofollow"&gt;
    &lt;img src="https://user-images.githubusercontent.com/26833433/50044365-9b22ac00-0082-11e9-862f-e77aee7aa7b0.png" width="180" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC, and &lt;strong&gt;is freely available for redistribution under the GPL-3.0 license&lt;/strong&gt;. For more information please visit &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;https://www.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/ultralytics/yolov3"&gt;https://github.com/ultralytics/yolov3&lt;/a&gt; repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux, MacOS and Windows. Training is done on the COCO dataset by default: &lt;a href="https://cocodataset.org/#home" rel="nofollow"&gt;https://cocodataset.org/#home&lt;/a&gt;. &lt;strong&gt;Credit to Joseph Redmon for YOLO:&lt;/strong&gt; &lt;a href="https://pjreddie.com/darknet/yolo/" rel="nofollow"&gt;https://pjreddie.com/darknet/yolo/&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Python 3.7 or later with the following &lt;code&gt;pip3 install -U -r requirements.txt&lt;/code&gt; packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;torch &amp;gt;= 1.1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;opencv-python&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart"&gt;GCP Quickstart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning"&gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image"&gt;Train Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class"&gt;Train Single Class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data"&gt;Train Custom Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-jupyter-notebook" class="anchor" aria-hidden="true" href="#jupyter-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook&lt;/h1&gt;
&lt;p&gt;Our Jupyter &lt;a href="https://colab.research.google.com/github/ultralytics/yolov3/blob/master/examples.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt; provides quick training, inference and testing examples.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Start Training:&lt;/strong&gt; &lt;code&gt;python3 train.py&lt;/code&gt; to begin training after downloading COCO data with &lt;code&gt;data/get_coco_dataset.sh&lt;/code&gt;. Each epoch trains on 117,263 images from the train and validate COCO sets, and tests on 5000 images from the COCO validate set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resume Training:&lt;/strong&gt; &lt;code&gt;python3 train.py --resume&lt;/code&gt; to resume training from &lt;code&gt;weights/last.pt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Plot Training:&lt;/strong&gt; &lt;code&gt;from utils import utils; utils.plot_results()&lt;/code&gt; plots training results from &lt;code&gt;coco_16img.data&lt;/code&gt;, &lt;code&gt;coco_64img.data&lt;/code&gt;, 2 example datasets available in the &lt;code&gt;data/&lt;/code&gt; folder, which train and test on the first 16 and 64 images of the COCO2014-trainval dataset.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-image-augmentation" class="anchor" aria-hidden="true" href="#image-augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;datasets.py&lt;/code&gt; applies random OpenCV-powered (&lt;a href="https://opencv.org/" rel="nofollow"&gt;https://opencv.org/&lt;/a&gt;) augmentation to the input images in accordance with the following specifications. Augmentation is applied &lt;strong&gt;only&lt;/strong&gt; during training, not during inference. Bounding boxes are automatically tracked and updated with the images. 416 x 416 examples pictured below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Augmentation&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Translation&lt;/td&gt;
&lt;td&gt;+/- 10% (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotation&lt;/td&gt;
&lt;td&gt;+/- 5 degrees&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Shear&lt;/td&gt;
&lt;td&gt;+/- 2 degrees (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale&lt;/td&gt;
&lt;td&gt;+/- 10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reflection&lt;/td&gt;
&lt;td&gt;50% probability (horizontal-only)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;strong&gt;S&lt;/strong&gt;V Saturation&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HS&lt;strong&gt;V&lt;/strong&gt; Intensity&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/deep-learning-vm/" rel="nofollow"&gt;https://cloud.google.com/deep-learning-vm/&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Machine type:&lt;/strong&gt; n1-standard-8 (8 vCPUs, 30 GB memory)&lt;br&gt;
&lt;strong&gt;CPU platform:&lt;/strong&gt; Intel Skylake&lt;br&gt;
&lt;strong&gt;GPUs:&lt;/strong&gt; K80 ($0.20/hr), T4 ($0.35/hr), V100 ($0.83/hr) CUDA with &lt;a href="https://github.com/NVIDIA/apex"&gt;Nvidia Apex&lt;/a&gt; FP16/32&lt;br&gt;
&lt;strong&gt;HDD:&lt;/strong&gt; 100 GB SSD&lt;br&gt;
&lt;strong&gt;Dataset:&lt;/strong&gt; COCO train 2014 (117,263 images)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GPUs&lt;/th&gt;
&lt;th&gt;&lt;code&gt;batch_size&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;images/sec&lt;/th&gt;
&lt;th&gt;epoch time&lt;/th&gt;
&lt;th&gt;epoch cost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;K80&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;175 min&lt;/td&gt;
&lt;td&gt;$0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;49 min&lt;/td&gt;
&lt;td&gt;$0.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4 x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;61&lt;/td&gt;
&lt;td&gt;32 min&lt;/td&gt;
&lt;td&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;115&lt;/td&gt;
&lt;td&gt;17 min&lt;/td&gt;
&lt;td&gt;$0.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100 x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;td&gt;13 min&lt;/td&gt;
&lt;td&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti&lt;/td&gt;
&lt;td&gt;64 (32x2)&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;24 min&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti x2&lt;/td&gt;
&lt;td&gt;64 (64x1)&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;14 min&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-inference" class="anchor" aria-hidden="true" href="#inference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt; runs inference on any sources:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 detect.py --source ...&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Image:  &lt;code&gt;--source file.jpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Video:  &lt;code&gt;--source file.mp4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Directory:  &lt;code&gt;--source dir/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Webcam:  &lt;code&gt;--source 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;RTSP stream:  &lt;code&gt;--source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;HTTP stream:  &lt;code&gt;--source http://wmccpinetop.axiscam.net/mjpg/video.mjpg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To run a specific models:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3.cfg --weights weights/yolov3.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-tiny:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-tiny.cfg --weights weights/yolov3-tiny.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-SPP:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-spp.cfg --weights weights/yolov3-spp.weights&lt;/code&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Darknet &lt;code&gt;*.weights&lt;/code&gt; format: &lt;a href="https://pjreddie.com/media/files/yolov3.weights" rel="nofollow"&gt;https://pjreddie.com/media/files/yolov3.weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyTorch &lt;code&gt;*.pt&lt;/code&gt; format: &lt;a href="https://drive.google.com/drive/folders/1uxgUBemJVw9wZsdpboYbzUN4bcRhsuAI" rel="nofollow"&gt;https://drive.google.com/drive/folders/1uxgUBemJVw9wZsdpboYbzUN4bcRhsuAI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-darknet-conversion" class="anchor" aria-hidden="true" href="#darknet-conversion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Darknet Conversion&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/ultralytics/yolov3 &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolov3

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert darknet cfg/weights to pytorch model&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.weights')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert cfg/pytorch model to darknet weights&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.pt')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-map" class="anchor" aria-hidden="true" href="#map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mAP&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/yolov3.weights&lt;/code&gt; tests official YOLOv3 weights.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/last.pt&lt;/code&gt; tests most recent checkpoint.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.py --weights weights/best.pt&lt;/code&gt; tests best checkpoint.&lt;/li&gt;
&lt;li&gt;Compare to darknet published results &lt;a href="https://arxiv.org/abs/1804.02767" rel="nofollow"&gt;https://arxiv.org/abs/1804.02767&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/ultralytics/yolov3"&gt;ultralytics/yolov3&lt;/a&gt; mAP@0.5 (&lt;a href="https://arxiv.org/abs/1804.02767" rel="nofollow"&gt;darknet&lt;/a&gt;-reported mAP@0.5)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;i&gt;&lt;/i&gt;&lt;/th&gt;
&lt;th&gt;320&lt;/th&gt;
&lt;th&gt;416&lt;/th&gt;
&lt;th&gt;608&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;YOLOv3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;51.8 (51.5)&lt;/td&gt;
&lt;td&gt;55.4 (55.3)&lt;/td&gt;
&lt;td&gt;58.2 (57.9)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;YOLOv3-SPP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;53.7&lt;/td&gt;
&lt;td&gt;57.7&lt;/td&gt;
&lt;td&gt;60.7 (60.6)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;YOLOv3-tiny&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;29.0&lt;/td&gt;
&lt;td&gt;32.9 (33.1)&lt;/td&gt;
&lt;td&gt;35.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 test.py --save-json --img-size 608
Namespace(batch_size=16, cfg=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;cfg/yolov3-spp.cfg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, conf_thres=0.001, data=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/coco.data&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, img_size=608, iou_thres=0.5, nms_thres=0.5, save_json=True, weights=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
Using CUDA device0 _CudaDeviceProperties(name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Tesla T4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, total_memory=15079MB)
                Class    Images   Targets         P         R       mAP        F1: 100% 313/313 [07:&lt;span class="pl-k"&gt;40&amp;lt;&lt;/span&gt;00:00,  2.34s/it]
                  all     5e+03  3.58e+04     0.119     0.788     0.594     0.201
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.367 &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;---
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.607 &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;---
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.75      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.387
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.208
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.392
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.487
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;  1 ] = 0.297
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt; 10 ] = 0.465
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.495
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.332
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.518
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.621

$ python3 test.py --save-json --img-size 416
Namespace(batch_size=16, cfg=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;cfg/yolov3-spp.cfg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, conf_thres=0.001, data=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/coco.data&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, img_size=416, iou_thres=0.5, nms_thres=0.5, save_json=True, weights=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3s-ultralytics.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
Using CUDA device0 _CudaDeviceProperties(name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Tesla T4&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, total_memory=15079MB)
                Class    Images   Targets         P         R       mAP        F1: 100% 313/313 [07:&lt;span class="pl-k"&gt;01&amp;lt;&lt;/span&gt;00:00,  1.41s/it]
                  all     5e+03  3.58e+04      0.11     0.739     0.569     0.185
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.373
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.577
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.75      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.392
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.175
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.403
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.537
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;  1 ] = 0.313
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt; 10 ] = 0.482
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.501
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.266
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.541
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.693&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/146165888" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cd760a8900fd4be0105229509d566b7c9499ef8d/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134363136353838382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/146165888.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;Issues should be raised directly in the repository. For additional questions or comments please email Glenn Jocher at &lt;a href="mailto:glenn.jocher@ultralytics.com"&gt;glenn.jocher@ultralytics.com&lt;/a&gt; or visit us at &lt;a href="https://contact.ultralytics.com" rel="nofollow"&gt;https://contact.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ultralytics</author><guid isPermaLink="false">https://github.com/ultralytics/yolov3</guid><pubDate>Mon, 18 Nov 2019 00:12:00 GMT</pubDate></item><item><title>NVIDIA-AI-IOT/jetbot #13 in Jupyter Notebook, This week</title><link>https://github.com/NVIDIA-AI-IOT/jetbot</link><description>&lt;p&gt;&lt;i&gt;An educational AI robot based on NVIDIA Jetson Nano.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-jetbot" class="anchor" aria-hidden="true" href="#jetbot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;JetBot&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Interested in making projects like this with Jetson Nano?  Check out the &lt;a href="https://info.nvidia.com/ai-for-makers-learn-with-jetbot-reg-page.html?nvid=nv-int-84114" rel="nofollow"&gt;webinar&lt;/a&gt; we released on 5/16/2019!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="../..//wiki/images/jetson-jetbot-illustration_1600x1260.png"&gt;&lt;img src="../..//wiki/images/jetson-jetbot-illustration_1600x1260.png" height="256" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;JetBot is an open-source robot based on NVIDIA Jetson Nano that is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Affordable&lt;/strong&gt; - Less than $150 add-on to Jetson Nano&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Educational&lt;/strong&gt; - Tutorials from basic motion to AI based collision avoidance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fun!&lt;/strong&gt; - Interactively programmed from your web browser&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Building and using JetBot gives the hands on experience needed to create entirely new AI projects.&lt;/p&gt;
&lt;p&gt;To get started, read the &lt;a href="https://github.com/NVIDIA-AI-IOT/jetbot/wiki"&gt;JetBot Wiki&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/igrigorik/ga-beacon"&gt;&lt;img src="https://camo.githubusercontent.com/48228270fe5e7df2a81064448b94b395450e67a2/68747470733a2f2f67612d626561636f6e2e61707073706f742e636f6d2f55412d3133353931393531302d312f6a6574626f742f524541444d453f706978656c" alt="Analytics" data-canonical-src="https://ga-beacon.appspot.com/UA-135919510-1/jetbot/README?pixel" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA-AI-IOT</author><guid isPermaLink="false">https://github.com/NVIDIA-AI-IOT/jetbot</guid><pubDate>Mon, 18 Nov 2019 00:13:00 GMT</pubDate></item><item><title>slundberg/shap #14 in Jupyter Notebook, This week</title><link>https://github.com/slundberg/shap</link><description>&lt;p&gt;&lt;i&gt;A unified approach to explain the output of any machine learning model.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png"&gt;&lt;img src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/slundberg/shap" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/19de279a6f67f8eea3f52ccecc779c3e1aff55e7/68747470733a2f2f7472617669732d63692e6f72672f736c756e64626572672f736861702e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/slundberg/shap.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://mybinder.org/v2/gh/slundberg/shap/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our &lt;a href="#citations"&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;SHAP can be installed from either &lt;a href="https://pypi.org/project/shap" rel="nofollow"&gt;PyPI&lt;/a&gt; or &lt;a href="https://anaconda.org/conda-forge/shap" rel="nofollow"&gt;conda-forge&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;pip install shap
&lt;i&gt;or&lt;/i&gt;
conda install -c conda-forge shap
&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models" class="anchor" aria-hidden="true" href="#tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tree ensemble example with TreeExplainer (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt;
&lt;p&gt;While SHAP values can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (&lt;a href="https://arxiv.org/abs/1802.03888" rel="nofollow"&gt;Tree SHAP arXiv paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; xgboost
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load JS visualization code to notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train XGBoost model&lt;/span&gt;
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.boston()
model &lt;span class="pl-k"&gt;=&lt;/span&gt; xgboost.train({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.01&lt;/span&gt;}, xgboost.DMatrix(X, &lt;span class="pl-v"&gt;label&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;y), &lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain the model's predictions using SHAP values&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.TreeExplainer(model)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue (these force plots are introduced in our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature BME paper&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the training set predictions&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions &lt;code&gt;dependence_plot&lt;/code&gt; automatically selects another feature for coloring. In this case coloring by RAD (index of accessibility to radial highways) highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; create a SHAP dependence plot to show the effect of a single feature across the whole dataset&lt;/span&gt;
shap.dependence_plot(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;RM&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png"&gt;&lt;img width="544" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; summarize the effects of all the features&lt;/span&gt;
shap.summary_plot(shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;shap.summary_plot(shap_values, X, &lt;span class="pl-v"&gt;plot_type&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;bar&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png"&gt;&lt;img width="470" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-deepexplainer-tensorflowkeras-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-deepexplainer-tensorflowkeras-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt;
&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href="https://arxiv.org/abs/1704.02685" rel="nofollow"&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&lt;/span&gt;

&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; select a set of background examples to take an expectation over&lt;/span&gt;
background &lt;span class="pl-k"&gt;=&lt;/span&gt; x_train[np.random.choice(x_train.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;replace&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain predictions of the model on four images&lt;/span&gt;
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.DeepExplainer(model, background)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...or pass tensors directly&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&lt;/span&gt;
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the feature attributions&lt;/span&gt;
shap.image_plot(shap_values, &lt;span class="pl-k"&gt;-&lt;/span&gt;x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png"&gt;&lt;img width="820" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt;
&lt;p&gt;Expected gradients combines ideas from &lt;a href="https://arxiv.org/abs/1703.01365" rel="nofollow"&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href="https://arxiv.org/abs/1706.03825" rel="nofollow"&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;VGG16&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; preprocess_input
&lt;span class="pl-k"&gt;import&lt;/span&gt; keras.backend &lt;span class="pl-k"&gt;as&lt;/span&gt; K
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; json
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load pre-trained model and choose two images to explain&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; VGG16(&lt;span class="pl-v"&gt;weights&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;imagenet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;include_top&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.imagenet50()
to_explain &lt;span class="pl-k"&gt;=&lt;/span&gt; X[[&lt;span class="pl-c1"&gt;39&lt;/span&gt;,&lt;span class="pl-c1"&gt;41&lt;/span&gt;]]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load the ImageNet class names&lt;/span&gt;
url &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
fname &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.cache(url)
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(fname) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
    class_names &lt;span class="pl-k"&gt;=&lt;/span&gt; json.load(f)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain how the input to the 7th layer of the model explains the top two classes&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;map2layer&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;layer&lt;/span&gt;):
    feed_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;dict&lt;/span&gt;(&lt;span class="pl-c1"&gt;zip&lt;/span&gt;([model.layers[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].input], [preprocess_input(x.copy())]))
    &lt;span class="pl-k"&gt;return&lt;/span&gt; K.get_session().run(model.layers[layer].input, feed_dict)
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.GradientExplainer(
    (model.layers[&lt;span class="pl-c1"&gt;7&lt;/span&gt;].input, model.layers[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;].output),
    map2layer(X, &lt;span class="pl-c1"&gt;7&lt;/span&gt;),
    &lt;span class="pl-v"&gt;local_smoothing&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; std dev of smoothing noise&lt;/span&gt;
)
shap_values,indexes &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(map2layer(to_explain, &lt;span class="pl-c1"&gt;7&lt;/span&gt;), &lt;span class="pl-v"&gt;ranked_outputs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get the names for the classes&lt;/span&gt;
index_names &lt;span class="pl-k"&gt;=&lt;/span&gt; np.vectorize(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: class_names[&lt;span class="pl-c1"&gt;str&lt;/span&gt;(x)][&lt;span class="pl-c1"&gt;1&lt;/span&gt;])(indexes)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the explanations&lt;/span&gt;
shap.image_plot(shap_values, to_explain, index_names)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png"&gt;&lt;img width="500" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-agnostic-example-with-kernelexplainer-explains-any-function" class="anchor" aria-hidden="true" href="#model-agnostic-example-with-kernelexplainer-explains-any-function"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt;
&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sklearn
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class="pl-k"&gt;import&lt;/span&gt; train_test_split

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; print the JS visualization code to the notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train a SVM classifier&lt;/span&gt;
X_train,X_test,Y_train,Y_test &lt;span class="pl-k"&gt;=&lt;/span&gt; train_test_split(&lt;span class="pl-k"&gt;*&lt;/span&gt;shap.datasets.iris(), &lt;span class="pl-v"&gt;test_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-v"&gt;random_state&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
svm &lt;span class="pl-k"&gt;=&lt;/span&gt; sklearn.svm.SVC(&lt;span class="pl-v"&gt;kernel&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rbf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;probability&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
svm.fit(X_train, Y_train)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; use Kernel SHAP to explain test set predictions&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.KernelExplainer(svm.predict_proba, X_train, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X_test, &lt;span class="pl-v"&gt;nsamples&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of the first instance&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X_test.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png"&gt;&lt;img width="810" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of all instances&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], X_test, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png"&gt;&lt;img width="813" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-shap-interaction-values" class="anchor" aria-hidden="true" href="#shap-interaction-values"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SHAP Interaction Values&lt;/h2&gt;
&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sample-notebooks" class="anchor" aria-hidden="true" href="#sample-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample notebooks&lt;/h2&gt;
&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-treeexplainer" class="anchor" aria-hidden="true" href="#treeexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TreeExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html" rel="nofollow"&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html" rel="nofollow"&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deepexplainer" class="anchor" aria-hidden="true" href="#deepexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html" rel="nofollow"&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html" rel="nofollow"&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-gradientexplainer" class="anchor" aria-hidden="true" href="#gradientexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GradientExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html" rel="nofollow"&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-linearexplainer" class="anchor" aria-hidden="true" href="#linearexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LinearExplainer&lt;/h3&gt;
&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covaraince matrix. LinearExplainer supports both of these options.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html" rel="nofollow"&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kernelexplainer" class="anchor" aria-hidden="true" href="#kernelexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KernelExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes not assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html" rel="nofollow"&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation-notebooks" class="anchor" aria-hidden="true" href="#documentation-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation notebooks&lt;/h2&gt;
&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/decision_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-methods-unified-by-shap" class="anchor" aria-hidden="true" href="#methods-unified-by-shap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Methods Unified by SHAP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems." Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015): e0130140.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. "Analysis of regression in game theory approach." Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href="http://blog.datadive.net/interpreting-random-forests/" rel="nofollow"&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citations&lt;/h2&gt;
&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href="https://suinlee.cs.washington.edu" rel="nofollow"&gt;Su-In Lee's lab&lt;/a&gt; at the University of Washington. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For general use of SHAP you can read/cite our &lt;a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions" rel="nofollow"&gt;NeurIPS paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For TreeExplainer you can (for now) read/cite our &lt;a href="https://arxiv.org/abs/1905.04610" rel="nofollow"&gt;arXiv paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/treeshap_arxiv.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;; &lt;a href="https://rdcu.be/baVbR" rel="nofollow"&gt;free access&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31"&gt;&lt;img height="1" width="1" src="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31" data-canonical-src="https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>slundberg</author><guid isPermaLink="false">https://github.com/slundberg/shap</guid><pubDate>Mon, 18 Nov 2019 00:14:00 GMT</pubDate></item><item><title>jantic/DeOldify #15 in Jupyter Notebook, This week</title><link>https://github.com/jantic/DeOldify</link><description>&lt;p&gt;&lt;i&gt;A Deep Learning based project for colorizing and restoring old images (and video!)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deoldify" class="anchor" aria-hidden="true" href="#deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeOldify&lt;/h1&gt;
&lt;p&gt;Image &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; |
Video &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt; Instructions on how to use the Colabs above have been kindly provided in video tutorial form by Old Ireland in Colour's John Breslin.  It's great! Click video image below to watch.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=VaEl0faDw38" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9d812131195cc524d5fe03696fdc284208bedbde/687474703a2f2f696d672e796f75747562652e636f6d2f76692f5661456c306661447733382f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/VaEl0faDw38/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Get more updates on &lt;a href="https://twitter.com/citnaj" rel="nofollow"&gt;Twitter &lt;img src="resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg" width="16" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about-deoldify"&gt;About DeOldify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#example-videos"&gt;Example Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#example-images"&gt;Example Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stuff-that-should-probably-be-in-a-paper"&gt;Stuff That Should Probably Be In A Paper&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#how-to-achieve-stable-video"&gt;How to Achieve Stable Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-is-nogan"&gt;What is NoGAN?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#why-three-models"&gt;Why Three Models?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-technical-details"&gt;Technical Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#this-project-going-forward"&gt;Going Forward&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started-yourself"&gt;Getting Started Yourself&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#easiest-approach"&gt;Easiest Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#your-own-machine-not-as-easy"&gt;Your Own Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pretrained-weights"&gt;Pretrained Weights&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about-deoldify" class="anchor" aria-hidden="true" href="#about-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About DeOldify&lt;/h2&gt;
&lt;p&gt;Simply put, the mission of this project is to colorize and restore old images and film footage.
We'll get into the details in a bit, but first let's see some pretty pictures and videos!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-new-and-exciting-stuff-in-deoldify" class="anchor" aria-hidden="true" href="#new-and-exciting-stuff-in-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New and Exciting Stuff in DeOldify&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Glitches and artifacts are almost entirely eliminated&lt;/li&gt;
&lt;li&gt;Better skin (less zombies)&lt;/li&gt;
&lt;li&gt;More highly detailed and photorealistic renders&lt;/li&gt;
&lt;li&gt;Much less "blue bias"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt; - it actually looks good!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NoGAN&lt;/strong&gt; - a new and weird but highly effective way to do GAN training for image to image.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-example-videos" class="anchor" aria-hidden="true" href="#example-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Videos&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;  Click images to watch&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facebook-f8-demo" class="anchor" aria-hidden="true" href="#facebook-f8-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facebook F8 Demo&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=l3UXXid04Ys" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95e149f839667ddcd87e0a1970e3870f6a61c24a/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6c335558586964303459732f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/l3UXXid04Ys/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-silent-movie-examples" class="anchor" aria-hidden="true" href="#silent-movie-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Silent Movie Examples&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=EXn-n2iqEjI" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24d210457f7e8b57ef701788f013f2f72d2eda1c/687474703a2f2f696d672e796f75747562652e636f6d2f76692f45586e2d6e326971456a492f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-example-images" class="anchor" aria-hidden="true" href="#example-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Images&lt;/h2&gt;
&lt;p&gt;"Migrant Mother" by Dorothea Lange (1936)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067" alt="Migrant Mother" data-canonical-src="https://i.imgur.com/Bt0vnke.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Woman relaxing in her livingroom in Sweden (1920)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067" alt="Sweden Living Room" data-canonical-src="https://i.imgur.com/158d0oU.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Toffs and Toughs" by Jimmy Sime (1937)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067" alt="Class Divide" data-canonical-src="https://i.imgur.com/VYuav4I.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanksgiving Maskers (1911)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067" alt="Thanksgiving Maskers" data-canonical-src="https://i.imgur.com/n8qVJ5c.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Glen Echo Madame Careta Gypsy Camp in Maryland (1925)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067" alt="Gypsy Camp" data-canonical-src="https://i.imgur.com/1oYrJRI.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Mr. and Mrs. Lemuel Smith and their younger children in their farm house, Carroll County, Georgia." (1941)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067" alt="Georgia Farmhouse" data-canonical-src="https://i.imgur.com/I2j8ynm.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Building the Golden Gate Bridge" (est 1937)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067" alt="Golden Gate Bridge" data-canonical-src="https://i.imgur.com/6SbFjfq.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;  What you might be wondering is while this render looks cool, are the colors accurate? The original photo certainly makes it look like the towers of the bridge could be white. We looked into this and it turns out the answer is no - the towers were already covered in red primer by this time. So that's something to keep in mind- historical accuracy remains a huge challenge!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;"Terrasse de cafÃ©, Paris" (1925)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067" alt="Cafe Paris" data-canonical-src="https://i.imgur.com/WprQwP5.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Norwegian Bride (est late 1890s)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067" alt="Norwegian Bride" data-canonical-src="https://i.imgur.com/MmtvrZm.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ZitkÃ¡la-Å Ã¡ (Lakota: Red Bird), also known as Gertrude Simmons Bonnin (1898)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067" alt="Native Woman" data-canonical-src="https://i.imgur.com/zIGM043.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chinese Opium Smokers (1880)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067" alt="Opium Real" data-canonical-src="https://i.imgur.com/lVGq8Vq.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stuff-that-should-probably-be-in-a-paper" class="anchor" aria-hidden="true" href="#stuff-that-should-probably-be-in-a-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stuff That Should Probably Be In A Paper&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-achieve-stable-video" class="anchor" aria-hidden="true" href="#how-to-achieve-stable-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Achieve Stable Video&lt;/h3&gt;
&lt;p&gt;NoGAN training is crucial to getting the kind of stable and colorful images seen in this iteration of DeOldify. NoGAN training combines the benefits of GAN training (wonderful colorization) while eliminating the nasty side effects (like flickering objects in video). Believe it or not, video is rendered using isolated image generation without any sort of temporal modeling tacked on. The process performs 30-60 minutes of the GAN portion of "NoGAN" training, using 1% to 3% of imagenet data once.  Then, as with still image colorization, we "DeOldify" individual frames before rebuilding the video.&lt;/p&gt;
&lt;p&gt;In addition to improved video stability, there is an interesting thing going on here worth mentioning. It turns out the models I run, even different ones and with different training structures, keep arriving at more or less the same solution.  That's even the case for the colorization of things you may think would be arbitrary and unknowable, like the color of clothing, cars, and even special effects (as seen in "Metropolis").&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966" alt="Metropolis Special FX" data-canonical-src="https://thumbs.gfycat.com/HeavyLoneBlowfish-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My best guess is that the models are learning some interesting rules about how to colorize based on subtle cues present in the black and white images that I certainly wouldn't expect to exist.  This result leads to nicely deterministic and consistent results, and that means you don't have track model colorization decisions because they're not arbitrary.  Additionally, they seem remarkably robust so that even in moving scenes the renders are very consistent.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966" alt="Moving Scene Example" data-canonical-src="https://thumbs.gfycat.com/FamiliarJubilantAsp-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Other ways to stabilize video add up as well. First, generally speaking rendering at a higher resolution (higher render_factor) will increase stability of colorization decisions.  This stands to reason because the model has higher fidelity image information to work with and will have a greater chance of making the "right" decision consistently.  Closely related to this is the use of resnet101 instead of resnet34 as the backbone of the generator- objects are detected more consistently and correctly with this. This is especially important for getting good, consistent skin rendering.  It can be particularly visually jarring if you wind up with "zombie hands", for example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966" alt="Zombie Hand Example" data-canonical-src="https://thumbs.gfycat.com/ThriftyInferiorIsabellinewheatear-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Additionally, gaussian noise augmentation during training appears to help but at this point the conclusions as to just how much are bit more tenuous (I just haven't formally measured this yet).  This is loosely based on work done in style transfer video, described here:  &lt;a href="https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42" rel="nofollow"&gt;https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Special thanks go to Rani Horev for his contributions in implementing this noise augmentation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-is-nogan" class="anchor" aria-hidden="true" href="#what-is-nogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is NoGAN?&lt;/h3&gt;
&lt;p&gt;This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model. It provides the benefits of GAN training while spending minimal time doing direct GAN training.  Instead, most of the training time is spent pretraining the generator and critic separately with more straight-forward, fast and reliable conventional methods.  A key insight here is that those more "conventional" methods generally get you most of the results you need, and that GANs can be used to close the gap on realism. During the very short amount of actual GAN training the generator not only gets the full realistic colorization capabilities that used to take days of progressively resized GAN training, but it also doesn't accrue nearly as much of the artifacts and other ugly baggage of GANs. In fact, you can pretty much eliminate glitches and artifacts almost entirely depending on your approach. As far as I know this is a new technique. And it's incredibly effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original DeOldify Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966" alt="Before Flicker" data-canonical-src="https://thumbs.gfycat.com/CoordinatedVeneratedHogget-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NoGAN-Based DeOldify Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966" alt="After Flicker" data-canonical-src="https://thumbs.gfycat.com/OilyBlackArctichare-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The steps are as follows: First train the generator in a conventional way by itself with just the feature loss. Next, generate images from that, and train the critic on distinguishing between those outputs and real images as a basic binary classifier. Finally, train the generator and critic together in a GAN setting (starting right at the target size of 192px in this case).  Now for the weird part:  All the useful GAN training here only takes place within a very small window of time.  There's an inflection point where it appears the critic has transferred everything it can that is useful to the generator. Past this point, image quality oscillates between the best that you can get at the inflection point, or bad in a predictable way (orangish skin, overly red lips, etc).  There appears to be no productive training after the inflection point.  And this point lies within training on just 1% to 3% of the Imagenet Data!  That amounts to about 30-60 minutes of training at 192px.&lt;/p&gt;
&lt;p&gt;The hard part is finding this inflection point.  So far, I've accomplished this by making a whole bunch of model save checkpoints (every 0.1% of data iterated on) and then just looking for the point where images look great before they go totally bonkers with orange skin (always the first thing to go). Additionally, generator rendering starts immediately getting glitchy and inconsistent at this point, which is no good particularly for video. What I'd really like to figure out is what the tell-tale sign of the inflection point is that can be easily automated as an early stopping point.  Unfortunately, nothing definitive is jumping out at me yet.  For one, it's happening in the middle of training loss decreasing- not when it flattens out, which would seem more reasonable on the surface.&lt;/p&gt;
&lt;p&gt;Another key thing about NoGAN training is you can repeat pretraining the critic on generated images after the initial GAN training, then repeat the GAN training itself in the same fashion.  This is how I was able to get extra colorful results with the "artistic" model.  But this does come at a cost currently- the output of the generator becomes increasingly inconsistent and you have to experiment with render resolution (render_factor) to get the best result.  But the renders are still glitch free and way more consistent than I was ever able to achieve with the original DeOldify model. You can do about five of these repeat cycles, give or take, before you get diminishing returns, as far as I can tell.&lt;/p&gt;
&lt;p&gt;Keep in mind- I haven't been entirely rigorous in figuring out what all is going on in NoGAN- I'll save that for a paper. That means there's a good chance I'm wrong about something.  But I think it's definitely worth putting out there now because I'm finding it very useful- it's solving basically much of my remaining problems I had in DeOldify.&lt;/p&gt;
&lt;p&gt;This builds upon a technique developed in collaboration with Jeremy Howard and Sylvain Gugger for Fast.AI's Lesson 7 in version 3 of Practical Deep Learning for Coders Part I. The particular lesson notebook can be found here: &lt;a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb"&gt;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-why-three-models" class="anchor" aria-hidden="true" href="#why-three-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Three Models?&lt;/h2&gt;
&lt;p&gt;There are now three models to choose from in DeOldify. Each of these has key strengths and weaknesses, and so have different use cases.  Video is for video of course.  But stable and artistic are both for images, and sometimes one will do images better than the other.&lt;/p&gt;
&lt;p&gt;More details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Artistic&lt;/strong&gt; - This model achieves the highest quality results in image coloration, in terms of interesting details and vibrance. The most notable drawback however is that it's a bit of a pain to fiddle around with to get the best results (you have to adjust the rendering resolution or render_factor to achieve this).  Additionally, the model does not do as well as stable in a few key common scenarios- nature scenes and portraits.  The model uses a resnet34 backbone on a UNet with an emphasis on depth of layers on the decoder side.  This model was trained with 5 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 32% of Imagenet data trained once (12.5 hours of direct GAN training).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stable&lt;/strong&gt; - This model achieves the best results with landscapes and portraits. Notably, it produces less "zombies"- where faces or limbs stay gray rather than being colored in properly.  It generally has less weird miscolorations than artistic, but it's also less colorful in general.  This model uses a resnet101 backbone on a UNet with an emphasis on width of layers on the decoder side.  This model was trained with 3 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 7% of Imagenet data trained once (3 hours of direct GAN training).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt; - This model is optimized for smooth, consistent and flicker-free video.  This would definitely be the least colorful of the three models, but it's honestly not too far off from "stable". The model is the same as "stable" in terms of architecture, but differs in training.  It's trained for a mere 2.2% of Imagenet data once at 192px, using only the initial generator/critic pretrain/GAN NoGAN training (1 hour of direct GAN training).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because the training of the artistic and stable models was done before the "inflection point" of NoGAN training described in "What is NoGAN???" was discovered,  I believe this amount of training on them can be knocked down considerably. As far as I can tell, the models were stopped at "good points" that were well beyond where productive training was taking place.  I'll be looking into this in the future.&lt;/p&gt;
&lt;p&gt;Ideally, eventually these three models will be consolidated into one that has all these good desirable unified.  I think there's a path there, but it's going to require more work!  So for now, the most practical solution appears to be to maintain multiple models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-technical-details" class="anchor" aria-hidden="true" href="#the-technical-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Technical Details&lt;/h2&gt;
&lt;p&gt;This is a deep learning based model.  More specifically, what I've done is combined the following approaches:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-self-attention-generative-adversarial-network" class="anchor" aria-hidden="true" href="#self-attention-generative-adversarial-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://arxiv.org/abs/1805.08318" rel="nofollow"&gt;Self-Attention Generative Adversarial Network&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Except the generator is a &lt;strong&gt;pretrained U-Net&lt;/strong&gt;, and I've just modified it to have the spectral normalization and self-attention.  It's a pretty straightforward translation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-two-time-scale-update-rule" class="anchor" aria-hidden="true" href="#two-time-scale-update-rule"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://arxiv.org/abs/1706.08500" rel="nofollow"&gt;Two Time-Scale Update Rule&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is also very straightforward â€“ it's just one to one generator/critic iterations and higher critic learning rate.
This is modified to incorporate a "threshold" critic loss that makes sure that the critic is "caught up" before moving on to generator training.
This is particularly useful for the "NoGAN" method described below.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nogan" class="anchor" aria-hidden="true" href="#nogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NoGAN&lt;/h3&gt;
&lt;p&gt;There's no paper here! This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model.
The gist is that you get the benefits of GAN training while spending minimal time doing direct GAN training.
More details are in the &lt;a href="#what-is-nogan"&gt;What is NoGAN?&lt;/a&gt; section (it's a doozy).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generator-loss" class="anchor" aria-hidden="true" href="#generator-loss"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generator Loss&lt;/h3&gt;
&lt;p&gt;Loss during NoGAN learning is two parts:  One is a basic Perceptual Loss (or Feature Loss) based on VGG16 â€“ this just biases the generator model to replicate the input image.
The second is the loss score from the critic.  For the curious â€“ Perceptual Loss isn't sufficient by itself to produce good results.
It tends to just encourage a bunch of brown/green/blue â€“ you know, cheating to the test, basically, which neural networks are really good at doing!
Key thing to realize here is that GANs essentially are learning the loss function for you â€“ which is really one big step closer to toward the ideal that we're shooting for in machine learning.
And of course you generally get much better results when you get the machine to learn something you were previously hand coding.
That's certainly the case here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Of note:&lt;/strong&gt;  There's no longer any "Progressive Growing of GANs" type training going on here.  It's just not needed in lieu of the superior results obtained by the "NoGAN" technique described above.&lt;/p&gt;
&lt;p&gt;The beauty of this model is that it should be generally useful for all sorts of image modification, and it should do it quite well.
What you're seeing above are the results of the colorization model, but that's just one component in a pipeline that I'm developing with the exact same approach.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-this-project-going-forward" class="anchor" aria-hidden="true" href="#this-project-going-forward"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;This Project, Going Forward&lt;/h2&gt;
&lt;p&gt;So that's the gist of this project â€“ I'm looking to make old photos and film look reeeeaaally good with GANs, and more importantly, make the project &lt;em&gt;useful&lt;/em&gt;.
In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.
I'll try to make this as user-friendly as possible, but I'm sure there's going to be hiccups along the way.&lt;/p&gt;
&lt;p&gt;Oh and I swear I'll document the code properly...eventually.  Admittedly I'm &lt;em&gt;one of those&lt;/em&gt; people who believes in "self documenting code" (LOL).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-yourself" class="anchor" aria-hidden="true" href="#getting-started-yourself"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started Yourself&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-easiest-approach" class="anchor" aria-hidden="true" href="#easiest-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Easiest Approach&lt;/h3&gt;
&lt;p&gt;The easiest way to get started is to go straight to the Colab notebooks:&lt;/p&gt;
&lt;p&gt;Image &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
| Video &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Special thanks to Matt Robinson and MarÃ­a Benavente for their image Colab notebook contributions, and Robert Bell for the video Colab notebook work!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-your-own-machine-not-as-easy" class="anchor" aria-hidden="true" href="#your-own-machine-not-as-easy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Your Own Machine (not as easy)&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-hardware-and-operating-system-requirements" class="anchor" aria-hidden="true" href="#hardware-and-operating-system-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hardware and Operating System Requirements&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(Training Only) BEEFY Graphics card&lt;/strong&gt;.  I'd really like to have more memory than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.  The Generators and Critic are ridiculously large.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Colorization Alone) A decent graphics card&lt;/strong&gt;. Approximately 4GB+ memory video cards should be sufficient.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linux (or maybe Windows 10)&lt;/strong&gt;  I'm using Ubuntu 16.04, but nothing about this precludes Windows 10 support as far as I know.  I just haven't tested it and am not going to make it a priority for now.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-easy-install" class="anchor" aria-hidden="true" href="#easy-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Easy Install&lt;/h4&gt;
&lt;p&gt;You should now be able to do a simple install with Anaconda. Here are the steps:&lt;/p&gt;
&lt;p&gt;Open the command line and navigate to the root folder you wish to install.  Then type the following commands&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd DeOldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;conda env create -f environment.yml&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then start running with these commands:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;source activate deoldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;jupyter lab&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From there you can start running the notebooks in Jupyter Lab, via the url they provide you in the console.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can also now do "conda activate deoldify" if you have the latest version of conda and in fact that's now recommended. But a lot of people don't have that yet so I'm not going to make it the default instruction here yet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-note-on-test_images-folder" class="anchor" aria-hidden="true" href="#note-on-test_images-folder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note on test_images Folder&lt;/h4&gt;
&lt;p&gt;The images in the &lt;code&gt;test_images&lt;/code&gt; folder have been removed because they were using Git LFS and that costs a lot of money when GitHub actually charges for bandwidth on a popular open source project (they had a billing bug for while that was recently fixed).  The notebooks that use them (the image test ones) still point to images in that directory that I (Jason) have personally and I'd like to keep it that way because, after all, I'm by far the primary and most active developer.  But they won't work for you.  Still, those notebooks are a convenient template for making your own tests if you're so inclined.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-docker-for-jupyter" class="anchor" aria-hidden="true" href="#docker-for-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker for Jupyter&lt;/h3&gt;
&lt;p&gt;You can build and run the docker using the following process:&lt;/p&gt;
&lt;p&gt;Cloning&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd DeOldify &amp;amp;&amp;amp; docker build -t deoldify_jupyter -f Dockerfile .&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;echo "http://$(curl ifconfig.io):8888" &amp;amp;&amp;amp; nvidia-docker run --ipc=host --env NOTEBOOK_PASSWORD="pass123" -p 8888:8888 -it deoldify_jupyter&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-docker-for-api" class="anchor" aria-hidden="true" href="#docker-for-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker for API&lt;/h3&gt;
&lt;p&gt;You can build and run the docker using the following process:&lt;/p&gt;
&lt;p&gt;Cloning&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd DeOldify &amp;amp;&amp;amp; docker build -t deoldify_api -f Dockerfile-api .&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;echo "http://$(curl ifconfig.io):5000" &amp;amp;&amp;amp; nvidia-docker run --ipc=host -p 5000:5000 -d deoldify_api&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Calling the API for image processing&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: image/png" -H "Content-Type: application/json" -d "{\"source_url\":\"http://www.afrikanheritage.com/wp-content/uploads/2015/08/slave-family-P.jpeg\", \"render_factor\":35}" --output colorized_image.png&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Calling the API for video processing&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: application/octet-stream" -H "Content-Type: application/json" -d "{\"source_url\":\"https://v.redd.it/d1ku57kvuf421/HLSPlaylist.m3u8\", \"render_factor\":35}" --output colorized_video.mp4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don't have Nvidia Docker, &lt;a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#installing-version-20"&gt;here&lt;/a&gt; is the installation guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-installation-details" class="anchor" aria-hidden="true" href="#installation-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Details&lt;/h3&gt;
&lt;p&gt;This project is built around the wonderful Fast.AI library.  Prereqs, in summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.AI 1.0.51&lt;/strong&gt; (and its dependencies).  If you use any higher version you'll see grid artifacts in rendering and tensorboard will malfunction. So yeah...don't do that.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 1.0.1&lt;/strong&gt; Not the latest version of PyTorch- that will not play nicely with the version of FastAI above.  Note however that the conda install of FastAI 1.0.51 grabs the latest PyTorch, which doesn't work.  This is patched over by our own conda install but fyi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; &lt;code&gt;conda install -c conda-forge jupyterlab&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensorboard&lt;/strong&gt; (i.e. install Tensorflow) and &lt;strong&gt;TensorboardX&lt;/strong&gt; (&lt;a href="https://github.com/lanpa/tensorboardX"&gt;https://github.com/lanpa/tensorboardX&lt;/a&gt;).  I guess you don't &lt;em&gt;have&lt;/em&gt; to but man, life is so much better with it.  FastAI now comes with built in support for this- you just  need to install the prereqs: &lt;code&gt;conda install -c anaconda tensorflow-gpu&lt;/code&gt; and &lt;code&gt;pip install tensorboardX&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageNet&lt;/strong&gt; â€“ Only if you're training, of course. It has proven to be a great dataset for my purposes.  &lt;a href="http://www.image-net.org/download-images" rel="nofollow"&gt;http://www.image-net.org/download-images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h2&gt;
&lt;p&gt;To start right away on your own machine with your own images or videos without training the models yourself, you'll need to download the "Completed Generator Weights" listed below and drop them in the /models/ folder.&lt;/p&gt;
&lt;p&gt;The colorization inference notebooks should be able to guide you from here. The notebooks to use are named ImageColorizerArtistic.ipynb, ImageColorizerStable.ipynb, and VideoColorizer.ipynb.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-completed-generator-weights" class="anchor" aria-hidden="true" href="#completed-generator-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Completed Generator Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/336vn9y4qwyg9yz/ColorizeVideo_gen.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-completed-critic-weights" class="anchor" aria-hidden="true" href="#completed-critic-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Completed Critic Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/8g5txfzt2fw8mf5/ColorizeArtistic_crit.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/7a8u20e7xdu1dtd/ColorizeStable_crit.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/0401djgo1dfxdzt/ColorizeVideo_crit.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pretrain-only-generator-weights" class="anchor" aria-hidden="true" href="#pretrain-only-generator-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrain Only Generator Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/9zexurvrve141n9/ColorizeArtistic_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/mdnuo1563bb8nh4/ColorizeStable_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/avzixh1ujf86e8x/ColorizeVideo_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pretrain-only-critic-weights" class="anchor" aria-hidden="true" href="#pretrain-only-critic-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrain Only Critic Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/lakxe8akzjgjnmh/ColorizeArtistic_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/b3wka56iyv1fvdc/ColorizeStable_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/j7og84cbhpa94gs/ColorizeVideo_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-want-the-old-deoldify" class="anchor" aria-hidden="true" href="#want-the-old-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want the Old DeOldify?&lt;/h2&gt;
&lt;p&gt;We suspect some of you are going to want access to the original DeOldify model for various reasons.  We have that archived here:  &lt;a href="https://github.com/dana-kelley/DeOldify"&gt;https://github.com/dana-kelley/DeOldify&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-want-more" class="anchor" aria-hidden="true" href="#want-more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want More?&lt;/h2&gt;
&lt;p&gt;Follow &lt;a href="https://twitter.com/search?q=%23Deoldify" rel="nofollow"&gt;#DeOldify&lt;/a&gt; or &lt;a href="https://twitter.com/citnaj" rel="nofollow"&gt;Jason Antic&lt;/a&gt; on Twitter.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;All code in this repository is under the MIT license as specified by the LICENSE file.&lt;/p&gt;
&lt;p&gt;The model weights listed in this readme under the "Pretrained Weights" section are trained by ourselves and are released under the MIT license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jantic</author><guid isPermaLink="false">https://github.com/jantic/DeOldify</guid><pubDate>Mon, 18 Nov 2019 00:15:00 GMT</pubDate></item><item><title>czy36mengfei/tensorflow2_tutorials_chinese #16 in Jupyter Notebook, This week</title><link>https://github.com/czy36mengfei/tensorflow2_tutorials_chinese</link><description>&lt;p&gt;&lt;i&gt;tensorflow2ä¸­æ–‡æ•™ç¨‹ï¼ŒæŒç»­æ›´æ–°(å½“å‰ç‰ˆæœ¬:tensorflow2.0)ï¼Œtag: tensorflow 2.0 tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow2_tutorials_chinese" class="anchor" aria-hidden="true" href="#tensorflow2_tutorials_chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;tensorflow2_tutorials_chinese&lt;/h1&gt;
&lt;p&gt;tensorflow2ä¸­æ–‡æ•™ç¨‹ï¼ŒæŒç»­æ›´æ–°ï¼ˆä¸å®šæœŸæ›´æ–°ï¼‰&lt;/p&gt;
&lt;p&gt;tensorflow 2.0 æ­£å¼ç‰ˆå·²ä¸Šçº¿ï¼Œ åé¢å°†æŒç»­æ ¹æ®TensorFlow2çš„ç›¸å…³æ•™ç¨‹å’Œå­¦ä¹ èµ„æ–™ã€‚&lt;/p&gt;
&lt;p&gt;æœ€æ–°tensorflowæ•™ç¨‹å’Œç›¸å…³èµ„æºï¼Œè¯·å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼šDoitNLPï¼Œ
åé¢æˆ‘ä¼šåœ¨DoitNLPä¸Šï¼ŒæŒç»­æ›´æ–°æ·±åº¦å­¦ä¹ ã€NLPã€Tensorflowçš„ç›¸å…³æ•™ç¨‹å’Œå‰æ²¿èµ„è®¯ï¼Œå®ƒå°†æˆä¸ºæˆ‘ä»¬ä¸€èµ·å­¦ä¹ tensorflowçš„å¤§æœ¬è¥ã€‚&lt;/p&gt;
&lt;p&gt;å½“å‰tensorflowç‰ˆæœ¬ï¼štensorflow2.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;æœ€å…¨Tensorflow 2.0 æ•™ç¨‹æŒç»­æ›´æ–°ï¼š&lt;/strong&gt;
&lt;a href="https://zhuanlan.zhihu.com/p/59507137" rel="nofollow"&gt;https://zhuanlan.zhihu.com/p/59507137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;æœ¬æ•™ç¨‹ä¸»è¦ç”±tensorflow2.0å®˜æ–¹æ•™ç¨‹çš„ä¸ªäººå­¦ä¹ å¤ç°ç¬”è®°æ•´ç†è€Œæ¥ï¼Œå¹¶å€Ÿé‰´äº†ä¸€äº›kerasæ„é€ ç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œä¸­æ–‡è®²è§£ï¼Œæ–¹ä¾¿å–œæ¬¢é˜…è¯»ä¸­æ–‡æ•™ç¨‹çš„æœ‹å‹ï¼Œtensorflowå®˜æ–¹æ•™ç¨‹ï¼š&lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;https://www.tensorflow.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58825020" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹- Keras å¿«é€Ÿå…¥é—¨&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58825710" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹-keras å‡½æ•°api&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58826227" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹-ä½¿ç”¨kerasè®­ç»ƒæ¨¡å‹&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59481536" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹-ç”¨kerasæ„å»ºè‡ªå·±çš„ç½‘ç»œå±‚&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59481985" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹-kerasæ¨¡å‹ä¿å­˜å’Œåºåˆ—åŒ–&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482373" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹-eageræ¨¡å¼&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482589" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹-Variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59482934" rel="nofollow"&gt;TensorFlow 2.0 æ•™ç¨‹--AutoGraph&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 æ·±åº¦å­¦ä¹ å®è·µ&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506238" rel="nofollow"&gt;TensorFlow2.0 æ•™ç¨‹-å›¾åƒåˆ†ç±»&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506402" rel="nofollow"&gt;TensorFlow2.0 æ•™ç¨‹-æ–‡æœ¬åˆ†ç±»&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/59506543" rel="nofollow"&gt;TensorFlow2.0 æ•™ç¨‹-è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60232704" rel="nofollow"&gt;TensorFlow2.0æ•™ç¨‹-ç»“æ„åŒ–æ•°æ®åˆ†ç±»&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60238056" rel="nofollow"&gt;TensorFlow2.0æ•™ç¨‹-å›å½’&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60485936" rel="nofollow"&gt;TensorFlow2.0æ•™ç¨‹-ä¿æŒå’Œè¯»å–æ¨¡å‹&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 åŸºç¡€ç½‘ç»œç»“æ„&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60899040" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-åŸºç¡€MLPç½‘ç»œ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900318" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-MLPåŠæ·±åº¦å­¦ä¹ å¸¸è§æŠ€å·§&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900649" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-åŸºç¡€CNNç½‘ç»œ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60900902" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-CNNå˜ä½“ç½‘ç»œ&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60901179" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-æ–‡æœ¬å·ç§¯&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/60966714" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-LSTMå’ŒGRU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61077346" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-è‡ªç¼–ç å™¨&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61080045" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-å·ç§¯è‡ªç¼–ç å™¨&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61224215" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-è¯åµŒå…¥&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61280722" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-DCGAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61400276" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-ä½¿ç”¨Estimatoræ„å»ºBoosted trees&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TensorFlow 2.0 å®‰è£…&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/61472293" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-Ubuntuå®‰è£…TensorFlow 2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/62036280" rel="nofollow"&gt;TensorFlow2æ•™ç¨‹-Windowså®‰è£…tensorflow2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;å®Œæ•´tensorflow2.0æ•™ç¨‹ä»£ç è¯·çœ‹&lt;a href="https://github.com/czy36mengfei/tensorflow2_tutorials_chinese"&gt;tensorflow2.0ï¼šä¸­æ–‡æ•™ç¨‹tensorflow2_tutorials_chinese(æ¬¢è¿star)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;æ›´å¤šTensorFlow 2.0 å…¥é—¨æ•™ç¨‹è¯·æŒç»­å…³æ³¨ä¸“æ ï¼š&lt;a href="https://zhuanlan.zhihu.com/c_1091021863043624960" rel="nofollow"&gt;Tensorflow2æ•™ç¨‹&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;æ·±åº¦å­¦ä¹ å…¥é—¨ä¹¦ç±å’Œèµ„æºæ¨èï¼š&lt;a href="https://zhuanlan.zhihu.com/p/65371424" rel="nofollow"&gt;https://zhuanlan.zhihu.com/p/65371424&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>czy36mengfei</author><guid isPermaLink="false">https://github.com/czy36mengfei/tensorflow2_tutorials_chinese</guid><pubDate>Mon, 18 Nov 2019 00:16:00 GMT</pubDate></item><item><title>REMitchell/python-scraping #17 in Jupyter Notebook, This week</title><link>https://github.com/REMitchell/python-scraping</link><description>&lt;p&gt;&lt;i&gt;Code samples from the book Web Scraping with Python http://shop.oreilly.com/product/0636920034391.do&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-web-scraping-with-python-code-samples" class="anchor" aria-hidden="true" href="#web-scraping-with-python-code-samples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Scraping with Python Code Samples&lt;/h1&gt;
&lt;p&gt;These code samples are for the book &lt;a href="http://shop.oreilly.com/product/0636920078067.do" rel="nofollow"&gt;Web Scraping with Python 2nd Edition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you're looking for the first edition code files, they can be found in the &lt;a href="https://github.com/REMitchell/python-scraping/tree/master/v1"&gt;v1&lt;/a&gt; directory.&lt;/p&gt;
&lt;p&gt;Most code for the second edition is contained in &lt;a href="https://jupyter.org/install.html" rel="nofollow"&gt;Jupyter notebooks&lt;/a&gt;. Although these files can be viewed directly in your browser in Github, some formatting changes and oddities may occur. I recommend that you clone the repository, install Jupyter, and view them locally for the best experience.&lt;/p&gt;
&lt;p&gt;The web changes, libraries update, and make mistakes and typos more frequently than I'd like to admit! If you think you've spotted an error, please feel free to make a pull request against this repository.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>REMitchell</author><guid isPermaLink="false">https://github.com/REMitchell/python-scraping</guid><pubDate>Mon, 18 Nov 2019 00:17:00 GMT</pubDate></item><item><title>bangoc123/learn-machine-learning-in-two-months #18 in Jupyter Notebook, This week</title><link>https://github.com/bangoc123/learn-machine-learning-in-two-months</link><description>&lt;p&gt;&lt;i&gt;Nhá»¯ng kiáº¿n thá»©c cáº§n thiáº¿t Ä‘á»ƒ há»c tá»‘t Machine Learning trong vÃ²ng 2 thÃ¡ng. Essential Knowledge for learning Machine Learning in two months.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body MD" data-path="README.MD"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-lá»™-trÃ¬nh-há»c-machine-learning-deep-learning-cho-ngÆ°á»i-má»›i-báº¯t-Ä‘áº§u" class="anchor" aria-hidden="true" href="#lá»™-trÃ¬nh-há»c-machine-learning-deep-learning-cho-ngÆ°á»i-má»›i-báº¯t-Ä‘áº§u"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lá»™ trÃ¬nh há»c Machine Learning, Deep Learning cho ngÆ°á»i má»›i báº¯t Ä‘áº§u&lt;/h2&gt;
&lt;p&gt;TÃ´i Ä‘Ã£ tá»«ng há»c Machine Learning trong vÃ²ng 2 thÃ¡ng vÃ  tÃ´i tin báº¡n cÅ©ng cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c.&lt;/p&gt;
&lt;p&gt;Lá»™ trÃ¬nh sáº½ giÃºp báº¡n náº¯m cháº¯c cÃ´ng nghá»‡ nÃ y tá»« cÆ¡ báº£n Ä‘áº¿n nÃ¢ng cao, xÃ¢y dá»±ng Machine Learning model tá»« python thuáº§n cho Ä‘áº¿n cÃ¡c thÆ° viá»‡n cao cáº¥p nhÆ° TensorFlow hay Keras. Äi sÃ¢u phÃ¢n tÃ­ch báº£n cháº¥t váº¥n Ä‘á» lÃ  giÃ¡ trá»‹ cá»‘t lÃµi cá»§a khÃ³a há»c nÃ y.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P/S:&lt;/strong&gt; HÃ£y Ä‘á»ƒ láº¡i &lt;strong&gt;1 star&lt;/strong&gt; Ä‘á»ƒ team cÃ³ Ä‘á»™ng lá»±c xuáº¥t báº£n cÃ¡c pháº§n tiáº¿p theo vÃ  cÅ©ng Ä‘á»«ng quÃªn chia sáº» tá»›i báº¡n bÃ¨ cá»§a báº¡n.&lt;/p&gt;
&lt;p&gt;TÃ´i lÃ  má»™t trong cÃ¡c lecturer cá»§a lá»›p VietAI Hanoi khoÃ¡ 3 vÃ  khoÃ¡ 4. Hiá»‡n táº¡i tÃ´i Ä‘Ã£ vÃ o SÃ i GÃ²n nÃªn khÃ´ng tham gia tiáº¿p Ä‘á»ƒ giáº£ng dáº¡y nhÆ°ng váº«n Ä‘Ã³ng vai trÃ² Advisor cho lá»›p khoÃ¡ 5.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/8073cb447e2f2da7f44ab32e293189b7b4c5fbc0/68747470733a2f2f6c68332e676f6f676c6575736572636f6e74656e742e636f6d2f2d33415942613769544e54616c576d506b73555a306368356c7648747a4475484d57423641776c476769617059706f61514d575a416d513355426c4d4c504d48715132674c32416b476a433949645341704a3165624c586e3066344f684e782d6b697962636d7a674863614c57545a4c5241355258453353726a52464d776b74316a345551344b31304a5442677052675845555944344b513379536d32456f77774e70584c3841562d6948454251563174684a78494178547233467434764e6c456b6757583542644a556371684d6b665848476f77544e69555454726e3230423757686a55386335535a5576535453716d763543325a79306f52424c6f47527145306b555f7a494c5a71327958655042516b504c2d704c71307459376e75376777617a454c696644444a3169704f4d477276737857457435674649474d4f4a47323537764d6f32667147657a6d4349765337666c483465434b47546d6866556f644a377464717a35553863556c326e5f7a3473585f78765064645f78745f5448766b534a6b5a36714f5a55784d32786d46387065736d67496161414b4c5755736e74764d695179542d4d4c4d6468623744764d4361307a366f4c78664c333761663548746b5a395866494f624b38506d4e426b37545879455573594a4e316d643752644f5241495050557541337673712d72587834454c6f726d4d37535f665442685a424e7331575a3233354775787165455066574e73414d654c544132304b65626b64596d2d5945774e305356704a3657394f47395f3846635f6e35762d78376278576168696b58744a59636454786341574441437647787a552d71564c6b4c3351657533676e377041306e51706d4b66485547656f795f7643474e436f326c6a597039435264796b586f64683935704f6431316e714c485662626172486234544557625a4f53316246347159716a4a654e596176325156476e75784b594d39333369475249713434767a6b59783761755449534437475838316a52307868774f5469754f5a43564d4561304274433d77313136352d683838312d6e6f"&gt;&lt;img src="https://camo.githubusercontent.com/8073cb447e2f2da7f44ab32e293189b7b4c5fbc0/68747470733a2f2f6c68332e676f6f676c6575736572636f6e74656e742e636f6d2f2d33415942613769544e54616c576d506b73555a306368356c7648747a4475484d57423641776c476769617059706f61514d575a416d513355426c4d4c504d48715132674c32416b476a433949645341704a3165624c586e3066344f684e782d6b697962636d7a674863614c57545a4c5241355258453353726a52464d776b74316a345551344b31304a5442677052675845555944344b513379536d32456f77774e70584c3841562d6948454251563174684a78494178547233467434764e6c456b6757583542644a556371684d6b665848476f77544e69555454726e3230423757686a55386335535a5576535453716d763543325a79306f52424c6f47527145306b555f7a494c5a71327958655042516b504c2d704c71307459376e75376777617a454c696644444a3169704f4d477276737857457435674649474d4f4a47323537764d6f32667147657a6d4349765337666c483465434b47546d6866556f644a377464717a35553863556c326e5f7a3473585f78765064645f78745f5448766b534a6b5a36714f5a55784d32786d46387065736d67496161414b4c5755736e74764d695179542d4d4c4d6468623744764d4361307a366f4c78664c333761663548746b5a395866494f624b38506d4e426b37545879455573594a4e316d643752644f5241495050557541337673712d72587834454c6f726d4d37535f665442685a424e7331575a3233354775787165455066574e73414d654c544132304b65626b64596d2d5945774e305356704a3657394f47395f3846635f6e35762d78376278576168696b58744a59636454786341574441437647787a552d71564c6b4c3351657533676e377041306e51706d4b66485547656f795f7643474e436f326c6a597039435264796b586f64683935704f6431316e714c485662626172486234544557625a4f53316246347159716a4a654e596176325156476e75784b594d39333369475249713434767a6b59783761755449534437475838316a52307868774f5469754f5a43564d4561304274433d77313136352d683838312d6e6f" width="500" data-canonical-src="https://lh3.googleusercontent.com/-3AYBa7iTNTalWmPksUZ0ch5lvHtzDuHMWB6AwlGgiapYpoaQMWZAmQ3UBlMLPMHqQ2gL2AkGjC9IdSApJ1ebLXn0f4OhNx-kiybcmzgHcaLWTZLRA5RXE3SrjRFMwkt1j4UQ4K10JTBgpRgXEUYD4KQ3ySm2EowwNpXL8AV-iHEBQV1thJxIAxTr3Ft4vNlEkgWX5BdJUcqhMkfXHGowTNiUTTrn20B7WhjU8c5SZUvSTSqmv5C2Zy0oRBLoGRqE0kU_zILZq2yXePBQkPL-pLq0tY7nu7gwazELifDDJ1ipOMGrvsxWEt5gFIGMOJG257vMo2fqGezmCIvS7flH4eCKGTmhfUodJ7tdqz5U8cUl2n_z4sX_xvPdd_xt_THvkSJkZ6qOZUxM2xmF8pesmgIaaAKLWUsntvMiQyT-MLMdhb7DvMCa0z6oLxfL37af5HtkZ9XfIObK8PmNBk7TXyEUsYJN1md7RdORAIPPUuA3vsq-rXx4ELormM7S_fTBhZBNs1WZ235GuxqeEPfWNsAMeLTA20KebkdYm-YEwN0SVpJ6W9OG9_8Fc_n5v-x7bxWahikXtJYcdTxcAWDACvGxzU-qVLkL3Qeu3gn7pA0nQpmKfHUGeoy_vCGNCo2ljYp9CRdykXodh95pOd11nqLHVbbarHb4TEWbZOS1bF4qYqjJeNYav2QVGnuxKYM933iGRIq44vzkYx7auTISD7GX81jR0xhwOTiuOZCVMEa0BtC=w1165-h881-no" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;BÃ i giáº£ng tÃ´i thÃ­ch dáº¡y nháº¥t á»Ÿ VietAI chÃ­nh lÃ  máº¡ng Neural Network.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/class.jpg"&gt;&lt;img src="./images/class.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Giá»›i thiá»‡u vá» thÃ nh tá»±u vÃ  má»¥c tiÃªu cá»§a VietAI &lt;a href="https://docs.google.com/presentation/d/1A_oDWZyC6NhYPeHNrWJbESxSPUT7f0Gg-PLfXDtVKus/edit?usp=sharing" rel="nofollow"&gt;táº¡i Ä‘Ã¢y&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-má»¥c-lá»¥c" class="anchor" aria-hidden="true" href="#má»¥c-lá»¥c"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Má»¥c lá»¥c&lt;/h3&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/math"&gt;1. Kiáº¿n thá»©c toÃ¡n há»c cáº§n thiáº¿t&lt;/a&gt; (HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/python-tutorials"&gt;2. Ká»¹ nÄƒng láº­p trÃ¬nh Python&lt;/a&gt; (HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/numpy"&gt;3. ThÆ° viá»‡n Numpy vÃ  TensorFlow&lt;/a&gt; (HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/blob/master/models/linear-regression/"&gt;4. BÃ i toÃ¡n há»“i quy (Regression)&lt;/a&gt; (HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/blob/master/models/logistic-regression"&gt;5. BÃ i toÃ¡n phÃ¢n loáº¡i (Classification)&lt;/a&gt; (HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/blob/master/models/random-forests"&gt;6. XÃ¢y dá»±ng mÃ´ hÃ¬nh Decision Trees vÃ  Random Forests &lt;/a&gt; (ChÆ°a HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/models/nn"&gt;7. XÃ¢y dá»±ng máº¡ng Neural Network&lt;/a&gt; (Äang tiáº¿n hÃ nh)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/models/cnn"&gt;8. XÃ¢y dá»±ng máº¡ng Convolutional Neural Network (CNN)&lt;/a&gt; (ChÆ°a HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/models/rnn"&gt;9. XÃ¢y dá»±ng máº¡ng Recurrent Neural Network (RNN)&lt;/a&gt; (ChÆ°a HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/gan"&gt;10. MÃ´ hÃ¬nh sinh GAN vÃ  CycleGAN&lt;/a&gt; (Äang tiáº¿n hÃ nh)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/deployment/distributed-tensorflow"&gt;11. Triá»ƒn khai (Deploy) Machine Learning Model trÃªn Production&lt;/a&gt; (Äang tiáº¿n hÃ nh)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/deployment/tensorflow-browser"&gt;12. Machine Learning trÃªn trÃ¬nh duyá»‡t vÃ  TensorFlowJS&lt;/a&gt; (HoÃ n táº¥t)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/tf2.0"&gt;13. Cáº­p nháº­t má»›i nháº¥t&lt;/a&gt; (Äang tiáº¿n hÃ nh)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/tf2.0"&gt;TensorFlow 2.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/articles/GradientPaperSpace.MD"&gt;Tráº£i nghiá»‡m PaperSpace Gradient Community&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bangoc123/learn-machine-learning-in-two-months/tree/master/algorithms"&gt;14. Ã”n luyá»‡n thuáº­t toÃ¡n má»—i ngÃ y&lt;/a&gt; (Äang tiáº¿n hÃ nh)
&lt;ul&gt;
&lt;li&gt;&lt;a href="./algorithms/graph/backtracking/backtracking.MD"&gt;Backtracking Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bangoc123</author><guid isPermaLink="false">https://github.com/bangoc123/learn-machine-learning-in-two-months</guid><pubDate>Mon, 18 Nov 2019 00:18:00 GMT</pubDate></item><item><title>ljpzzz/machinelearning #19 in Jupyter Notebook, This week</title><link>https://github.com/ljpzzz/machinelearning</link><description>&lt;p&gt;&lt;i&gt;My blogs and code for machine learning. http://cnblogs.com/pinard&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-åˆ˜å»ºå¹³pinardçš„åšå®¢é…å¥—ä»£ç " class="anchor" aria-hidden="true" href="#åˆ˜å»ºå¹³pinardçš„åšå®¢é…å¥—ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;åˆ˜å»ºå¹³Pinardçš„åšå®¢é…å¥—ä»£ç &lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/pinard" rel="nofollow"&gt;http://www.cnblogs.com/pinard&lt;/a&gt; åˆ˜å»ºå¹³Pinard&lt;/p&gt;
&lt;p&gt;ä¹‹å‰ä¸å°‘æœ‹å‹ååº”æˆ‘åšå®¢ä¸­çš„ä»£ç éƒ½æ˜¯è¿ç»­çš„ç‰‡æ®µï¼Œä¸å¥½å­¦ä¹ ï¼Œå› æ­¤è¿™é‡ŒæŠŠæ–‡ç« å’Œä»£ç åšä¸€ä¸ªæ•´ç†ã€‚
ä»£ç æœ‰éƒ¨åˆ†æ¥æºäºç½‘ç»œï¼Œå·²åŠ ä¸Šç›¸å…³æ–¹ç‰ˆæƒä¿¡æ¯ã€‚éƒ¨åˆ†ä¸ºè‡ªå·±åŸåˆ›ï¼Œå·²åŠ ä¸Šæˆ‘çš„ç‰ˆæƒä¿¡æ¯ã€‚&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ç›®å½•" class="anchor" aria-hidden="true" href="#ç›®å½•"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç›®å½•&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#2"&gt;æœºå™¨å­¦ä¹ åŸºç¡€ä¸å›å½’ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#3"&gt;æœºå™¨å­¦ä¹ åˆ†ç±»ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#4"&gt;æœºå™¨å­¦ä¹ èšç±»ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#5"&gt;æœºå™¨å­¦ä¹ é™ç»´ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#6"&gt;æœºå™¨å­¦ä¹ é›†æˆå­¦ä¹ ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#7"&gt;æ•°å­¦ç»Ÿè®¡å­¦&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#8"&gt;æœºå™¨å­¦ä¹ å…³è”ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#9"&gt;æœºå™¨å­¦ä¹ æ¨èç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#10"&gt;æ·±åº¦å­¦ä¹ ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#11"&gt;è‡ªç„¶è¯­è¨€å¤„ç†ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#1"&gt;å¼ºåŒ–å­¦ä¹ ç®—æ³•&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#12"&gt;ç‰¹å¾å·¥ç¨‹ä¸ç®—æ³•è½åœ°&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-æ³¨æ„" class="anchor" aria-hidden="true" href="#æ³¨æ„"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æ³¨æ„&lt;/h2&gt;
&lt;p&gt;2016-2017å¹´å†™çš„åšå®¢ä½¿ç”¨çš„pythonç‰ˆæœ¬æ˜¯2.7ï¼Œ 2018å¹´å› ä¸ºTensorFlowå¯¹Python3çš„ä¸€äº›è¦æ±‚ï¼Œæ‰€ä»¥å†™åšå®¢ä½¿ç”¨çš„Pythonç‰ˆæœ¬æ˜¯3.6ã€‚å°‘éƒ¨åˆ†2016ï¼Œ2017å¹´çš„åšå®¢ä»£ç æ— æ³•æ‰¾åˆ°ï¼Œé‡æ–°ç”¨Python3.6è·‘è¿‡ä¸Šä¼ ï¼Œå› æ­¤å¯èƒ½ä¼šå‡ºç°å’Œåšå®¢ä¸­ä»£ç ç¨æœ‰ä¸ä¸€è‡´çš„åœ°æ–¹ï¼Œä¸»è¦æ¶‰åŠåˆ°printçš„è¯­æ³•å’Œrangeçš„ç”¨æ³•ï¼Œè‹¥é‡åˆ°é—®é¢˜ï¼Œç¨å¾®ä¿®æ”¹å³å¯è·‘é€šã€‚&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-èµåŠ©æˆ‘" class="anchor" aria-hidden="true" href="#èµåŠ©æˆ‘"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#13"&gt;èµåŠ©æˆ‘&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="user-content-1"&gt;&lt;a id="user-content-å¼ºåŒ–å­¦ä¹ æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#å¼ºåŒ–å­¦ä¹ æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;å¼ºåŒ–å­¦ä¹ æ–‡ç« ä¸ä»£ç ï¼š:&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9385570.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆä¸€ï¼‰æ¨¡å‹åŸºç¡€&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/introduction.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9426283.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆäºŒï¼‰é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹(MDP)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9463815.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆä¸‰ï¼‰ç”¨åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰æ±‚è§£&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9492980.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆå››ï¼‰ç”¨è’™ç‰¹å¡ç½—æ³•ï¼ˆMCï¼‰æ±‚è§£&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9529828.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆäº”ï¼‰ç”¨æ—¶åºå·®åˆ†æ³•ï¼ˆTDï¼‰æ±‚è§£&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9614290.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆå…­ï¼‰æ—¶åºå·®åˆ†åœ¨çº¿æ§åˆ¶ç®—æ³•SARSA&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/sarsa_windy_world.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9669263.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆä¸ƒï¼‰æ—¶åºå·®åˆ†ç¦»çº¿æ§åˆ¶ç®—æ³•Q-Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9714655.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆå…«ï¼‰ä»·å€¼å‡½æ•°çš„è¿‘ä¼¼è¡¨ç¤ºä¸Deep Q-Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9756075.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆä¹ï¼‰Deep Q-Learningè¿›é˜¶ä¹‹Nature DQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9778063.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ ï¼ˆåï¼‰Double DQN (DDQN)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9797695.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åä¸€) Prioritized Replay DQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9923859.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åäºŒ) Dueling DQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10137696.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åä¸‰) ç­–ç•¥æ¢¯åº¦(Policy Gradient)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10272023.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åå››) Actor-Critic&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10334127.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åäº”) A3C&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/a3c.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10345762.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åå…­) æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦(DDPG)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10384424.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åä¸ƒ) åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¸Dynaç®—æ³•æ¡†æ¶&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10470571.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åå…«) åŸºäºæ¨¡æ‹Ÿçš„æœç´¢ä¸è’™ç‰¹å¡ç½—æ ‘æœç´¢(MCTS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10609228.html" rel="nofollow"&gt;å¼ºåŒ–å­¦ä¹ (åä¹) AlphaGo Zeroå¼ºåŒ–å­¦ä¹ åŸç†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-2"&gt;&lt;a id="user-content-æœºå™¨å­¦ä¹ åŸºç¡€ä¸å›å½’ç®—æ³•æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æœºå™¨å­¦ä¹ åŸºç¡€ä¸å›å½’ç®—æ³•æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ åŸºç¡€ä¸å›å½’ç®—æ³•æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5970503.html" rel="nofollow"&gt;æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5976811.html" rel="nofollow"&gt;æœ€å°äºŒä¹˜æ³•å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5992719.html" rel="nofollow"&gt;äº¤å‰éªŒè¯(Cross Validation)åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5993450.html" rel="nofollow"&gt;ç²¾ç¡®ç‡ä¸å¬å›ç‡ï¼ŒRoCæ›²çº¿ä¸PRæ›²çº¿&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6004041.html" rel="nofollow"&gt;çº¿æ€§å›å½’åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6007200.html" rel="nofollow"&gt;æœºå™¨å­¦ä¹ ç ”ç©¶ä¸å¼€å‘å¹³å°çš„é€‰æ‹©&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6013484.html" rel="nofollow"&gt;scikit-learn å’Œpandas åŸºäºwindowså•æœºæœºå™¨å­¦ä¹ ç¯å¢ƒçš„æ­å»º&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6016029.html" rel="nofollow"&gt;ç”¨scikit-learnå’Œpandaså­¦ä¹ çº¿æ€§å›å½’&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/linear-regression.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6018889.html" rel="nofollow"&gt;Lassoå›å½’ç®—æ³•ï¼š åæ ‡è½´ä¸‹é™æ³•ä¸æœ€å°è§’å›å½’æ³•å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6023000.html" rel="nofollow"&gt;ç”¨scikit-learnå’Œpandaså­¦ä¹ Ridgeå›å½’&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/ridge_regression_1.ipynb"&gt;ä»£ç 1&lt;/a&gt; &lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/ridge_regression.ipynb"&gt;ä»£ç 2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6026343.html" rel="nofollow"&gt;scikit-learn çº¿æ€§å›å½’ç®—æ³•åº“å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9314198.html" rel="nofollow"&gt;å¼‚å¸¸ç‚¹æ£€æµ‹ç®—æ³•å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-3"&gt;&lt;a id="user-content-æœºå™¨å­¦ä¹ åˆ†ç±»ç®—æ³•æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æœºå™¨å­¦ä¹ åˆ†ç±»ç®—æ³•æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ åˆ†ç±»ç®—æ³•æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6029432.html" rel="nofollow"&gt;é€»è¾‘å›å½’åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6035872.html" rel="nofollow"&gt;scikit-learn é€»è¾‘å›å½’ç±»åº“ä½¿ç”¨å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6042320.html" rel="nofollow"&gt;æ„ŸçŸ¥æœºåŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6050306.html" rel="nofollow"&gt;å†³ç­–æ ‘ç®—æ³•åŸç†(ä¸Š)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6053344.html" rel="nofollow"&gt;å†³ç­–æ ‘ç®—æ³•åŸç†(ä¸‹)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6056319.html" rel="nofollow"&gt;scikit-learnå†³ç­–æ ‘ç®—æ³•ç±»åº“ä½¿ç”¨å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/decision_tree_classifier.ipynb"&gt;ä»£ç 1&lt;/a&gt; &lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/decision_tree_classifier_1.ipynb"&gt;ä»£ç 2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6061661.html" rel="nofollow"&gt;Kè¿‘é‚»æ³•(KNN)åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6065607.html" rel="nofollow"&gt;scikit-learn Kè¿‘é‚»æ³•ç±»åº“ä½¿ç”¨å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/knn_classifier.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6069267.html" rel="nofollow"&gt;æœ´ç´ è´å¶æ–¯ç®—æ³•åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6074222.html" rel="nofollow"&gt;scikit-learn æœ´ç´ è´å¶æ–¯ç±»åº“ä½¿ç”¨å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/native_bayes.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6093948.html" rel="nofollow"&gt;æœ€å¤§ç†µæ¨¡å‹åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6097604.html" rel="nofollow"&gt;æ”¯æŒå‘é‡æœºåŸç†(ä¸€) çº¿æ€§æ”¯æŒå‘é‡æœº&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6100722.html" rel="nofollow"&gt;æ”¯æŒå‘é‡æœºåŸç†(äºŒ) çº¿æ€§æ”¯æŒå‘é‡æœºçš„è½¯é—´éš”æœ€å¤§åŒ–æ¨¡å‹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6103615.html" rel="nofollow"&gt;æ”¯æŒå‘é‡æœºåŸç†(ä¸‰)çº¿æ€§ä¸å¯åˆ†æ”¯æŒå‘é‡æœºä¸æ ¸å‡½æ•°&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6111471.html" rel="nofollow"&gt;æ”¯æŒå‘é‡æœºåŸç†(å››)SMOç®—æ³•åŸç†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6113120.html" rel="nofollow"&gt;æ”¯æŒå‘é‡æœºåŸç†(äº”)çº¿æ€§æ”¯æŒå›å½’&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6117515.html" rel="nofollow"&gt;scikit-learn æ”¯æŒå‘é‡æœºç®—æ³•åº“ä½¿ç”¨å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6126077.html" rel="nofollow"&gt;æ”¯æŒå‘é‡æœºé«˜æ–¯æ ¸è°ƒå‚å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/svm_classifier.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-7"&gt;&lt;a id="user-content-æ•°å­¦ç»Ÿè®¡å­¦æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æ•°å­¦ç»Ÿè®¡å­¦æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æ•°å­¦ç»Ÿè®¡å­¦æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6047802.html" rel="nofollow"&gt;æœºå™¨å­¦ä¹ ç®—æ³•çš„éšæœºæ•°æ®ç”Ÿæˆ&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/random_data_generation.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6625739.html" rel="nofollow"&gt;MCMC(ä¸€)è’™ç‰¹å¡ç½—æ–¹æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6632399.html" rel="nofollow"&gt;MCMC(äºŒ)é©¬å°”ç§‘å¤«é“¾&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/mcmc_2.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6638955.html" rel="nofollow"&gt;MCMC(ä¸‰)MCMCé‡‡æ ·å’ŒM-Hé‡‡æ ·&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/mcmc_3_4.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6645766.html" rel="nofollow"&gt;MCMC(å››)Gibbsé‡‡æ ·&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/mcmc_3_4.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10750718.html" rel="nofollow"&gt;æœºå™¨å­¦ä¹ ä¸­çš„çŸ©é˜µå‘é‡æ±‚å¯¼(ä¸€) æ±‚å¯¼å®šä¹‰ä¸æ±‚å¯¼å¸ƒå±€&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10773942.html" rel="nofollow"&gt;æœºå™¨å­¦ä¹ ä¸­çš„çŸ©é˜µå‘é‡æ±‚å¯¼(äºŒ) çŸ©é˜µå‘é‡æ±‚å¯¼ä¹‹å®šä¹‰æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10791506.html" rel="nofollow"&gt;æœºå™¨å­¦ä¹ ä¸­çš„çŸ©é˜µå‘é‡æ±‚å¯¼(ä¸‰) çŸ©é˜µå‘é‡æ±‚å¯¼ä¹‹å¾®åˆ†æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10825264.html" rel="nofollow"&gt;æœºå™¨å­¦ä¹ ä¸­çš„çŸ©é˜µå‘é‡æ±‚å¯¼(å››) çŸ©é˜µå‘é‡æ±‚å¯¼é“¾å¼æ³•åˆ™&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10930902.html" rel="nofollow"&gt;æœºå™¨å­¦ä¹ ä¸­çš„çŸ©é˜µå‘é‡æ±‚å¯¼(äº”) çŸ©é˜µå¯¹çŸ©é˜µçš„æ±‚å¯¼&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-6"&gt;&lt;a id="user-content-æœºå™¨å­¦ä¹ é›†æˆå­¦ä¹ æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æœºå™¨å­¦ä¹ é›†æˆå­¦ä¹ æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ é›†æˆå­¦ä¹ æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6131423.html" rel="nofollow"&gt;é›†æˆå­¦ä¹ åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6133937.html" rel="nofollow"&gt;é›†æˆå­¦ä¹ ä¹‹Adaboostç®—æ³•åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6136914.html" rel="nofollow"&gt;scikit-learn Adaboostç±»åº“ä½¿ç”¨å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/adaboost-classifier.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6140514.html" rel="nofollow"&gt;æ¢¯åº¦æå‡æ ‘(GBDT)åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6143927.html" rel="nofollow"&gt;scikit-learn æ¢¯åº¦æå‡æ ‘(GBDT)è°ƒå‚å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/gbdt_classifier.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6156009.html" rel="nofollow"&gt;Baggingä¸éšæœºæ£®æ—ç®—æ³•åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6160412.html" rel="nofollow"&gt;scikit-learnéšæœºæ£®æ—è°ƒå‚å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/random_forest_classifier.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10979808.html" rel="nofollow"&gt;XGBoostç®—æ³•åŸç†å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/11114748.html" rel="nofollow"&gt;XGBoostç±»åº“ä½¿ç”¨å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/xgboost-example.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-4"&gt;&lt;a id="user-content-æœºå™¨å­¦ä¹ èšç±»ç®—æ³•æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æœºå™¨å­¦ä¹ èšç±»ç®—æ³•æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ èšç±»ç®—æ³•æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6164214.html" rel="nofollow"&gt;K-Meansèšç±»ç®—æ³•åŸç†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6169370.html" rel="nofollow"&gt;ç”¨scikit-learnå­¦ä¹ K-Meansèšç±»&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/kmeans_cluster.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6179132.html" rel="nofollow"&gt;BIRCHèšç±»ç®—æ³•åŸç†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6200579.html" rel="nofollow"&gt;ç”¨scikit-learnå­¦ä¹ BIRCHèšç±»&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/birch_cluster.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6208966.html" rel="nofollow"&gt;DBSCANå¯†åº¦èšç±»ç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6217852.html" rel="nofollow"&gt;ç”¨scikit-learnå­¦ä¹ DBSCANèšç±»&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/dbscan_cluster.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6221564.html" rel="nofollow"&gt;è°±èšç±»ï¼ˆspectral clusteringï¼‰åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6235920.html" rel="nofollow"&gt;ç”¨scikit-learnå­¦ä¹ è°±èšç±»&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/spectral_cluster.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-5"&gt;&lt;a id="user-content-æœºå™¨å­¦ä¹ é™ç»´ç®—æ³•æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æœºå™¨å­¦ä¹ é™ç»´ç®—æ³•æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ é™ç»´ç®—æ³•æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6239403.html" rel="nofollow"&gt;ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6243025.html" rel="nofollow"&gt;ç”¨scikit-learnå­¦ä¹ ä¸»æˆåˆ†åˆ†æ(PCA)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/pca.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6244265.html" rel="nofollow"&gt;çº¿æ€§åˆ¤åˆ«åˆ†æLDAåŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6249328.html" rel="nofollow"&gt;ç”¨scikit-learnè¿›è¡ŒLDAé™ç»´&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/lda.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6251584.html" rel="nofollow"&gt;å¥‡å¼‚å€¼åˆ†è§£(SVD)åŸç†ä¸åœ¨é™ç»´ä¸­çš„åº”ç”¨&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6266408.html" rel="nofollow"&gt;å±€éƒ¨çº¿æ€§åµŒå…¥(LLE)åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6273377.html" rel="nofollow"&gt;ç”¨scikit-learnç ”ç©¶å±€éƒ¨çº¿æ€§åµŒå…¥(LLE)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/lle.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-8"&gt;&lt;a id="user-content-æœºå™¨å­¦ä¹ å…³è”ç®—æ³•æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æœºå™¨å­¦ä¹ å…³è”ç®—æ³•æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ å…³è”ç®—æ³•æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6288716.html" rel="nofollow"&gt;å…¸å‹å…³è”åˆ†æ(CCA)åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6293298.html" rel="nofollow"&gt;Aprioriç®—æ³•åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6307064.html" rel="nofollow"&gt;FP Treeç®—æ³•åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6323182.html" rel="nofollow"&gt;PrefixSpanç®—æ³•åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6340162.html" rel="nofollow"&gt;ç”¨Sparkå­¦ä¹ FP Treeç®—æ³•å’ŒPrefixSpanç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/fp_tree_prefixspan.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6039099.html" rel="nofollow"&gt;æ—¥å¿—å’Œå‘Šè­¦æ•°æ®æŒ–æ˜ç»éªŒè°ˆ&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-9"&gt;&lt;a id="user-content-æœºå™¨å­¦ä¹ æ¨èç®—æ³•æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æœºå™¨å­¦ä¹ æ¨èç®—æ³•æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æœºå™¨å­¦ä¹ æ¨èç®—æ³•æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6349233.html" rel="nofollow"&gt;ååŒè¿‡æ»¤æ¨èç®—æ³•æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6351319.html" rel="nofollow"&gt;çŸ©é˜µåˆ†è§£åœ¨ååŒè¿‡æ»¤æ¨èç®—æ³•ä¸­çš„åº”ç”¨&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6362647.html" rel="nofollow"&gt;SimRankååŒè¿‡æ»¤æ¨èç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6364932.html" rel="nofollow"&gt;ç”¨Sparkå­¦ä¹ çŸ©é˜µåˆ†è§£æ¨èç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/matrix_factorization.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6370127.html" rel="nofollow"&gt;åˆ†è§£æœº(Factorization Machines)æ¨èç®—æ³•åŸç†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9128682.html" rel="nofollow"&gt;è´å¶æ–¯ä¸ªæ€§åŒ–æ’åº(BPR)ç®—æ³•å°ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9163481.html" rel="nofollow"&gt;ç”¨tensorflowå­¦ä¹ è´å¶æ–¯ä¸ªæ€§åŒ–æ’åº(BPR)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/bpr.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-10"&gt;&lt;a id="user-content-æ·±åº¦å­¦ä¹ ç®—æ³•æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#æ·±åº¦å­¦ä¹ ç®—æ³•æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;æ·±åº¦å­¦ä¹ ç®—æ³•æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6418668.html" rel="nofollow"&gt;æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¨¡å‹ä¸å‰å‘ä¼ æ’­ç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6422831.html" rel="nofollow"&gt;æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åå‘ä¼ æ’­ç®—æ³•(BP)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6437495.html" rel="nofollow"&gt;æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æŸå¤±å‡½æ•°å’Œæ¿€æ´»å‡½æ•°çš„é€‰æ‹©&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6472666.html" rel="nofollow"&gt;æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„æ­£åˆ™åŒ–&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6483207.html" rel="nofollow"&gt;å·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ¨¡å‹ç»“æ„&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6489633.html" rel="nofollow"&gt;å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å‰å‘ä¼ æ’­ç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6494810.html" rel="nofollow"&gt;å·ç§¯ç¥ç»ç½‘ç»œ(CNN)åå‘ä¼ æ’­ç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6509630.html" rel="nofollow"&gt;å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)æ¨¡å‹ä¸å‰å‘åå‘ä¼ æ’­ç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6519110.html" rel="nofollow"&gt;LSTMæ¨¡å‹ä¸å‰å‘åå‘ä¼ æ’­ç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6530523.html" rel="nofollow"&gt;å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-11"&gt;&lt;a id="user-content-è‡ªç„¶è¯­è¨€å¤„ç†æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#è‡ªç„¶è¯­è¨€å¤„ç†æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;è‡ªç„¶è¯­è¨€å¤„ç†æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6677078.html" rel="nofollow"&gt;æ–‡æœ¬æŒ–æ˜çš„åˆ†è¯åŸç†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6688348.html" rel="nofollow"&gt;æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¹‹å‘é‡åŒ–ä¸Hash Trick&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/hash_trick.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6693230.html" rel="nofollow"&gt;æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†ä¹‹TF-IDF&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/tf-idf.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6744056.html" rel="nofollow"&gt;ä¸­æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†æµç¨‹æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/chinese_digging.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6756534.html" rel="nofollow"&gt;è‹±æ–‡æ–‡æœ¬æŒ–æ˜é¢„å¤„ç†æµç¨‹æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/english_digging.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6805861.html" rel="nofollow"&gt;æ–‡æœ¬ä¸»é¢˜æ¨¡å‹ä¹‹æ½œåœ¨è¯­ä¹‰ç´¢å¼•(LSI)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6812011.html" rel="nofollow"&gt;æ–‡æœ¬ä¸»é¢˜æ¨¡å‹ä¹‹éè´ŸçŸ©é˜µåˆ†è§£(NMF)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/nmf.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6831308.html" rel="nofollow"&gt;æ–‡æœ¬ä¸»é¢˜æ¨¡å‹ä¹‹LDA(ä¸€) LDAåŸºç¡€&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6867828.html" rel="nofollow"&gt;æ–‡æœ¬ä¸»é¢˜æ¨¡å‹ä¹‹LDA(äºŒ) LDAæ±‚è§£ä¹‹Gibbsé‡‡æ ·ç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6873703.html" rel="nofollow"&gt;æ–‡æœ¬ä¸»é¢˜æ¨¡å‹ä¹‹LDA(ä¸‰) LDAæ±‚è§£ä¹‹å˜åˆ†æ¨æ–­EMç®—æ³•&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6908150.html" rel="nofollow"&gt;ç”¨scikit-learnå­¦ä¹ LDAä¸»é¢˜æ¨¡å‹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/lda.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6912636.html" rel="nofollow"&gt;EMç®—æ³•åŸç†æ€»ç»“&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6945257.html" rel="nofollow"&gt;éšé©¬å°”ç§‘å¤«æ¨¡å‹HMMï¼ˆä¸€ï¼‰HMMæ¨¡å‹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6955871.html" rel="nofollow"&gt;éšé©¬å°”ç§‘å¤«æ¨¡å‹HMMï¼ˆäºŒï¼‰å‰å‘åå‘ç®—æ³•è¯„ä¼°è§‚å¯Ÿåºåˆ—æ¦‚ç‡&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6972299.html" rel="nofollow"&gt;éšé©¬å°”ç§‘å¤«æ¨¡å‹HMMï¼ˆä¸‰ï¼‰é²å§†-éŸ¦å°”å¥‡ç®—æ³•æ±‚è§£HMMå‚æ•°&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6991852.html" rel="nofollow"&gt;éšé©¬å°”ç§‘å¤«æ¨¡å‹HMMï¼ˆå››ï¼‰ç»´ç‰¹æ¯”ç®—æ³•è§£ç éšè—çŠ¶æ€åºåˆ—&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7001397.html" rel="nofollow"&gt;ç”¨hmmlearnå­¦ä¹ éšé©¬å°”ç§‘å¤«æ¨¡å‹HMM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/hmm.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7048333.html" rel="nofollow"&gt;æ¡ä»¶éšæœºåœºCRF(ä¸€)ä»éšæœºåœºåˆ°çº¿æ€§é“¾æ¡ä»¶éšæœºåœº&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7055072.html" rel="nofollow"&gt;æ¡ä»¶éšæœºåœºCRF(äºŒ) å‰å‘åå‘ç®—æ³•è¯„ä¼°æ ‡è®°åºåˆ—æ¦‚ç‡&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7068574.html" rel="nofollow"&gt;æ¡ä»¶éšæœºåœºCRF(ä¸‰) æ¨¡å‹å­¦ä¹ ä¸ç»´ç‰¹æ¯”ç®—æ³•è§£ç &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7160330.html" rel="nofollow"&gt;word2vecåŸç†(ä¸€) CBOWä¸Skip-Gramæ¨¡å‹åŸºç¡€&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7243513.html" rel="nofollow"&gt;word2vecåŸç†(äºŒ) åŸºäºHierarchical Softmaxçš„æ¨¡å‹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7249903.html" rel="nofollow"&gt;word2vecåŸç†(ä¸‰) åŸºäºNegative Samplingçš„æ¨¡å‹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7278324.html" rel="nofollow"&gt;ç”¨gensimå­¦ä¹ word2vec&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/word2vec.ipynb"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-12"&gt;&lt;a id="user-content-ç‰¹å¾å·¥ç¨‹ä¸ç®—æ³•è½åœ°æ–‡ç« ä¸ä»£ç " class="anchor" aria-hidden="true" href="#ç‰¹å¾å·¥ç¨‹ä¸ç®—æ³•è½åœ°æ–‡ç« ä¸ä»£ç "&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ç‰¹å¾å·¥ç¨‹ä¸ç®—æ³•è½åœ°æ–‡ç« ä¸ä»£ç ï¼š&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ–‡ç« &lt;/th&gt;
&lt;th&gt;ä»£ç &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9032759.html" rel="nofollow"&gt;ç‰¹å¾å·¥ç¨‹ä¹‹ç‰¹å¾é€‰æ‹©&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9061549.html" rel="nofollow"&gt;ç‰¹å¾å·¥ç¨‹ä¹‹ç‰¹å¾è¡¨è¾¾&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9093890.html" rel="nofollow"&gt;ç‰¹å¾å·¥ç¨‹ä¹‹ç‰¹å¾é¢„å¤„ç†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;æ— &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9220199.html" rel="nofollow"&gt;ç”¨PMMLå®ç°æœºå™¨å­¦ä¹ æ¨¡å‹çš„è·¨å¹³å°ä¸Šçº¿&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/model-in-product/sklearn-jpmml"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9251296.html" rel="nofollow"&gt;tensorflowæœºå™¨å­¦ä¹ æ¨¡å‹çš„è·¨å¹³å°ä¸Šçº¿&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/model-in-product/tensorflow-java"&gt;ä»£ç &lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-13"&gt;&lt;a id="user-content-èµåŠ©æˆ‘-1" class="anchor" aria-hidden="true" href="#èµåŠ©æˆ‘-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;èµåŠ©æˆ‘&lt;/h3&gt;
&lt;p&gt;ä½ çš„æ”¯æŒæ˜¯æˆ‘å†™ä½œçš„åŠ¨åŠ›(1.å¾®ä¿¡/2.æ”¯ä»˜å®)ï¼š&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./assert/invoice.bmp"&gt;&lt;img src="./assert/invoice.bmp" alt="å¾®ä¿¡èµåŠ©" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./assert/invoice_ali.bmp"&gt;&lt;img src="./assert/invoice_ali.bmp" alt="æ”¯ä»˜å®èµåŠ©" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;License MIT.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ljpzzz</author><guid isPermaLink="false">https://github.com/ljpzzz/machinelearning</guid><pubDate>Mon, 18 Nov 2019 00:19:00 GMT</pubDate></item><item><title>microsoft/QuantumKatas #20 in Jupyter Notebook, This week</title><link>https://github.com/microsoft/QuantumKatas</link><description>&lt;p&gt;&lt;i&gt;Tutorials and programming exercises for learning Q# and quantum computing&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Quantum Katas are a series of self-paced tutorials to help you learn quantum computing and Q# programming.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;ğŸ†•&lt;/g-emoji&gt; &lt;em&gt;(July 2019)&lt;/em&gt; The Quantum Katas now include Jupyter Notebook tutorials on quantum computing! Each tutorial combines theoretical explanations with Q# code snippets and programming exercises. See &lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all tutorials and instructions on running them online.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;ğŸ†•&lt;/g-emoji&gt; &lt;em&gt;(April 2019)&lt;/em&gt; The Quantum Katas are now available as Jupyter Notebooks! See &lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all Kata Notebooks and instructions on running them online.&lt;/p&gt;
&lt;p&gt;Each kata is a separate project that includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A sequence of tasks progressing from easy to hard.
Each task requires you to fill in some code. The first task might require just one line, and the last one might require rather complicated code.&lt;/li&gt;
&lt;li&gt;A testing framework that sets up, runs, and validates your solutions.
Each task is covered by a &lt;a href="https://docs.microsoft.com/en-us/visualstudio/test/getting-started-with-unit-testing" rel="nofollow"&gt;unit test&lt;/a&gt; which initially fails. Once you write the code to make the test pass, you can move on to the next task.&lt;/li&gt;
&lt;li&gt;Links to quantum computing and Q# reference material you might need to solve the tasks.&lt;/li&gt;
&lt;li&gt;Hints and reference solutions to help you if you're stuck.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#tutorial-topics"&gt;List of tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-topics"&gt;List of katas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#run-as-notebook"&gt;Run the katas and tutorials online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-locally"&gt;Run the katas locally&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#download"&gt;Download the Quantum Katas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#kata-as-project"&gt;Run a kata as a Q# project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tests"&gt;Run kata tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker"&gt;Run katas locally with Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#code-of-conduct"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-list-of-tutorials-" class="anchor" aria-hidden="true" href="#list-of-tutorials-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of tutorials &lt;a name="user-content-tutorial-topics"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-tutorial-topics"&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-tutorial-topics"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-tutorial-topics"&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="./tutorials/ComplexArithmetic/"&gt;Complex arithmetic&lt;/a&gt;&lt;/strong&gt;.
Learn about complex numbers and the mathematics required to work with quantum computing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/LinearAlgebra/"&gt;Linear algebra&lt;/a&gt;&lt;/strong&gt;.
Learn about vectors and matrices used to represent quantum states and quantum operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/Qubit/"&gt;The qubit&lt;/a&gt;&lt;/strong&gt;.
Learn what a qubit is.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/SingleQubitGates/"&gt;Single-qubit gates&lt;/a&gt;&lt;/strong&gt;.
Learn what a quantum gate is and about the most common single-qubit gates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/MultiQubitSystems/"&gt;Multi-qubit systems&lt;/a&gt;&lt;/strong&gt;.
Learn to represent multi-qubit systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/MultiQubitGates/"&gt;Multi-qubit gates&lt;/a&gt;&lt;/strong&gt;.
Learn about the most common multi-qubit gates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/ExploringDeutschJozsaAlgorithm/"&gt;Exploring Deutschâ€“Jozsa algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn to implement classical functions and equivalent quantum oracles, and compare the quantum
solution to the Deutschâ€“Jozsa problem to a classical one.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./tutorials/ExploringGroversAlgorithm/"&gt;Exploring Grover's search algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn more about Grover's search algorithm, picking up where the &lt;a href="./GroversAlgorithm/"&gt;Grover's algorithm kata&lt;/a&gt; left off.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-list-of-katas-" class="anchor" aria-hidden="true" href="#list-of-katas-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of Katas &lt;a name="user-content-kata-topics"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-kata-topics"&gt;
&lt;h4&gt;&lt;a id="user-content-quantum-computing-concepts" class="anchor" aria-hidden="true" href="#quantum-computing-concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantum computing concepts&lt;/h4&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-kata-topics"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-kata-topics"&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;a href="./BasicGates/"&gt;Basic quantum computing gates&lt;/a&gt;&lt;/strong&gt;.
Learn to apply the most common gates used in quantum computing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Superposition/"&gt;Superposition&lt;/a&gt;&lt;/strong&gt;.
Learn to prepare superposition states.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Measurements/"&gt;Measurements&lt;/a&gt;&lt;/strong&gt;.
Learn to distinguish quantum states using measurements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./JointMeasurements/"&gt;Joint measurements&lt;/a&gt;&lt;/strong&gt;.
Learn about using joint (parity) measurements to distinguish quantum states and to perform state transformations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-simple-algorithms" class="anchor" aria-hidden="true" href="#simple-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Simple algorithms&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./Teleportation/"&gt;Teleportation&lt;/a&gt;&lt;/strong&gt;.
Implement standard teleportation protocol and its variations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SuperdenseCoding/"&gt;Superdense coding&lt;/a&gt;&lt;/strong&gt;.
Implement the superdense coding protocol.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./DeutschJozsaAlgorithm/"&gt;Deutschâ€“Jozsa algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about quantum oracles which implement classical functions, and implement Bernsteinâ€“Vazirani and Deutschâ€“Jozsa algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SimonsAlgorithm/"&gt;Simon's algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about Simon's algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-grovers-algorithm" class="anchor" aria-hidden="true" href="#grovers-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grover's algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GroversAlgorithm/"&gt;Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Learn about Grover's search algorithm and how to write quantum oracles to use with it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./SolveSATWithGrover/"&gt;Solving SAT problems using Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Explore Grover's search algorithm, using SAT problems as an example. Learn to implement quantum oracles based on the problem description instead of a hard-coded answer. Use Grover's algorithm to solve problems with an unknown number of solutions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GraphColoring/"&gt;Solving graph coloring problems using Grover's algorithm&lt;/a&gt;&lt;/strong&gt;.
Continue the exploration of Grover's search algorithm, using graph coloring problems as an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-entanglement-games" class="anchor" aria-hidden="true" href="#entanglement-games"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Entanglement games&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./CHSHGame/"&gt;CHSH game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./GHZGame/"&gt;GHZ game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./MagicSquareGame"&gt;Mermin-Peres magic square game&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-miscellaneous" class="anchor" aria-hidden="true" href="#miscellaneous"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Miscellaneous&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./KeyDistribution_BB84/"&gt;BB84 protocol&lt;/a&gt;&lt;/strong&gt;.
Implement the BB84 key distribution algorithm.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./PhaseEstimation/"&gt;Phase estimation&lt;/a&gt;&lt;/strong&gt;.
Learn about phase estimation algorithms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./QEC_BitFlipCode/"&gt;Bit-flip error correcting code&lt;/a&gt;&lt;/strong&gt;.
Learn about a 3-qubit error correcting code for protecting against bit-flip errors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./RippleCarryAdder/"&gt;Ripple-carry adder&lt;/a&gt;&lt;/strong&gt;.
Build a ripple-carry adder on a quantum computer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="./UnitaryPatterns/"&gt;Unitary Patterns*&lt;/a&gt;&lt;/strong&gt;.
Learn to implement unitaries with matrices that follow certain patterns of zero and non-zero elements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-run-the-katas-and-tutorials-online-" class="anchor" aria-hidden="true" href="#run-the-katas-and-tutorials-online-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run the katas and tutorials online &lt;a name="user-content-run-as-notebook"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-run-as-notebook"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-run-as-notebook"&gt;The Quantum Katas are now available as Jupyter Notebooks! See &lt;/a&gt;&lt;a href="https://mybinder.org/v2/gh/Microsoft/QuantumKatas/master?filepath=index.ipynb" rel="nofollow"&gt;index.ipynb&lt;/a&gt; for the list of all katas and tutorials, and instructions to run them online.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-run-katas-locally-" class="anchor" aria-hidden="true" href="#run-katas-locally-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run katas locally &lt;a name="user-content-kata-locally"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-kata-locally"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-kata-locally"&gt;To use the Quantum Katas locally, you'll need the &lt;/a&gt;&lt;a href="https://docs.microsoft.com/quantum" rel="nofollow"&gt;Quantum Development Kit&lt;/a&gt;, available for Windows 10, macOS, and Linux.
If you don't already have the Quantum Development Kit installed, see &lt;a href="https://docs.microsoft.com/quantum/install-guide/" rel="nofollow"&gt;install guide for the Quantum Development Kit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a quick Q# programming language reference sheet, see &lt;a href="./quickref/qsharp-quick-reference.pdf"&gt;Q# Language Quick Reference&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-download-the-quantum-katas-" class="anchor" aria-hidden="true" href="#download-the-quantum-katas-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download the Quantum Katas &lt;a name="user-content-download"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-download"&gt;
&lt;p&gt;If you have Git installed, clone the Microsoft/QuantumKatas repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/Microsoft/QuantumKatas.git&lt;/pre&gt;&lt;/div&gt;
&lt;/a&gt;&lt;blockquote&gt;&lt;a name="user-content-download"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-download"&gt;[!TIP]
Both Visual Studio 2019 and Visual Studio Code make it easy to clone repositories from within your development environment.
For details, see the &lt;/a&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/devops/repos/git/clone?view=azure-devops&amp;amp;tabs=visual-studio#clone-from-another-git-provider" rel="nofollow"&gt;Visual Studio 2019&lt;/a&gt; and &lt;a href="https://code.visualstudio.com/docs/editor/versioncontrol#_cloning-a-repository" rel="nofollow"&gt;Visual Studio Code&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you don't have Git installed, download the katas from &lt;a href="https://github.com/Microsoft/QuantumKatas/archive/master.zip"&gt;https://github.com/Microsoft/QuantumKatas/archive/master.zip&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run-a-kata-as-a-q-project-" class="anchor" aria-hidden="true" href="#run-a-kata-as-a-q-project-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run a kata as a Q# project &lt;a name="user-content-kata-as-project"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-kata-as-project"&gt;
&lt;p&gt;Each kata is in its own directory as a self-contained Q# project, solution and Jupyter Notebook triplet.
For instance, the BasicGates directory structure is:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;QuantumKatas/
  BasicGates/
    README.md                  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Instructions specific to this kata.&lt;/span&gt;
    .vscode/                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Metadata used by Visual Studio Code.&lt;/span&gt;
    BasicGates.sln             &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Visual Studio 2019 solution file.&lt;/span&gt;
    BasicGates.csproj          &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Project file used to build both classical and quantum code.&lt;/span&gt;
    BasicGates.ipynb           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Jupyter Notebook front-end for this kata.&lt;/span&gt;

    Tasks.qs                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# source code that you will fill as you solve each task.&lt;/span&gt;
    Tests.qs                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# tests that verify your solutions.&lt;/span&gt;
    TestSuiteRunner.cs         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; C# source code used to run the Q# tests.&lt;/span&gt;
    ReferenceImplementation.qs &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Q# source code containing solutions to the tasks.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To open the &lt;strong&gt;BasicGates&lt;/strong&gt; kata in Visual Studio 2019, open the &lt;strong&gt;QuantumKatas/BasicGates/BasicGates.sln&lt;/strong&gt; solution file.&lt;/p&gt;
&lt;p&gt;To open the &lt;strong&gt;BasicGates&lt;/strong&gt; kata in Visual Studio Code, open the &lt;strong&gt;QuantumKatas/BasicGates/&lt;/strong&gt; folder.
Press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;âŒ˜ + Shift + P&lt;/strong&gt; on macOS) to open the &lt;strong&gt;Command Palette&lt;/strong&gt;. Type &lt;strong&gt;Open Folder&lt;/strong&gt; on Windows 10 or Linux or &lt;strong&gt;Open&lt;/strong&gt; on macOS.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[!TIP]
Almost all commands available in Visual Studio Code are in the Command Palette.
If you get stuck, press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;âŒ˜ + Shift + P&lt;/strong&gt; on macOS) and start typing to search through all available commands.&lt;/p&gt;
&lt;p&gt;You can also launch Visual Studio Code from the command line:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ code QuantumKatas/BasicGates/&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-run-kata-tests-" class="anchor" aria-hidden="true" href="#run-kata-tests-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-kata-as-project"&gt;Run kata tests &lt;/a&gt;&lt;a name="user-content-tests"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-tests"&gt;
&lt;p&gt;Once you have a kata open, it's time to run the tests using the following instructions.
Initially all tests will fail. Don't panic!
Open &lt;strong&gt;Tasks.qs&lt;/strong&gt; and start filling in the code to complete the tasks. Each task is covered by a unit test. Once you fill in the correct code for a task, rebuild the project and re-run the tests, and the corresponding unit test will pass.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-visual-studio-2019" class="anchor" aria-hidden="true" href="#visual-studio-2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio 2019&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Build the solution.&lt;/li&gt;
&lt;li&gt;From the main menu, open &lt;strong&gt;Test Explorer&lt;/strong&gt; (&lt;strong&gt;Test&lt;/strong&gt; &amp;gt; &lt;strong&gt;Windows&lt;/strong&gt;) and select &lt;strong&gt;Run All&lt;/strong&gt; to run all unit tests at once.&lt;/li&gt;
&lt;li&gt;Work on the tasks in the &lt;strong&gt;Tasks.qs&lt;/strong&gt; file.&lt;/li&gt;
&lt;li&gt;To test your code changes for a task, rebuild the solution and re-run all unit tests using &lt;strong&gt;Run All&lt;/strong&gt;, or run just the test for that task by right-clicking the test and selecting &lt;strong&gt;Run Selected Tests&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Press &lt;strong&gt;Ctrl + `&lt;/strong&gt; (or &lt;strong&gt;âŒ˜ + `&lt;/strong&gt; on macOS) to open the integrated terminal.
The terminal should open to the kata directory. If it doesn't, navigate to the folder containing the *.csproj file for the kata using &lt;code&gt;cd&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;dotnet test&lt;/code&gt; in the integrated terminal.
This should build the kata project and run all of the unit tests. All of the unit tests should fail.&lt;/li&gt;
&lt;li&gt;Work on the tasks in the &lt;strong&gt;Tasks.qs&lt;/strong&gt; file.&lt;/li&gt;
&lt;li&gt;To test your code changes for a task, from the integrated terminal run &lt;code&gt;dotnet test&lt;/code&gt; again.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For convenience, a tasks.json configuration file exists for each kata. It allows Visual Studio Code to run the build and test steps from the Command Palette.
Press &lt;strong&gt;Ctrl + Shift + P&lt;/strong&gt; (or &lt;strong&gt;âŒ˜ + Shift + P&lt;/strong&gt; on macOS) to open the Palette and type &lt;strong&gt;Run Build Task&lt;/strong&gt; or &lt;strong&gt;Run Test Task&lt;/strong&gt; and press &lt;strong&gt;Enter&lt;/strong&gt;.&lt;/p&gt;
&lt;/a&gt;&lt;h2&gt;&lt;a id="user-content-run-katas-locally-with-docker-" class="anchor" aria-hidden="true" href="#run-katas-locally-with-docker-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-tests"&gt;Run katas locally with Docker &lt;/a&gt;&lt;a name="user-content-docker"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-docker"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-docker"&gt;You can use the included &lt;/a&gt;&lt;a href="./Dockerfile"&gt;Dockerfile&lt;/a&gt; to create a docker image with all the necessary tools to run the katas from the command line or Jupyter.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="https://docs.docker.com/install/" rel="nofollow"&gt;Docker&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Build the docker image and tag it &lt;code&gt;katas&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker build -t katas &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Run the image in the container named &lt;code&gt;katas-container&lt;/code&gt; with interactive command-line and redirect container port &lt;code&gt;8888&lt;/code&gt; to local port &lt;code&gt;8888&lt;/code&gt; (needed to run Jupyter):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker run -it --name katas-container -p 8888:8888 katas /bin/bash&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="4"&gt;
&lt;li&gt;From the same command line that you used to run the container, run the C# version of the &lt;strong&gt;BasicGates&lt;/strong&gt; kata:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/BasicGates/
dotnet &lt;span class="pl-c1"&gt;test&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Start a Jupyter Notebook within the image for the &lt;strong&gt;BasicGates&lt;/strong&gt; kata:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/BasicGates/ &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; jupyter notebook --ip=0.0.0.0 --no-browser&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="6"&gt;
&lt;li&gt;Once Jupyter has started, use your browser to open the kata in notebook format. You
will need a token generated by Jupyter when it started on the previous step:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:8888/notebooks/BasicGates.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit a docker container without killing it (daemon mode), press &lt;strong&gt;Ctrl+P, Ctrl+Q&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To re-enter the existing &lt;code&gt;katas-container&lt;/code&gt; (in daemon mode):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker attach katas-container&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you're done, remove the &lt;code&gt;katas-container&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docker rm --force katas-container&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-contributing-" class="anchor" aria-hidden="true" href="#contributing-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing &lt;a name="user-content-contributing"&gt;&lt;/a&gt;&lt;/h1&gt;&lt;a name="user-content-contributing"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-contributing"&gt;This project welcomes contributions and suggestions.  See &lt;/a&gt;&lt;a href=".github/CONTRIBUTING.md"&gt;How Can I Contribute?&lt;/a&gt; for details.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-code-of-conduct-" class="anchor" aria-hidden="true" href="#code-of-conduct-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code of Conduct &lt;a name="user-content-code-of-conduct"&gt;&lt;/a&gt;&lt;/h1&gt;&lt;a name="user-content-code-of-conduct"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-code-of-conduct"&gt;This project has adopted the &lt;/a&gt;&lt;a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;.
For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow"&gt;Code of Conduct FAQ&lt;/a&gt; or
contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/QuantumKatas</guid><pubDate>Mon, 18 Nov 2019 00:20:00 GMT</pubDate></item><item><title>tensorflow/tpu #21 in Jupyter Notebook, This week</title><link>https://github.com/tensorflow/tpu</link><description>&lt;p&gt;&lt;i&gt;Reference models and tools for Cloud TPUs.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cloud-tpus" class="anchor" aria-hidden="true" href="#cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cloud TPUs&lt;/h1&gt;
&lt;p&gt;This repository is a collection of reference models and tools used with
&lt;a href="https://cloud.google.com/tpu/" rel="nofollow"&gt;Cloud TPUs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The fastest way to get started training a model on a Cloud TPU is by following
the tutorial. Click the button below to launch the tutorial using Google Cloud
Shell.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/cloudshell/open?git_repo=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftpu&amp;amp;page=shell&amp;amp;tutorial=tools%2Fctpu%2Ftutorial.md" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/acf32864fdae185325f992b48fb2132badef1e9a/687474703a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e737667" alt="Open in Cloud Shell" data-canonical-src="http://gstatic.com/cloudssh/images/open-btn.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This repository is a public mirror, pull requests will not be accepted.
Please file an issue if you have a feature or bug request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-models" class="anchor" aria-hidden="true" href="#running-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Models&lt;/h2&gt;
&lt;p&gt;To run models in the &lt;code&gt;models&lt;/code&gt; subdirectory, you may need to add the top-level
&lt;code&gt;/models&lt;/code&gt; folder to the Python path with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export PYTHONPATH="$PYTHONPATH:/path/to/models"
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/tpu</guid><pubDate>Mon, 18 Nov 2019 00:21:00 GMT</pubDate></item><item><title>onnx/models #22 in Jupyter Notebook, This week</title><link>https://github.com/onnx/models</link><description>&lt;p&gt;&lt;i&gt;A collection of pre-trained, state-of-the-art models in the ONNX format &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-onnx-model-zoo" class="anchor" aria-hidden="true" href="#onnx-model-zoo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ONNX Model Zoo&lt;/h1&gt;
&lt;p&gt;Open Neural Network Exchange (ONNX) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools.&lt;/p&gt;
&lt;p&gt;The ONNX Model Zoo is a collection of pre-trained, state-of-the-art models in the &lt;a href="http://onnx.ai" rel="nofollow"&gt;ONNX&lt;/a&gt; format contributed by community members like you. Accompanying each model are &lt;a href="http://jupyter.org" rel="nofollow"&gt;Jupyter notebooks&lt;/a&gt; for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-read-the-usage-section-below-for-more-details-on-the-file-formats-in-the-onnx-model-zoo-onnx-pb-npz-and-starter-python-code-for-validating-your-onnx-model-using-test-data" class="anchor" aria-hidden="true" href="#read-the-usage-section-below-for-more-details-on-the-file-formats-in-the-onnx-model-zoo-onnx-pb-npz-and-starter-python-code-for-validating-your-onnx-model-using-test-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Read the &lt;a href="#usage-"&gt;Usage&lt;/a&gt; section below for more details on the file formats in the ONNX Model Zoo (.onnx, .pb, .npz) and starter Python code for validating your ONNX model using test data.&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-vision" class="anchor" aria-hidden="true" href="#vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vision&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#image_classification"&gt;Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#object_detection"&gt;Object Detection &amp;amp; Image Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#body_analysis"&gt;Body, Face &amp;amp; Gesture Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image_manipulation"&gt;Image Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-language" class="anchor" aria-hidden="true" href="#language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Language&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#machine_comprehension"&gt;Machine Comprehension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#machine_translation"&gt;Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#language"&gt;Language Modelling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#visual_qna"&gt;Visual Question Answering &amp;amp; Dialog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speech"&gt;Speech &amp;amp; Audio Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#others"&gt;Other interesting models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-image-classification-" class="anchor" aria-hidden="true" href="#image-classification-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Classification &lt;a name="user-content-image_classification"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-image_classification"&gt;
&lt;p&gt;This collection of models take images as input, then classifies the major objects in the images into 1000 object categories such as keyboard, mouse, pencil, and many animals.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/mobilenet"&gt;MobileNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1801.04381" rel="nofollow"&gt;Sandler et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Light-weight deep neural network best suited for mobile and embedded vision applications. &lt;br&gt;Top-5 error from paper - ~10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/resnet"&gt;ResNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;He et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN model (up to 152 layers). Uses shortcut connections to achieve higher accuracy when classifying images. &lt;br&gt; Top-5 error from paper - ~3.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/squeezenet"&gt;SqueezeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1602.07360" rel="nofollow"&gt;Iandola et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A light-weight CNN model providing AlexNet level accuracy with 50x fewer parameters. &lt;br&gt;Top-5 error from paper - ~20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/vgg"&gt;VGG&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1409.1556" rel="nofollow"&gt;Simonyan et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model(up to 19 layers). Similar to AlexNet but uses multiple smaller kernel-sized filters that provides more accuracy when classifying images. &lt;br&gt;Top-5 error from paper - ~8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/alexnet"&gt;AlexNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow"&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A Deep CNN model (up to 8 layers) where the input is an image and the output is a vector of 1000 numbers. &lt;br&gt; Top-5 error from paper - ~15%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/inception_and_googlenet/googlenet"&gt;GoogleNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1409.4842.pdf" rel="nofollow"&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model(up to 22 layers). Comparatively smaller and faster than VGG and more accurate in detailing than AlexNet. &lt;br&gt; Top-5 error from paper - ~6.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/caffenet"&gt;CaffeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf" rel="nofollow"&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN variation of AlexNet for Image Classification in Caffe where the max pooling precedes the local response normalization (LRN) so that the LRN takes less compute and memory.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/rcnn_ilsvrc13"&gt;RCNN_ILSVRC13&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1311.2524" rel="nofollow"&gt;Girshick et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Pure Caffe implementation of R-CNN for image classification. This model uses localization of regions to classify and extract features from images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/densenet-121"&gt;DenseNet-121&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1608.06993" rel="nofollow"&gt;Huang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Model that has every layer connected to every other layer and passes on its own feature providing strong gradient flow and more diversified features.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/inception_and_googlenet/inception_v1"&gt;Inception_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1409.4842" rel="nofollow"&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model is same as GoogLeNet, implemented through Caffe2 that has improved utilization of the computing resources inside the network and helps with the vanishing gradient problem. &lt;br&gt; Top-5 error from paper - ~6.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/inception_and_googlenet/inception_v2"&gt;Inception_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.00567" rel="nofollow"&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model for Image Classification as an adaptation to Inception v1 with batch normalization. This model has reduced computational cost and improved image resolution compared to Inception v1. &lt;br&gt; Top-5 error from paper ~4.82%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/shufflenet"&gt;ShuffleNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1707.01083" rel="nofollow"&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This model greatly reduces the computational cost and provides a ~13x speedup over AlexNet on ARM-based mobile devices. Compared to MobileNet, ShuffleNet achieves superior performance by a significant margin due to it's efficient structure. &lt;br&gt; Top-1 error from paper - ~7.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/zfnet-512"&gt;ZFNet-512&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1311.2901" rel="nofollow"&gt;Zeiler et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model (up to 8 layers) that increased the number of features that the network is capable of detecting that helps to pick image features at a finer level of resolution. &lt;br&gt; Top-5 error from paper - ~14.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h4&gt;&lt;a id="user-content-domain-based-image-classification-" class="anchor" aria-hidden="true" href="#domain-based-image-classification-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-image_classification"&gt;Domain-based Image Classification &lt;/a&gt;&lt;a name="user-content-domain_based_image"&gt;&lt;/a&gt;&lt;/h4&gt;&lt;a name="user-content-domain_based_image"&gt;
&lt;p&gt;This subset of models classify images for specific domains and datasets.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/classification/mnist"&gt;MNIST-Handwritten Digit Recognition&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.ipynb"&gt;Convolutional Neural Network with MNIST&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN model for handwritten digit identification&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-object-detection--image-segmentation-" class="anchor" aria-hidden="true" href="#object-detection--image-segmentation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-domain_based_image"&gt;Object Detection &amp;amp; Image Segmentation &lt;/a&gt;&lt;a name="user-content-object_detection"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-object_detection"&gt;
&lt;p&gt;Object detection models detect the presence of multiple objects in an image and segment out areas of the image where the objects are detected. Semantic segmentation models partition an input image by labeling each pixel into a set of pre-defined categories.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/tiny_yolov2"&gt;Tiny YOLOv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1612.08242.pdf" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A real-time CNN for object detection that detects 20 different classes. A smaller version of the more complex full YOLOv2 network.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/ssd"&gt;SSD&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1512.02325" rel="nofollow"&gt;Liu et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Single Stage Detector: real-time CNN for object detection that detects 80 different classes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/faster-rcnn"&gt;Faster-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1506.01497" rel="nofollow"&gt;Ren et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Increases efficiency from R-CNN by connecting a RPN with a CNN to create a single, unified network for object detection that detects 80 different classes.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/mask-rcnn"&gt;Mask-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1703.06870" rel="nofollow"&gt;He et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A real-time neural network for object instance segmentation that detects 80 different classes. Extends Faster R-CNN as each of the 300 elected ROIs go through 3 parallel branches of the network: label prediction, bounding box prediction and mask prediction.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLO v2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1612.08242" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN model for real-time object detection system that can detect over 9000 object categories. It uses a single network evaluation, enabling it to be more than 1000x faster than R-CNN and 100x faster than Faster R-CNN. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/yolov3"&gt;YOLO v3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1804.02767.pdf" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A deep CNN model for real-time object detection that detects 80 different classes. A little bigger than YOLOv2 but still very fast. As accurate as SSD but 3 times faster.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/tiny_yolov3"&gt;Tiny YOLOv3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1804.02767.pdf" rel="nofollow"&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A smaller version of YOLOv3 model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/object_detection_segmentation/duc"&gt;DUC&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.08502" rel="nofollow"&gt;Wang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN based pixel-wise semantic segmentation model with &amp;gt;80% &lt;a href="/models/semantic_segmentation/DUC/README.md/#metric"&gt;mIOU&lt;/a&gt; (mean Intersection Over Union). Trained on cityscapes dataset, which can be effectively implemented in self driving vehicle systems.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FCN&lt;/td&gt;
&lt;td&gt;&lt;a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" rel="nofollow"&gt;Long et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN based segmentation model trained end-to-end, pixel-to-pixel that produces efficient inference and learning. Built off of AlexNet, VGG net, GoogLeNet classification methods. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-body-face--gesture-analysis-" class="anchor" aria-hidden="true" href="#body-face--gesture-analysis-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-object_detection"&gt;Body, Face &amp;amp; Gesture Analysis &lt;/a&gt;&lt;a name="user-content-body_analysis"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-body_analysis"&gt;
&lt;p&gt;Face detection models identify and/or recognize human faces and emotions in given images. Body and Gesture Analysis models identify gender and age in given image.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/body_analysis/arcface"&gt;ArcFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1801.07698" rel="nofollow"&gt;Deng et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN based model for face recognition which learns discriminative features of faces and produces embeddings for input face images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN Cascade&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Li_A_Convolutional_Neural_2015_CVPR_paper.pdf" rel="nofollow"&gt;Li et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The model operates at multiple resolutions, quickly rejecting the background regions in the fast low resolution stages in an image and carefully evaluates a small number of challenging candidates in the last high resolution stage. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/body_analysis/emotion_ferplus"&gt;Emotion FerPlus&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1608.01041" rel="nofollow"&gt;Barsoum et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep CNN for emotion recognition trained on images of faces.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age and Gender Classification using Convolutional Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf" rel="nofollow"&gt;Levi et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model accurately classifies gender and age even the amount of learning data is limited.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-image-manipulation-" class="anchor" aria-hidden="true" href="#image-manipulation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-body_analysis"&gt;Image Manipulation &lt;/a&gt;&lt;a name="user-content-image_manipulation"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-image_manipulation"&gt;
&lt;p&gt;Image manipulation models use neural networks to transform input images to modified output images. Some popular models in this category involve style transfer or enhancing images by increasing resolution.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Unpaired Image to Image Translation using Cycle consistent Adversarial Network&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1703.10593" rel="nofollow"&gt;Zhu et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The model uses learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/super_resolution/sub_pixel_cnn_2016"&gt;Super Resolution with sub-pixel CNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1609.05158" rel="nofollow"&gt;Shi et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A deep CNN that uses sub-pixel convolution layers to upscale the input image.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="vision/style_transfer/fast_neural_style"&gt;Fast Neural Style Transfer&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1603.08155" rel="nofollow"&gt;Johnson et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This method uses a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-speech--audio-processing-" class="anchor" aria-hidden="true" href="#speech--audio-processing-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-image_manipulation"&gt;Speech &amp;amp; Audio Processing &lt;/a&gt;&lt;a name="user-content-speech"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-speech"&gt;
&lt;p&gt;This class of models uses audio data to train models that can identify voice, generate music, or even read text out loud.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Speech recognition with deep recurrent neural networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf" rel="nofollow"&gt;Graves et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A RNN model for sequential data for speech recognition. Labels problems where the input-output alignment is unknown&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deep voice: Real time neural text to speech&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1702.07825" rel="nofollow"&gt;Arik et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DNN model that performs end-to-end neural speech synthesis. Requires fewer parameters and it is faster than other systems. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sound Generative models&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1609.03499" rel="nofollow"&gt;WaveNet: A Generative Model for Raw Audio &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A CNN model that generates raw audio waveforms. Has predictive distribution for each audio sample. Generates realistic music fragments. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-machine-comprehension-" class="anchor" aria-hidden="true" href="#machine-comprehension-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-speech"&gt;Machine Comprehension &lt;/a&gt;&lt;a name="user-content-machine_comprehension"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-machine_comprehension"&gt;
&lt;p&gt;This subset of natural language processing models that answer questions about a given context paragraph.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="text/machine_comprehension/bidirectional_attention_flow"&gt;Bidirectional Attention Flow&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1611.01603" rel="nofollow"&gt;Seo et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A model that answers a query about a given context paragraph.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="text/machine_comprehension/bert-squad"&gt;BERT-Squad&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1810.04805.pdf" rel="nofollow"&gt;Devlin et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model answers questions based on the context of the given input paragraph.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;b&gt;&lt;a href="text/machine_comprehension/gpt-2"&gt;GPT-2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow"&gt;Radford et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A large transformer-based language model that given a sequence of words within some text, predicts the next word.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-machine-translation-" class="anchor" aria-hidden="true" href="#machine-translation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-machine_comprehension"&gt;Machine Translation &lt;/a&gt;&lt;a name="user-content-machine_translation"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-machine_translation"&gt;
&lt;p&gt;This class of natural language processing models learns how to translate input text to another language.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Neural Machine Translation by jointly learning to align and translate&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1409.0473" rel="nofollow"&gt;Bahdanau et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Aims to build a single neural network that can be jointly tuned to maximize the translation performance. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Google's Neural Machine Translation System&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1609.08144" rel="nofollow"&gt;Wu et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;This model helps to improve issues faced by the Neural Machine Translation (NMT) systems like parallelism that helps accelerate the final translation speed.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-language-modelling-" class="anchor" aria-hidden="true" href="#language-modelling-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-machine_translation"&gt;Language Modelling &lt;/a&gt;&lt;a name="user-content-language"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-language"&gt;
&lt;p&gt;This subset of natural language processing models learns representations of language from large corpuses of text.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Deep Neural Network Language Models&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pdfs.semanticscholar.org/a177/45f1d7045636577bcd5d513620df5860e9e5.pdf" rel="nofollow"&gt;Arisoy et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DNN acoustic model. Used in many natural language technologies. Represents a probability distribution over all possible word strings in a language. &lt;br&gt; &lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-visual-question-answering--dialog-" class="anchor" aria-hidden="true" href="#visual-question-answering--dialog-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-language"&gt;Visual Question Answering &amp;amp; Dialog &lt;/a&gt;&lt;a name="user-content-visual_qna"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-visual_qna"&gt;
&lt;p&gt;This subset of natural language processing models uses input images to answer questions about those images.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;VQA: Visual Question Answering&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1505.00468v6.pdf" rel="nofollow"&gt;Agrawal et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A model that takes an image and a free-form, open-ended natural language question about the image and outputs a natural-language answer. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Yin and Yang: Balancing and Answering Binary Visual Questions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1511.05099.pdf" rel="nofollow"&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Addresses VQA by converting the question to a tuple that concisely summarizes the visual concept to be detected in the image. Next, if the concept can be found in the image, it provides a â€œyesâ€ or â€œnoâ€ answer. Its performance matches the traditional VQA approach on unbalanced dataset, and outperforms it on the balanced dataset. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Making the V in VQA Matter&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1612.00837.pdf" rel="nofollow"&gt;Goyal et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Balances the VQA dataset by collecting complementary images such that every question is associated with a pair of similar images that result in two different answers to the question, providing a unique interpretable model that provides a counter-example based explanation.  &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Visual Dialog&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1611.08669" rel="nofollow"&gt;Das et al.&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An AI agent that holds a meaningful dialog with humans in natural, conversational language about visual content. Curates a large-scale Visual Dialog dataset (VisDial). &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;/a&gt;&lt;h3&gt;&lt;a id="user-content-other-interesting-models-" class="anchor" aria-hidden="true" href="#other-interesting-models-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-visual_qna"&gt;Other interesting models &lt;/a&gt;&lt;a name="user-content-others"&gt;&lt;/a&gt;&lt;/h3&gt;&lt;a name="user-content-others"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-others"&gt;There are many interesting deep learning models that do not fit into the categories described above. The ONNX team would like to highly encourage users and researchers to &lt;/a&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt; their models to the growing model zoo.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Class&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Text to Image&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1605.05396" rel="nofollow"&gt;Generative Adversarial Text to image Synthesis &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Effectively bridges the advances in text and image modeling, translating visual concepts from characters to pixels. Generates plausible images of birds and flowers from detailed text descriptions. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Time Series Forecasting&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.07015.pdf" rel="nofollow"&gt;Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The model extracts short-term local dependency patterns among variables and to discover long-term patterns for time series trends. It helps to predict solar plant energy output, electricity consumption, and traffic jam situations. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recommender systems&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf" rel="nofollow"&gt;DropoutNet: Addressing Cold Start in Recommender Systems&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A collaborative filtering method that makes predictions about an individualâ€™s preference based on preference information from other users.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Collaborative filtering&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1708.05031.pdf" rel="nofollow"&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A DNN model based on the interaction between user and item features using matrix factorization. &lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Autoencoders&lt;/td&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/abs/1506.01057" rel="nofollow"&gt;A Hierarchical Neural Autoencoder for Paragraphs and Documents&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;An LSTM (long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs.&lt;br&gt;&lt;a href="contribute.md"&gt;contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-usage-" class="anchor" aria-hidden="true" href="#usage-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage &lt;a name="user-content-usage-"&gt;&lt;/a&gt;&lt;/h2&gt;&lt;a name="user-content-usage-"&gt;
&lt;p&gt;Every ONNX backend should support running the models out of the box. After downloading and extracting the tarball of each model, you will find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A protobuf file &lt;code&gt;model.onnx&lt;/code&gt; that represents the serialized ONNX model.&lt;/li&gt;
&lt;li&gt;Test data (in the form of serialized protobuf TensorProto files or serialized NumPy archives).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The test data files can be used to validate ONNX models from the Model Zoo. We have provided the following interface examples for you to get started. Please replace &lt;code&gt;onnx_backend&lt;/code&gt; in your code with the appropriate framework of your choice that provides ONNX inferencing support, and likewise replace &lt;code&gt;backend.run_model&lt;/code&gt; with the framework's model evaluation logic.&lt;/p&gt;
&lt;p&gt;There are two different formats for the test data files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Serialized protobuf TensorProtos (.pb), stored in folders with the naming convention &lt;code&gt;test_data_set_*&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx
&lt;span class="pl-k"&gt;import&lt;/span&gt; os
&lt;span class="pl-k"&gt;import&lt;/span&gt; glob
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx_backend &lt;span class="pl-k"&gt;as&lt;/span&gt; backend

&lt;span class="pl-k"&gt;from&lt;/span&gt; onnx &lt;span class="pl-k"&gt;import&lt;/span&gt; numpy_helper

model &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;model.onnx&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
test_data_dir &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;test_data_set_0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load inputs&lt;/span&gt;
inputs &lt;span class="pl-k"&gt;=&lt;/span&gt; []
inputs_num &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(glob.glob(os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_*.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)))
&lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(inputs_num):
    input_file &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_&lt;span class="pl-c1"&gt;{}&lt;/span&gt;.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.format(i))
    tensor &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.TensorProto()
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(input_file, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        tensor.ParseFromString(f.read())
    inputs.append(numpy_helper.to_array(tensor))

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load reference outputs&lt;/span&gt;
ref_outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; []
ref_outputs_num &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(glob.glob(os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;output_*.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)))
&lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(ref_outputs_num):
    output_file &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(test_data_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;output_&lt;span class="pl-c1"&gt;{}&lt;/span&gt;.pb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.format(i))
    tensor &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.TensorProto()
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(output_file, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        tensor.ParseFromString(f.read())
    ref_outputs.append(numpy_helper.to_array(tensor))

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run the model on the backend&lt;/span&gt;
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(backend.run_model(model, inputs))

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Compare the results with reference outputs.&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; ref_o, o &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;zip&lt;/span&gt;(ref_outputs, outputs):
    np.testing.assert_almost_equal(ref_o, o)&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Serialized Numpy archives, stored in files with the naming convention &lt;code&gt;test_data_*.npz&lt;/code&gt;. Each file contains one set of test inputs and outputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx
&lt;span class="pl-k"&gt;import&lt;/span&gt; onnx_backend &lt;span class="pl-k"&gt;as&lt;/span&gt; backend

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load the model and sample inputs and outputs&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; onnx.load(model_pb_path)
sample &lt;span class="pl-k"&gt;=&lt;/span&gt; np.load(npz_path, &lt;span class="pl-v"&gt;encoding&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bytes&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
inputs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(sample[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;inputs&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(sample[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;outputs&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run the model with an onnx backend and verify the results&lt;/span&gt;
np.testing.assert_almost_equal(outputs, backend.run_model(model, inputs))&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-model-visualization" class="anchor" aria-hidden="true" href="#model-visualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Visualization&lt;/h2&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-usage-"&gt;You can see visualizations of each model's network architecture by using &lt;/a&gt;&lt;a href="https://lutzroeder.github.io/Netron" rel="nofollow"&gt;Netron&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributions" class="anchor" aria-hidden="true" href="#contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions&lt;/h2&gt;
&lt;p&gt;Do you want to contribute a model? To get started, pick any model presented above with the &lt;a href="contribute.md"&gt;contribute&lt;/a&gt; link under the Description column. The links point to a page containing guidelines for making a contribution.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>onnx</author><guid isPermaLink="false">https://github.com/onnx/models</guid><pubDate>Mon, 18 Nov 2019 00:22:00 GMT</pubDate></item><item><title>justmarkham/scikit-learn-videos #23 in Jupyter Notebook, This week</title><link>https://github.com/justmarkham/scikit-learn-videos</link><description>&lt;p&gt;&lt;i&gt;Jupyter notebooks from the scikit-learn video series&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introduction-to-machine-learning-with-scikit-learn" class="anchor" aria-hidden="true" href="#introduction-to-machine-learning-with-scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to machine learning with scikit-learn&lt;/h1&gt;
&lt;p&gt;This video series will teach you how to solve machine learning problems using Python's popular scikit-learn library. There are &lt;strong&gt;10 video tutorials&lt;/strong&gt; totaling 4.5 hours, each with a corresponding &lt;strong&gt;Jupyter notebook&lt;/strong&gt;. The notebook contains everything you see in the video: code, output, images, and comments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The notebooks in this repository have been updated to use Python 3.6 and scikit-learn 0.19.1. The original notebooks (shown in the video) used Python 2.7 and scikit-learn 0.16, and can be downloaded from the &lt;a href="https://github.com/justmarkham/scikit-learn-videos/tree/archive"&gt;archive branch&lt;/a&gt;. You can read about how I updated the code in this &lt;a href="https://www.dataschool.io/how-to-update-your-scikit-learn-code-for-2018/" rel="nofollow"&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can &lt;a href="https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A" rel="nofollow"&gt;watch the entire series&lt;/a&gt; on YouTube, and &lt;a href="http://nbviewer.jupyter.org/github/justmarkham/scikit-learn-videos/tree/master/" rel="nofollow"&gt;view all of the notebooks&lt;/a&gt; using nbviewer.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=elojMnjn4kk&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=1" title="Watch the first tutorial video" rel="nofollow"&gt;&lt;img src="images/youtube.png" alt="Watch the first tutorial video" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once you complete this video series, I recommend enrolling in my online course, &lt;a href="http://www.dataschool.io/learn/" rel="nofollow"&gt;Machine Learning with Text in Python&lt;/a&gt;, to gain a deeper understanding of scikit-learn and Natural Language Processing.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What is machine learning, and how does it work? (&lt;a href="https://www.youtube.com/watch?v=elojMnjn4kk&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=1" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="01_machine_learning_intro.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is machine learning?&lt;/li&gt;
&lt;li&gt;What are the two main categories of machine learning?&lt;/li&gt;
&lt;li&gt;What are some examples of machine learning?&lt;/li&gt;
&lt;li&gt;How does machine learning "work"?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setting up Python for machine learning: scikit-learn and Jupyter Notebook (&lt;a href="https://www.youtube.com/watch?v=IsXXlYVBt1M&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=2" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="02_machine_learning_setup.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the benefits and drawbacks of scikit-learn?&lt;/li&gt;
&lt;li&gt;How do I install scikit-learn?&lt;/li&gt;
&lt;li&gt;How do I use the Jupyter Notebook?&lt;/li&gt;
&lt;li&gt;What are some good resources for learning Python?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Getting started in scikit-learn with the famous iris dataset (&lt;a href="https://www.youtube.com/watch?v=hd1W4CyPX58&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=3" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="03_getting_started_with_iris.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the famous iris dataset, and how does it relate to machine learning?&lt;/li&gt;
&lt;li&gt;How do we load the iris dataset into scikit-learn?&lt;/li&gt;
&lt;li&gt;How do we describe a dataset using machine learning terminology?&lt;/li&gt;
&lt;li&gt;What are scikit-learn's four key requirements for working with data?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training a machine learning model with scikit-learn (&lt;a href="https://www.youtube.com/watch?v=RlQuVL6-qe8&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=4" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="04_model_training.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the K-nearest neighbors classification model?&lt;/li&gt;
&lt;li&gt;What are the four steps for model training and prediction in scikit-learn?&lt;/li&gt;
&lt;li&gt;How can I apply this pattern to other machine learning models?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Comparing machine learning models in scikit-learn (&lt;a href="https://www.youtube.com/watch?v=0pP4EwWJgIU&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=5" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="05_model_evaluation.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do I choose which model to use for my supervised learning task?&lt;/li&gt;
&lt;li&gt;How do I choose the best tuning parameters for that model?&lt;/li&gt;
&lt;li&gt;How do I estimate the likely performance of my model on out-of-sample data?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data science pipeline: pandas, seaborn, scikit-learn (&lt;a href="https://www.youtube.com/watch?v=3ZWuPVWq7p4&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=6" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="06_linear_regression.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do I use the pandas library to read data into Python?&lt;/li&gt;
&lt;li&gt;How do I use the seaborn library to visualize data?&lt;/li&gt;
&lt;li&gt;What is linear regression, and how does it work?&lt;/li&gt;
&lt;li&gt;How do I train and interpret a linear regression model in scikit-learn?&lt;/li&gt;
&lt;li&gt;What are some evaluation metrics for regression problems?&lt;/li&gt;
&lt;li&gt;How do I choose which features to include in my model?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross-validation for parameter tuning, model selection, and feature selection (&lt;a href="https://www.youtube.com/watch?v=6dbrR-WymjI&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=7" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="07_cross_validation.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the drawback of using the train/test split procedure for model evaluation?&lt;/li&gt;
&lt;li&gt;How does K-fold cross-validation overcome this limitation?&lt;/li&gt;
&lt;li&gt;How can cross-validation be used for selecting tuning parameters, choosing between models, and selecting features?&lt;/li&gt;
&lt;li&gt;What are some possible improvements to cross-validation?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficiently searching for optimal tuning parameters (&lt;a href="https://www.youtube.com/watch?v=Gol_qOgRqfA&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=8" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="08_grid_search.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can K-fold cross-validation be used to search for an optimal tuning parameter?&lt;/li&gt;
&lt;li&gt;How can this process be made more efficient?&lt;/li&gt;
&lt;li&gt;How do you search for multiple tuning parameters at once?&lt;/li&gt;
&lt;li&gt;What do you do with those tuning parameters before making real predictions?&lt;/li&gt;
&lt;li&gt;How can the computational expense of this process be reduced?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluating a classification model (&lt;a href="https://www.youtube.com/watch?v=85dtiMz9tSo&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=9" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="09_classification_metrics.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the purpose of model evaluation, and what are some common evaluation procedures?&lt;/li&gt;
&lt;li&gt;What is the usage of classification accuracy, and what are its limitations?&lt;/li&gt;
&lt;li&gt;How does a confusion matrix describe the performance of a classifier?&lt;/li&gt;
&lt;li&gt;What metrics can be computed from a confusion matrix?&lt;/li&gt;
&lt;li&gt;How can you adjust classifier performance by changing the classification threshold?&lt;/li&gt;
&lt;li&gt;What is the purpose of an ROC curve?&lt;/li&gt;
&lt;li&gt;How does Area Under the Curve (AUC) differ from classification accuracy?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Encoding categorical features (&lt;a href="https://www.youtube.com/watch?v=irHhDMbw3xo&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=10" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="10_categorical_features.ipynb"&gt;notebook&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why should you use a Pipeline?&lt;/li&gt;
&lt;li&gt;How do you encode categorical features with OneHotEncoder?&lt;/li&gt;
&lt;li&gt;How do you apply OneHotEncoder to selected columns with ColumnTransformer?&lt;/li&gt;
&lt;li&gt;How do you build and cross-validate a Pipeline?&lt;/li&gt;
&lt;li&gt;How do you make predictions on new data using a Pipeline?&lt;/li&gt;
&lt;li&gt;Why should you use scikit-learn (rather than pandas) for preprocessing?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-bonus-video" class="anchor" aria-hidden="true" href="#bonus-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bonus Video&lt;/h2&gt;
&lt;p&gt;At the PyCon 2016 conference, I taught a &lt;strong&gt;3-hour tutorial&lt;/strong&gt; that builds upon this video series and focuses on &lt;strong&gt;text-based data&lt;/strong&gt;. You can watch the &lt;a href="https://www.youtube.com/watch?v=ZiKMIuYidY0&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=10" rel="nofollow"&gt;tutorial video&lt;/a&gt; on YouTube.&lt;/p&gt;
&lt;p&gt;Here are the topics I covered:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model building in scikit-learn (refresher)&lt;/li&gt;
&lt;li&gt;Representing text as numerical data&lt;/li&gt;
&lt;li&gt;Reading a text-based dataset into pandas&lt;/li&gt;
&lt;li&gt;Vectorizing our dataset&lt;/li&gt;
&lt;li&gt;Building and evaluating a model&lt;/li&gt;
&lt;li&gt;Comparing models&lt;/li&gt;
&lt;li&gt;Examining a model for further insight&lt;/li&gt;
&lt;li&gt;Practicing this workflow on another dataset&lt;/li&gt;
&lt;li&gt;Tuning the vectorizer (discussion)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Visit this &lt;a href="https://github.com/justmarkham/pycon-2016-tutorial"&gt;GitHub repository&lt;/a&gt; to access the tutorial notebooks and many other recommended resources.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>justmarkham</author><guid isPermaLink="false">https://github.com/justmarkham/scikit-learn-videos</guid><pubDate>Mon, 18 Nov 2019 00:23:00 GMT</pubDate></item><item><title>blue-yonder/tsfresh #24 in Jupyter Notebook, This week</title><link>https://github.com/blue-yonder/tsfresh</link><description>&lt;p&gt;&lt;i&gt;Automatic extraction of relevant features from time series:&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://tsfresh.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/36acd223668ed9b8757d4b53b31463c5052e705f/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f747366726573682f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/tsfresh/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/blue-yonder/tsfresh" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9d7ac3b7a4be38dda0e1acb1e2df0a452e132ab8/68747470733a2f2f7472617669732d63692e6f72672f626c75652d796f6e6465722f747366726573682e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/blue-yonder/tsfresh.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/blue-yonder/tsfresh?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d600de5532fa69596b8e6c08d848381f3d1fae78/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f626c75652d796f6e6465722f747366726573682f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/blue-yonder/tsfresh/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/blue-yonder/tsfresh/blob/master/LICENSE.txt"&gt;&lt;img src="https://camo.githubusercontent.com/b0224997019dec4e51d692c722ea9bee2818c837/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e737667" alt="license" data-canonical-src="https://img.shields.io/github/license/mashape/apistatus.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/tsfresh/Lobby?utm_source=share-link&amp;amp;utm_medium=link&amp;amp;utm_campaign=share-link" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e3f3756ed14ab89e92954154ae3210ee867e5bf3/68747470733a2f2f6261646765732e6769747465722e696d2f747366726573682f4c6f6262792e737667" alt="Gitter chat" data-canonical-src="https://badges.gitter.im/tsfresh/Lobby.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/eb8187b66388b85396094b47955c6d801fe7feb4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e322e372d737570706f727465642d677265656e2e737667"&gt;&lt;img src="https://camo.githubusercontent.com/eb8187b66388b85396094b47955c6d801fe7feb4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e322e372d737570706f727465642d677265656e2e737667" alt="py27 status" data-canonical-src="https://img.shields.io/badge/python2.7-supported-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/blue-yonder/tsfresh/issues/8"&gt;&lt;img src="https://camo.githubusercontent.com/848b334aabca64e10f1535e7b6a8dd952a025db0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e332e352e322d737570706f727465642d677265656e2e737667" alt="py352 status" data-canonical-src="https://img.shields.io/badge/python3.5.2-supported-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://mybinder.org/v2/gh/blue-yonder/tsfresh/master?filepath=notebooks" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/tsfresh" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5e21bea1727201bd2b3270bd63d7d29796444bb9/68747470733a2f2f706570792e746563682f62616467652f74736672657368" alt="Downloads" data-canonical-src="https://pepy.tech/badge/tsfresh" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-tsfresh" class="anchor" aria-hidden="true" href="#tsfresh"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;tsfresh&lt;/h1&gt;
&lt;p&gt;This repository contains the &lt;em&gt;TSFRESH&lt;/em&gt; python package. The abbreviation stands for&lt;/p&gt;
&lt;p&gt;&lt;em&gt;"Time Series Feature extraction based on scalable hypothesis tests"&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The package contains many feature extraction methods and a robust feature selection algorithm.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-spend-less-time-on-feature-engineering" class="anchor" aria-hidden="true" href="#spend-less-time-on-feature-engineering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spend less time on feature engineering&lt;/h2&gt;
&lt;p&gt;Data Scientists often spend most of their time either cleaning data or building features.
While we cannot change the first thing, the second can be automated.
&lt;em&gt;TSFRESH&lt;/em&gt; frees your time spent on building features by extracting them automatically.
Hence, you have more time to study the newest deep learning paper, read hacker news or build better models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-automatic-extraction-of-100s-of-features" class="anchor" aria-hidden="true" href="#automatic-extraction-of-100s-of-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automatic extraction of 100s of features&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;TSFRESH&lt;/em&gt; automatically extracts 100s of features from time series.
Those features describe basic characteristics of the time series such as the number of peaks, the average or maximal value or more complex features such as the time reversal symmetry statistic.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/images/introduction_ts_exa_features.png"&gt;&lt;img src="docs/images/introduction_ts_exa_features.png" alt="The features extracted from a exemplary time series" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The set of features can then be used to construct statistical or machine learning models on the time series to be used for example in regression or
classification tasks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-forget-irrelevant-features" class="anchor" aria-hidden="true" href="#forget-irrelevant-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Forget irrelevant features&lt;/h2&gt;
&lt;p&gt;Time series often contain noise, redundancies or irrelevant information.
As a result most of the extracted features will not be useful for the machine learning task at hand.&lt;/p&gt;
&lt;p&gt;To avoid extracting irrelevant features, the &lt;em&gt;TSFRESH&lt;/em&gt; package has a built-in filtering procedure.
This filtering procedure evaluates the explaining power and importance of each characteristic for the regression or classification tasks at hand.&lt;/p&gt;
&lt;p&gt;It is based on the well developed theory of hypothesis testing and uses a multiple test procedure.
As a result the filtering process mathematically controls the percentage of irrelevant extracted features.&lt;/p&gt;
&lt;p&gt;The  &lt;em&gt;TSFRESH&lt;/em&gt; package is described in the following open access paper&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Christ, M., Braun, N., Neuffer, J. and Kempa-Liehr A.W. (2018).
&lt;em&gt;Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh -- A Python package).&lt;/em&gt;
Neurocomputing 307 (2018) 72-77, &lt;a href="https://doi.org/10.1016/j.neucom.2018.03.067" rel="nofollow"&gt;doi:10.1016/j.neucom.2018.03.067&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The FRESH algorithm is described in the following whitepaper&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Christ, M., Kempa-Liehr, A.W. and Feindt, M. (2017).&lt;br&gt;
&lt;em&gt;Distributed and parallel time series feature extraction for industrial big data applications.&lt;/em&gt;&lt;br&gt;
ArXiv e-print 1610.07717,  &lt;a href="https://arxiv.org/abs/1610.07717" rel="nofollow"&gt;https://arxiv.org/abs/1610.07717&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-advantages-of-tsfresh" class="anchor" aria-hidden="true" href="#advantages-of-tsfresh"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advantages of tsfresh&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;TSFRESH&lt;/em&gt; has several selling points, for example&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;it is field tested&lt;/li&gt;
&lt;li&gt;it is unit tested&lt;/li&gt;
&lt;li&gt;the filtering process is statistically/mathematically correct&lt;/li&gt;
&lt;li&gt;it has a comprehensive documentation&lt;/li&gt;
&lt;li&gt;it is compatible with sklearn, pandas and numpy&lt;/li&gt;
&lt;li&gt;it allows anyone to easily add their favorite features&lt;/li&gt;
&lt;li&gt;it both runs on your local machine or even on a cluster&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-next-steps" class="anchor" aria-hidden="true" href="#next-steps"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Next steps&lt;/h2&gt;
&lt;p&gt;If you are interested in the technical workings, go to see our comprehensive Read-The-Docs documentation at &lt;a href="http://tsfresh.readthedocs.io" rel="nofollow"&gt;http://tsfresh.readthedocs.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The algorithm, especially the filtering part are also described in the paper mentioned above.&lt;/p&gt;
&lt;p&gt;If you have some questions or feedback you can find the developers in the &lt;a href="https://gitter.im/tsfresh/Lobby?utm_source=share-link&amp;amp;utm_medium=link&amp;amp;utm_campaign=share-link" rel="nofollow"&gt;gitter chatroom.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We appreciate any contributions, if you are interested in helping us to make &lt;em&gt;TSFRESH&lt;/em&gt; the biggest archive of feature extraction methods in python, just head over to our &lt;a href="http://tsfresh.readthedocs.io/en/latest/text/how_to_contribute.html" rel="nofollow"&gt;How-To-Contribute&lt;/a&gt; instructions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;The research and development of &lt;em&gt;TSFRESH&lt;/em&gt; was funded in part by the German Federal Ministry of Education and Research under grant number 01IS14004 (project iPRODICT).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>blue-yonder</author><guid isPermaLink="false">https://github.com/blue-yonder/tsfresh</guid><pubDate>Mon, 18 Nov 2019 00:24:00 GMT</pubDate></item><item><title>MIT-LCP/mimic-code #25 in Jupyter Notebook, This week</title><link>https://github.com/MIT-LCP/mimic-code</link><description>&lt;p&gt;&lt;i&gt;MIMIC Code Repository: Code shared by the research community for the MIMIC-III database&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mimic-code-repository---" class="anchor" aria-hidden="true" href="#mimic-code-repository---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MIMIC Code Repository &lt;a href="https://travis-ci.org/MIT-LCP/mimic-code" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3a8549e8ed9d29e6450a16f792328b2a6ca7404c/68747470733a2f2f7472617669732d63692e6f72672f4d49542d4c43502f6d696d69632d636f64652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/MIT-LCP/mimic-code.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://doi.org/10.5281/zenodo.821872" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4fd2b90d54ec2e72b9df903adf933401aacbfb00/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e3832313837322e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.821872.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://gitter.im/MIT-LCP/mimic-code?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9ea1ba8c6f0013ba8d39281b626b9e64c8ef4694/68747470733a2f2f6261646765732e6769747465722e696d2f4d49542d4c43502f6d696d69632d636f64652e737667" alt="Join the chat at https://gitter.im/MIT-LCP/mimic-code" data-canonical-src="https://badges.gitter.im/MIT-LCP/mimic-code.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;This is a repository of code shared by the research community. The repository is intended to be a central hub for sharing, refining, and reusing code used for analysis of the &lt;a href="https://mimic.physionet.org" rel="nofollow"&gt;MIMIC critical care database&lt;/a&gt;. To find out more about MIMIC, please see: &lt;a href="https://mimic.physionet.org" rel="nofollow"&gt;https://mimic.physionet.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can read more about the code repository in the following open access paper: &lt;a href="https://doi.org/10.1093/jamia/ocx084" rel="nofollow"&gt;The MIMIC Code Repository: enabling reproducibility in critical care research&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-brief-introduction" class="anchor" aria-hidden="true" href="#brief-introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Brief introduction&lt;/h2&gt;
&lt;p&gt;The repository is organized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/benchmark"&gt;benchmark&lt;/a&gt; - Various speed tests for indices&lt;/li&gt;
&lt;li&gt;&lt;a href="/buildmimic"&gt;buildmimic&lt;/a&gt; - Scripts to build MIMIC-III in a relational database management system (RDMS), in particular &lt;a href="/buildmimic/postgres"&gt;postgres&lt;/a&gt; is our RDMS of choice&lt;/li&gt;
&lt;li&gt;&lt;a href="/concepts"&gt;concepts&lt;/a&gt; - Useful views/summaries of the data in MIMIC-III, e.g. demographics, organ failure scores, severity of illness scores, durations of treatment, easier to analyze views, etc. The paper above describes these in detail.&lt;/li&gt;
&lt;li&gt;&lt;a href="/notebooks"&gt;notebooks&lt;/a&gt; - A collection of R markdown and Jupyter notebooks which give examples of how to extract and analyze data&lt;/li&gt;
&lt;li&gt;&lt;a href="/notebooks/aline"&gt;notebooks/aline&lt;/a&gt; - An entire study reproduced in the MIMIC-III database - from cohort generation to hypothesis testing&lt;/li&gt;
&lt;li&gt;&lt;a href="/notebooks/aline-aws"&gt;notebooks/aline-aws&lt;/a&gt; - As above, &lt;a href="#launch-mimic-iii-in-aws"&gt;launchable immediately on AWS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tests"&gt;tests&lt;/a&gt; - You should always have tests!&lt;/li&gt;
&lt;li&gt;&lt;a href="/tutorials"&gt;tutorials&lt;/a&gt; - Similar to the notebooks folder, but focuses on explaining concepts to new users&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-launch-mimic-iii-in-aws" class="anchor" aria-hidden="true" href="#launch-mimic-iii-in-aws"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Launch MIMIC-III in AWS&lt;/h2&gt;
&lt;p&gt;Use the below Launch Stack button to deploy access to the MIMIC-III dataset into your AWS account.  This will give you real-time access to the MIMIC-III data in your AWS account without having to download a copy of the MIMIC-III dataset.  It will also deploy a Jupyter Notebook with access to the content of this GitHub repository in your AWS account.    Prior to launching this, please login to the &lt;a href="https://mimic.physionet.org/" rel="nofollow"&gt;MIMIC PhysioNet website&lt;/a&gt;, &lt;a href="https://physionet.org/settings/cloud/" rel="nofollow"&gt;input your AWS account number&lt;/a&gt;, and &lt;a href="https://physionet.org/projects/mimiciii/1.4/request_access/2" rel="nofollow"&gt;request access to the MIMIC-III Clinical Database on AWS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To start this deployment, click the Launch Stack button.  On the first screen, the template link has already been specified, so just click next.  On the second screen, provide a Stack name (letters and numbers) and click next, on the third screen, just click next.  On the forth screen, at the bottom, there is a box that says &lt;strong&gt;I acknowledge that AWS CloudFormation might create IAM resources.&lt;/strong&gt;.  Check that box, and then click &lt;strong&gt;Create&lt;/strong&gt;.  Once the Stack has complete deploying, look at the &lt;strong&gt;Outputs&lt;/strong&gt; tab of the AWS CloudFormation console for links to your Juypter Notebooks instance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=MIMIC&amp;amp;templateURL=https://aws-bigdata-blog.s3.amazonaws.com/artifacts/biomedical-informatics-studies/mimic-iii-athena.yaml" rel="nofollow"&gt;&lt;img src="buildmimic/aws-athena/cloudformation-launch-stack.png" alt="cloudformation-launch-stack" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;If you use code or concepts available in this repository, we would be grateful if you would cite the above paper as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Johnson, Alistair EW, David J. Stone, Leo A. Celi, and Tom J. Pollard. "The MIMIC Code Repository: enabling reproducibility in critical care research." Journal of the American Medical Informatics Association (2017): ocx084.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If including a hyperlink to the code, we recommend you use the DOI from Zenodo rather than a GitHub URL: &lt;a href="https://doi.org/10.5281/zenodo.821872" rel="nofollow"&gt;https://doi.org/10.5281/zenodo.821872&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-contribute" class="anchor" aria-hidden="true" href="#how-to-contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to contribute&lt;/h2&gt;
&lt;p&gt;Our team has worked hard to create and share the MIMIC dataset. We encourage you to share the code that you use for data processing and analysis. Sharing code helps to make studies reproducible and promotes collaborative research. To contribute, please:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fork the repository using the following link: &lt;a href="https://github.com/MIT-LCP/mimic-code/fork"&gt;https://github.com/MIT-LCP/mimic-code/fork&lt;/a&gt;. For a background on GitHub forks, see: &lt;a href="https://help.github.com/articles/fork-a-repo/"&gt;https://help.github.com/articles/fork-a-repo/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Commit your changes to the forked repository.&lt;/li&gt;
&lt;li&gt;Submit a pull request to the &lt;a href="https://github.com/MIT-LCP/mimic-code"&gt;MIMIC code repository&lt;/a&gt;, using the method described at: &lt;a href="https://help.github.com/articles/using-pull-requests/"&gt;https://help.github.com/articles/using-pull-requests/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We encourage users to share concepts they have extracted by writing code which generates a materialized view. These materialized views can then be used by researchers around the world to speed up data extraction. For example, ventilation durations can be acquired by creating the ventdurations view in &lt;a href="https://github.com/MIT-LCP/mimic-code/blob/master/concepts/durations/ventilation-durations.sql"&gt;concepts/durations/ventilation-durations.sql&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;By committing your code to the &lt;a href="https://github.com/mit-lcp/mimic-code"&gt;MIMIC Code Repository&lt;/a&gt; you agree to release the code under the &lt;a href="https://github.com/mit-lcp/mimic-code/blob/master/LICENSE"&gt;MIT License attached to the repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-coding-style" class="anchor" aria-hidden="true" href="#coding-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Coding style&lt;/h2&gt;
&lt;p&gt;Please refer to the &lt;a href="https://github.com/MIT-LCP/mimic-code/blob/master/styleguide.md"&gt;style guide&lt;/a&gt; for guidelines on formatting your code for the repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-building-mimic" class="anchor" aria-hidden="true" href="#building-mimic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building MIMIC&lt;/h2&gt;
&lt;p&gt;A Makefile build system has been created to facilitate the building of the MIMIC database, and optionally contributed views from the community. Please refer to the &lt;a href="https://github.com/MIT-LCP/mimic-code/blob/master/Makefile.md"&gt;Makefile guide&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>MIT-LCP</author><guid isPermaLink="false">https://github.com/MIT-LCP/mimic-code</guid><pubDate>Mon, 18 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>