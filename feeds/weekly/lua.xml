<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Lua, This week</title><link>https://github.com/trending/lua?since=weekly</link><description>The top repositories on GitHub for lua, measured weekly</description><pubDate>Mon, 28 Oct 2019 04:49:49 GMT</pubDate><lastBuildDate>Mon, 28 Oct 2019 04:49:49 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>1400</ttl><item><title>Kong/kong #1 in Lua, This week</title><link>https://github.com/Kong/kong</link><description>&lt;p&gt;&lt;i&gt;ü¶ç The Cloud-Native API Gateway &lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://konghq.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9e4fe7914c7357861223aa535d7ca9858253c96e/68747470733a2f2f6b6f6e6768712e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30352f6b6f6e672d6c6f676f2d6769746875622d726561646d652e706e67" alt="" data-canonical-src="https://konghq.com/wp-content/uploads/2018/05/kong-logo-github-readme.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/Kong/kong/branches" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/032b58c2a2e0a2a8dbb0c1fe60a0236e1042b7ad/68747470733a2f2f7472617669732d63692e6f72672f4b6f6e672f6b6f6e672e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/Kong/kong.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/Kong/kong/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/intent/follow?screen_name=thekonginc" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/295bb78a3be8393e728bb4ad7470bd98a1c5062d/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7468656b6f6e67696e632e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/thekonginc.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kong is a cloud-native, fast, scalable, and distributed Microservice
Abstraction Layer &lt;em&gt;(also known as an API Gateway, API Middleware or in some
cases Service Mesh)&lt;/em&gt;. Made available as an open-source project in 2015, its
core values are high performance and extensibility.&lt;/p&gt;
&lt;p&gt;Actively maintained, Kong is widely used in production at companies ranging
from startups to Global 5000 as well as government organizations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://konghq.com/install" rel="nofollow"&gt;Installation&lt;/a&gt; |
&lt;a href="https://docs.konghq.com" rel="nofollow"&gt;Documentation&lt;/a&gt; |
&lt;a href="https://discuss.konghq.com" rel="nofollow"&gt;Forum&lt;/a&gt; |
&lt;a href="https://konghq.com/blog" rel="nofollow"&gt;Blog&lt;/a&gt; |
IRC (freenode): &lt;a href="https://webchat.freenode.net/?channels=kong" rel="nofollow"&gt;#kong&lt;/a&gt; |
&lt;a href="https://bintray.com/kong/kong-nightly/master" rel="nofollow"&gt;Nightly Builds&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#why-kong"&gt;&lt;strong&gt;Why Kong?&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#features"&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#distributions"&gt;&lt;strong&gt;Distributions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#development"&gt;&lt;strong&gt;Development&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enterprise-support--demo"&gt;&lt;strong&gt;Enterprise Support &amp;amp; Demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-kong" class="anchor" aria-hidden="true" href="#why-kong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Kong?&lt;/h2&gt;
&lt;p&gt;If you are building for the web, mobile, or IoT (Internet of Things) you will
likely end up needing common functionality to run your actual software. Kong
can help by acting as a gateway (or a sidecar) for microservices requests while
providing load balancing, logging, authentication, rate-limiting,
transformations, and more through plugins.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://konghq.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d4d0dcb22c223db0bf2e301aab0dddb3015f1729/68747470733a2f2f6b6f6e6768712e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031382f30352f6b6f6e672d62656e65666974732d6769746875622d726561646d652e706e67" alt="" data-canonical-src="https://konghq.com/wp-content/uploads/2018/05/kong-benefits-github-readme.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cloud-Native&lt;/strong&gt;: Platform agnostic, Kong can run from bare metal to
Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Load Balancing&lt;/strong&gt;: Load balance traffic across multiple upstream
services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hash-based Load Balancing&lt;/strong&gt;: Load balance with consistent hashing/sticky
sessions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Circuit-Breaker&lt;/strong&gt;: Intelligent tracking of unhealthy upstream services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Health Checks:&lt;/strong&gt; Active and passive monitoring of your upstream services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Discovery&lt;/strong&gt;: Resolve SRV records in third-party DNS resolvers like
Consul.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Serverless&lt;/strong&gt;: Invoke and secure AWS Lambda or OpenWhisk functions directly
from Kong.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WebSockets&lt;/strong&gt;: Communicate to your upstream services via WebSockets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OAuth2.0&lt;/strong&gt;: Easily add OAuth2.0 authentication to your APIs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logging&lt;/strong&gt;: Log requests and responses to your system over HTTP, TCP, UDP,
or to disk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: ACL, Bot detection, whitelist/blacklist IPs, etc...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Syslog&lt;/strong&gt;: Logging to System log.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSL&lt;/strong&gt;: Setup a Specific SSL Certificate for an underlying service or API.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Live monitoring provides key load and performance server
metrics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Proxy&lt;/strong&gt;: Make Kong connect to intermediary transparent HTTP proxies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Authentications&lt;/strong&gt;: HMAC, JWT, Basic, and more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rate-limiting&lt;/strong&gt;: Block and throttle requests based on many variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformations&lt;/strong&gt;: Add, remove, or manipulate HTTP requests and responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt;: Cache and serve responses at the proxy layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI&lt;/strong&gt;: Control your Kong cluster from the command line.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: Kong can be operated with its RESTful API for maximum
flexibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Geo-Replicated&lt;/strong&gt;: Configs are always up-to-date across different regions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Failure Detection &amp;amp; Recovery&lt;/strong&gt;: Kong is unaffected if one of your Cassandra
nodes goes down.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: All Kong nodes auto-join the cluster keeping their config
updated across nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Distributed by nature, Kong scales horizontally by simply
adding nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Kong handles load with ease by scaling and using NGINX at
the core.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugins&lt;/strong&gt;: Extendable architecture for adding functionality to Kong and
APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more info about plugins and integrations, you can check out the &lt;a href="https://docs.konghq.com/hub/" rel="nofollow"&gt;Kong
Hub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-distributions" class="anchor" aria-hidden="true" href="#distributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Distributions&lt;/h2&gt;
&lt;p&gt;Kong comes in many shapes. While this repository contains its core's source
code, other repos are also under active development:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/docker-kong"&gt;Kong Docker&lt;/a&gt;: A Dockerfile for
running Kong in Docker.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong/releases"&gt;Kong Packages&lt;/a&gt;: Pre-built packages
for Debian, Red Hat, and OS X distributions (shipped with each release).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-vagrant"&gt;Kong Vagrant&lt;/a&gt;: A Vagrantfile for
provisioning a development-ready environment for Kong.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/homebrew-kong"&gt;Kong Homebrew&lt;/a&gt;: Homebrew Formula
for Kong.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-dist-cloudformation"&gt;Kong CloudFormation&lt;/a&gt;:
Kong in a 1-click deployment for AWS EC2.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/marketplace/pp/B06WP4TNKL" rel="nofollow"&gt;Kong AWS AMI&lt;/a&gt;: Kong AMI on
the AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kong-dist-azure"&gt;Kong on Microsoft Azure&lt;/a&gt;: Run Kong
using Azure Resource Manager.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/heroku/heroku-kong"&gt;Kong on Heroku&lt;/a&gt;: Deploy Kong on
Heroku in one click.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.instaclustr.com/solutions/managed-cassandra-for-kong/" rel="nofollow"&gt;Kong and Instaclustr&lt;/a&gt;: Let
Instaclustr manage your Cassandra cluster.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Kong/kubernetes-ingress-controller"&gt;Kubernetes Ingress Controller for Kong&lt;/a&gt;:
Use Kong for Kubernetes Ingress.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bintray.com/kong/kong-nightly/master" rel="nofollow"&gt;Nightly Builds&lt;/a&gt;: Builds of the master branch available
every morning at about 9AM PST.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;If you are planning on developing on Kong, you'll need a development
installation. The &lt;code&gt;next&lt;/code&gt; branch holds the latest unreleased source code.&lt;/p&gt;
&lt;p&gt;You can read more about writing your own plugins in the &lt;a href="https://docs.konghq.com/latest/plugin-development/" rel="nofollow"&gt;Plugin Development
Guide&lt;/a&gt;, or browse an
online version of Kong's source code documentation in the &lt;a href="https://docs.konghq.com/latest/pdk/" rel="nofollow"&gt;Plugin Development
Kit (PDK) Reference&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h4&gt;
&lt;p&gt;You can use Docker / docker-compose and a mounted volume to develop Kong by
following the instructions on &lt;a href="https://github.com/Kong/kong-build-tools#developing-kong"&gt;Kong/kong-build-tools&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-vagrant" class="anchor" aria-hidden="true" href="#vagrant"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vagrant&lt;/h4&gt;
&lt;p&gt;You can use a Vagrant box running Kong and Postgres that you can find at
&lt;a href="https://github.com/Kong/kong-vagrant"&gt;Kong/kong-vagrant&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-source-install" class="anchor" aria-hidden="true" href="#source-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source Install&lt;/h4&gt;
&lt;p&gt;Kong mostly is an OpenResty application made of Lua source files, but also
requires some additional third-party dependencies. We recommend installing
those by following the source install instructions at
&lt;a href="https://docs.konghq.com/install/source/" rel="nofollow"&gt;https://docs.konghq.com/install/source/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Instead of following the second step (Install Kong), clone this repository
and install the latest Lua sources instead of the currently released ones:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/Kong/kong
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; kong/

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; you might want to switch to the development branch. See CONTRIBUTING.md&lt;/span&gt;
$ git checkout next

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install the Lua sources&lt;/span&gt;
$ luarocks make&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-running-for-development" class="anchor" aria-hidden="true" href="#running-for-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running for development&lt;/h4&gt;
&lt;p&gt;Check out the &lt;a href="https://github.com/Kong/kong/blob/next/kong.conf.default#L244"&gt;development section&lt;/a&gt;
of the default configuration file for properties to tweak in order to ease
the development process for Kong.&lt;/p&gt;
&lt;p&gt;Modifying the &lt;a href="https://github.com/openresty/lua-nginx-module#lua_package_path"&gt;&lt;code&gt;lua_package_path&lt;/code&gt;&lt;/a&gt;
and &lt;a href="https://github.com/openresty/lua-nginx-module#lua_package_cpath"&gt;&lt;code&gt;lua_package_cpath&lt;/code&gt;&lt;/a&gt;
directives will allow Kong to find your custom plugin's source code wherever it
might be in your system.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h4&gt;
&lt;p&gt;Install the development dependencies (&lt;a href="https://github.com/Olivine-Labs/busted"&gt;busted&lt;/a&gt;, &lt;a href="https://github.com/mpeterv/luacheck"&gt;luacheck&lt;/a&gt;) with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make dev&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Kong relies on three test suites using the &lt;a href="https://github.com/Olivine-Labs/busted"&gt;busted&lt;/a&gt; testing library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unit tests&lt;/li&gt;
&lt;li&gt;Integration tests, which require Postgres and Cassandra to be up and running&lt;/li&gt;
&lt;li&gt;Plugins tests, which require Postgres to be running&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first can simply be run after installing busted and running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the integration and plugins tests will spawn a Kong instance and
perform their tests against it. As so, consult/edit the &lt;code&gt;spec/kong_tests.conf&lt;/code&gt;
configuration file to make your test instance point to your Postgres/Cassandra
servers, depending on your needs.&lt;/p&gt;
&lt;p&gt;You can run the integration tests (assuming &lt;strong&gt;both&lt;/strong&gt; Postgres and Cassandra are
running and configured according to &lt;code&gt;spec/kong_tests.conf&lt;/code&gt;) with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-integration
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the plugins tests with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-plugins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, all suites can be run at once by simply using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make test-all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consult the &lt;a href=".ci/run_tests.sh"&gt;run_tests.sh&lt;/a&gt; script for a more advanced example
usage of the tests suites and the Makefile.&lt;/p&gt;
&lt;p&gt;Finally, a very useful tool in Lua development (as with many other dynamic
languages) is performing static linting of your code. You can use &lt;a href="https://github.com/mpeterv/luacheck"&gt;luacheck&lt;/a&gt;
(installed with &lt;code&gt;make dev&lt;/code&gt;) for this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make lint
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-makefile" class="anchor" aria-hidden="true" href="#makefile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile&lt;/h4&gt;
&lt;p&gt;When developing, you can use the &lt;code&gt;Makefile&lt;/code&gt; for doing the following operations:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;install&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install the Kong luarock globally&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;dev&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Install development dependencies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;lint&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Lint Lua files in &lt;code&gt;kong/&lt;/code&gt; and &lt;code&gt;spec/&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the unit tests suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-integration&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the integration tests suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-plugins&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run the plugins test suite&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;&lt;code&gt;test-all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Run all unit + integration + plugins tests at once&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-enterprise-support--demo" class="anchor" aria-hidden="true" href="#enterprise-support--demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enterprise Support &amp;amp; Demo&lt;/h2&gt;
&lt;p&gt;If you are working in a large organization you should learn more about &lt;a href="https://konghq.com/kong-enterprise-edition/" rel="nofollow"&gt;Kong
Enterprise&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Copyright 2016-2019 Kong Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Kong</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>iresty/apisix #2 in Lua, This week</title><link>https://github.com/iresty/apisix</link><description>&lt;p&gt;&lt;i&gt;Cloud-Native Microservices API Gateway&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="README_CN.md"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-apisix" class="anchor" aria-hidden="true" href="#apisix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;APISIX&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/iresty/apisix" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0dcd92e44e87cce3ac38b5a886c38219eaf5bab5/68747470733a2f2f7472617669732d63692e6f72672f6972657374792f6170697369782e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/iresty/apisix.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/iresty/apisix/blob/master/LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8051e9938a1ab39cf002818dfceb6b6092f34d68/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;QQ group&lt;/strong&gt;: 552030619&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitter.im/apisix/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/568da9652db68475b3c235e1274c91d42c378149/68747470733a2f2f6261646765732e6769747465722e696d2f6170697369782f636f6d6d756e6974792e737667" alt="Gitter" data-canonical-src="https://badges.gitter.im/apisix/community.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/intent/follow?screen_name=apisixfast" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8aeaef778a0e54bb395d635f5b1bfdf8a48abde2/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f617069736978666173742e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/apisixfast.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;APISIX is a cloud-native microservices API gateway, delivering the ultimate performance, security, open source and scalable platform for all your APIs and microservices.&lt;/p&gt;
&lt;p&gt;APISIX is based on OpenResty and etcd. Compared with traditional API gateways, APISIX has dynamic routing and plug-in hot loading, which is especially suitable for API management under micro-service system.&lt;/p&gt;
&lt;p&gt;&lt;a href="#Installation"&gt;Installation&lt;/a&gt; | &lt;a href="doc/README.md"&gt;Documentation&lt;/a&gt; | &lt;a href="#development-manual-of-apisix"&gt;Development ENV&lt;/a&gt; | &lt;a href="FAQ.md"&gt;FAQ&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-why-apisix" class="anchor" aria-hidden="true" href="#why-apisix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why APISIX?&lt;/h2&gt;
&lt;p&gt;If you are building a website, mobile device or IoT (Internet of Things) application, you may need to use an API gateway to handle interface traffic.&lt;/p&gt;
&lt;p&gt;APISIX is a cloud-based microservices API gateway that handles traditional north-south traffic and handles east-west traffic between services.&lt;/p&gt;
&lt;p&gt;APISIX provides dynamic load balancing, authentication, rate limiting, and other plugins through plugin mechanisms, and supports plugins you develop yourself.&lt;/p&gt;
&lt;p&gt;For more detailed information, see the &lt;a href="https://www.iresty.com/download/Choosing%20the%20Right%20Microservice%20API%20Gateway%20for%20the%20Enterprise%20User.pdf" rel="nofollow"&gt;White Paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="doc/images/apisix.png"&gt;&lt;img src="doc/images/apisix.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cloud-Native&lt;/strong&gt;: Platform agnostic, No vendor lock-in, APISIX can run from bare-metal to Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins.md"&gt;Hot Updates And Hot Plugins&lt;/a&gt;&lt;/strong&gt;: Continuously updates its configurations and plugins without restarts!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Load Balancing&lt;/strong&gt;: Round-robin load balancing with weight.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hash-based Load Balancing&lt;/strong&gt;: Load balance with consistent hashing sessions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/https.md"&gt;SSL&lt;/a&gt;&lt;/strong&gt;: Dynamically load an SSL certificate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HTTP(S) Forward Proxy&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/health-check.md"&gt;Health Checks&lt;/a&gt;&lt;/strong&gt;ÔºöEnable health check on the upstream node, and will automatically filter unhealthy nodes during load balancing to ensure system stability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Circuit-Breaker&lt;/strong&gt;: Intelligent tracking of unhealthy upstream services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Authentications&lt;/strong&gt;: &lt;a href="doc/plugins/key-auth.md"&gt;key-auth&lt;/a&gt;, &lt;a href="doc/plugins/jwt-auth.md"&gt;JWT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins/limit-req.md"&gt;Limit-req&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins/limit-count.md"&gt;Limit-count&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins/limit-conn.md"&gt;Limit-concurrency&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins/proxy-rewrite.md"&gt;Proxy Rewrite&lt;/a&gt;&lt;/strong&gt;: Support for rewriting the &lt;code&gt;host&lt;/code&gt;, &lt;code&gt;uri&lt;/code&gt;, &lt;code&gt;schema&lt;/code&gt;, &lt;code&gt;enable_websocket&lt;/code&gt;, &lt;code&gt;headers&lt;/code&gt; information upstream of the request.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenTracing: &lt;a href="doc/plugins/zipkin.md"&gt;support Apache Skywalking and Zipkin&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring And Metrics&lt;/strong&gt;: &lt;a href="doc/plugins/prometheus.md"&gt;Prometheus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins/grpc-transcoding.md"&gt;gRPC transcoding&lt;/a&gt;&lt;/strong&gt;ÔºöSupports protocol transcoding so that clients can access your gRPC API by using HTTP/JSON.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins/serverless.md"&gt;Serverless&lt;/a&gt;&lt;/strong&gt;: Invoke functions in each phase in APISIX.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom plugins&lt;/strong&gt;: Allows hooking of common phases, such as &lt;code&gt;rewrite&lt;/code&gt;, &lt;code&gt;access&lt;/code&gt;, &lt;code&gt;header filer&lt;/code&gt;, &lt;code&gt;body filter&lt;/code&gt; and &lt;code&gt;log&lt;/code&gt;, also allows to hook the &lt;code&gt;balancer&lt;/code&gt; stage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dashboard&lt;/strong&gt;: Built-in dashboard to control APISIX.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Version Control&lt;/strong&gt;: Supports rollbacks of operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI&lt;/strong&gt;: start\stop\reload APISIX through the command line.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proxy Websocket&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IPv6&lt;/strong&gt;: Use IPv6 to match route.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clustering&lt;/strong&gt;: APISIX nodes are stateless, creates clustering of the configuration center, please refer to &lt;a href="https://github.com/etcd-io/etcd/blob/master/Documentation/v2/clustering.md"&gt;etcd Clustering Guide&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: plug-in mechanism is easy to extend.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: The single-core QPS reaches 24k with an average delay of less than 0.6 milliseconds.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anti-ReDoS(Regular expression Denial of Service)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IP Whitelist/Blacklist&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IdP&lt;/strong&gt;: Support external authentication services, such as Auth0, okta, etc., users can use this to connect to Oauth2.0 and other authentication methods.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/stand-alone.md"&gt;Stand-alone mode&lt;/a&gt;&lt;/strong&gt;: Supports to load route rules from local yaml file, it is more friendly such as under the kubernetes(k8s).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Global Rule&lt;/strong&gt;: Allows to run any plugin for all request, eg: limit rate, IP filter etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/stream-proxy.md"&gt;TCP/UDP Proxy&lt;/a&gt;&lt;/strong&gt;: Dynamic TCP/UDP proxy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="doc/plugins/mqtt-proxy.md"&gt;Dynamic MQTT Proxy&lt;/a&gt;&lt;/strong&gt;: Supports to load balance MQTT by &lt;code&gt;client_id&lt;/code&gt;, both support MQTT &lt;a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html" rel="nofollow"&gt;3.1.*&lt;/a&gt;, &lt;a href="https://docs.oasis-open.org/mqtt/mqtt/v5.0/mqtt-v5.0.html" rel="nofollow"&gt;5.0&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ACL&lt;/strong&gt;: TODO.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bot detection&lt;/strong&gt;: TODO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-online-demo-dashboard" class="anchor" aria-hidden="true" href="#online-demo-dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online Demo Dashboard&lt;/h2&gt;
&lt;p&gt;We provide an online dashboard &lt;a href="http://apisix.iresty.com" rel="nofollow"&gt;demo version&lt;/a&gt;Ôºå make it easier for you to understand APISIX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;APISIX Installed and tested in the following systems, and the version of OpenResty MUST &amp;gt;= 1.15.8.1:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CentOS 7&lt;/li&gt;
&lt;li&gt;Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;Ubuntu 18.04&lt;/li&gt;
&lt;li&gt;Debian 9&lt;/li&gt;
&lt;li&gt;Debian 10&lt;/li&gt;
&lt;li&gt;macOS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ARM64&lt;/strong&gt; Ubuntu 18.04&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You now have four ways to install APISIX:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if you are using CentOS 7, it is recommended to use &lt;a href="#install-from-rpm-for-centos-7"&gt;RPM&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;if using macOS, only support git clone and install by manual, please take a look at &lt;a href="doc/dev-manual.md"&gt;dev manual&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;other systems please use &lt;a href="#install-from-luarocks-not-support-macos"&gt;Luarocks&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;You can also install from &lt;a href="https://github.com/iresty/docker-apisix"&gt;Docker image&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: APISIX currently only supports the v2 protocol storage to etcd, but the latest version of etcd (starting with 3.4) has turned off the v2 protocol by default. You need to add &lt;code&gt;--enable-v2=true&lt;/code&gt; to the startup parameter to enable the v2 protocol. The development of the v3 protocol supporting etcd has begun and will soon be available.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-install-from-rpm-for-centos-7" class="anchor" aria-hidden="true" href="#install-from-rpm-for-centos-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install from RPM for CentOS 7&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo yum install yum-utils
sudo yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo
sudo yum install -y openresty etcd
sudo service etcd start

sudo yum install -y https://github.com/iresty/apisix/releases/download/v0.8/apisix-0.8-0.el7.noarch.rpm&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can try APISIX with the &lt;a href="#quickstart"&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt; now.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-install-from-luarocks-not-support-macos" class="anchor" aria-hidden="true" href="#install-from-luarocks-not-support-macos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install from Luarocks (not support macOS)&lt;/h3&gt;
&lt;h5&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h5&gt;
&lt;p&gt;APISIX is based on &lt;a href="https://openresty.org/" rel="nofollow"&gt;OpenResty&lt;/a&gt;, the configures data storage and distribution via &lt;a href="https://github.com/etcd-io/etcd"&gt;etcd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We recommend that you use &lt;a href="https://luarocks.org/" rel="nofollow"&gt;luarocks&lt;/a&gt; to install APISIX, and for different operating systems have different dependencies, see more: &lt;a href="doc/install-dependencies.md"&gt;Install Dependencies&lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-install-apisix" class="anchor" aria-hidden="true" href="#install-apisix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install APISIX&lt;/h5&gt;
&lt;p&gt;APISIX is installed by running the following commands in your terminal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;install the master branch via curl&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo sh -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;curl -fsSL https://raw.githubusercontent.com/iresty/apisix/master/utils/install-apisix.sh&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;install the specified version via Luarock:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; install apisix with version v0.8&lt;/span&gt;
sudo luarocks install --lua-dir=/path/openresty/luajit apisix 0.8

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; old luarocks may not support `lua-dir`, we can remove option `lua-dir`&lt;/span&gt;
sudo luarocks install apisix 0.8&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Installation complete&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If all goes well, you will see the message like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    apisix 0.7-0 is now built and installed in /usr/local/apisix/deps (license: Apache License 2.0)

    + sudo rm -f /usr/local/bin/apisix
    + sudo ln -s /usr/local/apisix/deps/bin/apisix /usr/local/bin/apisix
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Congratulations, you have already installed APISIX successfully.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-development-manual-of-apisix" class="anchor" aria-hidden="true" href="#development-manual-of-apisix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development Manual of APISIX&lt;/h2&gt;
&lt;p&gt;If you are a developer, you can view the &lt;a href="doc/dev-manual.md"&gt;dev manual&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quickstart" class="anchor" aria-hidden="true" href="#quickstart"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quickstart&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;start server:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo apisix start&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;note&lt;/em&gt;: If you are in a development environment, start server by command &lt;code&gt;make run&lt;/code&gt;.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;try limit count plugin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Limit count plugin is a good start to try APISIX,
you can follow the &lt;a href="doc/plugins/limit-count.md"&gt;documentation of limit count&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then you can try more &lt;a href="doc/README.md#plugins"&gt;plugins&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-dashboard" class="anchor" aria-hidden="true" href="#dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dashboard&lt;/h2&gt;
&lt;p&gt;APISIX has the built-in dashboardÔºåopen &lt;code&gt;http://127.0.0.1:9080/apisix/dashboard/&lt;/code&gt; with a browser and try it.&lt;/p&gt;
&lt;p&gt;Do not need to fill the user name and password, log in directly.&lt;/p&gt;
&lt;p&gt;Dashboard allow any remote IP by default, and you can modify &lt;code&gt;allow_admin&lt;/code&gt; in &lt;code&gt;conf/config.yaml&lt;/code&gt; by yourself, to list the list of IPs allowed to access.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-benchmark" class="anchor" aria-hidden="true" href="#benchmark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark&lt;/h2&gt;
&lt;p&gt;Using Google Cloud's 4 core server, APISIX's QPS reach to 60,000 with a latency of only 500 microseconds.&lt;/p&gt;
&lt;p&gt;You can view the &lt;a href="doc/benchmark.md"&gt;benchmark documentation&lt;/a&gt; for more detailed information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-architecture-design" class="anchor" aria-hidden="true" href="#architecture-design"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Architecture Design&lt;/h2&gt;
&lt;p&gt;&lt;a href="doc/architecture-design.md"&gt;Development Documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-videos-and-slides" class="anchor" aria-hidden="true" href="#videos-and-slides"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Videos and slides&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.upyun.com/opentalk/432.html" rel="nofollow"&gt;APISIX technology selection, testing and continuous integration(Chinese)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.upyun.com/opentalk/429.html" rel="nofollow"&gt;APISIX high performance practice(Chinese)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-who-uses-apisix" class="anchor" aria-hidden="true" href="#who-uses-apisix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Who Uses APISIX?&lt;/h2&gt;
&lt;p&gt;A wide variety of companies and organizations use APISIX for research, production and commercial product.
Here is the User Wall of APISIX.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="doc/images/user-wall.jpg"&gt;&lt;img src="doc/images/user-wall.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Users are encouraged to add themselves to the &lt;a href="doc/powered-by.md"&gt;Powered By&lt;/a&gt; page.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-landscape" class="anchor" aria-hidden="true" href="#landscape"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Landscape&lt;/h2&gt;
&lt;p&gt;APISIX enriches the &lt;a href="https://landscape.cncf.io/category=api-gateway&amp;amp;format=card-mode&amp;amp;grouping=category" rel="nofollow"&gt;CNCF API Gateway Landscape&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="doc/images/cncf-landscope.jpg"&gt;&lt;img src="doc/images/cncf-landscope.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;There are often some questions asked by developers in the community. We have arranged them in the &lt;a href="FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If your concerns are not among them, please submit issue to communicate with us.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;See &lt;a href="Contributing.md"&gt;CONTRIBUTING&lt;/a&gt; for details on submitting patches and the contribution workflow.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;inspired by Kong and Orange.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iresty</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>rubbertoe98/FiveM-Scripts #3 in Lua, This week</title><link>https://github.com/rubbertoe98/FiveM-Scripts</link><description>&lt;p&gt;&lt;i&gt;Compilation of my publically released code&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fivem-scripts" class="anchor" aria-hidden="true" href="#fivem-scripts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FiveM-Scripts&lt;/h1&gt;
&lt;p&gt;Compilation of my publicly released code&lt;/p&gt;
&lt;p&gt;Feel free to make improvements with PRs&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rubbertoe98</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>DiscworldZA/gta-resources #4 in Lua, This week</title><link>https://github.com/DiscworldZA/gta-resources</link><description>&lt;p&gt;&lt;i&gt;All the DiscworldZA GTA Resources&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Hi, my name is &lt;a href="https://twitter.com/DiscworldZA" rel="nofollow"&gt;DiscworldZA&lt;/a&gt;. I create GTA V mods for FiveM.&lt;/p&gt;
&lt;p&gt;I create mods while streaming. Give me a follow here for notifications &lt;a href="https://www.twitch.tv/DiscworldZA" rel="nofollow"&gt;Twitch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is my repo for all the mods I create.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-feedback" class="anchor" aria-hidden="true" href="#feedback"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback.&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Highly recommended&lt;/li&gt;
&lt;li&gt;Report Issues &lt;a href="https://github.com/DiscworldZA/gta-resources/issues"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Want a Mod or Feature? Request &lt;a href="https://github.com/DiscworldZA/gta-resources/issues"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Changelog &lt;a href="https://github.com/DiscworldZA/gta-resources/blob/master/changelog.md"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will create almost anything you can think of. Feel free to request it and I will see about creating it.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-configuration" class="anchor" aria-hidden="true" href="#configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration&lt;/h1&gt;
&lt;p&gt;Most server owners are not developers but understand the basics. My mods are created highly configurable. Every location or price for the mods will be able to be configured. If you want more configuration options create an &lt;a href="https://github.com/DiscworldZA/gta-resources/issues"&gt;issue&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All ReadMe contains Configuration sections&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-explanation" class="anchor" aria-hidden="true" href="#explanation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Explanation&lt;/h1&gt;
&lt;p&gt;Most of my mods work with the &lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-base"&gt;Base&lt;/a&gt; mod and &lt;a href="https://github.com/ESX-Org/es_extended"&gt;ESX&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The reasoning behind this is to simplify my process and have all my mods up to the same standard and address bug fixing over the whole platform rather individual broken sections.
It is the same concept that &lt;a href="https://github.com/ESX-Org/es_extended"&gt;ESX&lt;/a&gt; works on but for all my needs.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements-for-all-mods" class="anchor" aria-hidden="true" href="#requirements-for-all-mods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements for all mods&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ESX-Org/es_extended"&gt;ESX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mythicrp/mythic_notify"&gt;Mythic Notify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ESX-Org/esx_addonaccount"&gt;ESX Add on Account&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-mod-list" class="anchor" aria-hidden="true" href="#mod-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mod List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-base"&gt;Disc-Base&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-ammo"&gt;Disc-Ammo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-armory"&gt;Disc-Armory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-autorepair"&gt;Disc-AutoRepair&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-billing"&gt;Disc-Billing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-boot"&gt;Disc-Boot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-compensation"&gt;Disc-Compensation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-carthief"&gt;Disc-CarThief&lt;/a&gt; (my version of &lt;a href="https://github.com/KlibrDM/esx_carthief"&gt;esx_carthief&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-dragme"&gt;Disc-DragMe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-drugruns"&gt;Disc-DrugRuns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-drugsales"&gt;Disc-DrugSales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-gcphone"&gt;Disc-GcPhone&lt;/a&gt; (add-on to &lt;a href="https://github.com/N3MTV/gcphone"&gt;gcphone&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-hotwire"&gt;Disc-HotWire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-hud"&gt;Disc-HUD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-identification"&gt;Disc-Identification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-import"&gt;Disc-Import&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-inventoryhud"&gt;Disc-InventoryHUD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-jobcars"&gt;Disc-JobCars&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-panic"&gt;Disc-Panic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-property"&gt;Disc-Property&lt;/a&gt; (BETA)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-showid"&gt;Disc-ShowId&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-social"&gt;Disc-Social&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-tax"&gt;Disc-Tax&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-teleport"&gt;Disc-Teleport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-vehiclepick"&gt;Disc-VehiclePick&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-vehiclesales"&gt;Disc-VehicleSales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-warrant"&gt;Disc-Warrant&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-ideas" class="anchor" aria-hidden="true" href="#ideas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ideas&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;NPC Jobs (These are all mods to add depth to the ESX versions)
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-cops"&gt;Disc-Cops&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;Disc-EMS&lt;/li&gt;
&lt;li&gt;Disc-Mechanic&lt;/li&gt;
&lt;li&gt;Disc-CarSales&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unique Garages&lt;/li&gt;
&lt;li&gt;Server Wide Robbery Mechanic&lt;/li&gt;
&lt;li&gt;Search Warrants for Disc-Properties via Disc-Warrants&lt;/li&gt;
&lt;li&gt;Automatic BOLO/Facial Detection&lt;/li&gt;
&lt;li&gt;Handheld Police and EMS devices&lt;/li&gt;
&lt;li&gt;Variable Drug Import and Selling (Allow Drug importing via ship or truck) (Allow Large shipments of drugs)&lt;/li&gt;
&lt;li&gt;Expanding EMS and Cop Procedure (Require documentation to be filed)&lt;/li&gt;
&lt;li&gt;Mechanic Repair Expansion&lt;/li&gt;
&lt;li&gt;Vehicle Shop Documentation (License and Registration)&lt;/li&gt;
&lt;li&gt;Cop Evidence Mechanic&lt;/li&gt;
&lt;li&gt;Judge Job (Allow search warrant authorization, Allow cases to be handled)&lt;/li&gt;
&lt;li&gt;Daily Login Reward System&lt;/li&gt;
&lt;li&gt;Vehicle Keys (Compatible With Hotwire)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h1&gt;
&lt;p&gt;Feel free to use my &lt;a href="https://github.com/DiscworldZA/gta-resources/tree/master/disc-base"&gt;Base&lt;/a&gt; mod to create your own! Just credit me in some way. Share your mods you created with my base mod with me!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-credit" class="anchor" aria-hidden="true" href="#credit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credit&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Did I forget to credit you for your code?&lt;/li&gt;
&lt;li&gt;Am I using your code without permission?&lt;/li&gt;
&lt;li&gt;Do you want to use my code?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Message me on Discord. I will help you out.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-discord" class="anchor" aria-hidden="true" href="#discord"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Discord&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://discord.gg/S2SckF6" rel="nofollow"&gt;https://discord.gg/S2SckF6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>DiscworldZA</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>cardwing/Codes-for-Lane-Detection #5 in Lua, This week</title><link>https://github.com/cardwing/Codes-for-Lane-Detection</link><description>&lt;p&gt;&lt;i&gt;Learning Lightweight Lane Detection CNNs by Self Attention Distillation (ICCV 2019)&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Codes for &lt;a href="https://arxiv.org/abs/1908.00821" rel="nofollow"&gt;"Learning Lightweight Lane Detection CNNs by Self Attention Distillation"&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repo also contains Tensorflow implementation of &lt;a href="https://arxiv.org/abs/1712.06080" rel="nofollow"&gt;"Spatial As Deep: Spatial CNN for Traffic Scene Understanding"&lt;/a&gt;. (SCNN-Tensorflow)&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./ERFNet-CULane-PyTorch"&gt;ERFNet-CULane-PyTorch&lt;/a&gt; has been released. (It can achieve &lt;strong&gt;73.1&lt;/strong&gt; F1-measure in CULane testing set)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./ENet-Label-Torch"&gt;ENet-Label-Torch&lt;/a&gt;, &lt;a href="./ENet-TuSimple-Torch"&gt;ENet-TuSimple-Torch&lt;/a&gt; and &lt;a href="./ENet-BDD100K-Torch"&gt;ENet-BDD100K-Torch&lt;/a&gt; have been released.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Key features:&lt;/p&gt;
&lt;p&gt;(1) ENet-label is a &lt;strong&gt;light-weight&lt;/strong&gt; lane detection model based on &lt;a href="https://arxiv.org/abs/1606.02147" rel="nofollow"&gt;ENet&lt;/a&gt; and adopts &lt;strong&gt;self attention distillation&lt;/strong&gt; (more details can be found in our paper).&lt;/p&gt;
&lt;p&gt;(2) It has &lt;strong&gt;20&lt;/strong&gt; √ó fewer parameters and runs &lt;strong&gt;10&lt;/strong&gt; √ó faster compared to the state-of-the-art SCNN, and achieves &lt;strong&gt;72.0&lt;/strong&gt; (F1-measure) on CULane testing set (better than SCNN which achieves 71.6). It also achieves &lt;strong&gt;96.64%&lt;/strong&gt; accuracy in TuSimple testing set (better than SCNN which achieves 96.53%) and &lt;strong&gt;36.56%&lt;/strong&gt; accuracy in BDD100K testing set (better than SCNN which achieves 35.79%).&lt;/p&gt;
&lt;p&gt;(3) Applying ENet-SAD to &lt;a href="https://unsupervised-llamas.com/llamas/" rel="nofollow"&gt;LLAMAS&lt;/a&gt; dataset yields &lt;strong&gt;0.635&lt;/strong&gt; mAP in the &lt;a href="https://unsupervised-llamas.com/llamas/benchmark_multi" rel="nofollow"&gt;multi-class lane marker segmentation task&lt;/a&gt;, which is much better than the baseline algorithm which achieves 0.500 mAP. Details can be found in &lt;a href="https://github.com/cardwing/unsupervised_llamas/tree/master/ENet-SAD-Simple"&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(Do not hesitate to try our model!!!)&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Multi-GPU training has been supported. Just change BATCH_SIZE and GPU_NUM in global_config.py, and then use &lt;code&gt;CUDA_VISIBLE_DEVICES="0,1,2,3" python file_name.py&lt;/code&gt;. Thanks @ yujincheng08.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#Installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Datasets"&gt;Datasets&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#TuSimple"&gt;TuSimple&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#CULane"&gt;CULane&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#BDD100K"&gt;BDD100K&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#SCNN-Tensorflow"&gt;SCNN-Tensorflow&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#Test"&gt;Test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Train"&gt;Train&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#Performance"&gt;Performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Others"&gt;Others&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#Citation"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Contact"&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Install necessary packages:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;    conda create -n tensorflow_gpu pip python=3.5
    source activate tensorflow_gpu
    pip install --upgrade tensorflow-gpu==1.3.0
    pip3 install -r SCNN-Tensorflow/lane-detection-model/requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Download VGG-16:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Download the vgg.npy &lt;a href="https://github.com/machrisaa/tensorflow-vgg"&gt;here&lt;/a&gt; and put it in SCNN-Tensorflow/lane-detection-model/data.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Pre-trained model for testing:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Download the pre-trained model &lt;a href="https://drive.google.com/open?id=1-E0Bws7-v35vOVfqEXDTJdfovUTQ2sf5" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-tusimple" class="anchor" aria-hidden="true" href="#tusimple"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TuSimple&lt;/h2&gt;
&lt;p&gt;The ground-truth labels of TuSimple testing set is now available at &lt;a href="https://github.com/TuSimple/tusimple-benchmark/issues/3"&gt;TuSimple&lt;/a&gt;. The annotated training (#frame = 3268) and validation labels (#frame = 358) can be found &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/11"&gt;here&lt;/a&gt;, please use them (list-name.txt) to replace the train_gt.txt and val_gt.txt in &lt;a href="./SCNN-Tensorflow/lane-detection-model/tools/train_lanenet.py"&gt;train_lanenet.py&lt;/a&gt;. Moreover, you need to resize the image to 256 x 512 instead of 288 x 800 in TuSimple. Remember to change the maximum index of rows and columns, and detailed explanations can be seen &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/18"&gt;here&lt;/a&gt;. Please evaluate your pred.json using the labels and &lt;a href="https://github.com/TuSimple/tusimple-benchmark/blob/master/evaluate/lane.py"&gt;this script&lt;/a&gt;. Besides, to generate pred.json, you can refer to &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/4"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-culane" class="anchor" aria-hidden="true" href="#culane"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CULane&lt;/h2&gt;
&lt;p&gt;The whole dataset is available at &lt;a href="https://xingangpan.github.io/projects/CULane.html" rel="nofollow"&gt;CULane&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-bdd100k" class="anchor" aria-hidden="true" href="#bdd100k"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BDD100K&lt;/h2&gt;
&lt;p&gt;The whole dataset is available at &lt;a href="http://bdd-data.berkeley.edu/" rel="nofollow"&gt;BDD100K&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-scnn-tensorflow" class="anchor" aria-hidden="true" href="#scnn-tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SCNN-Tensorflow&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-test" class="anchor" aria-hidden="true" href="#test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Test&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;cd SCNN-Tensorflow/lane-detection-model
CUDA_VISIBLE_DEVICES="0" python tools/test_lanenet.py --weights_path path/to/model_weights_file --image_path path/to/image_name_list --save_dir to_be_saved_dir
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that path/to/image_name_list should be like &lt;a href="./SCNN-Tensorflow/lane-detection-model/demo_file/test_img.txt"&gt;test_img.txt&lt;/a&gt;. Now, you get the probability maps from our model. To get the final performance, you need to follow &lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN&lt;/a&gt; to get curve lines from probability maps as well as calculate precision, recall and F1-measure.&lt;/p&gt;
&lt;p&gt;Reminder: you should check &lt;a href="./SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor.py"&gt;lanenet_data_processor.py&lt;/a&gt; and &lt;a href="./SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor.py"&gt;lanenet_data_processor_test.py&lt;/a&gt; to ensure that the processing of image path is right. You are recommended to use the absolute path in your image path list. Besides, this code needs batch size used in training and testing to be consistent. To enable arbitrary batch size in the testing phase, please refer to &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/10"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-train" class="anchor" aria-hidden="true" href="#train"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES="0" python tools/train_lanenet.py --net vgg --dataset_dir path/to/CULane-dataset/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that path/to/CULane-dataset/ should contain files like &lt;a href="./SCNN-Tensorflow/lane-detection-model/demo_file/train_gt.txt"&gt;train_gt.txt&lt;/a&gt; and &lt;a href="./SCNN-Tensorflow/lane-detection-model/demo_file/train_gt.txt"&gt;val_gt.txt&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Performance&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;TuSimple testing set:&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Accuracy&lt;/th&gt;
&lt;th align="center"&gt;FP&lt;/th&gt;
&lt;th align="center"&gt;FN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN-Torch&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;96.53%&lt;/td&gt;
&lt;td align="center"&gt;0.0617&lt;/td&gt;
&lt;td align="center"&gt;0.0180&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SCNN-Tensorflow&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ENet-Label-Torch&lt;/td&gt;
&lt;td align="center"&gt;96.64%&lt;/td&gt;
&lt;td align="center"&gt;0.0602&lt;/td&gt;
&lt;td align="center"&gt;0.0205&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The pre-trained model for testing is here. (coming soon!) Note that in TuSimple, SCNN-Torch is based on ResNet-101 while SCNN-Tensorflow is based on VGG-16. In CULane and BDD100K, both SCNN-Torch and SCNN-Tensorflow are based on VGG-16.&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;CULane testing set (F1-measure):&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Category&lt;/th&gt;
&lt;th align="center"&gt;&lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN-Torch&lt;/a&gt;&lt;/th&gt;
&lt;th align="center"&gt;SCNN-Tensorflow&lt;/th&gt;
&lt;th align="center"&gt;ENet-Label-Torch&lt;/th&gt;
&lt;th align="center"&gt;ERFNet-CULane-PyTorch&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Normal&lt;/td&gt;
&lt;td align="center"&gt;90.6&lt;/td&gt;
&lt;td align="center"&gt;90.2&lt;/td&gt;
&lt;td align="center"&gt;90.7&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.5&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Crowded&lt;/td&gt;
&lt;td align="center"&gt;69.7&lt;/td&gt;
&lt;td align="center"&gt;71.9&lt;/td&gt;
&lt;td align="center"&gt;70.8&lt;/td&gt;
&lt;td align="center"&gt;71.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Night&lt;/td&gt;
&lt;td align="center"&gt;66.1&lt;/td&gt;
&lt;td align="center"&gt;64.6&lt;/td&gt;
&lt;td align="center"&gt;65.9&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;67.1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;No line&lt;/td&gt;
&lt;td align="center"&gt;43.4&lt;/td&gt;
&lt;td align="center"&gt;45.8&lt;/td&gt;
&lt;td align="center"&gt;44.7&lt;/td&gt;
&lt;td align="center"&gt;45.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Shadow&lt;/td&gt;
&lt;td align="center"&gt;66.9&lt;/td&gt;
&lt;td align="center"&gt;73.8&lt;/td&gt;
&lt;td align="center"&gt;70.6&lt;/td&gt;
&lt;td align="center"&gt;71.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Arrow&lt;/td&gt;
&lt;td align="center"&gt;84.1&lt;/td&gt;
&lt;td align="center"&gt;83.8&lt;/td&gt;
&lt;td align="center"&gt;85.8&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Dazzle light&lt;/td&gt;
&lt;td align="center"&gt;58.5&lt;/td&gt;
&lt;td align="center"&gt;59.5&lt;/td&gt;
&lt;td align="center"&gt;64.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;66.0&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Curve&lt;/td&gt;
&lt;td align="center"&gt;64.4&lt;/td&gt;
&lt;td align="center"&gt;63.4&lt;/td&gt;
&lt;td align="center"&gt;65.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;66.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Crossroad&lt;/td&gt;
&lt;td align="center"&gt;1990&lt;/td&gt;
&lt;td align="center"&gt;4137&lt;/td&gt;
&lt;td align="center"&gt;2729&lt;/td&gt;
&lt;td align="center"&gt;2199&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Total&lt;/td&gt;
&lt;td align="center"&gt;71.6&lt;/td&gt;
&lt;td align="center"&gt;71.3&lt;/td&gt;
&lt;td align="center"&gt;72.0&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;73.1&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Runtime(ms)&lt;/td&gt;
&lt;td align="center"&gt;133.5&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;13.4&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;10.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Parameter(M)&lt;/td&gt;
&lt;td align="center"&gt;20.72&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;0.98&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;2.49&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The pre-trained model for testing is &lt;a href="https://drive.google.com/open?id=1-E0Bws7-v35vOVfqEXDTJdfovUTQ2sf5" rel="nofollow"&gt;here&lt;/a&gt;. Note that you need to exchange the order of VGG-MEAN in test_lanenet.py and change the order of input images from RGB to BGR since the pre-trained model uses opencv to read images. You can further boost the performance by referring to &lt;a href="https://github.com/cardwing/Codes-for-Lane-Detection/issues/5"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;BDD100K testing set:&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Accuracy&lt;/th&gt;
&lt;th align="center"&gt;IoU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN-Torch&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;35.79%&lt;/td&gt;
&lt;td align="center"&gt;15.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SCNN-Tensorflow&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;ENet-Label-Torch&lt;/td&gt;
&lt;td align="center"&gt;36.56%&lt;/td&gt;
&lt;td align="center"&gt;16.02&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The accuracy and IoU of lane pixels are computed. The pre-trained model for testing is here. (coming soon!)&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-others" class="anchor" aria-hidden="true" href="#others"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Others&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use the codes, please cite the following publications:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{hou2019learning,
  title={Learning Lightweight Lane Detection CNNs by Self Attention Distillation},
  author={Hou, Yuenan and Ma, Zheng and Liu, Chunxiao and Loy, Chen Change},
  journal={arXiv preprint arXiv:1908.00821},
  year={2019}
}

@inproceedings{pan2018SCNN,  
  author = {Xingang Pan, Jianping Shi, Ping Luo, Xiaogang Wang, and Xiaoou Tang},  
  title = {Spatial As Deep: Spatial CNN for Traffic Scene Understanding},  
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},  
  month = {February},  
  year = {2018}  
}

@misc{hou2019agnostic,
    title={Agnostic Lane Detection},
    author={Yuenan Hou},
    year={2019},
    eprint={1905.03704},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This repo is built upon &lt;a href="https://github.com/XingangPan/SCNN"&gt;SCNN&lt;/a&gt; and &lt;a href="https://github.com/MaybeShewill-CV/lanenet-lane-detection"&gt;LaneNet&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;If you have any problems in reproducing the results, just raise an issue in this repo.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-to-do-list" class="anchor" aria-hidden="true" href="#to-do-list"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;To-Do List&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Test SCNN-Tensorflow in TuSimple and BDD100K&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Provide detailed instructions to run SCNN-Tensorflow in TuSimple and BDD100K&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Upload our light-weight model (ENet-SAD) and its training &amp;amp; testing scripts&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cardwing</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>openwrt/luci #6 in Lua, This week</title><link>https://github.com/openwrt/luci</link><description>&lt;p&gt;&lt;i&gt;LuCI - OpenWrt Configuration Interface&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openwrt-luci-feed" class="anchor" aria-hidden="true" href="#openwrt-luci-feed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenWrt luci feed&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://hosted.weblate.org/engage/openwrt/?utm_source=widget" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6f6dda98b1ee7b17ac7b1253073a21ebc7468446/68747470733a2f2f686f737465642e7765626c6174652e6f72672f776964676574732f6f70656e7772742f2d2f6c7563692f7376672d62616467652e737667" alt="Translation status" data-canonical-src="https://hosted.weblate.org/widgets/openwrt/-/luci/svg-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h2&gt;
&lt;p&gt;This is the OpenWrt "luci"-feed containing LuCI - OpenWrt Configuration Interface.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;This feed is enabled by default. Your feeds.conf.default (or feeds.conf) should contain a line like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;src-git luci https://github.com/openwrt/luci.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To install all its package definitions, run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./scripts/feeds update luci
./scripts/feeds install -a -p luci
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-api-reference" class="anchor" aria-hidden="true" href="#api-reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;API Reference&lt;/h2&gt;
&lt;p&gt;You can browse the generated API documentation &lt;a href="http://htmlpreview.github.io/?http://raw.githubusercontent.com/openwrt/luci/master/documentation/api/index.html" rel="nofollow"&gt;directly on Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;Documentation for developing and extending LuCI can be found &lt;a href="https://github.com/openwrt/luci/wiki"&gt;in the Wiki&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;See &lt;a href="LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-package-guidelines" class="anchor" aria-hidden="true" href="#package-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Package Guidelines&lt;/h2&gt;
&lt;p&gt;See &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openwrt</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>Stephan-S/FS19_AutoDrive #7 in Lua, This week</title><link>https://github.com/Stephan-S/FS19_AutoDrive</link><description>&lt;p&gt;&lt;i&gt;FS19 version of AutoDrive - Developer Version&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fs19_autodrive" class="anchor" aria-hidden="true" href="#fs19_autodrive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FS19_AutoDrive&lt;/h1&gt;
&lt;p&gt;FS19 version of AutoDrive&lt;/p&gt;
&lt;p&gt;If you want to support my development effort, the best way is to open issues on any bugs you encounter or for features you would like to be added to the mod.&lt;/p&gt;
&lt;p&gt;Wer die Weiterentwicklung des Mods unterst√ºtzen m√∂chte, kann dies am Besten durch flei√üiges Erstellen von Issues zu gefundenen Bugs und/oder gew√ºnschten Erweiterungen zum Mod tun.&lt;/p&gt;
&lt;p&gt;If you like my work, feel free to buy me a coffee (of which I drink quite a lot :D )
&lt;a href="https://www.buymeacoffee.com/9Di7EUSI2" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/031fc5a134cdca5ae3460822aba371e63f794233/68747470733a2f2f7777772e6275796d6561636f666665652e636f6d2f6173736574732f696d672f637573746f6d5f696d616765732f6f72616e67655f696d672e706e67" alt="Buy Me A Coffee" data-canonical-src="https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/StephanSchlosser" rel="nofollow"&gt;https://www.paypal.me/StephanSchlosser&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Stephan-S</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>openresty/lua-resty-core #8 in Lua, This week</title><link>https://github.com/openresty/lua-resty-core</link><description>&lt;p&gt;&lt;i&gt;New FFI-based API for lua-nginx-module&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body markdown" data-path="README.markdown"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-name" class="anchor" aria-hidden="true" href="#name"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Name&lt;/h1&gt;
&lt;p&gt;lua-resty-core - New FFI-based Lua API for ngx_http_lua_module and/or ngx_stream_lua_module&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#name"&gt;Name&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#status"&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#synopsis"&gt;Synopsis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#description"&gt;Description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#api-implemented"&gt;API Implemented&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#restycorehash"&gt;resty.core.hash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycorebase64"&gt;resty.core.base64&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoreuri"&gt;resty.core.uri&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoreregex"&gt;resty.core.regex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoreexit"&gt;resty.core.exit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoreshdict"&gt;resty.core.shdict&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycorevar"&gt;resty.core.var&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycorectx"&gt;resty.core.ctx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycorerequest"&gt;resty.core.request&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoreresponse"&gt;resty.core.response&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoremisc"&gt;resty.core.misc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoretime"&gt;resty.core.time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycoreworker"&gt;resty.core.worker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#restycorephase"&gt;resty.core.phase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxsemaphore"&gt;ngx.semaphore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxbalancer"&gt;ngx.balancer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxssl"&gt;ngx.ssl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxsslsession"&gt;ngx.ssl.session&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxre"&gt;ngx.re&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxresp"&gt;ngx.resp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxpipe"&gt;ngx.pipe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxprocess"&gt;ngx.process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxerrlog"&gt;ngx.errlog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ngxbase64"&gt;ngx.base64&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#caveat"&gt;Caveat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#todo"&gt;TODO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#author"&gt;Author&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#copyright-and-license"&gt;Copyright and License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#see-also"&gt;See Also&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-status" class="anchor" aria-hidden="true" href="#status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Status&lt;/h1&gt;
&lt;p&gt;This library is production ready.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-synopsis" class="anchor" aria-hidden="true" href="#synopsis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Synopsis&lt;/h1&gt;
&lt;p&gt;This library is automatically loaded by default in OpenResty 1.15.8.1. This
behavior can be disabled via the
&lt;a href="https://github.com/openresty/lua-nginx-module#lua_load_resty_core"&gt;lua_load_resty_core&lt;/a&gt;
directive, but note that the use of this library is vividly recommended, as its
FFI implementation is both faster, safer, and more complete than the Lua C API
of the ngx_lua module.&lt;/p&gt;
&lt;p&gt;If you are using an older version of OpenResty, you must load this library like
so:&lt;/p&gt;
&lt;div class="highlight highlight-source-nginx"&gt;&lt;pre&gt;    &lt;span class="pl-c"&gt;# nginx.conf&lt;/span&gt;

    &lt;span class="pl-k"&gt;http&lt;/span&gt; {
        &lt;span class="pl-c"&gt;# you do NOT need to configure the following line when you&lt;/span&gt;
        &lt;span class="pl-c"&gt;# are using the OpenResty bundle 1.4.3.9+.&lt;/span&gt;
        &lt;span class="pl-k"&gt;lua_package_path&lt;/span&gt; &lt;span class="pl-s"&gt;"/path/to/lua-resty-core/lib/?.lua;;"&lt;/span&gt;;

        &lt;span class="pl-k"&gt;init_by_lua_block&lt;/span&gt; {
            &lt;span class="pl-k"&gt;require&lt;/span&gt; &lt;span class="pl-s"&gt;"resty.core"&lt;/span&gt;
            collectgarbage("collect")  -- &lt;span class="pl-k"&gt;just&lt;/span&gt; to collect any garbage
        }

        ...
    }&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h1&gt;
&lt;p&gt;This pure Lua library reimplements part of the &lt;a href="https://github.com/openresty/lua-nginx-module#readme"&gt;ngx_lua&lt;/a&gt; module's
&lt;a href="https://github.com/openresty/lua-nginx-module#nginx-api-for-lua"&gt;Nginx API for Lua&lt;/a&gt;
with LuaJIT FFI and installs the new FFI-based Lua API into the ngx.* and ndk.* namespaces
used by the ngx_lua module.&lt;/p&gt;
&lt;p&gt;In addition, this Lua library implements any significant new Lua APIs of
the &lt;a href="https://github.com/openresty/lua-nginx-module#readme"&gt;ngx_lua&lt;/a&gt; module
as proper Lua modules, like &lt;a href="#ngxsemaphore"&gt;ngx.semaphore&lt;/a&gt; and &lt;a href="#ngxbalancer"&gt;ngx.balancer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The FFI-based Lua API can work with LuaJIT's JIT compiler. ngx_lua's default API is based on the standard
Lua C API, which will never be JIT compiled and the user Lua code is always interpreted (slowly).&lt;/p&gt;
&lt;p&gt;Support for the new &lt;a href="https://github.com/openresty/stream-lua-nginx-module"&gt;ngx_stream_lua_module&lt;/a&gt; has also begun.&lt;/p&gt;
&lt;p&gt;This library is shipped with the OpenResty bundle by default. So you do not really need to worry about the dependencies
and requirements.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt; This library is included with every OpenResty release. You should use the bundled version
of this library in the particular OpenResty release you are using. Otherwise you may run
into serious compatibility issues.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LuaJIT 2.1 (for now, it is the v2.1 git branch in the official luajit-2.0 git repository: &lt;a href="http://luajit.org/download.html" rel="nofollow"&gt;http://luajit.org/download.html&lt;/a&gt; )&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module"&gt;ngx_http_lua_module&lt;/a&gt; v0.10.16.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module"&gt;ngx_stream_lua_module&lt;/a&gt; v0.0.8.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-resty-lrucache"&gt;lua-resty-lrucache&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-api-implemented" class="anchor" aria-hidden="true" href="#api-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;API Implemented&lt;/h1&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycorehash" class="anchor" aria-hidden="true" href="#restycorehash"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.hash&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxmd5"&gt;ngx.md5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxmd5_bin"&gt;ngx.md5_bin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxsha1_bin"&gt;ngx.sha1_bin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycorebase64" class="anchor" aria-hidden="true" href="#restycorebase64"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.base64&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxencode_base64"&gt;ngx.encode_base64&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxdecode_base64"&gt;ngx.decode_base64&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoreuri" class="anchor" aria-hidden="true" href="#restycoreuri"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.uri&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxescape_uri"&gt;ngx.escape_uri&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxunescape_uri"&gt;ngx.unescape_uri&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoreregex" class="anchor" aria-hidden="true" href="#restycoreregex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.regex&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxrematch"&gt;ngx.re.match&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxregmatch"&gt;ngx.re.gmatch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxrefind"&gt;ngx.re.find&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxresub"&gt;ngx.re.sub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxregsub"&gt;ngx.re.gsub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoreexit" class="anchor" aria-hidden="true" href="#restycoreexit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.exit&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxexit"&gt;ngx.exit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoreshdict" class="anchor" aria-hidden="true" href="#restycoreshdict"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.shdict&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictget"&gt;ngx.shared.DICT.get&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictget_stale"&gt;ngx.shared.DICT.get_stale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictincr"&gt;ngx.shared.DICT.incr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictset"&gt;ngx.shared.DICT.set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictsafe_set"&gt;ngx.shared.DICT.safe_set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictadd"&gt;ngx.shared.DICT.add&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictsafe_add"&gt;ngx.shared.DICT.safe_add&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictreplace"&gt;ngx.shared.DICT.replace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictdelete"&gt;ngx.shared.DICT.delete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictttl"&gt;ngx.shared.DICT.ttl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictexpire"&gt;ngx.shared.DICT.expire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictflush_all"&gt;ngx.shared.DICT.flush_all&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictfree_space"&gt;ngx.shared.DICT.free_space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxshareddictcapacity"&gt;ngx.shared.DICT.capacity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycorevar" class="anchor" aria-hidden="true" href="#restycorevar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.var&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxvarvariable"&gt;ngx.var.VARIABLE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycorectx" class="anchor" aria-hidden="true" href="#restycorectx"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.ctx&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxctx"&gt;ngx.ctx&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycorerequest" class="anchor" aria-hidden="true" href="#restycorerequest"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.request&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxreqget_headers"&gt;ngx.req.get_headers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxreqget_uri_args"&gt;ngx.req.get_uri_args&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxreqstart_time"&gt;ngx.req.start_time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxreqget_method"&gt;ngx.req.get_method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxreqset_method"&gt;ngx.req.set_method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxreqset_header"&gt;ngx.req.set_header&lt;/a&gt;  (partial: table-typed header values not supported yet)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxreqclear_header"&gt;ngx.req.clear_header&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoreresponse" class="anchor" aria-hidden="true" href="#restycoreresponse"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.response&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxheaderheader"&gt;ngx.header.HEADER&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoremisc" class="anchor" aria-hidden="true" href="#restycoremisc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.misc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxstatus"&gt;ngx.status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxis_subrequest"&gt;ngx.is_subrequest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxheaders_sent"&gt;ngx.headers_sent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoretime" class="anchor" aria-hidden="true" href="#restycoretime"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.time&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxtime"&gt;ngx.time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxnow"&gt;ngx.now&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxupdate_time"&gt;ngx.update_time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxlocaltime"&gt;ngx.localtime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxutctime"&gt;ngx.utctime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxcookie_time"&gt;ngx.cookie_time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxhttp_time"&gt;ngx.http_time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxparse_http_time"&gt;ngx.parse_http_time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycoreworker" class="anchor" aria-hidden="true" href="#restycoreworker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.worker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxworkerexiting"&gt;ngx.worker.exiting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxworkerpid"&gt;ngx.worker.pid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxworkerid"&gt;ngx.worker.id&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxworkercount"&gt;ngx.worker.count&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycorephase" class="anchor" aria-hidden="true" href="#restycorephase"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.phase&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ngxget_phase"&gt;ngx.get_phase&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-restycorendk" class="anchor" aria-hidden="true" href="#restycorendk"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;resty.core.ndk&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/openresty/lua-nginx-module#ndkset_vardirective"&gt;ndk.set_var&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ngxsemaphore" class="anchor" aria-hidden="true" href="#ngxsemaphore"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.semaphore&lt;/h2&gt;
&lt;p&gt;This Lua module implements a semaphore API for efficient "light thread" synchronization,
which can work across different requests (but not across nginx worker processes).&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/semaphore.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxbalancer" class="anchor" aria-hidden="true" href="#ngxbalancer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.balancer&lt;/h2&gt;
&lt;p&gt;This Lua module implements for defining dynamic upstream balancers in Lua.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/balancer.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxssl" class="anchor" aria-hidden="true" href="#ngxssl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.ssl&lt;/h2&gt;
&lt;p&gt;This Lua module provides a Lua API for controlling SSL certificates, private keys,
SSL protocol versions, and etc in NGINX downstream SSL handshakes.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/ssl.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxsslsession" class="anchor" aria-hidden="true" href="#ngxsslsession"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.ssl.session&lt;/h2&gt;
&lt;p&gt;This Lua module provides a Lua API for manipulating SSL session data and IDs
for NGINX downstream SSL connections.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/ssl/session.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxre" class="anchor" aria-hidden="true" href="#ngxre"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.re&lt;/h2&gt;
&lt;p&gt;This Lua module provides a Lua API which implements convenience utilities for
the &lt;code&gt;ngx.re&lt;/code&gt; API.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/re.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxresp" class="anchor" aria-hidden="true" href="#ngxresp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.resp&lt;/h2&gt;
&lt;p&gt;This Lua module provides Lua API which could be used to handle HTTP response.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/resp.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxpipe" class="anchor" aria-hidden="true" href="#ngxpipe"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.pipe&lt;/h2&gt;
&lt;p&gt;This module provides a Lua API to spawn processes and communicate with them in
a non-blocking fashion.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/pipe.md"&gt;documentation&lt;/a&gt; for this Lua module for more
details.&lt;/p&gt;
&lt;p&gt;This module was first introduced in lua-resty-core v0.1.16.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxprocess" class="anchor" aria-hidden="true" href="#ngxprocess"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.process&lt;/h2&gt;
&lt;p&gt;This Lua module is used to manage the nginx process in Lua.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/process.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;This module was first introduced in lua-resty-core v0.1.12.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxerrlog" class="anchor" aria-hidden="true" href="#ngxerrlog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.errlog&lt;/h2&gt;
&lt;p&gt;This Lua module provides Lua API to capture and manage nginx error log messages.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/errlog.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;This module was first introduced in lua-resty-core v0.1.12.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ngxbase64" class="anchor" aria-hidden="true" href="#ngxbase64"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ngx.base64&lt;/h2&gt;
&lt;p&gt;This Lua module provides Lua API to urlsafe base64 encode/decode.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="./lib/ngx/base64.md"&gt;documentation&lt;/a&gt; for this Lua module for more details.&lt;/p&gt;
&lt;p&gt;This module was first introduced in lua-resty-core v0.1.14.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-caveat" class="anchor" aria-hidden="true" href="#caveat"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Caveat&lt;/h1&gt;
&lt;p&gt;If the user Lua code is not JIT compiled, then use of this library may
lead to performance drop in interpreted mode. You will only observe
speedup when you get a good part of your user Lua code JIT compiled.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Re-implement &lt;code&gt;ngx_lua&lt;/code&gt;'s cosocket API with FFI.&lt;/li&gt;
&lt;li&gt;Re-implement &lt;code&gt;ngx_lua&lt;/code&gt;'s &lt;code&gt;ngx.eof&lt;/code&gt; and &lt;code&gt;ngx.flush&lt;/code&gt; API functions with FFI.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-author" class="anchor" aria-hidden="true" href="#author"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Author&lt;/h1&gt;
&lt;p&gt;Yichun "agentzh" Zhang (Á´†‰∫¶Êò•) &lt;a href="mailto:agentzh@gmail.com"&gt;agentzh@gmail.com&lt;/a&gt;, OpenResty Inc.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-copyright-and-license" class="anchor" aria-hidden="true" href="#copyright-and-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright and License&lt;/h1&gt;
&lt;p&gt;This module is licensed under the BSD license.&lt;/p&gt;
&lt;p&gt;Copyright (C) 2013-2019, by Yichun "agentzh" Zhang, OpenResty Inc.&lt;/p&gt;
&lt;p&gt;All rights reserved.&lt;/p&gt;
&lt;p&gt;Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.&lt;/p&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;See Also&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;the ngx_lua module: &lt;a href="https://github.com/openresty/lua-nginx-module#readme"&gt;https://github.com/openresty/lua-nginx-module#readme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LuaJIT FFI: &lt;a href="http://luajit.org/ext_ffi.html" rel="nofollow"&gt;http://luajit.org/ext_ffi.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openresty</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>jcjohnson/fast-neural-style #9 in Lua, This week</title><link>https://github.com/jcjohnson/fast-neural-style</link><description>&lt;p&gt;&lt;i&gt;Feedforward style transfer&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fast-neural-style" class="anchor" aria-hidden="true" href="#fast-neural-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;fast-neural-style&lt;/h1&gt;
&lt;p&gt;This is the code for the paper&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="http://cs.stanford.edu/people/jcjohns/eccv16/" rel="nofollow"&gt;Perceptual Losses for Real-Time Style Transfer and Super-Resolution&lt;/a&gt;&lt;/strong&gt;
&lt;br&gt;
&lt;a href="http://cs.stanford.edu/people/jcjohns/" rel="nofollow"&gt;Justin Johnson&lt;/a&gt;,
&lt;a href="http://web.stanford.edu/~alahi/" rel="nofollow"&gt;Alexandre Alahi&lt;/a&gt;,
&lt;a href="http://vision.stanford.edu/feifeili/" rel="nofollow"&gt;Li Fei-Fei&lt;/a&gt;
&lt;br&gt;
Presented at &lt;a href="http://www.eccv2016.org/" rel="nofollow"&gt;ECCV 2016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper builds on
&lt;a href="https://arxiv.org/abs/1508.06576" rel="nofollow"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;
by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge by training
feedforward neural networks that apply artistic styles to images.
After training, our feedforward networks can stylize images
&lt;strong&gt;hundreds of times faster&lt;/strong&gt; than the optimization-based method presented
by Gatys et al.&lt;/p&gt;
&lt;p&gt;This repository also includes an implementation of instance normalization as
described in the paper &lt;a href="https://arxiv.org/abs/1607.08022" rel="nofollow"&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;
by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. This simple trick
significantly improves the quality of feedforward style transfer models.&lt;/p&gt;
&lt;p&gt;Stylizing this image of the Stanford campus at a resolution of 1200x630
takes &lt;strong&gt;50 milliseconds&lt;/strong&gt; on a Pascal Titan X:&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/styles/candy.jpg"&gt;&lt;img src="images/styles/candy.jpg" height="225px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/content/hoovertowernight.jpg"&gt;&lt;img src="images/content/hoovertowernight.jpg" height="225px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/hoovertowernight_candy.jpg"&gt;&lt;img src="images/outputs/hoovertowernight_candy.jpg" height="346px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;In this repository we provide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The style transfer models &lt;a href="#models-from-the-paper"&gt;used in the paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Additional models &lt;a href="#models-with-instance-normalization"&gt;using instance normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code for &lt;a href="#running-on-new-images"&gt;running models on new images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A demo that runs models in &lt;a href="#webcam-demo"&gt;real-time off a webcam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code for &lt;a href="doc/training.md"&gt;training new feedforward style transfer models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An implementation of &lt;a href="#optimization-based-style-transfer"&gt;optimization-based style transfer&lt;/a&gt;
method described by Gatys et al.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you find this code useful for your research, please cite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{Johnson2016Perceptual,
  title={Perceptual losses for real-time style transfer and super-resolution},
  author={Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  booktitle={European Conference on Computer Vision},
  year={2016}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;p&gt;All code is implemented in &lt;a href="http://torch.ch/" rel="nofollow"&gt;Torch&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First &lt;a href="http://torch.ch/docs/getting-started.html#installing-torch" rel="nofollow"&gt;install Torch&lt;/a&gt;, then
update / install the following packages:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;luarocks install torch
luarocks install nn
luarocks install image
luarocks install lua-cjson&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-optional-gpu-acceleration" class="anchor" aria-hidden="true" href="#optional-gpu-acceleration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(Optional) GPU Acceleration&lt;/h3&gt;
&lt;p&gt;If you have an NVIDIA GPU, you can accelerate all operations with CUDA.&lt;/p&gt;
&lt;p&gt;First &lt;a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow"&gt;install CUDA&lt;/a&gt;, then
update / install the following packages:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;luarocks install cutorch
luarocks install cunn&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-optional-cudnn" class="anchor" aria-hidden="true" href="#optional-cudnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(Optional) cuDNN&lt;/h3&gt;
&lt;p&gt;When using CUDA, you can use cuDNN to accelerate convolutions.&lt;/p&gt;
&lt;p&gt;First &lt;a href="https://developer.nvidia.com/cudnn" rel="nofollow"&gt;download cuDNN&lt;/a&gt; and copy the
libraries to &lt;code&gt;/usr/local/cuda/lib64/&lt;/code&gt;. Then install the Torch bindings for cuDNN:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;luarocks install cudnn&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Models&lt;/h3&gt;
&lt;p&gt;Download all pretrained style transfer models by running the script&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash models/download_style_transfer_models.sh&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will download ten model files (~200MB) to the folder &lt;code&gt;models/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-models-from-the-paper" class="anchor" aria-hidden="true" href="#models-from-the-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models from the paper&lt;/h2&gt;
&lt;p&gt;The style transfer models we used in the paper will be located in the folder &lt;code&gt;models/eccv16&lt;/code&gt;.
Here are some example results where we use these models to stylize this
image of the Chicago skyline with at an image size of 512:&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/content/chicago.jpg"&gt;&lt;img src="images/content/chicago.jpg" height="185px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/styles/starry_night_crop.jpg"&gt;&lt;img src="images/styles/starry_night_crop.jpg" height="155px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/styles/la_muse.jpg"&gt;&lt;img src="images/styles/la_muse.jpg" height="155px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/styles/composition_vii.jpg"&gt;&lt;img src="images/styles/composition_vii.jpg" height="155px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/styles/wave_crop.jpg"&gt;&lt;img src="images/styles/wave_crop.jpg" height="155px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/eccv16/chicago_starry_night.jpg"&gt;&lt;img src="images/outputs/eccv16/chicago_starry_night.jpg" height="142px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/eccv16/chicago_la_muse.jpg"&gt;&lt;img src="images/outputs/eccv16/chicago_la_muse.jpg" height="142px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/eccv16/chicago_composition_vii.jpg"&gt;&lt;img src="images/outputs/eccv16/chicago_composition_vii.jpg" height="142px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/eccv16/chicago_wave.jpg"&gt;&lt;img src="images/outputs/eccv16/chicago_wave.jpg" height="142px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-models-with-instance-normalization" class="anchor" aria-hidden="true" href="#models-with-instance-normalization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models with instance normalization&lt;/h2&gt;
&lt;p&gt;As discussed in the paper
&lt;a href="https://arxiv.org/abs/1607.08022" rel="nofollow"&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;
by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, replacing batch
normalization with instance normalization significantly improves the quality
of feedforward style transfer models.&lt;/p&gt;
&lt;p&gt;We have trained several models with instance normalization; after downloading
pretrained models they will be in the folder &lt;code&gt;models/instance_norm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;These models use the same architecture as those used in our paper, except with
half the number of filters per layer and with instance normalization instead of
batch normalization. Using narrower layers makes the models smaller and faster
without sacrificing model quality.&lt;/p&gt;
&lt;p&gt;Here are some example outputs from these models, with an image size of 1024:&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/styles/candy.jpg"&gt;&lt;img src="images/styles/candy.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/chicago_candy.jpg"&gt;&lt;img src="images/outputs/chicago_candy.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/chicago_udnie.jpg"&gt;&lt;img src="images/outputs/chicago_udnie.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/styles/udnie.jpg"&gt;&lt;img src="images/styles/udnie.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/styles/the_scream.jpg"&gt;&lt;img src="images/styles/the_scream.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/chicago_scream.jpg"&gt;&lt;img src="images/outputs/chicago_scream.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/chicago_mosaic.jpg"&gt;&lt;img src="images/outputs/chicago_mosaic.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/styles/mosaic.jpg"&gt;&lt;img src="images/styles/mosaic.jpg" height="174px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/styles/feathers.jpg"&gt;&lt;img src="images/styles/feathers.jpg" height="173px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/chicago_feathers.jpg"&gt;&lt;img src="images/outputs/chicago_feathers.jpg" height="173px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/outputs/chicago_muse.jpg"&gt;&lt;img src="images/outputs/chicago_muse.jpg" height="173px" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/styles/la_muse.jpg"&gt;&lt;img src="images/styles/la_muse.jpg" height="173px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-running-on-new-images" class="anchor" aria-hidden="true" href="#running-on-new-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running on new images&lt;/h2&gt;
&lt;p&gt;The script &lt;code&gt;fast_neural_style.lua&lt;/code&gt; lets you use a trained model to stylize new images:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;th fast_neural_style.lua \
  -model models/eccv16/starry_night.t7 \
  -input_image images/content/chicago.jpg \
  -output_image out.png&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can run the same model on an entire directory of images like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;th fast_neural_style.lua \
  -model models/eccv16/starry_night.t7 \
  -input_dir images/content/ \
  -output_dir out/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can control the size of the output images using the &lt;code&gt;-image_size&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;By default this script runs on CPU; to run on GPU, add the flag &lt;code&gt;-gpu&lt;/code&gt;
specifying the GPU on which to run.&lt;/p&gt;
&lt;p&gt;The full set of options for this script is &lt;a href="doc/flags.md#fast_neural_stylelua"&gt;described here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-webcam-demo" class="anchor" aria-hidden="true" href="#webcam-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Webcam demo&lt;/h2&gt;
&lt;p&gt;You can use the script &lt;code&gt;webcam_demo.lua&lt;/code&gt; to run one or more models in real-time
off a webcam stream. To run this demo you need to use &lt;code&gt;qlua&lt;/code&gt; instead of &lt;code&gt;th&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;qlua webcam_demo.lua -models models/instance_norm/candy.t7 -gpu 0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can run multiple models at the same time by passing a comma-separated list
to the &lt;code&gt;-models&lt;/code&gt; flag:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;qlua webcam_demo.lua \
  -models models/instance_norm/candy.t7,models/instance_norm/udnie.t7 \
  -gpu 0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With a Pascal Titan X you can easily run four models in realtime at 640x480:&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="images/webcam.gif"&gt;&lt;img src="images/webcam.gif" width="700px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;The webcam demo depends on a few extra Lua packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/clementfarabet/lua---camera"&gt;clementfarabet/lua---camera&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/torch/qtlua"&gt;torch/qtlua&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can install / update these packages by running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;luarocks install camera
luarocks install qtlua&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The full set of options for this script is &lt;a href="doc/flags.md#webcam_demolua"&gt;described here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-training-new-models" class="anchor" aria-hidden="true" href="#training-new-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training new models&lt;/h2&gt;
&lt;p&gt;You can &lt;a href="doc/training.md"&gt;find instructions for training new models here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-optimization-based-style-transfer" class="anchor" aria-hidden="true" href="#optimization-based-style-transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimization-based Style Transfer&lt;/h2&gt;
&lt;p&gt;The script &lt;code&gt;slow_neural_style.lua&lt;/code&gt; is similar to the
&lt;a href="https://github.com/jcjohnson/neural-style"&gt;original neural-style&lt;/a&gt;, and uses
the optimization-based style-transfer method described by Gatys et al.&lt;/p&gt;
&lt;p&gt;This script uses the same code for computing losses as the feedforward training
script, allowing for fair comparisons between feedforward style transfer networks
and optimization-based style transfer.&lt;/p&gt;
&lt;p&gt;Compared to the original &lt;a href="https://github.com/jcjohnson/neural-style"&gt;neural-style&lt;/a&gt;,
this script has the following improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remove dependency on protobuf and &lt;a href="https://github.com/szagoruyko/loadcaffe"&gt;loadcaffe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for many more CNN architectures, including ResNets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full set of options for this script is &lt;a href="doc/flags.md#slow_neural_stylelua"&gt;described here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Free for personal or research use; for commercial use please contact me.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jcjohnson</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>phillipi/pix2pix #10 in Lua, This week</title><link>https://github.com/phillipi/pix2pix</link><description>&lt;p&gt;&lt;i&gt;Image-to-image translation with conditional adversarial nets&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pix2pix" class="anchor" aria-hidden="true" href="#pix2pix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pix2pix&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://phillipi.github.io/pix2pix/" rel="nofollow"&gt;Project&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/1611.07004" rel="nofollow"&gt;Arxiv&lt;/a&gt; |
&lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"&gt;PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Torch implementation for learning a mapping from input images to output images, for example:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="imgs/examples.jpg"&gt;&lt;img src="imgs/examples.jpg" width="900px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;br&gt;
&lt;a href="http://web.mit.edu/phillipi/" rel="nofollow"&gt;Phillip Isola&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~junyanz/" rel="nofollow"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~tinghuiz/" rel="nofollow"&gt;Tinghui Zhou&lt;/a&gt;, &lt;a href="https://people.eecs.berkeley.edu/~efros/" rel="nofollow"&gt;Alexei A. Efros&lt;/a&gt;&lt;br&gt;
CVPR, 2017.&lt;/p&gt;
&lt;p&gt;On some tasks, decent results can be obtained fairly quickly and on small datasets. For example, to learn to generate facades (example shown above), we trained on just 400 images for about 2 hours (on a single Pascal Titan X GPU). However, for harder problems it may be important to train on far larger datasets, and for many hours or even days.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please check out our &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"&gt;PyTorch&lt;/a&gt; implementation for pix2pix and CycleGAN. The PyTorch version is under active development and can produce results comparable to or better than this Torch version.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linux or OSX&lt;/li&gt;
&lt;li&gt;NVIDIA GPU + CUDA CuDNN (CPU mode and CUDA without CuDNN may work with minimal modification, but untested)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install torch and dependencies from &lt;a href="https://github.com/torch/distro"&gt;https://github.com/torch/distro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install torch packages &lt;code&gt;nngraph&lt;/code&gt; and &lt;code&gt;display&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;luarocks install nngraph
luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Clone this repo:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone git@github.com:phillipi/pix2pix.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; pix2pix&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Download the dataset (e.g., &lt;a href="http://cmp.felk.cvut.cz/~tylecr1/facade/" rel="nofollow"&gt;CMP Facades&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_dataset.sh facades&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Train the model&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA th train.lua&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(CPU only) The same training command without using a GPU or CUDNN. Setting the environment variables &lt;code&gt;gpu=0 cudnn=0&lt;/code&gt; forces CPU only&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;(Optionally) start the display server to view results as the model trains. ( See &lt;a href="#display-ui"&gt;Display UI&lt;/a&gt; for more details):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;th -ldisplay.start 8000 0.0.0.0&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Finally, test the model:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The test results will be saved to an html file here: &lt;code&gt;./results/facades_generation/latest_net_G_val/index.html&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-train" class="anchor" aria-hidden="true" href="#train"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Train&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB th train.lua&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Switch &lt;code&gt;AtoB&lt;/code&gt; to &lt;code&gt;BtoA&lt;/code&gt; to train translation in opposite direction.&lt;/p&gt;
&lt;p&gt;Models are saved to &lt;code&gt;./checkpoints/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;checkpoint_dir=your_dir&lt;/code&gt; in train.lua).&lt;/p&gt;
&lt;p&gt;See &lt;code&gt;opt&lt;/code&gt; in train.lua for additional training options.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-test" class="anchor" aria-hidden="true" href="#test"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Test&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;DATA_ROOT=/path/to/data/ name=expt_name which_direction=AtoB phase=val th test.lua&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will run the model named &lt;code&gt;expt_name&lt;/code&gt; in direction &lt;code&gt;AtoB&lt;/code&gt; on all images in &lt;code&gt;/path/to/data/val&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Result images, and a webpage to view them, are saved to &lt;code&gt;./results/expt_name&lt;/code&gt; (can be changed by passing &lt;code&gt;results_dir=your_dir&lt;/code&gt; in test.lua).&lt;/p&gt;
&lt;p&gt;See &lt;code&gt;opt&lt;/code&gt; in test.lua for additional testing options.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h2&gt;
&lt;p&gt;Download the datasets using the following script. Some of the datasets are collected by other researchers. Please cite their papers if you use the data.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./datasets/download_dataset.sh dataset_name&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;facades&lt;/code&gt;: 400 images from &lt;a href="http://cmp.felk.cvut.cz/~tylecr1/facade/" rel="nofollow"&gt;CMP Facades dataset&lt;/a&gt;. [&lt;a href="datasets/bibtex/facades.tex"&gt;Citation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cityscapes&lt;/code&gt;: 2975 images from the &lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes training set&lt;/a&gt;.  [&lt;a href="datasets/bibtex/cityscapes.tex"&gt;Citation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;maps&lt;/code&gt;: 1096 training images scraped from Google Maps&lt;/li&gt;
&lt;li&gt;&lt;code&gt;edges2shoes&lt;/code&gt;: 50k training images from &lt;a href="http://vision.cs.utexas.edu/projects/finegrained/utzap50k/" rel="nofollow"&gt;UT Zappos50K dataset&lt;/a&gt;. Edges are computed by &lt;a href="https://github.com/s9xie/hed"&gt;HED&lt;/a&gt; edge detector + post-processing.
[&lt;a href="datasets/bibtex/shoes.tex"&gt;Citation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;edges2handbags&lt;/code&gt;: 137K Amazon Handbag images from &lt;a href="https://github.com/junyanz/iGAN"&gt;iGAN project&lt;/a&gt;. Edges are computed by &lt;a href="https://github.com/s9xie/hed"&gt;HED&lt;/a&gt; edge detector + post-processing. [&lt;a href="datasets/bibtex/handbags.tex"&gt;Citation&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;night2day&lt;/code&gt;: around 20K natural scene images from  &lt;a href="http://transattr.cs.brown.edu/" rel="nofollow"&gt;Transient Attributes dataset&lt;/a&gt; [&lt;a href="datasets/bibtex/transattr.tex"&gt;Citation&lt;/a&gt;]. To train a &lt;code&gt;day2night&lt;/code&gt; pix2pix model, you need to add &lt;code&gt;which_direction=BtoA&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models&lt;/h2&gt;
&lt;p&gt;Download the pre-trained models with the following script. You need to rename the model (e.g., &lt;code&gt;facades_label2image&lt;/code&gt; to &lt;code&gt;/checkpoints/facades/latest_net_G.t7&lt;/code&gt;) after the download has finished.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./models/download_model.sh model_name&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;facades_label2image&lt;/code&gt; (label -&amp;gt; facade): trained on the CMP Facades dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cityscapes_label2image&lt;/code&gt; (label -&amp;gt; street scene): trained on the Cityscapes dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cityscapes_image2label&lt;/code&gt; (street scene -&amp;gt; label): trained on the Cityscapes dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;edges2shoes&lt;/code&gt; (edge -&amp;gt; photo): trained on UT Zappos50K dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;edges2handbags&lt;/code&gt; (edge -&amp;gt; photo): trained on Amazon handbags images.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;day2night&lt;/code&gt; (daytime scene -&amp;gt; nighttime scene): trained on around 100 &lt;a href="http://transattr.cs.brown.edu/" rel="nofollow"&gt;webcams&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-setup-training-and-test-data" class="anchor" aria-hidden="true" href="#setup-training-and-test-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup Training and Test data&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-generating-pairs" class="anchor" aria-hidden="true" href="#generating-pairs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating Pairs&lt;/h3&gt;
&lt;p&gt;We provide a python script to generate training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A:&lt;/p&gt;
&lt;p&gt;Create folder &lt;code&gt;/path/to/data&lt;/code&gt; with subfolders &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; should each have their own subfolders &lt;code&gt;train&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, etc. In &lt;code&gt;/path/to/data/A/train&lt;/code&gt;, put training images in style A. In &lt;code&gt;/path/to/data/B/train&lt;/code&gt;, put the corresponding images in style B. Repeat same for other data splits (&lt;code&gt;val&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, etc).&lt;/p&gt;
&lt;p&gt;Corresponding images in a pair {A,B} must be the same size and have the same filename, e.g., &lt;code&gt;/path/to/data/A/train/1.jpg&lt;/code&gt; is considered to correspond to &lt;code&gt;/path/to/data/B/train/1.jpg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once the data is formatted this way, call:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python scripts/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will combine each pair of images (A,B) into a single image file, ready for training.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-notes-on-colorization" class="anchor" aria-hidden="true" href="#notes-on-colorization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notes on Colorization&lt;/h3&gt;
&lt;p&gt;No need to run &lt;code&gt;combine_A_and_B.py&lt;/code&gt; for colorization. Instead, you need to prepare some natural images and set &lt;code&gt;preprocess=colorization&lt;/code&gt; in the script. The program will automatically convert each RGB image into Lab color space, and create  &lt;code&gt;L -&amp;gt; ab&lt;/code&gt; image pair during the training. Also set &lt;code&gt;input_nc=1&lt;/code&gt; and &lt;code&gt;output_nc=2&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-extracting-edges" class="anchor" aria-hidden="true" href="#extracting-edges"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extracting Edges&lt;/h3&gt;
&lt;p&gt;We provide python and Matlab scripts to extract coarse edges from photos. Run &lt;code&gt;scripts/edges/batch_hed.py&lt;/code&gt; to compute &lt;a href="https://github.com/s9xie/hed"&gt;HED&lt;/a&gt; edges. Run &lt;code&gt;scripts/edges/PostprocessHED.m&lt;/code&gt; to simplify edges with additional post-processing steps. Check the code documentation for more details.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-evaluating-labels2photos-on-cityscapes" class="anchor" aria-hidden="true" href="#evaluating-labels2photos-on-cityscapes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluating Labels2Photos on Cityscapes&lt;/h3&gt;
&lt;p&gt;We provide scripts for running the evaluation of the Labels2Photos task on the Cityscapes &lt;strong&gt;validation&lt;/strong&gt; set. We assume that you have installed &lt;code&gt;caffe&lt;/code&gt; (and &lt;code&gt;pycaffe&lt;/code&gt;) in your system. If not, see the &lt;a href="http://caffe.berkeleyvision.org/installation.html" rel="nofollow"&gt;official website&lt;/a&gt; for installation instructions. Once &lt;code&gt;caffe&lt;/code&gt; is successfully installed, download the pre-trained FCN-8s semantic segmentation model (512MB) by running&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash ./scripts/eval_cityscapes/download_fcn8s.sh&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then make sure &lt;code&gt;./scripts/eval_cityscapes/&lt;/code&gt; is in your system's python path. If not, run the following command to add it&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; PYTHONPATH=&lt;span class="pl-smi"&gt;${PYTHONPATH}&lt;/span&gt;:./scripts/eval_cityscapes/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you can run the following command to evaluate your predictions:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/to/output/directory/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Images stored under &lt;code&gt;--result_dir&lt;/code&gt; should contain your model predictions on the Cityscapes &lt;strong&gt;validation&lt;/strong&gt; split, and have the original Cityscapes naming convention (e.g., &lt;code&gt;frankfurt_000001_038418_leftImg8bit.png&lt;/code&gt;). The script will output a text file under &lt;code&gt;--output_dir&lt;/code&gt; containing the metric.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Further notes&lt;/strong&gt;: The pre-trained model is &lt;strong&gt;not&lt;/strong&gt; supposed to work on Cityscapes in the original resolution (1024x2048) as it was trained on 256x256 images that are upsampled to 1024x2048. The purpose of the resizing was to 1) keep the label maps in the original high resolution untouched and 2) avoid the need of changing the standard FCN training code for Cityscapes. To get the &lt;em&gt;ground-truth&lt;/em&gt; numbers in the paper, you need to resize the original Cityscapes images to 256x256 before running the evaluation code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-display-ui" class="anchor" aria-hidden="true" href="#display-ui"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Display UI&lt;/h2&gt;
&lt;p&gt;Optionally, for displaying images during training and test, use the &lt;a href="https://github.com/szym/display"&gt;display package&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install it with: &lt;code&gt;luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Then start the server with: &lt;code&gt;th -ldisplay.start&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open this URL in your browser: &lt;a href="http://localhost:8000" rel="nofollow"&gt;http://localhost:8000&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By default, the server listens on localhost. Pass &lt;code&gt;0.0.0.0&lt;/code&gt; to allow external connections on any interface:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;th -ldisplay.start 8000 0.0.0.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then open &lt;code&gt;http://(hostname):(port)/&lt;/code&gt; in your browser to load the remote desktop.&lt;/p&gt;
&lt;p&gt;L1 error is plotted to the display by default. Set the environment variable &lt;code&gt;display_plot&lt;/code&gt; to a comma-separated list of values &lt;code&gt;errL1&lt;/code&gt;, &lt;code&gt;errG&lt;/code&gt; and &lt;code&gt;errD&lt;/code&gt; to visualize the L1, generator, and discriminator error respectively. For example, to plot only the generator and discriminator errors to the display instead of the default L1 error, set &lt;code&gt;display_plot="errG,errD"&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use this code for your research, please cite our paper &lt;a href="https://arxiv.org/pdf/1611.07004v1.pdf" rel="nofollow"&gt;Image-to-Image Translation Using Conditional Adversarial Networks&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{pix2pix2017,
  title={Image-to-Image Translation with Conditional Adversarial Networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  journal={CVPR},
  year={2017}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-cat-paper-collection" class="anchor" aria-hidden="true" href="#cat-paper-collection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cat Paper Collection&lt;/h2&gt;
&lt;p&gt;If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection:&lt;br&gt;
&lt;a href="https://github.com/junyanz/CatPapers"&gt;[Github]&lt;/a&gt; &lt;a href="http://people.eecs.berkeley.edu/~junyanz/cat/cat_papers.html" rel="nofollow"&gt;[Webpage]&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgments" class="anchor" aria-hidden="true" href="#acknowledgments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Code borrows heavily from &lt;a href="https://github.com/soumith/dcgan.torch"&gt;DCGAN&lt;/a&gt;. The data loader is modified from &lt;a href="https://github.com/soumith/dcgan.torch"&gt;DCGAN&lt;/a&gt; and  &lt;a href="https://github.com/pathak22/context-encoder"&gt;Context-Encoder&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>phillipi</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>karpathy/char-rnn #11 in Lua, This week</title><link>https://github.com/karpathy/char-rnn</link><description>&lt;p&gt;&lt;i&gt;Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="Readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-char-rnn" class="anchor" aria-hidden="true" href="#char-rnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;char-rnn&lt;/h1&gt;
&lt;p&gt;This code implements &lt;strong&gt;multi-layer Recurrent Neural Network&lt;/strong&gt; (RNN, LSTM, and GRU) for training/sampling from character-level language models. In other words the model takes one text file as input and trains a Recurrent Neural Network that learns to predict the next character in a sequence. The RNN can then be used to generate text character by character that will look like the original training data. The context of this code base is described in detail in my &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="nofollow"&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are new to Torch/Lua/Neural Nets, it might be helpful to know that this code is really just a slightly more fancy version of this &lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086"&gt;100-line gist&lt;/a&gt; that I wrote in Python/numpy. The code in this repo additionally: allows for multiple layers, uses an LSTM instead of a vanilla RNN, has more supporting code for model checkpointing, and is of course much more efficient since it uses mini-batches and can run on a GPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-update-torch-rnn" class="anchor" aria-hidden="true" href="#update-torch-rnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Update: torch-rnn&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://cs.stanford.edu/people/jcjohns/" rel="nofollow"&gt;Justin Johnson&lt;/a&gt; (@jcjohnson) recently re-implemented char-rnn from scratch with a much nicer/smaller/cleaner/faster Torch code base. It's under the name &lt;a href="https://github.com/jcjohnson/torch-rnn"&gt;torch-rnn&lt;/a&gt;. It uses Adam for optimization and hard-codes the RNN/LSTM forward/backward passes for space/time efficiency. This also avoids headaches with cloning models in this repo. In other words, torch-rnn should be the default char-rnn implemention to use now instead of the one in this code base.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;p&gt;This code is written in Lua and requires &lt;a href="http://torch.ch/" rel="nofollow"&gt;Torch&lt;/a&gt;. If you're on Ubuntu, installing Torch in your home directory may look something like:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps &lt;span class="pl-k"&gt;|&lt;/span&gt; bash
$ git clone https://github.com/torch/distro.git &lt;span class="pl-k"&gt;~&lt;/span&gt;/torch --recursive
$ &lt;span class="pl-c1"&gt;cd&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/torch&lt;span class="pl-k"&gt;;&lt;/span&gt; 
$ ./install.sh      &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; and enter "yes" at the end to modify your bashrc&lt;/span&gt;
$ &lt;span class="pl-c1"&gt;source&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/.bashrc&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See the Torch installation documentation for more details. After Torch is installed we need to get a few more packages using &lt;a href="https://luarocks.org/" rel="nofollow"&gt;LuaRocks&lt;/a&gt; (which already came with the Torch install). In particular:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ luarocks install nngraph 
$ luarocks install optim
$ luarocks install nn&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you'd like to train on an NVIDIA GPU using CUDA (this can be to about 15x faster), you'll of course need the GPU, and you will have to install the &lt;a href="https://developer.nvidia.com/cuda-toolkit" rel="nofollow"&gt;CUDA Toolkit&lt;/a&gt;. Then get the &lt;code&gt;cutorch&lt;/code&gt; and &lt;code&gt;cunn&lt;/code&gt; packages:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ luarocks install cutorch
$ luarocks install cunn&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you'd like to use OpenCL GPU instead (e.g. ATI cards), you will instead need to install the &lt;code&gt;cltorch&lt;/code&gt; and &lt;code&gt;clnn&lt;/code&gt; packages, and then use the option &lt;code&gt;-opencl 1&lt;/code&gt; during training (&lt;a href="https://github.com/hughperkins/cltorch/issues"&gt;cltorch issues&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ luarocks install cltorch
$ luarocks install clnn&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-data" class="anchor" aria-hidden="true" href="#data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data&lt;/h3&gt;
&lt;p&gt;All input data is stored inside the &lt;code&gt;data/&lt;/code&gt; directory. You'll notice that there is an example dataset included in the repo (in folder &lt;code&gt;data/tinyshakespeare&lt;/code&gt;) which consists of a subset of works of Shakespeare. I'm providing a few more datasets on &lt;a href="http://cs.stanford.edu/people/karpathy/char-rnn/" rel="nofollow"&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your own data&lt;/strong&gt;: If you'd like to use your own data then create a single file &lt;code&gt;input.txt&lt;/code&gt; and place it into a folder in the &lt;code&gt;data/&lt;/code&gt; directory. For example, &lt;code&gt;data/some_folder/input.txt&lt;/code&gt;. The first time you run the training script it will do some preprocessing and write two more convenience cache files into &lt;code&gt;data/some_folder&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset sizes&lt;/strong&gt;: Note that if your data is too small (1MB is already considered very small) the RNN won't learn very effectively. Remember that it has to learn everything completely from scratch. Conversely if your data is large (more than about 2MB), feel confident to increase &lt;code&gt;rnn_size&lt;/code&gt; and train a bigger model (see details of training below). It will work &lt;em&gt;significantly better&lt;/em&gt;. For example with 6MB you can easily go up to &lt;code&gt;rnn_size&lt;/code&gt; 300 or even more. The biggest that fits on my GPU and that I've trained with this code is &lt;code&gt;rnn_size&lt;/code&gt; 700 with &lt;code&gt;num_layers&lt;/code&gt; 3 (2 is default).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h3&gt;
&lt;p&gt;Start training the model using &lt;code&gt;train.lua&lt;/code&gt;. As a sanity check, to run on the included example dataset simply try:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th train.lua -gpuid -1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that here we are setting the flag &lt;code&gt;gpuid&lt;/code&gt; to -1, which tells the code to train using CPU, otherwise it defaults to GPU 0.  There are many other flags for various options. Consult &lt;code&gt;$ th train.lua -help&lt;/code&gt; for comprehensive settings. Here's another example that trains a bigger network and also shows how you can run on your own custom dataset (this already assumes that &lt;code&gt;data/some_folder/input.txt&lt;/code&gt; exists):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th train.lua -data_dir data/some_folder -rnn_size 512 -num_layers 2 -dropout 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Checkpoints.&lt;/strong&gt; While the model is training it will periodically write checkpoint files to the &lt;code&gt;cv&lt;/code&gt; folder. The frequency with which these checkpoints are written is controlled with number of iterations, as specified with the &lt;code&gt;eval_val_every&lt;/code&gt; option (e.g. if this is 1 then a checkpoint is written every iteration). The filename of these checkpoints contains a very important number: the &lt;strong&gt;loss&lt;/strong&gt;. For example, a checkpoint with filename &lt;code&gt;lm_lstm_epoch0.95_2.0681.t7&lt;/code&gt; indicates that at this point the model was on epoch 0.95 (i.e. it has almost done one full pass over the training data), and the loss on validation data was 2.0681. This number is very important because the lower it is, the better the checkpoint works. Once you start to generate data (discussed below), you will want to use the model checkpoint that reports the lowest validation loss. Notice that this might not necessarily be the last checkpoint at the end of training (due to possible overfitting).&lt;/p&gt;
&lt;p&gt;Another important quantities to be aware of are &lt;code&gt;batch_size&lt;/code&gt; (call it B), &lt;code&gt;seq_length&lt;/code&gt; (call it S), and the &lt;code&gt;train_frac&lt;/code&gt; and &lt;code&gt;val_frac&lt;/code&gt; settings. The batch size specifies how many streams of data are processed in parallel at one time. The sequence length specifies the length of each stream, which is also the limit at which the gradients can propagate backwards in time. For example, if &lt;code&gt;seq_length&lt;/code&gt; is 20, then the gradient signal will never backpropagate more than 20 time steps, and the model might not &lt;em&gt;find&lt;/em&gt; dependencies longer than this length in number of characters. Thus, if you have a very difficult dataset where there are a lot of long-term dependencies you will want to increase this setting. Now, if at runtime your input text file has N characters, these first all get split into chunks of size &lt;code&gt;BxS&lt;/code&gt;. These chunks then get allocated across three splits: train/val/test according to the &lt;code&gt;frac&lt;/code&gt; settings. By default &lt;code&gt;train_frac&lt;/code&gt; is 0.95 and &lt;code&gt;val_frac&lt;/code&gt; is 0.05, which means that 95% of our data chunks will be trained on and 5% of the chunks will be used to estimate the validation loss (and hence the generalization). If your data is small, it's possible that with the default settings you'll only have very few chunks in total (for example 100). This is bad: In these cases you may want to decrease batch size or sequence length.&lt;/p&gt;
&lt;p&gt;Note that you can also initialize parameters from a previously saved checkpoint using &lt;code&gt;init_from&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sampling" class="anchor" aria-hidden="true" href="#sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sampling&lt;/h3&gt;
&lt;p&gt;Given a checkpoint file (such as those written to &lt;code&gt;cv&lt;/code&gt;) we can generate new text. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th sample.lua cv/some_checkpoint.t7 -gpuid -1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure that if your checkpoint was trained with GPU it is also sampled from with GPU, or vice versa. Otherwise the code will (currently) complain. As with the train script, see &lt;code&gt;$ th sample.lua -help&lt;/code&gt; for full options. One important one is (for example) &lt;code&gt;-length 10000&lt;/code&gt; which would generate 10,000 characters (default = 2000).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Temperature&lt;/strong&gt;. An important parameter you may want to play with is &lt;code&gt;-temperature&lt;/code&gt;, which takes a number in range (0, 1] (0 not included), default = 1. The temperature is dividing the predicted log probabilities before the Softmax, so lower temperature will cause the model to make more likely, but also more boring and conservative predictions. Higher temperatures cause the model to take more chances and increase diversity of results, but at a cost of more mistakes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Priming&lt;/strong&gt;. It's also possible to prime the model with some starting text using &lt;code&gt;-primetext&lt;/code&gt;. This starts out the RNN with some hardcoded characters to &lt;em&gt;warm&lt;/em&gt; it up with some context before it starts generating text. E.g. a fun primetext might be &lt;code&gt;-primetext "the meaning of life is "&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training with GPU but sampling on CPU&lt;/strong&gt;. Right now the solution is to use the &lt;code&gt;convert_gpu_cpu_checkpoint.lua&lt;/code&gt; script to convert your GPU checkpoint to a CPU checkpoint. In near future you will not have to do this explicitly. E.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ th convert_gpu_cpu_checkpoint.lua cv/lm_lstm_epoch30.00_1.3950.t7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will create a new file &lt;code&gt;cv/lm_lstm_epoch30.00_1.3950.t7_cpu.t7&lt;/code&gt; that you can use with the sample script and with &lt;code&gt;-gpuid -1&lt;/code&gt; for CPU mode.&lt;/p&gt;
&lt;p&gt;Happy sampling!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-monitoring-validation-loss-vs-training-loss" class="anchor" aria-hidden="true" href="#monitoring-validation-loss-vs-training-loss"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monitoring Validation Loss vs. Training Loss&lt;/h3&gt;
&lt;p&gt;If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If your training loss is much lower than validation loss then this means the network might be &lt;strong&gt;overfitting&lt;/strong&gt;. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.&lt;/li&gt;
&lt;li&gt;If your training/validation loss are about equal then your model is &lt;strong&gt;underfitting&lt;/strong&gt;. Increase the size of your model (either number of layers or the raw number of neurons per layer)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-approximate-number-of-parameters" class="anchor" aria-hidden="true" href="#approximate-number-of-parameters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Approximate number of parameters&lt;/h3&gt;
&lt;p&gt;The two most important parameters that control the model are &lt;code&gt;rnn_size&lt;/code&gt; and &lt;code&gt;num_layers&lt;/code&gt;. I would advise that you always use &lt;code&gt;num_layers&lt;/code&gt; of either 2/3. The &lt;code&gt;rnn_size&lt;/code&gt; can be adjusted based on how much data you have. The two important quantities to keep track of here are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of parameters in your model. This is printed when you start training.&lt;/li&gt;
&lt;li&gt;The size of your dataset. 1MB file is approximately 1 million characters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil &amp;gt;&amp;gt; 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make &lt;code&gt;rnn_size&lt;/code&gt; larger.&lt;/li&gt;
&lt;li&gt;I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that heps the validation loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-best-models-strategy" class="anchor" aria-hidden="true" href="#best-models-strategy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Best models strategy&lt;/h3&gt;
&lt;p&gt;The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.&lt;/p&gt;
&lt;p&gt;It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.&lt;/p&gt;
&lt;p&gt;By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-additional-pointers-and-acknowledgements" class="anchor" aria-hidden="true" href="#additional-pointers-and-acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional Pointers and Acknowledgements&lt;/h2&gt;
&lt;p&gt;This code was originally based on Oxford University Machine Learning class &lt;a href="https://github.com/oxford-cs-ml-2015/practical6"&gt;practical 6&lt;/a&gt;, which is in turn based on &lt;a href="https://github.com/wojciechz/learning_to_execute"&gt;learning to execute&lt;/a&gt; code from Wojciech Zaremba. Chunks of it were also developed in collaboration with my labmate &lt;a href="http://cs.stanford.edu/people/jcjohns/" rel="nofollow"&gt;Justin Johnson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To learn more about RNN language models I recommend looking at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks" rel="nofollow"&gt;My recent talk&lt;/a&gt; on char-rnn&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1308.0850" rel="nofollow"&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt; by Alex Graves&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf" rel="nofollow"&gt;Generating Text with Recurrent Neural Networks&lt;/a&gt; by Ilya Sutskever&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf" rel="nofollow"&gt;Tomas Mikolov's Thesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>karpathy</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>norcalli/nvim-colorizer.lua #12 in Lua, This week</title><link>https://github.com/norcalli/nvim-colorizer.lua</link><description>&lt;p&gt;&lt;i&gt;The fastest Neovim colorizer.&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-colorizerlua" class="anchor" aria-hidden="true" href="#colorizerlua"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;colorizer.lua&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://norcalli.github.io/luadoc/nvim-colorizer.lua/modules/colorizer.html" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/60fb5c444d142958b0261a908b886d2d8ff2ff92/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c7561646f632d302e312d626c7565" alt="luadoc" data-canonical-src="https://img.shields.io/badge/luadoc-0.1-blue" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A high-performance color highlighter for Neovim which has &lt;strong&gt;no external dependencies&lt;/strong&gt;! Written in performant Luajit.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/norcalli/github-assets/master/nvim-colorizer.lua-demo-short.gif"&gt;&lt;img src="https://raw.githubusercontent.com/norcalli/github-assets/master/nvim-colorizer.lua-demo-short.gif" alt="Demo.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/norcalli/github-assets/master/nvim-colorizer.lua-demo-short.mp4"&gt;&lt;img src="https://raw.githubusercontent.com/norcalli/github-assets/master/nvim-colorizer.lua-demo-short.mp4" alt="Demo.mp4" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and Usage&lt;/h2&gt;
&lt;p&gt;Requires Neovim &amp;gt;= 0.4.0 and &lt;code&gt;set termguicolors&lt;/code&gt; (I'm looking into relaxing
these constraints). If you don't have true color for your terminal or are
unsure, &lt;a href="https://github.com/termstandard/colors"&gt;read this excellent guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use your plugin manager or clone directly into your &lt;code&gt;runtimepath&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-viml"&gt;&lt;pre&gt;Plug &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;norcalli/nvim-colorizer.lua&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As long as you have &lt;code&gt;malloc()&lt;/code&gt; and &lt;code&gt;free()&lt;/code&gt; on your system, this will work.
Which includes Linux, OSX, and Windows.&lt;/p&gt;
&lt;p&gt;One line setup. This will create an &lt;code&gt;autocmd&lt;/code&gt; for &lt;code&gt;FileType *&lt;/code&gt; to highlight
every filetype.&lt;/p&gt;
&lt;div class="highlight highlight-source-viml"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;lua&lt;/span&gt; require&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;colorizer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.&lt;span class="pl-en"&gt;setup&lt;/span&gt;()&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-why-another-highlighter" class="anchor" aria-hidden="true" href="#why-another-highlighter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why another highlighter?&lt;/h3&gt;
&lt;p&gt;Mostly, &lt;strong&gt;RAW SPEED&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This has no external dependencies, which means you install it and &lt;strong&gt;it just
works&lt;/strong&gt;. Other colorizers typically were synchronous and slow, as well. Being
written with performance in mind and leveraging the excellent LuaJIT and a
handwritten parser, updates can be done in real time. There are plugins such as
&lt;a href="https://github.com/RRethy/vim-hexokinase"&gt;hexokinase&lt;/a&gt; which have good
performance, but it has some difficulty with becoming out of sync. The downside
is that &lt;em&gt;this only works for Neovim&lt;/em&gt;, and that will never change.&lt;/p&gt;
&lt;p&gt;Additionally, having a Lua API that's available means users can use this as a
library to do custom highlighting themselves.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-customization" class="anchor" aria-hidden="true" href="#customization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Customization&lt;/h3&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;  DEFAULT_OPTIONS &lt;span class="pl-k"&gt;=&lt;/span&gt; {
	RGB      &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;;         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; #RGB hex codes&lt;/span&gt;
	RRGGBB   &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;;         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; #RRGGBB hex codes&lt;/span&gt;
	names    &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;;         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; "Name" codes like Blue&lt;/span&gt;
	RRGGBBAA &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; #RRGGBBAA hex codes&lt;/span&gt;
	rgb_fn   &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; CSS rgb() and rgba() functions&lt;/span&gt;
	hsl_fn   &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; CSS hsl() and hsla() functions&lt;/span&gt;
	css      &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Enable all CSS features: rgb_fn, hsl_fn, names, RGB, RRGGBB&lt;/span&gt;
	css_fn   &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Enable all CSS *functions*: rgb_fn, hsl_fn&lt;/span&gt;
	&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Available modes: foreground, background&lt;/span&gt;
	mode     &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;background&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Set the display mode.&lt;/span&gt;
  }&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;MODES:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;foreground&lt;/code&gt;: sets the foreground text color.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;background&lt;/code&gt;: sets the background text color.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For basic setup, you can use a command like the following.&lt;/p&gt;
&lt;div class="highlight highlight-source-lua"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Attaches to every FileType mode&lt;/span&gt;
&lt;span class="pl-c1"&gt;require&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;colorizer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.&lt;span class="pl-c1"&gt;setup&lt;/span&gt;()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Attach to certain Filetypes, add special configuration for `html`&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Use `background` for everything else.&lt;/span&gt;
&lt;span class="pl-c1"&gt;require&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;colorizer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.&lt;span class="pl-c1"&gt;setup&lt;/span&gt; {
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;css&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;javascript&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
  html &lt;span class="pl-k"&gt;=&lt;/span&gt; {
    mode &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;foreground&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
  }
}

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Use the `default_options` as the second parameter, which uses&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; `foreground` for every mode. This is the inverse of the previous&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; setup configuration.&lt;/span&gt;
&lt;span class="pl-c1"&gt;require&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;colorizer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.&lt;span class="pl-c1"&gt;setup&lt;/span&gt;({
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;css&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;javascript&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
  html &lt;span class="pl-k"&gt;=&lt;/span&gt; { mode &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;background&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; };
}, { mode &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;foreground&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; })

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Use the `default_options` as the second parameter, which uses&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; `foreground` for every mode. This is the inverse of the previous&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; setup configuration.&lt;/span&gt;
&lt;span class="pl-c1"&gt;require&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;colorizer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.&lt;span class="pl-c1"&gt;setup&lt;/span&gt; {
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;*&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Highlight all files, but customize some others.&lt;/span&gt;
  css &lt;span class="pl-k"&gt;=&lt;/span&gt; { rgb_fn &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;true&lt;/span&gt;; }; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Enable parsing rgb(...) functions in css.&lt;/span&gt;
  html &lt;span class="pl-k"&gt;=&lt;/span&gt; { names &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;false&lt;/span&gt;; } &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Disable parsing "names" like Blue or Gray&lt;/span&gt;
}

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Exclude some filetypes from highlighting by using `!`&lt;/span&gt;
&lt;span class="pl-c1"&gt;require&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;colorizer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.&lt;span class="pl-c1"&gt;setup&lt;/span&gt; {
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;*&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Highlight all files, but customize some others.&lt;/span&gt;
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;!vim&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Exclude vim from highlighting.&lt;/span&gt;
  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;--&lt;/span&gt; Exclusion Only makes sense if '*' is specified!&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For lower level interface, see the &lt;a href="https://norcalli.github.io/luadoc/nvim-colorizer.lua/modules/colorizer.html" rel="nofollow"&gt;LuaDocs for API details&lt;/a&gt; or use &lt;code&gt;:h colorizer.lua&lt;/code&gt; once installed.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-commands" class="anchor" aria-hidden="true" href="#commands"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Commands&lt;/h2&gt;
&lt;pre lang="help"&gt;&lt;code&gt;|:ColorizerAttachToBuffer|

Attach to the current buffer and start highlighting with the settings as
specified in setup (or the defaults).

If the buffer was already attached (i.e. being highlighted), the settings will
be reloaded with the ones from setup. This is useful for reloading settings
for just one buffer.

|:ColorizerDetachFromBuffer|

Stop highlighting the current buffer (detach).

|:ColorizerReloadAllBuffers|

Reload all buffers that are being highlighted with new settings from the setup
settings (or the defaults). Shortcut for ColorizerAttachToBuffer on every
buffer.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-caveats" class="anchor" aria-hidden="true" href="#caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Caveats&lt;/h2&gt;
&lt;p&gt;If the file you are editing has no filetype, the plugin won't be attached, as
it relies on AutoCmd to do so. You can still make it work by running the
following command: &lt;code&gt;:ColorizerAttachToBuffer&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://github.com/norcalli/nvim-colorizer.lua/issues/9#issuecomment-543742619"&gt;this comment&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add more display modes?&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Use a more space efficient trie implementation.&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Create a COMMON_SETUP which does obvious things like enable &lt;code&gt;rgb_fn&lt;/code&gt; for css&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>norcalli</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>liuzhuang13/DenseNet #13 in Lua, This week</title><link>https://github.com/liuzhuang13/DenseNet</link><description>&lt;p&gt;&lt;i&gt;Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award).&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-densely-connected-convolutional-networks-densenets" class="anchor" aria-hidden="true" href="#densely-connected-convolutional-networks-densenets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Densely Connected Convolutional Networks (DenseNets)&lt;/h1&gt;
&lt;p&gt;This repository contains the code for DenseNet introduced in the following paper&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1608.06993" rel="nofollow"&gt;Densely Connected Convolutional Networks&lt;/a&gt; (CVPR 2017, Best Paper Award)&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cs.cornell.edu/~gaohuang/" rel="nofollow"&gt;Gao Huang&lt;/a&gt;*, &lt;a href="https://liuzhuang13.github.io/" rel="nofollow"&gt;Zhuang Liu&lt;/a&gt;*, &lt;a href="https://lvdmaaten.github.io/" rel="nofollow"&gt;Laurens van der Maaten&lt;/a&gt; and &lt;a href="https://www.cs.cornell.edu/~kilian/" rel="nofollow"&gt;Kilian Weinberger&lt;/a&gt; (* Authors contributed equally).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now with much more memory efficient implementation!&lt;/strong&gt; Please check the &lt;a href="https://arxiv.org/pdf/1707.06990.pdf" rel="nofollow"&gt;technical report&lt;/a&gt; and &lt;a href="https://github.com/liuzhuang13/DenseNet/tree/master/models"&gt;code&lt;/a&gt; for more infomation.&lt;/p&gt;
&lt;p&gt;The code is built on &lt;a href="https://github.com/facebook/fb.resnet.torch"&gt;fb.resnet.torch&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you find DenseNet useful in your research, please consider citing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q },
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-other-implementations" class="anchor" aria-hidden="true" href="#other-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other Implementations&lt;/h2&gt;
&lt;p&gt;Our &lt;a href="https://github.com/liuzhuang13/DenseNetCaffe"&gt;[Caffe]&lt;/a&gt;, Our memory-efficient &lt;a href="https://github.com/Tongcheng/DN_CaffeScript"&gt;[Caffe]&lt;/a&gt;, Our memory-efficient &lt;a href="https://github.com/gpleiss/efficient_densenet_pytorch"&gt;[PyTorch]&lt;/a&gt;,
&lt;a href="https://github.com/andreasveit/densenet-pytorch"&gt;[PyTorch]&lt;/a&gt; by Andreas Veit, &lt;a href="https://github.com/bamos/densenet.pytorch"&gt;[PyTorch]&lt;/a&gt; by Brandon Amos, &lt;a href="https://github.com/baldassarreFe/pytorch-densenet-tiramisu"&gt;[PyTorch]&lt;/a&gt; by Federico Baldassarre,
&lt;a href="https://github.com/Nicatio/Densenet/tree/master/mxnet"&gt;[MXNet]&lt;/a&gt; by Nicatio,
&lt;a href="https://github.com/bruinxiong/densenet.mxnet"&gt;[MXNet]&lt;/a&gt; by Xiong Lin,
&lt;a href="https://github.com/miraclewkf/DenseNet"&gt;[MXNet]&lt;/a&gt; by miraclewkf,
&lt;a href="https://github.com/YixuanLi/densenet-tensorflow"&gt;[Tensorflow]&lt;/a&gt; by Yixuan Li,
&lt;a href="https://github.com/LaurentMazare/deep-models/tree/master/densenet"&gt;[Tensorflow]&lt;/a&gt; by Laurent Mazare,
&lt;a href="https://github.com/ikhlestov/vision_networks"&gt;[Tensorflow]&lt;/a&gt; by Illarion Khlestov,
&lt;a href="https://github.com/Lasagne/Recipes/tree/master/papers/densenet"&gt;[Lasagne]&lt;/a&gt; by Jan Schl√ºter,
&lt;a href="https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet"&gt;[Keras]&lt;/a&gt; by tdeboissiere,&lt;br&gt;
&lt;a href="https://github.com/robertomest/convnet-study"&gt;[Keras]&lt;/a&gt; by Roberto de Moura Estev√£o Filho,
&lt;a href="https://github.com/titu1994/DenseNet"&gt;[Keras]&lt;/a&gt; by Somshubra Majumdar,
&lt;a href="https://github.com/t-hanya/chainer-DenseNet"&gt;[Chainer]&lt;/a&gt; by Toshinori Hanya,
&lt;a href="https://github.com/yasunorikudo/chainer-DenseNet"&gt;[Chainer]&lt;/a&gt; by Yasunori Kudo,
&lt;a href="https://github.com/barrykui/3ddensenet.torch"&gt;[Torch 3D-DenseNet]&lt;/a&gt; by Barry Kui,
&lt;a href="https://github.com/cmasch/densenet"&gt;[Keras]&lt;/a&gt; by Christopher Masch,
&lt;a href="https://github.com/okason97/DenseNet-Tensorflow2"&gt;[Tensorflow2]&lt;/a&gt; by Gaston Rios and Ulises Jeremias Cornejo Fandos.&lt;/p&gt;
&lt;p&gt;Note that we only listed some early implementations here. If you would like to add yours, please submit a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-some-following-up-projects" class="anchor" aria-hidden="true" href="#some-following-up-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Some Following up Projects&lt;/h2&gt;
&lt;ol start="0"&gt;
&lt;li&gt;&lt;a href="https://github.com/gaohuang/MSDNet"&gt;Multi-Scale Dense Convolutional Networks for Efficient Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/szq0214/DSOD"&gt;DSOD: Learning Deeply Supervised Object Detectors from Scratch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ShichenLiu/CondenseNet"&gt;CondenseNet: An Efficient DenseNet using Learned Group Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SimJeg/FC-DenseNet"&gt;Fully Convolutional DenseNets for Semantic Segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Robert-JunWang/Pelee"&gt;Pelee: A Real-Time Object Detection System on Mobile Devices&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#usage"&gt;Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#results-on-cifar"&gt;Results on CIFAR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#results-on-imagenet-and-pretrained-models"&gt;Results on ImageNet and Pretrained Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#updates"&gt;Updates&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each &lt;em&gt;dense block&lt;/em&gt;). For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. This connectivity pattern yields state-of-the-art accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale ILSVRC 2012 (ImageNet) dataset, DenseNet achieves a similar accuracy as ResNet, but using less than half the amount of parameters and roughly half the number of FLOPs.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg"&gt;&lt;img src="https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg" width="480" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: A dense block with 5 layers and growth rate 4.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg"&gt;&lt;img src="https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg" alt="densenet" style="max-width:100%;"&gt;&lt;/a&gt;
Figure 2: A deep DenseNet with three dense blocks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Install Torch and required dependencies like cuDNN. See the instructions &lt;a href="https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md"&gt;here&lt;/a&gt; for a step-by-step guide.&lt;/li&gt;
&lt;li&gt;Clone this repo: &lt;code&gt;git clone https://github.com/liuzhuang13/DenseNet.git&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As an example, the following command trains a DenseNet-BC with depth L=100 and growth rate k=12 on CIFAR-10:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;th main.lua -netType densenet -dataset cifar10 -batchSize 64 -nEpochs 300 -depth 100 -growthRate 12
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As another example, the following command trains a DenseNet-BC with depth L=121 and growth rate k=32 on ImageNet:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;th main.lua -netType densenet -dataset imagenet -data [dataFolder] -batchSize 256 -nEpochs 90 -depth 121 -growthRate 32 -nGPU 4 -nThreads 16 -optMemory 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please refer to &lt;a href="https://github.com/facebook/fb.resnet.torch"&gt;fb.resnet.torch&lt;/a&gt; for data preparation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-densenet-and-densenet-bc" class="anchor" aria-hidden="true" href="#densenet-and-densenet-bc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet and DenseNet-BC&lt;/h3&gt;
&lt;p&gt;By default, the code runs with the DenseNet-BC architecture, which has 1x1 convolutional &lt;em&gt;bottleneck&lt;/em&gt; layers, and &lt;em&gt;compresses&lt;/em&gt; the number of channels at each transition layer by 0.5. To run with the original DenseNet, simply use the options &lt;em&gt;-bottleneck false&lt;/em&gt; and &lt;em&gt;-reduction 1&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-memory-efficient-implementation-newly-added-feature-on-june-6-2017" class="anchor" aria-hidden="true" href="#memory-efficient-implementation-newly-added-feature-on-june-6-2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Memory efficient implementation (newly added feature on June 6, 2017)&lt;/h3&gt;
&lt;p&gt;There is an option &lt;em&gt;-optMemory&lt;/em&gt; which is very useful for reducing GPU memory footprint when training a DenseNet. By default, the value is set to 2, which activates the &lt;em&gt;shareGradInput&lt;/em&gt; function (with small modifications from &lt;a href="https://github.com/facebook/fb.resnet.torch/blob/master/models/init.lua#L102"&gt;here&lt;/a&gt;). There are two extreme memory efficient modes (&lt;em&gt;-optMemory 3&lt;/em&gt; or &lt;em&gt;-optMemory 4&lt;/em&gt;) which use a customized densely connected layer. With &lt;em&gt;-optMemory 4&lt;/em&gt;, the largest 190-layer DenseNet-BC on CIFAR can be trained on a single NVIDIA TitanX GPU (uses 8.3G of 12G) instead of fully using four GPUs with the standard (recursive concatenation) implementation .&lt;/p&gt;
&lt;p&gt;More details about the memory efficient implementation are discussed &lt;a href="https://github.com/liuzhuang13/DenseNet/tree/master/models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-results-on-cifar" class="anchor" aria-hidden="true" href="#results-on-cifar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results on CIFAR&lt;/h2&gt;
&lt;p&gt;The table below shows the results of DenseNets on CIFAR datasets. The "+" mark at the end denotes for standard data augmentation (random crop after zero-padding, and horizontal flip). For a DenseNet model, L denotes its depth and k denotes its growth rate. On CIFAR-10 and CIFAR-100 without data augmentation, a Dropout layer with drop rate 0.2 is introduced after each convolutional layer except the very first one.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Parameters&lt;/th&gt;
&lt;th align="center"&gt;CIFAR-10&lt;/th&gt;
&lt;th align="center"&gt;CIFAR-10+&lt;/th&gt;
&lt;th align="center"&gt;CIFAR-100&lt;/th&gt;
&lt;th align="center"&gt;CIFAR-100+&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet (L=40, k=12)&lt;/td&gt;
&lt;td align="center"&gt;1.0M&lt;/td&gt;
&lt;td align="center"&gt;7.00&lt;/td&gt;
&lt;td align="center"&gt;5.24&lt;/td&gt;
&lt;td align="center"&gt;27.55&lt;/td&gt;
&lt;td align="center"&gt;24.42&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet (L=100, k=12)&lt;/td&gt;
&lt;td align="center"&gt;7.0M&lt;/td&gt;
&lt;td align="center"&gt;5.77&lt;/td&gt;
&lt;td align="center"&gt;4.10&lt;/td&gt;
&lt;td align="center"&gt;23.79&lt;/td&gt;
&lt;td align="center"&gt;20.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet (L=100, k=24)&lt;/td&gt;
&lt;td align="center"&gt;27.2M&lt;/td&gt;
&lt;td align="center"&gt;5.83&lt;/td&gt;
&lt;td align="center"&gt;3.74&lt;/td&gt;
&lt;td align="center"&gt;23.42&lt;/td&gt;
&lt;td align="center"&gt;19.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-BC (L=100, k=12)&lt;/td&gt;
&lt;td align="center"&gt;0.8M&lt;/td&gt;
&lt;td align="center"&gt;5.92&lt;/td&gt;
&lt;td align="center"&gt;4.51&lt;/td&gt;
&lt;td align="center"&gt;24.15&lt;/td&gt;
&lt;td align="center"&gt;22.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-BC (L=250, k=24)&lt;/td&gt;
&lt;td align="center"&gt;15.3M&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;5.19&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;3.62&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;19.64&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;17.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-BC (L=190, k=40)&lt;/td&gt;
&lt;td align="center"&gt;25.6M&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;3.46&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;17.18&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-results-on-imagenet-and-pretrained-models" class="anchor" aria-hidden="true" href="#results-on-imagenet-and-pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results on ImageNet and Pretrained Models&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-torch" class="anchor" aria-hidden="true" href="#torch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Torch&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-models-in-the-original-paper" class="anchor" aria-hidden="true" href="#models-in-the-original-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models in the original paper&lt;/h4&gt;
&lt;p&gt;The Torch models are trained under the same setting as in &lt;a href="https://github.com/facebook/fb.resnet.torch"&gt;fb.resnet.torch&lt;/a&gt;. The error rates shown are 224x224 1-crop test errors.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Network&lt;/th&gt;
&lt;th&gt;Top-1 error&lt;/th&gt;
&lt;th&gt;Torch Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-121 (k=32)&lt;/td&gt;
&lt;td&gt;25.0&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/open?id=0B8ReS-sYUS-HWFViYlVlZk9sdHc" rel="nofollow"&gt;Download (64.5MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-169 (k=32)&lt;/td&gt;
&lt;td&gt;23.6&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/open?id=0B8ReS-sYUS-HY25Dc2VrUGlVSGc" rel="nofollow"&gt;Download (114.4MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-201 (k=32)&lt;/td&gt;
&lt;td&gt;22.5&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/open?id=0B8ReS-sYUS-HaDdpNmlWRjJkd3c" rel="nofollow"&gt;Download (161.8MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-161 (k=48)&lt;/td&gt;
&lt;td&gt;22.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/open?id=0B8ReS-sYUS-HVXp2RExSTmMzZVU" rel="nofollow"&gt;Download (230.8MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-models-in-the-tech-report" class="anchor" aria-hidden="true" href="#models-in-the-tech-report"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models in the tech report&lt;/h4&gt;
&lt;p&gt;More accurate models trained with the memory efficient implementation in the &lt;a href="https://arxiv.org/pdf/1707.06990.pdf" rel="nofollow"&gt;technical report&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Network&lt;/th&gt;
&lt;th&gt;Top-1 error&lt;/th&gt;
&lt;th&gt;Torch Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-264 (k=32)&lt;/td&gt;
&lt;td&gt;22.1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/0By1NwtA2JPGzdVRqOEotMUZrbTA/view?usp=sharing" rel="nofollow"&gt;Download (256MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-232 (k=48)&lt;/td&gt;
&lt;td&gt;21.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/open?id=0By1NwtA2JPGzdkRDaWQ5M3VHTDg" rel="nofollow"&gt;Download (426MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-cosine-264 (k=32)&lt;/td&gt;
&lt;td&gt;21.6&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/0By1NwtA2JPGzRDhxWGo2a3pOTjA/view?usp=sharing" rel="nofollow"&gt;Download (256MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-cosine-264 (k=48)&lt;/td&gt;
&lt;td&gt;20.4&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/0By1NwtA2JPGzcnFDSE1HQVh4c0k/view?usp=sharing" rel="nofollow"&gt;Download (557MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-caffe" class="anchor" aria-hidden="true" href="#caffe"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Caffe&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/shicai/DenseNet-Caffe"&gt;https://github.com/shicai/DenseNet-Caffe&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pytorch" class="anchor" aria-hidden="true" href="#pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://pytorch.org/docs/torchvision/models.html?highlight=densenet" rel="nofollow"&gt;PyTorch documentation on models&lt;/a&gt;. We would like to thank @gpleiss for this nice work in PyTorch.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-keras-tensorflow-and-theano" class="anchor" aria-hidden="true" href="#keras-tensorflow-and-theano"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Keras, Tensorflow and Theano&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/flyyufelix/DenseNet-Keras"&gt;https://github.com/flyyufelix/DenseNet-Keras&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-mxnet" class="anchor" aria-hidden="true" href="#mxnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MXNet&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/miraclewkf/DenseNet"&gt;https://github.com/miraclewkf/DenseNet&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wide-densenet-for-better-timeaccuracy-and-memoryaccuracy-tradeoff" class="anchor" aria-hidden="true" href="#wide-densenet-for-better-timeaccuracy-and-memoryaccuracy-tradeoff"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wide-DenseNet for better Time/Accuracy and Memory/Accuracy Tradeoff&lt;/h2&gt;
&lt;p&gt;If you use DenseNet as a model in your learning task, to reduce the memory and time consumption, we recommend use a wide and shallow DenseNet, following the strategy of &lt;a href="https://github.com/szagoruyko/wide-residual-networks"&gt;wide residual networks&lt;/a&gt;. To obtain a wide DenseNet we set the depth to be smaller (e.g., L=40) and the growthRate to be larger (e.g., k=48).&lt;/p&gt;
&lt;p&gt;We test a set of Wide-DenseNet-BCs and compared the memory and time with the DenseNet-BC (L=100, k=12) shown above. We obtained the statistics using a single TITAN X card, with batch size 64, and without any memory optimization.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Parameters&lt;/th&gt;
&lt;th align="center"&gt;CIFAR-10+&lt;/th&gt;
&lt;th align="center"&gt;CIFAR-100+&lt;/th&gt;
&lt;th align="center"&gt;Time per Iteration&lt;/th&gt;
&lt;th align="center"&gt;Memory&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DenseNet-BC (L=100, k=12)&lt;/td&gt;
&lt;td align="center"&gt;0.8M&lt;/td&gt;
&lt;td align="center"&gt;4.51&lt;/td&gt;
&lt;td align="center"&gt;22.27&lt;/td&gt;
&lt;td align="center"&gt;0.156s&lt;/td&gt;
&lt;td align="center"&gt;5452MB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wide-DenseNet-BC (L=40, k=36)&lt;/td&gt;
&lt;td align="center"&gt;1.5M&lt;/td&gt;
&lt;td align="center"&gt;4.58&lt;/td&gt;
&lt;td align="center"&gt;22.30&lt;/td&gt;
&lt;td align="center"&gt;0.130s&lt;/td&gt;
&lt;td align="center"&gt;4008MB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wide-DenseNet-BC (L=40, k=48)&lt;/td&gt;
&lt;td align="center"&gt;2.7M&lt;/td&gt;
&lt;td align="center"&gt;3.99&lt;/td&gt;
&lt;td align="center"&gt;20.29&lt;/td&gt;
&lt;td align="center"&gt;0.165s&lt;/td&gt;
&lt;td align="center"&gt;5245MB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wide-DenseNet-BC (L=40, k=60)&lt;/td&gt;
&lt;td align="center"&gt;4.3M&lt;/td&gt;
&lt;td align="center"&gt;4.01&lt;/td&gt;
&lt;td align="center"&gt;19.99&lt;/td&gt;
&lt;td align="center"&gt;0.223s&lt;/td&gt;
&lt;td align="center"&gt;6508MB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Obersevations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Wide-DenseNet-BC (L=40, k=36) uses less memory/time while achieves about the same accuracy as DenseNet-BC (L=100, k=12).&lt;/li&gt;
&lt;li&gt;Wide-DenseNet-BC (L=40, k=48) uses about the same memory/time as DenseNet-BC (L=100, k=12), while is much more accurate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, for practical use, we suggest picking one model from those Wide-DenseNet-BCs.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;08/23/2017:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add supporting code, so one can simply &lt;em&gt;git clone&lt;/em&gt; and run.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;06/06/2017:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Support &lt;strong&gt;ultra memory efficient&lt;/strong&gt; training of DenseNet with &lt;em&gt;customized densely connected layer&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support &lt;strong&gt;memory efficient&lt;/strong&gt; training of DenseNet with &lt;em&gt;standard densely connected layer&lt;/em&gt; (recursive concatenation) by fixing the &lt;em&gt;shareGradInput&lt;/em&gt; function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;05/17/2017:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add Wide-DenseNet.&lt;/li&gt;
&lt;li&gt;Add keras, tf, theano link for pretrained models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;04/20/2017:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add usage of models in PyTorch.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;03/29/2017:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add the code for imagenet training.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;12/03/2016:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add Imagenet results and pretrained models.&lt;/li&gt;
&lt;li&gt;Add DenseNet-BC structures.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;liuzhuangthu at gmail.com&lt;br&gt;
gh349 at cornell.edu&lt;br&gt;
Any discussions, suggestions and questions are welcome!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>liuzhuang13</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>DGVaniX/vRP #14 in Lua, This week</title><link>https://github.com/DGVaniX/vRP</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-vrp-scripts-collection" class="anchor" aria-hidden="true" href="#vrp-scripts-collection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;vRP Scripts Collection&lt;/h1&gt;
&lt;p&gt;A collection of scripts/system made for the FiveM multiplayer modification of Grand Theft Auto 5, for the vRP framework.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; &lt;em&gt;DGVaniX (&lt;a href="mailto:dgvanix@mystic.ro"&gt;dgvanix@mystic.ro&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributor:&lt;/strong&gt; &lt;em&gt;Muhammed Imran (&lt;a href="mailto:khanmi@aston.ac.uk"&gt;khanmi@aston.ac.uk&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>DGVaniX</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>Naikzer/HUD-GTAVRP #15 in Lua, This week</title><link>https://github.com/Naikzer/HUD-GTAVRP</link><description>&lt;p&gt;&lt;i&gt;HUD based Features for FiveM Roleplay servers&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-enhanced-hud" class="anchor" aria-hidden="true" href="#enhanced-hud"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enhanced HUD&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Do not use these scripts if you are not comfortable with development.&lt;/strong&gt;
&lt;strong&gt;If you have some issues with code, ask the community on the official &lt;a href="https://forum.fivem.net/t/preview-enhanced-hud/634217" rel="nofollow"&gt;FiveM's topic&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-credits--licence" class="anchor" aria-hidden="true" href="#credits--licence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credits &amp;amp; licence&lt;/h2&gt;
&lt;p&gt;Nicolas Marx (alias &lt;a href="https://twitter.com/naikzer_" rel="nofollow"&gt;Naiko&lt;/a&gt;) is the only owner of these scripts. You are free to use and edit the source code as you want for personal or commercial use.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ui" class="anchor" aria-hidden="true" href="#ui"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;UI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="skincreator"&gt;Character Creator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="menu"&gt;Menu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="speedometer"&gt;Speedometer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Inventory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Messaging service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hungerthirst"&gt;Hunger/Thirst&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-screens" class="anchor" aria-hidden="true" href="#screens"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Screens&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/52666643/61137448-d6c10f80-a4c5-11e9-90ce-5053cf641551.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/52666643/61137448-d6c10f80-a4c5-11e9-90ce-5053cf641551.jpg" alt="GTAV-RP-FiveM-SkinCreator-1" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/52666643/61137456-daed2d00-a4c5-11e9-8373-36432454aa9d.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/52666643/61137456-daed2d00-a4c5-11e9-8373-36432454aa9d.jpg" alt="GTAV-RP-FiveM-SkinCreator-2" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/52666643/61137474-e0e30e00-a4c5-11e9-978e-744b937de828.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/52666643/61137474-e0e30e00-a4c5-11e9-978e-744b937de828.jpg" alt="GTAV-RP-FiveM-Speedometer-HUD" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/52666643/61137488-e50f2b80-a4c5-11e9-8a87-c584f6fcd0a5.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/52666643/61137488-e50f2b80-a4c5-11e9-8a87-c584f6fcd0a5.jpg" alt="GTAV-RP-FiveM-Inventory" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/52666643/61137504-ea6c7600-a4c5-11e9-9792-b600b291f960.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/52666643/61137504-ea6c7600-a4c5-11e9-9792-b600b291f960.jpg" alt="GTAV-RP-FiveM-Menu" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/52666643/61137524-efc9c080-a4c5-11e9-927b-2cb71d963cd6.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/52666643/61137524-efc9c080-a4c5-11e9-927b-2cb71d963cd6.jpg" alt="GTAV-RP-FiveM-MessagingService" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Naikzer</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item><item><title>probable-basilisk/cheatgui #16 in Lua, This week</title><link>https://github.com/probable-basilisk/cheatgui</link><description>&lt;p&gt;&lt;i&gt;The Noita cheat gui mod, now in its own repo&lt;/i&gt;&lt;/p&gt; &lt;p&gt;Last seen &lt;b&gt;2019-10-28&lt;/b&gt;; First seen &lt;b&gt;2019-10-28&lt;/b&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/screenshot.jpg?raw=true"&gt;&lt;img src="/screenshot.jpg?raw=true" alt="Screenshot of the cheat menu as it appears in Noita" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-noita-cheat-gui" class="anchor" aria-hidden="true" href="#noita-cheat-gui"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Noita Cheat GUI&lt;/h1&gt;
&lt;p&gt;A basic in-game cheat menu. Note: if you just want to see the alchemy recipes without all the other cheat functionality, &lt;a href="https://github.com/probable-basilisk/alchemyrecipes"&gt;there is a mod for that&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;You can either download the mod manually or clone this Git repo into the Noita &lt;code&gt;mods&lt;/code&gt; sub-directory.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-recommended-opt-in-to-the-steam-beta-branch-of-noita" class="anchor" aria-hidden="true" href="#recommended-opt-in-to-the-steam-beta-branch-of-noita"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(Recommended: opt-in to the Steam beta branch of Noita)&lt;/h3&gt;
&lt;p&gt;Cheatgui is developed against, and really only tested with, the beta branch. It'll &lt;em&gt;probably&lt;/em&gt; work with the non-beta,
but it's not guaranteed.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-download-manually" class="anchor" aria-hidden="true" href="#download-manually"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download manually&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/probable-basilisk/cheatgui/archive/master.zip"&gt;Download this repo as a .zip&lt;/a&gt;,
and extract into your &lt;code&gt;Noita/mods/&lt;/code&gt; directory (so this README should end up in &lt;code&gt;Noita/mods/cheatgui/README.md&lt;/code&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-or-clone-the-git-repo" class="anchor" aria-hidden="true" href="#or-clone-the-git-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(or) Clone the Git repo&lt;/h3&gt;
&lt;p&gt;You can git clone this repo directly into mods:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd {your Noita install dir}/mods/
git clone https://github.com/probable-basilisk/cheatgui.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-enable-the-mod-in-noita" class="anchor" aria-hidden="true" href="#enable-the-mod-in-noita"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enable the mod in Noita&lt;/h3&gt;
&lt;p&gt;Enable the 'cheatgui' mod through the in-game pause menu.&lt;/p&gt;
&lt;p&gt;You will be prompted that "This mod has requested extra privileges." ‚Äì see the following section 'Note about scary warnings' for details.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-note-about-scary-warnings" class="anchor" aria-hidden="true" href="#note-about-scary-warnings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note about scary warnings&lt;/h4&gt;
&lt;p&gt;Cheatgui asks for scary permissions because it has to do some horrible workarounds in
order to be able to do the type-to-filter thing. Some day I hope the devs will give
us an actual textbox-type GUI component, but until then scary permissions and
hacks are the best we can do. If this permission really scares you, you
can edit &lt;code&gt;mod.xml&lt;/code&gt; to not request the permission, and it'll mostly still work
except for type-to-filter:&lt;/p&gt;
&lt;div class="highlight highlight-text-xml"&gt;&lt;pre&gt;&amp;lt;&lt;span class="pl-ent"&gt;Mod&lt;/span&gt;
	name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Cheatgui&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
	description=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Basic cheat menu&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
	request_no_api_restrictions=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&amp;gt;
&amp;lt;/&lt;span class="pl-ent"&gt;Mod&lt;/span&gt;&amp;gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-note-about-paths" class="anchor" aria-hidden="true" href="#note-about-paths"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note about paths&lt;/h2&gt;
&lt;p&gt;Right now I'm having the mod put all its files into the global &lt;code&gt;data/hax/&lt;/code&gt;
path rather than into the mod-specific path, both because I'm lazy, and
also because I might want to cross-load some of these files from other things.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>probable-basilisk</author><pubDate>Mon, 28 Oct 2019 00:00:00 GMT</pubDate></item></channel></rss>