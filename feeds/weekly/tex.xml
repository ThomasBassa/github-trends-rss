<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, This week</title><link>https://github.com/trending/tex?since=weekly</link><description>The top repositories on GitHub for tex, measured weekly</description><pubDate>Mon, 02 Dec 2019 01:06:04 GMT</pubDate><lastBuildDate>Mon, 02 Dec 2019 01:06:04 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>HarisIqbal88/PlotNeuralNet #1 in TeX, This week</title><link>https://github.com/HarisIqbal88/PlotNeuralNet</link><description>&lt;p&gt;&lt;i&gt;Latex code for making neural networks diagrams&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-plotneuralnet" class="anchor" aria-hidden="true" href="#plotneuralnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PlotNeuralNet&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.5281/zenodo.2526396" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/89c8c312f40c2d237b2319aececd5740a147b11c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e323532363339362e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.2526396.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Latex code for drawing neural networks for reports and presentation. Have a look into examples to see how they are made. Additionally, lets consolidate any improvements that you make and fix any bugs to help more people with this code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Install the following packages on Ubuntu.
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 16.04&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ubuntu 18.04.2
Base on this &lt;a href="https://gist.github.com/rain1024/98dd5e2c6c8c28f9ea9d"&gt;website&lt;/a&gt;, please install the following packages.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-base
sudo apt-get install texlive-fonts-recommended
sudo apt-get install texlive-fonts-extra
sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Execute the example as followed.
&lt;pre&gt;&lt;code&gt;cd pyexamples/
bash ../tikzmake.sh test_simple
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Python interface&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add easy legend functionality&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add more layer shapes like TruncatedPyramid, 2DSheet etc&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add examples for RNN and likes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-latex-usage" class="anchor" aria-hidden="true" href="#latex-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latex usage&lt;/h2&gt;
&lt;p&gt;See &lt;a href="examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for usage.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python-usage" class="anchor" aria-hidden="true" href="#python-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python usage&lt;/h2&gt;
&lt;p&gt;First, create a new directory and a new Python file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mkdir my_project
$ cd my_project
vim my_arch.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the following code to your new file:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sys
sys.path.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;../&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;from&lt;/span&gt; pycore.tikzeng &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; defined your arch&lt;/span&gt;
arch &lt;span class="pl-k"&gt;=&lt;/span&gt; [
    to_head( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;..&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; ),
    to_cor(),
    to_begin(),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;512&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Conv(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(1,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt; ),
    to_connection( &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;conv2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_Pool(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;offset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(0,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;to&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(conv2-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;depth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;28&lt;/span&gt;, &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;),
    to_SoftMax(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt; ,&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(3,0,0)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;(pool1-east)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;caption&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;SOFT&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;  ),
    to_connection(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;pool2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;soft1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;),
    to_end()
    ]

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;main&lt;/span&gt;():
    namefile &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;str&lt;/span&gt;(sys.argv[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]).split(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
    to_generate(arch, namefile &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;.tex&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; )

&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
    main()&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, run the program as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bash ../tikzmake.sh my_arch
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;Following are some network representations:&lt;/p&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-8" class="anchor" aria-hidden="true" href="#fcn-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-8&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-fcn-32" class="anchor" aria-hidden="true" href="#fcn-32"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FCN-32&lt;/h6&gt;
&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png"&gt;&lt;img src="https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png" width="85%" height="85%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h6 align="center"&gt;&lt;a id="user-content-holistically-nested-edge-detection" class="anchor" aria-hidden="true" href="#holistically-nested-edge-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Holistically-Nested Edge Detection&lt;/h6&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>HarisIqbal88</author><guid isPermaLink="false">https://github.com/HarisIqbal88/PlotNeuralNet</guid><pubDate>Mon, 02 Dec 2019 00:01:00 GMT</pubDate></item><item><title>abhat222/Data-Science--Cheat-Sheet #2 in TeX, This week</title><link>https://github.com/abhat222/Data-Science--Cheat-Sheet</link><description>&lt;p&gt;&lt;i&gt;Cheat Sheets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;
Taken from &lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet"&gt;&lt;strong&gt;Link to original GitHub page&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-data-science-cheatsheets" class="anchor" aria-hidden="true" href="#data-science-cheatsheets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Science Cheatsheets&lt;/h1&gt;
&lt;p&gt;List of Data Science Cheatsheets :&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="Artificial%20Intelligence/README.md"&gt;Artificial Intelligence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Big%20Data/README.md"&gt;Big Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Engineering/README.md"&gt;Data Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Mining/README.md"&gt;Data Mining&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Science/README.md"&gt;Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Visualization/README.md"&gt;Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Data%20Warehouse/README.md"&gt;Data Warehouse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Deep%20Learning/README.md"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DevOps/README.md"&gt;DevOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Docker%20&amp;amp;%20Kubernetes/README.md"&gt;Docker &amp;amp; Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Excel/README.md"&gt;Excel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Git/README.md"&gt;Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Images//README.md"&gt;Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Interview%20Questions/README.md"&gt;Interview Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Linux/README.md"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet#machine-learning"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Mathematics/README.md"&gt;Mathematics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Matlab/README.md"&gt;Matlab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="NLP/README.md"&gt;NLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Numpy/README.md"&gt;Numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Ordinary%20Differential%20Equations/README.md"&gt;Ordinary Differential Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Pandas/README.md"&gt;Pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Probability/README.md"&gt;Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Python/README.md"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Scala/README.md"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="SQL/README.md"&gt;SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Statistics/README.md"&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/Python_cheatsheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/python_cheatsheet.PNG?" alt="Illustration" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/spaCy.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/SpaCy.PNG?" alt="Illustration" width="415px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Python Cheat Sheet&lt;/b&gt;                                                                  &lt;b&gt;Spacy Cheat Sheet&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/pandas_cheat_sheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/DA_Pandas.PNG?" alt="Illustration" height="320px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/Reading%20and%20Writing%20data%20with%20PANDAS.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/RW_Panda.PNG?" alt="Illustration" width="415px" height="320px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Data Analysis with Pandas&lt;b&gt;                                              &lt;b&gt;Pandas (Reading and Writing Data)&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/python-cheatsheets-ds.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/PythonForDS.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Numpy/100_numpy_exercises.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/101_numpy_exercises.jpg?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                   &lt;b&gt;Python for Data Science&lt;b&gt;                                                                  &lt;b&gt;100 Numpy Exercises&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Python/Python%20for%20Data%20Analysis.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/pythonforDA.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/Pandas/pandas-10min.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/Pandas_10mins.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                             &lt;b&gt;Python for Data Analysis&lt;b&gt;                                                              &lt;b&gt;10 Minutes to Pandas&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-r-language" class="anchor" aria-hidden="true" href="#r-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;R Language&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/R%20Cheat%20Sheet/R%20Cheat%20Sheets.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/R_cheatsheet.PNG?" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;      &lt;/a&gt;&lt;a href="https://github.com/abhat222/Data-Science--Cheat-Sheet/blob/master/R%20Cheat%20Sheet/R%20Programming%20Cheat%20Sheet.pdf"&gt;&lt;img src="https://github.com/abhat222/Data-Science--Cheat-Sheet/raw/master/Images/R.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;R (Basics &amp;amp; Advanced)&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-machine-learning" class="anchor" aria-hidden="true" href="#machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Machine%20Learning/Applied%20Machine%20Learning%20Problem%20Solving%20Framework.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/abhat222/Data-Science--Cheat-Sheet/master/Machine%20Learning/Applied%20Machine%20Learning%20Problem%20Solving%20Framework.jpg" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;       &lt;/a&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Machine%20Learning/Cheat%20Sheet%20Algorithms%20for%20Supervised%20and%20Unsupervised%20Learning.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/supervised-and-unsupervised-learning.PNG?" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;Machine Learning&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Deep%20Learning/Coursera%20Deep%20Learning%20course%20Notes.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/coursera-deep-learning.PNG" alt="Illustration" height="300px" width="415px" style="max-width:100%;"&gt;       &lt;/a&gt;&lt;a href="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/blob/master/Deep%20Learning/super-cheatsheet-deep-learning.pdf"&gt;&lt;img src="https://github.com/cyborg-girl/Data-Science--Cheat-Sheet/raw/master/Images/super-cheatsheet-deep-learning.PNG" alt="Illustration" width="415px" height="300px" style="max-width:100%;"&gt;&lt;/a&gt;                                    &lt;b&gt;R Cheat Sheet&lt;b&gt;                                                                     &lt;b&gt;Machine Learning&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/p&gt;&lt;b&gt;&lt;b&gt;&lt;b&gt;
&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/b&gt;&lt;/article&gt;&lt;/div&gt;</description><author>abhat222</author><guid isPermaLink="false">https://github.com/abhat222/Data-Science--Cheat-Sheet</guid><pubDate>Mon, 02 Dec 2019 00:02:00 GMT</pubDate></item><item><title>lib-pku/libpku #3 in TeX, This week</title><link>https://github.com/lib-pku/libpku</link><description>&lt;p&gt;&lt;i&gt;贵校课程资料民间整理&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-libpku---贵校课程资料民间整理" class="anchor" aria-hidden="true" href="#libpku---贵校课程资料民间整理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;libpku - 贵校课程资料民间整理&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-preface" class="anchor" aria-hidden="true" href="#preface"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preface&lt;/h2&gt;
&lt;p&gt;（引用自 &lt;a href="https://github.com/QSCTech/zju-icicles"&gt;QSCTech/zju-icicles&lt;/a&gt; ）&lt;/p&gt;
&lt;p&gt;来到一所大学，从第一次接触许多课，直到一门一门完成，这个过程中我们时常收集起许多资料和情报。&lt;/p&gt;
&lt;p&gt;有些是需要在网上搜索的电子书，每次见到一门新课程，Google 一下教材名称，有的可以立即找到，有的却是要花费许多眼力；有些是历年试卷或者 A4 纸，前人精心收集制作，抱着能对他人有用的想法公开，却需要在各个群或者私下中摸索以至于从学长手中代代相传；有些是上完一门课才恍然领悟的技巧，原来这门课重点如此，当初本可以更轻松地完成得更好……&lt;/p&gt;
&lt;p&gt;我也曾很努力地收集各种课程资料，但到最后，某些重要信息的得到却往往依然是纯属偶然。这种状态时常令我感到后怕与不安。我也曾在课程结束后终于有了些许方法与总结，但这些想法无处诉说，最终只能把花费时间与精力才换来的经验耗散在了漫漫的遗忘之中。&lt;/p&gt;
&lt;p&gt;我为这一年一年，这么多人孤军奋战的重复劳动感到不平。&lt;/p&gt;
&lt;p&gt;我希望能够将这些隐晦的、不确定的、口口相传的资料和经验，变为公开的、易于获取的和大家能够共同完善、积累的共享资料。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我希望只要是前人走过的弯路，后人就不必再走。&lt;/strong&gt; 这是我的信念，也是我建立这个项目的原因。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;使用方法：访问 &lt;a href="https://lib-pku.github.io/" rel="nofollow"&gt;https://lib-pku.github.io/&lt;/a&gt; ，点击资料链接即可下载。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://minhaskamal.github.io/DownGit/#/home" rel="nofollow"&gt;https://minhaskamal.github.io/DownGit/#/home&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;——因为很重要所以说了三遍&lt;/p&gt;
&lt;p&gt;Issue、PR、纠错、资料、选课/考试攻略，完全欢迎！&lt;/p&gt;
&lt;p&gt;来自大家的关注、维护和贡献，才是让这个攻略继续存在的动力~&lt;/p&gt;
&lt;p&gt;对于课程的评价可写在对应课程文件夹的 &lt;code&gt;README.md&lt;/code&gt; 中。如果想上传课件（请确保无版权问题），推荐使用 PDF 格式，避免系统差。&lt;/p&gt;
&lt;p&gt;由于本项目体积很大，故推荐采用在 &lt;strong&gt;GitHub Web 端直接上传&lt;/strong&gt; 的方式，具体操作如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先 Fork 本项目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;上传文件到已有文件夹：打开对应文件夹，点击绿色 Download 按钮旁的 upload，上传你的文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传文件到新文件夹：打开任意文件夹，点击绿色 Download 按钮旁的 upload，&lt;strong&gt;把浏览器地址栏中文件夹名称改为你想要新建的文件夹名称，然后回车&lt;/strong&gt;，上传你的文件。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;提交 PR：Fork 本项目，然后在 GitHub 网页端点击 Upload File 上传文件，发起 PR 即可。留意一下项目的文件组织喔。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;或者也可以直接附加在 &lt;strong&gt;Issue&lt;/strong&gt; 中，由维护者进行添加。&lt;/p&gt;
&lt;p&gt;或者也可以发送邮件至 &lt;strong&gt;&lt;a href="mailto:libpku@protonmail.com"&gt;libpku@protonmail.com&lt;/a&gt;&lt;/strong&gt; ，由维护者进行添加。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;这不是北京大学图书馆。
我们也不对项目中信息的准确性或真实性做任何承诺。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果有侵权情况，麻烦您发送必要的信息至 &lt;a href="mailto:libpku@protonmail.com"&gt;libpku@protonmail.com&lt;/a&gt; ，带来不便还请您谅解。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;资料来自网络，相关权利由原作者所有，这个 repo 仅用于收集现有资料。&lt;/p&gt;
&lt;p&gt;当然，我们不会为收集到的资料收费，或是尝试收取捐赠。&lt;/p&gt;
&lt;p&gt;我们只是尝试为后来的同学节省一些时间。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-works" class="anchor" aria-hidden="true" href="#related-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related Works&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/QSCTech/zju-icicles"&gt;浙江大学课程攻略共享计划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/martinwu42/project-hover"&gt;气垫船计划——免费、去中心化的北京大学往年题资料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/EECS-PKU-XSB/Shared-learning-materials"&gt;北京大学信科学生会学术部资料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tongtzeho/PKUCourse"&gt;北大计算机课程大作业&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PKUanonym/REKCARC-TSC-UHT"&gt;清华大学计算机系课程攻略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zjdx1998/seucourseshare"&gt;东南大学课程共享计划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/USTC-Resource/USTC-Course"&gt;中国科学技术大学计算机学院课程资源&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CoolPhilChen/SJTU-Courses/"&gt;上海交通大学课程资料分享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sysuexam/SYSU-Exam"&gt;中山大学课程资料分享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/idealclover/NJU-Review-Materials"&gt;南京大学课程复习资料&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CooperNiu/ZZU-Courses-Resource"&gt;郑州大学课程复习资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(more to be added....)&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lib-pku</author><guid isPermaLink="false">https://github.com/lib-pku/libpku</guid><pubDate>Mon, 02 Dec 2019 00:03:00 GMT</pubDate></item><item><title>soulmachine/leetcode #4 in TeX, This week</title><link>https://github.com/soulmachine/leetcode</link><description>&lt;p&gt;&lt;i&gt;LeetCode题解，151道题完整版&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-leetcode题解" class="anchor" aria-hidden="true" href="#leetcode题解"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;#LeetCode题解&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-在线阅读" class="anchor" aria-hidden="true" href="#在线阅读"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;在线阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.gitbook.com/book/soulmachine/algorithm-essentials/" rel="nofollow"&gt;https://www.gitbook.com/book/soulmachine/algorithm-essentials/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;##PDF下载
&lt;a href="https://github.com/soulmachine/leetcode/raw/master/C%2B%2B/leetcode-cpp.pdf"&gt;LeetCode题解(C++版).pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;C++ 文件夹下是C++版，内容一模一样，代码是用C++写的。&lt;/p&gt;
&lt;p&gt;Java 文件夹下是Java版，目前正在编写中，由于拖延症，不知道猴年马月能完成。&lt;/p&gt;
&lt;p&gt;##LaTeX模板
本书使用的是陈硕开源的&lt;a href="https://github.com/chenshuo/typeset"&gt;模板&lt;/a&gt;。这个模板制作精良，很有taste，感谢陈硕 :)&lt;/p&gt;
&lt;p&gt;##在Windows下编译&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;安装Tex Live 2015 &lt;a href="http://www.tug.org/texlive/" rel="nofollow"&gt;http://www.tug.org/texlive/&lt;/a&gt;。把bin目录例如&lt;code&gt;D:\texlive\2015\bin\win32&lt;/code&gt;加入PATH环境变量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装字体。这个LaTex模板总共使用了10个字体，下载地址 &lt;a href="https://pan.baidu.com/s/1eRFJXnW" rel="nofollow"&gt;https://pan.baidu.com/s/1eRFJXnW&lt;/a&gt; ，有的字体Windows自带了，有的字体Ubuntu自带了，但都不全，还是一次性安装完所有字体比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装TeXstudio &lt;a href="http://texstudio.sourceforge.net/" rel="nofollow"&gt;http://texstudio.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(可选)启动Tex Live Manager，更新所有已安装的软件包。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置TeXstudio。&lt;/p&gt;
&lt;p&gt;启动Texstudio，选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Commands&lt;/code&gt;，XeLaTex 设置为 &lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode %.tex&lt;/code&gt;；&lt;/p&gt;
&lt;p&gt;选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Build&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Build &amp;amp; View 由默认的 PDF Chain 改为 Compile &amp;amp; View；&lt;/p&gt;
&lt;p&gt;Default Compiler 由默认的PdfLaTex 修改为 XeLaTex ；&lt;/p&gt;
&lt;p&gt;PDF Viewer 改为 “Internal PDF Viewer(windowed)”，这样预览时会弹出一个独立的窗口，这样比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编译。用TeXstudio打开&lt;code&gt;typeset.tex&lt;/code&gt;，点击界面上的绿色箭头就可以开始编译了。&lt;/p&gt;
&lt;p&gt;在下方的窗口可以看到TeXstudio正在使用的编译命令是&lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode "typeset".tex&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;##在Ubuntu下编译&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;安装Tex Live 2015 &lt;a href="http://www.tug.org/texlive/" rel="nofollow"&gt;http://www.tug.org/texlive/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.1. 下载TexLive 2015 的ISO 光盘，地址 &lt;a href="http://www.tug.org/texlive/acquire-iso.html" rel="nofollow"&gt;http://www.tug.org/texlive/acquire-iso.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1.2 mount 光盘，&lt;code&gt;sudo ./install-tl&lt;/code&gt; 开始安装&lt;/p&gt;
&lt;p&gt;1.3 加入环境变量&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; sudo vi /etc/profile
 export PATH=$PATH:/usr/local/texlive/2015/bin/x86_64-linux
 export MANPATH=$MANPATH:/usr/local/texlive/2015/texmf-dist/doc/man
 export INFPATH=$INFPATH:/usr/local/texlive/2015/texmf-dist/doc/info
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装字体。这个LaTex模板总共使用了10个字体，下载地址 &lt;a href="https://pan.baidu.com/s/1eRFJXnW" rel="nofollow"&gt;https://pan.baidu.com/s/1eRFJXnW&lt;/a&gt; ，有的字体Windows自带了，有的字体Ubuntu自带了，但都不全，还是一次性安装完所有字体比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装TeXstudio &lt;a href="http://texstudio.sourceforge.net/" rel="nofollow"&gt;http://texstudio.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置TeXstudio。&lt;/p&gt;
&lt;p&gt;启动Texstudio，选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Commands&lt;/code&gt;，XeLaTex 设置为 &lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode %.tex&lt;/code&gt;；&lt;/p&gt;
&lt;p&gt;选择 &lt;code&gt;Options--&amp;gt;Configure Texstudio--&amp;gt;Build&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Build &amp;amp; View 由默认的 PDF Chain 改为 Compile &amp;amp; View；&lt;/p&gt;
&lt;p&gt;Default Compiler 由默认的PdfLaTex 修改为 XeLaTex ；&lt;/p&gt;
&lt;p&gt;PDF Viewer 改为 “Internal PDF Viewer(windowed)”，这样预览时会弹出一个独立的窗口，这样比较方便。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编译。用TeXstudio打开&lt;code&gt;typeset.tex&lt;/code&gt;，点击界面上的绿色箭头就可以开始编译了。&lt;/p&gt;
&lt;p&gt;在下方的窗口可以看到TeXstudio正在使用的编译命令是&lt;code&gt;xelatex -synctex=1 -interaction=nonstopmode "typeset".tex&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;懒人版镜像。如果不想进行上面繁琐的安装过程，我做好了一个Ubuntu VMware虚拟机镜像，已经装好了 TexLive 2015, TexStudio和字体(详细的安装日志见压缩包注释)，开箱即用，下载地址 &lt;a href="http://pan.baidu.com/s/1cLWkgA" rel="nofollow"&gt;http://pan.baidu.com/s/1cLWkgA&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;##如何贡献代码
编译通过后，就具备了完整的LaTeX编译环境了。&lt;/p&gt;
&lt;p&gt;本书模板已经写好了，基本上不需要很多LaTeX知识就可以动手了。&lt;/p&gt;
&lt;p&gt;欢迎给本书添加内容或纠正错误，在自己本地编译成PDF，预览没问题后，就可以发pull request过来了。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-qq群" class="anchor" aria-hidden="true" href="#qq群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QQ群&lt;/h2&gt;
&lt;p&gt;237669375&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-小密圈" class="anchor" aria-hidden="true" href="#小密圈"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;小密圈&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/silicon-job.jpeg"&gt;&lt;img src="%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/silicon-job.jpeg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-algohub" class="anchor" aria-hidden="true" href="#algohub"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlgoHub&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.algohub.org" rel="nofollow"&gt;https://www.algohub.org&lt;/a&gt; 是我建立的一个刷题网站，即将上线，敬请期待&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-纸质书" class="anchor" aria-hidden="true" href="#纸质书"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;纸质书&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;本书即将由电子工业出版社出版，敬请期待&lt;/strong&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>soulmachine</author><guid isPermaLink="false">https://github.com/soulmachine/leetcode</guid><pubDate>Mon, 02 Dec 2019 00:04:00 GMT</pubDate></item><item><title>devonfw-forge/devonfw4flutter #5 in TeX, This week</title><link>https://github.com/devonfw-forge/devonfw4flutter</link><description>&lt;p&gt;&lt;i&gt; A guide aiming to bridge the gap between the absolute Flutter basics and clean, structured Flutter Development&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/devonfw-forge/devonfw4flutter/wiki//images/banner.png"&gt;&lt;img src="https://github.com/devonfw-forge/devonfw4flutter/wiki//images/banner.png" alt="Banner" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Guide is published in the &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki"&gt;Wiki of this repository&lt;/a&gt;. This Repository just hold a copy of the Wiki to make the commit history more readable. The README.md is a copy of the &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki"&gt;Introduction chapter&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Page Table of Contents&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#introduction"&gt;Introduction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-goal-of-this-guide"&gt;The Goal of this Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#who-is-this-guide-for"&gt;Who is this Guide for?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#topics-that-will-be-covered"&gt;Topics that will be Covered&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#creation-context"&gt;Creation Context&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#structure"&gt;Structure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#my-sources"&gt;My Sources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#creation-process"&gt;Creation Process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-the-goal-of-this-guide" class="anchor" aria-hidden="true" href="#the-goal-of-this-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Goal of this Guide&lt;/h2&gt;
&lt;p&gt;This guide aims to bridge the gap between the absolute Flutter &lt;a href="https://flutter.dev/" rel="nofollow"&gt;[1]&lt;/a&gt; basics and clean, structured Flutter development. It should bring you from the basics of knowing how to build an app with Flutter to an understanding of how to do it &lt;em&gt;properly&lt;/em&gt;. Or at least show you one possible way to make large scale Flutter projects clean and manageable.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-who-is-this-guide-for" class="anchor" aria-hidden="true" href="#who-is-this-guide-for"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Who is this Guide for?&lt;/h2&gt;
&lt;p&gt;For people with a basic knowledge of the Flutter Framework. I recommend following this tutorial by the Flutter team &lt;a href="https://flutter.dev/docs/get-started/codelab" rel="nofollow"&gt;[2]&lt;/a&gt;. It will walk you through developing your first Flutter application. You should also have a basic understanding of the Dart programming language &lt;a href="https://dart.dev/" rel="nofollow"&gt;[3]&lt;/a&gt;. No worries, it is very similar to Java &lt;a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" rel="nofollow"&gt;[4]&lt;/a&gt;, Kotlin &lt;a href="https://kotlinlang.org/" rel="nofollow"&gt;[5]&lt;/a&gt; and JavaScript &lt;a href="https://www.ecma-international.org/publications/standards/Ecma-262.htm" rel="nofollow"&gt;[6]&lt;/a&gt;. So if you know 1 or 2 of those languages you should be fine.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-topics-that-will-be-covered" class="anchor" aria-hidden="true" href="#topics-that-will-be-covered"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Topics that will be Covered&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A brief introduction to the &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/100-The-Flutter-Framework"&gt;Flutter Framework&lt;/a&gt; in general:
&lt;ul&gt;
&lt;li&gt;How the &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/110-Under-the-Hood"&gt;underlying technology&lt;/a&gt; works,&lt;/li&gt;
&lt;li&gt;how it’s &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/120-Thinking-Declaratively"&gt;programming style&lt;/a&gt; is little different from other frameworks,&lt;/li&gt;
&lt;li&gt;how Flutter apps are &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/130-The-Widget-Tree"&gt;structured&lt;/a&gt; on an abstract level and,&lt;/li&gt;
&lt;li&gt;how &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/140-Asynchronous-Flutter"&gt;asynchrony&lt;/a&gt; and communication with the web can be implemented.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A showcase of possible &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/210-State-Management-Alternatives"&gt;architectural styles&lt;/a&gt; you can use to build your app and
&lt;ul&gt;
&lt;li&gt;an &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/220-BLoC"&gt;in-depth guide&lt;/a&gt; on one of those possibilities (BLoC Pattern &lt;a href="https://www.youtube.com/watch?v=PLHln7wHgPE" rel="nofollow"&gt;[7]&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/300-Testing"&gt;test&lt;/a&gt; your app.&lt;/li&gt;
&lt;li&gt;Some &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/400-Conventions"&gt;conventions and best practices&lt;/a&gt; for Dart, and the Flutter Framework in general.&lt;/li&gt;
&lt;li&gt;My personal &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/500-Conclusion"&gt;evaluation of the framework&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-creation-context" class="anchor" aria-hidden="true" href="#creation-context"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creation Context&lt;/h2&gt;
&lt;p&gt;This guide was written by a student in the Bachelor of Science Program “Computer Science and Media Technology” at Technical University Cologne &lt;a href="https://www.th-koeln.de/en/homepage_26.php" rel="nofollow"&gt;[8]&lt;/a&gt;, and it was created for one of the modules in that Bachelor. In addition to this, the guide was written in collaboration with DevonFw &lt;a href="https://devonfw.com/index.html" rel="nofollow"&gt;[9]&lt;/a&gt;. DevonFw released a guide on building an application with Angular &lt;a href="https://github.com/devonfw/devon4ng"&gt;[10]&lt;/a&gt; in May of 2019, this guide is meant to be the Flutter version of that.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-structure" class="anchor" aria-hidden="true" href="#structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Structure&lt;/h2&gt;
&lt;p&gt;The guide is designed to be read in order, from chapter 0 (this one) to chapter 5. Code examples throughout the chapters will mainly be taken from Wisgen &lt;a href="https://github.com/Fasust/wisgen"&gt;[11]&lt;/a&gt;, an example Flutter application that was specifically built for the purposes of this guide. If you want to search for any specific terms in the guide, you can use &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/gfm-guide"&gt;this page&lt;/a&gt;. It is all chapters of the guide combined into one page. There is going to be a few common symbols throughout the guide, this is what they stand for:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Symbol&lt;/th&gt;
&lt;th align="left"&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="orange_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d9.png"&gt;📙&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="left"&gt;Definition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;g-emoji class="g-emoji" alias="clock1" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f550.png"&gt;🕐&lt;/g-emoji&gt;&lt;/td&gt;
&lt;td align="left"&gt;Shortened version (TLDR)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;⚠&lt;/td&gt;
&lt;td align="left"&gt;Important&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-my-sources" class="anchor" aria-hidden="true" href="#my-sources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;My Sources&lt;/h2&gt;
&lt;p&gt;I am basing this guide on a combination of conference talks, blog articles by respected Flutter developers, the official documentation, scientific papers that cover cross-platform mobile development in general and many other sources. All sources used in the guide are listed in chapter &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/600-References"&gt;&lt;em&gt;6 References&lt;/em&gt;&lt;/a&gt;. To put that theoretical knowledge into practice, I built the Wisgen application &lt;a href="https://github.com/Fasust/wisgen"&gt;[11]&lt;/a&gt; using the Flutter Framework, the BLoC Pattern &lt;a href="https://www.youtube.com/watch?v=PLHln7wHgPE" rel="nofollow"&gt;[7]&lt;/a&gt;, and a four-layered architecture.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-creation-process" class="anchor" aria-hidden="true" href="#creation-process"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creation Process&lt;/h2&gt;
&lt;p&gt;If your in interested in how this guide was created, how Wisgen was built, how a bridge between a citation software and Markdown was realized, or any other details about the creation process, check out the &lt;a href="https://github.com/devonfw-forge/devonfw4flutter/blob/master/Meta-Documentation.pdf"&gt;Meta-Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p align="right"&gt;&lt;a href="https://github.com/devonfw-forge/devonfw4flutter/wiki/100-The-Flutter-Framework"&gt;Next Chapter: The Flutter Framework &amp;gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;a href="#"&gt;Back to Top&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>devonfw-forge</author><guid isPermaLink="false">https://github.com/devonfw-forge/devonfw4flutter</guid><pubDate>Mon, 02 Dec 2019 00:05:00 GMT</pubDate></item><item><title>jindongwang/transferlearning-tutorial #6 in TeX, This week</title><link>https://github.com/jindongwang/transferlearning-tutorial</link><description>&lt;p&gt;&lt;i&gt;《迁移学习简明手册》LaTex源码&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-迁移学习简明手册" class="anchor" aria-hidden="true" href="#迁移学习简明手册"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;《迁移学习简明手册》&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6a40b4913be1852a0fe4a896c5912412202f9169/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667" alt="MIT License" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/jindongwang/transferlearning-tutorial"&gt;&lt;img src="https://camo.githubusercontent.com/79aacff803391d52db37e3a25682ff6d8bed70e9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4769746875622d56312e302d3531396464392e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/badge/Github-V1.0-519dd9.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/jindongwang/transferlearning-tutorial/issues"&gt;&lt;img src="https://camo.githubusercontent.com/003d89d87fd7db8619fd5437d30ed940022a57a5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6d6d6974732d312d3531396464392e737667" alt="GitHub commits" data-canonical-src="https://img.shields.io/badge/commits-1-519dd9.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d2e15fcc22b8462ec721b6cfa902ab11b72c6f0b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d5465782d6f72616e67652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/d2e15fcc22b8462ec721b6cfa902ab11b72c6f0b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d5465782d6f72616e67652e737667" alt="" data-canonical-src="https://img.shields.io/badge/language-Tex-orange.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这是《迁移学习简明手册》的LaTex源码。欢迎有兴趣的学者一起来贡献维护。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-im-considering-a-big-update-to-this-tutorial-later-this-year-if-you-are-interested-please-feel-free-to-contact-me" class="anchor" aria-hidden="true" href="#im-considering-a-big-update-to-this-tutorial-later-this-year-if-you-are-interested-please-feel-free-to-contact-me"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I'm considering a big update to this tutorial later this year. If you are interested, please feel free to contact me!&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-下载" class="anchor" aria-hidden="true" href="#下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;V1.1版本: &lt;a href="http://jd92.wang/assets/files/transfer_learning_tutorial_wjd.pdf" rel="nofollow"&gt;地址1&lt;/a&gt;   &lt;a href="https://github.com/jindongwang/transferlearning-tutorial/releases"&gt;地址2&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;开发版：&lt;a href="https://www.jianguoyun.com/p/DSI5P2YQjKnsBRiU_0w" rel="nofollow"&gt;地址&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://t.cn/RmasEFe" rel="nofollow"&gt;手册网站与勘误表&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-意见与建议" class="anchor" aria-hidden="true" href="#意见与建议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;意见与建议&lt;/h2&gt;
&lt;p&gt;对于不足和错误之处，以及新的意见，欢迎到&lt;a href="https://github.com/jindongwang/transferlearning-tutorial/issues/6"&gt;这里&lt;/a&gt;留言！&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-引用" class="anchor" aria-hidden="true" href="#引用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;引用&lt;/h3&gt;
&lt;p&gt;可以按如下方式进行引用：&lt;/p&gt;
&lt;p&gt;Jindong Wang et al. Transfer Learning Tutorial. 2018.&lt;/p&gt;
&lt;p&gt;王晋东等. 迁移学习简明手册. 2018.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-bibtex" class="anchor" aria-hidden="true" href="#bibtex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BibTeX&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;@misc{WangTLTutorial2018,
	Author = {Jindon Wang et al.},
	Title = {Transfer Learning Tutorial},
	Url = {https://github.com/jindongwang/transferlearning-tutorial},
	Year = {2018},
}

@misc{WangTLTutorial2018cn,
	Author = {王晋东等},
	Title = {迁移学习简明手册},
	Url = {https://github.com/jindongwang/transferlearning-tutorial},
	Year = {2018},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-参与贡献方式" class="anchor" aria-hidden="true" href="#参与贡献方式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参与贡献方式&lt;/h2&gt;
&lt;p&gt;以下部分为参与贡献的详细说明。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-编译方式" class="anchor" aria-hidden="true" href="#编译方式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;编译方式&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在任何装有较新版TexLive的电脑上，首先选择&lt;code&gt;xelatex&lt;/code&gt;引擎进行第一次编译&lt;/li&gt;
&lt;li&gt;再选择&lt;code&gt;BibTeX&lt;/code&gt;编译一次生成参考文献&lt;/li&gt;
&lt;li&gt;最后选择&lt;code&gt;xelatex&lt;/code&gt;引擎进行第三次编译即可生成带书签的PDF文档&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-主要文件介绍" class="anchor" aria-hidden="true" href="#主要文件介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;主要文件介绍&lt;/h3&gt;
&lt;p&gt;以下是本手册的主要文件与其内容介绍：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;章节&lt;/th&gt;
&lt;th align="center"&gt;名称&lt;/th&gt;
&lt;th align="center"&gt;文件名&lt;/th&gt;
&lt;th align="center"&gt;内容&lt;/th&gt;
&lt;th align="center"&gt;状态&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;主文件&lt;/td&gt;
&lt;td align="center"&gt;..&lt;/td&gt;
&lt;td align="center"&gt;main.tex&lt;/td&gt;
&lt;td align="center"&gt;题目、摘要、推荐语、目录、文件组织&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;写在前面等&lt;/td&gt;
&lt;td align="center"&gt;..&lt;/td&gt;
&lt;td align="center"&gt;prefix.tex&lt;/td&gt;
&lt;td align="center"&gt;写在前面、致谢、说明&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第1章&lt;/td&gt;
&lt;td align="center"&gt;迁移学习基本概念&lt;/td&gt;
&lt;td align="center"&gt;introduction.tex&lt;/td&gt;
&lt;td align="center"&gt;迁移学习基本介绍&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第2章&lt;/td&gt;
&lt;td align="center"&gt;迁移学习的研究领域&lt;/td&gt;
&lt;td align="center"&gt;research_area.tex&lt;/td&gt;
&lt;td align="center"&gt;研究领域&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第3章&lt;/td&gt;
&lt;td align="center"&gt;迁移学习的应用&lt;/td&gt;
&lt;td align="center"&gt;application.tex&lt;/td&gt;
&lt;td align="center"&gt;应用&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第4章&lt;/td&gt;
&lt;td align="center"&gt;基础知识&lt;/td&gt;
&lt;td align="center"&gt;basic.tex&lt;/td&gt;
&lt;td align="center"&gt;基础知识&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第5章&lt;/td&gt;
&lt;td align="center"&gt;迁移学习的基本方法&lt;/td&gt;
&lt;td align="center"&gt;method.tex&lt;/td&gt;
&lt;td align="center"&gt;四类基本方法&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第6章&lt;/td&gt;
&lt;td align="center"&gt;第一类方法：数据分布自适应&lt;/td&gt;
&lt;td align="center"&gt;distributionadapt.tex&lt;/td&gt;
&lt;td align="center"&gt;数据分布自适应&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第7章&lt;/td&gt;
&lt;td align="center"&gt;第二类方法：特征选择&lt;/td&gt;
&lt;td align="center"&gt;featureselect.tex&lt;/td&gt;
&lt;td align="center"&gt;特征选择&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第8章&lt;/td&gt;
&lt;td align="center"&gt;第三类方法：子空间学习&lt;/td&gt;
&lt;td align="center"&gt;subspacelearn.tex&lt;/td&gt;
&lt;td align="center"&gt;子空间学习法&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第9章&lt;/td&gt;
&lt;td align="center"&gt;深度迁移学习&lt;/td&gt;
&lt;td align="center"&gt;deep.tex&lt;/td&gt;
&lt;td align="center"&gt;深度和对抗迁移方法&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第10章&lt;/td&gt;
&lt;td align="center"&gt;上手实践&lt;/td&gt;
&lt;td align="center"&gt;practice.tex&lt;/td&gt;
&lt;td align="center"&gt;实践教程&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第11章&lt;/td&gt;
&lt;td align="center"&gt;迁移学习前沿&lt;/td&gt;
&lt;td align="center"&gt;future.tex&lt;/td&gt;
&lt;td align="center"&gt;展望&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第12章&lt;/td&gt;
&lt;td align="center"&gt;总结语&lt;/td&gt;
&lt;td align="center"&gt;conclusion&lt;/td&gt;
&lt;td align="center"&gt;总结&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;第13章&lt;/td&gt;
&lt;td align="center"&gt;附录&lt;/td&gt;
&lt;td align="center"&gt;appendix.tex&lt;/td&gt;
&lt;td align="center"&gt;附录&lt;/td&gt;
&lt;td align="center"&gt;V1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;所有的源码均在&lt;code&gt;src&lt;/code&gt;目录下。其中，除去主文件&lt;code&gt;main.tex&lt;/code&gt;外，所有章节都在&lt;code&gt;chaps/&lt;/code&gt;文件夹下。&lt;/p&gt;
&lt;p&gt;所有的图片都在&lt;code&gt;figures/&lt;/code&gt;文件夹下。推荐实用eps或pdf格式高清文件。&lt;/p&gt;
&lt;p&gt;参考文献采用&lt;code&gt;bibtex&lt;/code&gt;方式，见&lt;code&gt;refs.bib&lt;/code&gt;文件。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-未来计划" class="anchor" aria-hidden="true" href="#未来计划"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;未来计划&lt;/h3&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 丰富和完善现有的V1.0&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 单独写一章介绍基于实例的迁移学习方法(instance-based)，以及相关的instance selection method，如比较经典的tradaboost等&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 深度和对抗迁移学习方法分成两章，再结合有关文献进行补充&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; 上手实践部分增加对深度方法的说明&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; ……&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-参与方式" class="anchor" aria-hidden="true" href="#参与方式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参与方式&lt;/h3&gt;
&lt;p&gt;欢迎有兴趣的学者一起加入，让手册更完善！现阶段有2个branch：master用于开发和完善，V1.0是稳定的1.0版本。后续可根据进度增加更多的branch。&lt;/p&gt;
&lt;p&gt;具体参与方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在&lt;a href="https://github.com/jindongwang/transferlearning-tutorial/issues/1"&gt;这个issue&lt;/a&gt;下留言你的Github账号和邮箱，我将你添加到协作者中&lt;/li&gt;
&lt;li&gt;直接fork，然后将你的修改提交pull request&lt;/li&gt;
&lt;li&gt;如果不熟悉git，可直接下载本目录，然后将你修改的部分发给我(&lt;a href="mailto:jindongwang@outlook.com"&gt;jindongwang@outlook.com&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;有任何问题，均可以提交issue&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;贡献之后：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在下面的贡献者信息中加入自己的信息。&lt;/li&gt;
&lt;li&gt;如果是对错误的更正，在&lt;code&gt;web/transfer_tutorial.html&lt;/code&gt;中的"勘误表"部分加入勘误信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-如何提交-pull-request" class="anchor" aria-hidden="true" href="#如何提交-pull-request"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;如何提交 Pull Request&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-准备工作" class="anchor" aria-hidden="true" href="#准备工作"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;准备工作&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;在原始代码库上点 Fork ，在自己的账户下开一个分支代码库&lt;/li&gt;
&lt;li&gt;将自己的分支克隆到本地
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;git clone https://github.com/(YOUR_GIT_NAME)/transferlearning-tutorial.git&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;将本机自己的 fork 的代码库和 GitHub 上原始作者的代码库 ，即上游（ upstream ）连接起来
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;git remote add upstream https://github.com/jindongwang/transferlearning-tutorial.git&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-提交代码" class="anchor" aria-hidden="true" href="#提交代码"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;提交代码&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;每次修改之前，先将自己的本地分支同步到上游分支的最新状态
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;git pull upstream master&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;作出修改后 push 到自己名下的代码库&lt;/li&gt;
&lt;li&gt;在 GitHub 网页端自己的账户下看到最新修改后点击 New pull request 即可&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-贡献者信息" class="anchor" aria-hidden="true" href="#贡献者信息"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;贡献者信息&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jindongwang"&gt;@jindongwang&lt;/a&gt; 王晋东，中国科学院计算技术研究所&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Godblesswz"&gt;@Godblesswz&lt;/a&gt; 万震，重庆大学&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jindongwang</author><guid isPermaLink="false">https://github.com/jindongwang/transferlearning-tutorial</guid><pubDate>Mon, 02 Dec 2019 00:06:00 GMT</pubDate></item><item><title>zhanwen/MathModel #7 in TeX, This week</title><link>https://github.com/zhanwen/MathModel</link><description>&lt;p&gt;&lt;i&gt;研究生数学建模，数学建模竞赛优秀论文，数学建模算法，LaTeX论文模板，算法思维导图，参考书籍，Matlab软件教程，PPT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-数学建模资源" class="anchor" aria-hidden="true" href="#数学建模资源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模资源&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-2019-年研究生数模" class="anchor" aria-hidden="true" href="#2019-年研究生数模"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019 年研究生数模&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20191111-又是双十一比赛结果经过一个半月的评审在这一天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.11.11 又是双十一，比赛结果经过一个半月的评审，在这一天公布了&lt;a href="2019%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^o)&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-20199192019923-比赛已经结束大家耐心等待获奖吧oo" class="anchor" aria-hidden="true" href="#20199192019923-比赛已经结束大家耐心等待获奖吧oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019.9.19—2019.9.23 比赛已经结束，大家耐心等待获奖吧（(o^^o)）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-论文提交md5使用方法" class="anchor" aria-hidden="true" href="#论文提交md5使用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文提交（MD5使用方法）&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;MD5文件校验和使用说明：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md"&gt;&lt;strong&gt;MD5文件校验和使用说明&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-论文模版更新" class="anchor" aria-hidden="true" href="#论文模版更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;论文模版更新&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;LaTex 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/2019%E5%B9%B4Latex%E6%A8%A1%E7%89%88.zip"&gt;&lt;strong&gt;LaTex 论文模版&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;Word 论文模版：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/blob/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/%E2%80%9C%E5%8D%8E%E4%B8%BA%E6%9D%AF%E2%80%9D%E7%AC%AC%E5%8D%81%E5%85%AD%E5%B1%8A%E4%B8%AD%E5%9B%BD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AE%BA%E6%96%87%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83.doc"&gt;&lt;strong&gt;Word 论文模版（已更新最新）&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;LaTex 论文模版使用方式：&lt;/em&gt; &lt;a href="https://github.com/zhanwen/MathModel/tree/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/latex_note.md"&gt;&lt;strong&gt;如何编译 Latex 文件&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-下载方式仓库比较大建议单个文件下载" class="anchor" aria-hidden="true" href="#下载方式仓库比较大建议单个文件下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载方式(仓库比较大，建议单个文件下载)&lt;/h4&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo2.gif"&gt;&lt;img src="./images/downloaddemo2.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt;  
&lt;p&gt;&lt;em&gt;主题：&lt;/em&gt; &lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;&lt;strong&gt;“华为杯”第十六届中国研究生数学建模竞赛&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
&lt;em&gt;报名时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月10日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;审核时间：&lt;/em&gt; &lt;strong&gt;2019年6月1日8:00——9月12日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;交费时间：&lt;/em&gt; &lt;strong&gt;2019年7月1日8:00——9月15日17:00&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;比赛时间：&lt;/em&gt; &lt;strong&gt;2019年9月19日8:00——9月23日12:00&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-官网报名地址官网地址" class="anchor" aria-hidden="true" href="#官网报名地址官网地址"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;官网报名地址：&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;官网地址&lt;/a&gt;&lt;/h4&gt;
&lt;hr&gt;  
&lt;h3&gt;&lt;a id="user-content-2018915-祝大家比赛开心-_" class="anchor" aria-hidden="true" href="#2018915-祝大家比赛开心-_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.15 祝大家比赛开心 （^_^）&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-2018919-比赛已经结束大家耐心等待获奖吧o" class="anchor" aria-hidden="true" href="#2018919-比赛已经结束大家耐心等待获奖吧o"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.9.19 比赛已经结束，大家耐心等待获奖吧（^o^）&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo" class="anchor" aria-hidden="true" href="#20181111-比赛结果经过一个半月的评审终于在昨天公布了获奖名单大家的努力相信都会有所收获余生还有很多有意义的事情需要我们去做让我们一起努力oo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018.11.11 比赛结果经过一个半月的评审，终于在昨天公布了&lt;a href="https://github.com/zhanwen/MathModel/tree/master/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/2018%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95"&gt;获奖名单&lt;/a&gt;，大家的努力相信都会有所收获。余生还有很多有意义的事情需要我们去做，让我们一起努力。(o^^o)&lt;/h4&gt;
&lt;hr&gt;  
&lt;h4&gt;&lt;a id="user-content-更新添加比赛官网地址戳这里" class="anchor" aria-hidden="true" href="#更新添加比赛官网地址戳这里"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新/添加比赛官网地址&lt;a href="https://cpipc.chinadegrees.cn/" rel="nofollow"&gt;戳这里&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/4" rel="nofollow"&gt;数学建模竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/6" rel="nofollow"&gt;电子设计竞赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8101510bd" rel="nofollow"&gt;人工智能创新大赛&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8934810be" rel="nofollow"&gt;机器人创新设计大赛&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-下载与使用由于整个项目直接下载比较慢可以看方式四" class="anchor" aria-hidden="true" href="#下载与使用由于整个项目直接下载比较慢可以看方式四"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载与使用（由于整个项目直接下载比较慢，可以看方式四）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;方式一：使用 &lt;code&gt;git&lt;/code&gt; 下载。&lt;br&gt;
&lt;code&gt;git clone https://github.com/zhanwen/MathModel.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方式二：直接下载压缩包。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/downloaddemo.gif"&gt;&lt;img src="./images/downloaddemo.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式三
&lt;ul&gt;
&lt;li&gt;可以单个文件下载，选择自己需要的某篇论文，直接在对应的页面点击下载即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./images/download3.gif"&gt;&lt;img src="./images/download3.gif" height="250" width="500" align="center" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方式四：百度云下载（推荐）
&lt;ul&gt;
&lt;li&gt;使用百度云下载，正常的客户端会出现限速，导致下载的很慢，这里给大家推荐一个绕过百度云下载限速的方式。具体怎么下载，请参照 &lt;a href="https://github.com/iikira/BaiduPCS-Go"&gt;绕过限速&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;该项目的百度云链接 &lt;a href="https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw" rel="nofollow"&gt;https://pan.baidu.com/s/1UnngHxNR0EVoyBpKlPxFAw&lt;/a&gt;，密码：&lt;code&gt;ea2n&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛试题" class="anchor" aria-hidden="true" href="#国赛试题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛试题&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2019%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2019年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2018年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2017%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2017年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2016%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2016年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2015%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2015年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2014%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2014年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2013%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2013年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2012%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2012年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2011%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2011年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2010%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2010年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2009%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2009年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2008%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2008年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2007%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2007年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2006%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2006年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2005%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2005年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2004%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98"&gt;2004年研究生数学建模竞赛试题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-国赛论文" class="anchor" aria-hidden="true" href="#国赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;国赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2018年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：关于跳台跳水体型系数设置的建模分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：光传送网建模与价值评估&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：对恐怖袭击事件记录数据的量化分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星高度计海面高度异常资料获取潮汐调和常数方法及应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多无人机对组网雷达的协同干扰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：航站楼扩增评估&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2017年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：无人机在抢险救灾中的优化运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：面向下一代光通信的 VCSEL 激光器仿真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：航班恢复问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于监控视频的前景目标提取&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：多波次导弹发射中的规划问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：地下物流系统网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2016年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：多无人机协同任务规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：具有遗传性疾病和性状的遗传位点分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：基于无线通信基站的室内三维定位问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：军事行动避空侦察的时机和路线选择&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：粮食最低收购价政策问题研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2015年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：水面舰艇编队防空和信息化战争评估模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：数据的多流形结构分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：移动通信中的无线信道“指纹”特征建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：面向节能的单/多列车优化决策问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：数控加工刀具运动的优化控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：旅游路线规划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2014年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：小鼠视觉感受区电位信号(LFP)与视觉刺激之间的关系研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：机动目标的跟踪与反跟踪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：无线通信中的快时变信道建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：人体营养健康角度的中国果蔬发展战略研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：乘用车物流运输计划问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2013年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：变循环发动机部件法建模及优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：功率放大器非线性特性及预失真模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：微蜂窝环境中无线接收信号的特性分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：空气中PM2.5问题的研究 attachment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E"&gt;E题：中等收入定位与人口度量模型研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F"&gt;F题：可持续的中国城乡居民养老保险体系的数学模型研究&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2012年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基因识别问题及其算法实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：基于卫星无源探测的空间飞行器主动段轨道估计与误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：有杆抽油系统的数学建模及诊断&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：基于卫星云图的风失场(云导风)度量模型与算法探讨&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2011年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：基于光的波粒二象性一种猜想的数学仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：吸波材料与微波暗室问题的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：小麦发育后期茎杆抗倒性的数学模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：房地产行业的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2010年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：确定肿瘤的重要基因信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：与封堵渍口有关的重物落水后运动过程的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：神经元的形态分类和识别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：特殊工件磨削加工的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2009年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：我国就业人数或城镇登记失业率的数学建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：枪弹头痕迹，自动比对方法的研究&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：多传感器数据融合与航迹预测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：110 警车配置及巡逻方案&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2008年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：汶川地震中唐家山堪塞湖泄洪问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：城市道路交通信号实时控制问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;C题：货运列车的编组调度问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;D题：中央空调系统节能设计问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2007年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：建立食品卫生安全保障体系数学模型及改进模型的若干理论问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：械臂运动路径设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：探讨提高高速公路路面质量的改进方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：邮政运输网络中的邮路规划和邮车调运&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2006年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Ad Hoc 网络中的区域划分和资源分配问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：确定高精度参数问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：维修线性流量阀时的内筒设计问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：学生面试问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2005年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：Highway Traveling time Estimate and Optimal Routing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：空中加油&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：城市交通管理中的出租车规划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：仓库容量有限条件下的随机存贮管理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87"&gt;2004年优秀论文&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A"&gt;A题：发现黄球并定位&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B"&gt;B题：使用下料问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C"&gt;C题：售后服务数据的运用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D"&gt;D题：研究生录取问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-美赛论文" class="anchor" aria-hidden="true" href="#美赛论文"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;美赛论文&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2017年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2016年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2015年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2014年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2013年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2012年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2011年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2010年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2009年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2008年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2007年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2006年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2005年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86"&gt;2004年特等奖论文&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法" class="anchor" aria-hidden="true" href="#数学建模算法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AE%97%E6%B3%95"&gt;经典算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95"&gt;现代算法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BB%BF%E7%9C%9F"&gt;计算机仿真&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95"&gt;粒子群算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"&gt;马尔可夫链&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95"&gt;蒙特卡洛法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E6%B3%95"&gt;模拟退火法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"&gt;神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90"&gt;小波分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"&gt;遗传算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-教材及课件" class="anchor" aria-hidden="true" href="#教材及课件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;教材及课件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6"&gt;国防科技术大学&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AF%BE%E4%BB%B6/PPT%E8%AF%BE%E4%BB%B6"&gt;浙江大学课件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-数学建模算法思维导图" class="anchor" aria-hidden="true" href="#数学建模算法思维导图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数学建模算法思维导图&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Mind"&gt;思维导图&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-matlab-入门教程" class="anchor" aria-hidden="true" href="#matlab-入门教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Matlab 入门教程&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanwen/MathModel/tree/master/Matlab%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B"&gt;Matlab入门和在线性代数中的应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt; 
&lt;h3&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;其中有些内容整理自互联网，如有侵权，请联系，我将及时处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-个人微信公众号" class="anchor" aria-hidden="true" href="#个人微信公众号"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;个人微信公众号&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dotzhang&lt;/code&gt;：一名不羁的学僧，我的世界不只有学术。一条迷途的咸鱼，正在游向属于它的天地！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/images/donate/common.jpg"&gt;&lt;img src="/images/donate/common.jpg" width="150" height="150" alt="weixin" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-赞助和支持" class="anchor" aria-hidden="true" href="#赞助和支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;赞助和支持&lt;/h3&gt;
&lt;p&gt;这些内容都是我花了不少时间整理出来的, 如果你觉得它对你很有帮助, 请你也分享给需要学习的朋友们。如果你看好我的内容分享, 也可以考虑适当的赞助打赏, 让我有更多的动力去继续分享更好的内容给大家。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;微信&lt;/th&gt;
&lt;th&gt;支付宝&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/weixinpay.jpg"&gt;&lt;img src="images/donate/weixinpay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/alipay.jpg"&gt;&lt;img src="images/donate/alipay.jpg" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-联系" class="anchor" aria-hidden="true" href="#联系"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;联系&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Email：&lt;a href="https://mail.google.com/" rel="nofollow"&gt;hanwenme@gmail.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微  信（有任何问题都可以直接怼我）：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/donate/wechat.png"&gt;&lt;img src="images/donate/wechat.png" width="150" height="150" alt="pay check" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zhanwen</author><guid isPermaLink="false">https://github.com/zhanwen/MathModel</guid><pubDate>Mon, 02 Dec 2019 00:07:00 GMT</pubDate></item><item><title>mohuangrui/ucasthesis #8 in TeX, This week</title><link>https://github.com/mohuangrui/ucasthesis</link><description>&lt;p&gt;&lt;i&gt; [最新样式] 中国科学院大学学位论文 LaTeX 模板  LaTeX Thesis Template for the University of Chinese Academy of Sciences &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ucasthesis-国科大学位论文-latex-模板-最新样式" class="anchor" aria-hidden="true" href="#ucasthesis-国科大学位论文-latex-模板-最新样式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;ucasthesis&lt;/code&gt; 国科大学位论文 LaTeX 模板 [最新样式]&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-模板下载" class="anchor" aria-hidden="true" href="#模板下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板下载&lt;/h2&gt;
&lt;p&gt;请在页面右边点击：&lt;strong&gt;Clone or download -&amp;gt; Download Zip&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-重要建议" class="anchor" aria-hidden="true" href="#重要建议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要建议&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;关于 LaTeX 的知识性问题，请查阅 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki"&gt;LaTeX 知识小站&lt;/a&gt; 和 &lt;a href="https://en.wikibooks.org/wiki/LaTeX" rel="nofollow"&gt;LaTeX Wikibook&lt;/a&gt;，如发问需前往 &lt;a href="https://github.com/CTeX-org/forum"&gt;CTeX Forum&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;关于 ucasthesis 编译和设计的问题，请先读 &lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;，如发问需遵从&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"&gt;提问流程&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;使用邮件传播 ucasthesis 时，请先删除 &lt;code&gt;artratex.bat&lt;/code&gt; 以防范 Dos 脚本的潜在风险。&lt;/li&gt;
&lt;li&gt;开题报告请见：&lt;a href="https://github.com/mohuangrui/ucasproposal"&gt;ucasproposal: 中国科学院大学开题报告 LaTeX 模板&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;书脊制作请见：&lt;a href="https://github.com/mohuangrui/latexspine"&gt;latexspine: LaTeX 书脊模板&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-模板简介" class="anchor" aria-hidden="true" href="#模板简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板简介&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 为撰写中国科学院大学&lt;strong&gt;本&lt;/strong&gt;、&lt;strong&gt;硕&lt;/strong&gt;、&lt;strong&gt;博&lt;/strong&gt;学位论文和&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;&lt;strong&gt;博后&lt;/strong&gt;&lt;/a&gt;报告的 LaTeX 模版。ucasthesis 提供了简单明了的&lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;。无论你是否具有 LaTeX 使用经验，都可较为轻松地使用以完成学位论文的撰写和排版。谢谢大家的测试、反馈和支持，我们一起的努力让 ucasthesis 非常荣幸地得到了国科大本科部陆晴老师、本科部学位办丁云云老师和中科院数学与系统科学研究院吴凌云研究员的支持，并得到吴凌云学长在 &lt;a href="http://www.ctex.org/HomePage" rel="nofollow"&gt;CTEX&lt;/a&gt; 的发布。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;考虑到许多同学可能缺乏 LaTeX 使用经验，ucasthesis 将 LaTeX 的复杂性高度封装，开放出简单的接口，以便轻易使用。同时，对用 LaTeX 撰写论文的一些主要难题，如制图、制表、文献索引等，进行了详细说明，并提供了相应的代码样本，理解了上述问题后，对于初学者而言，使用此模板撰写学位论文将不存在实质性的困难。所以，如果你是初学者，请不要直接放弃，因为同样为初学者的我，十分明白让 LaTeX 简单易用的重要性，而这正是 ucasthesis 所追求和体现的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;此中国科学院大学学位论文模板 ucasthesis 基于中科院数学与系统科学研究院吴凌云研究员的 CASthesis 模板发展而来。当前 ucasthesis 模板满足最新的中国科学院大学学位论文撰写要求和封面设定。兼顾操作系统：Windows，Linux，MacOS 和 LaTeX 编译引擎：pdflatex，xelatex，lualatex。支持中文书签、中文渲染、中文粗体显示、拷贝 PDF 中的文本到其他文本编辑器等特性（&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE"&gt;Windows 系统 PDF 拷贝乱码的解决方案需见：字体配置&lt;/a&gt;）。此外，对模板的文档结构进行了精心设计，撰写了编译脚本提高模板的易用性和使用效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 的目标在于简化学位论文的撰写，利用 LaTeX 格式与内容分离的特征，模板将格式设计好后，作者可只需关注论文内容。 同时，ucasthesis 有着整洁一致的代码结构和扼要的注解，对文档的仔细阅读可为初学者提供一个学习 LaTeX 的窗口。此外，模板的架构十分注重通用性，事实上，ucasthesis 不仅是国科大学位论文模板，同时，通过少量修改即可成为使用 LaTeX 撰写中英文文章或书籍的通用模板，并为使用者的个性化设定提供了接口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-重要通知" class="anchor" aria-hidden="true" href="#重要通知"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要通知&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2019-10-21&lt;/code&gt; 模板样式进行了修改，请查看下面的修改描述，以决定是否需要更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/198"&gt;huiwenzhang, issue #198&lt;/a&gt; 修复&lt;code&gt;mainmatter&lt;/code&gt;下&lt;code&gt;\chapter*&lt;/code&gt;的页眉错误。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/195"&gt;Fancy0609, muzimuzhi, issue #195&lt;/a&gt; 调整由&lt;code&gt;AutoFakeBold&lt;/code&gt;控制的伪粗体加粗程度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-11&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/190"&gt;Pantrick, issue #190&lt;/a&gt; 采用 &lt;a href="https://github.com/muzimuzhi"&gt;muzimuzhi&lt;/a&gt; 提供的方法实现&lt;code&gt;\advisor{}&lt;/code&gt;和&lt;code&gt;\institute{}&lt;/code&gt;的自动换行功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-08-01&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/183"&gt;vectorliu, issue #183&lt;/a&gt; 修改英文模式下的&lt;code&gt;plain&lt;/code&gt;选项为&lt;code&gt;scheme=plain&lt;/code&gt;以消除对&lt;code&gt;Algorithm&lt;/code&gt;样式的修改。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-15&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/177"&gt;HaorenWang, issue #177&lt;/a&gt; 调整矢量、矩阵、张量字体样式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-09&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/170"&gt;DRjy, issue #170&lt;/a&gt; 轻微缩减目录中编号与标题的间距；根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/174"&gt;e71828, issue #174&lt;/a&gt; 轻微增加页眉中编号与标题的间距。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-05-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/169"&gt;CDMA2019, issue #169&lt;/a&gt; 提供横排图表环境下页眉页脚的横排，具体使用见 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%A8%AA%E6%8E%92%E5%9B%BE%E8%A1%A8"&gt;横排图表&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-24&lt;/code&gt; 拓展模版兼容 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;博后报告&lt;/a&gt;。修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/156"&gt;gsp2014, issue #156&lt;/a&gt; 文献引用中的连字符的间断显示和上标引用中逗号下沉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-19&lt;/code&gt; 修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/117"&gt;nihaomiao, issue #117&lt;/a&gt;&lt;code&gt;\mathbf&lt;/code&gt;失效问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-16&lt;/code&gt; 修复国际生需要的&lt;code&gt;plain&lt;/code&gt;模式下无法改变英文章标题字体大小的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-09&lt;/code&gt; 对部分宏命令进行调整，无功能及样式上的修改。若需更新，建议参考 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%9B%B4%E6%96%B0%E6%8C%87%E5%8D%97"&gt;更新指南&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-04&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/134"&gt;liuy334, songchunlin, issue #134&lt;/a&gt; ，调整行距使&lt;code&gt;LaTeX&lt;/code&gt;版与&lt;code&gt;Word&lt;/code&gt;版的行数和每行字数相一致。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-28&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/49"&gt;zssasa, allenwoods, issue #49&lt;/a&gt; ，修复&lt;code&gt;bicaption&lt;/code&gt;对&lt;code&gt;longtable&lt;/code&gt;的兼容性。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/133"&gt;BowenHou, issue #133&lt;/a&gt; ，使下划线能对长标题自动换行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/127"&gt;DRjy, muzimuzhi, issue #127&lt;/a&gt; ，为&lt;code&gt;摘要&lt;/code&gt;等无需在目录中显示的结构元素建立书签。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/130"&gt;muzimuzhi, issue #130&lt;/a&gt; ，修正对&lt;code&gt;\voffset&lt;/code&gt;的使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-14&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/121"&gt;opt-gaobin, issue #121&lt;/a&gt; ，修正中文标点使下划线断掉的问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/120"&gt;Guoqiang Zhang, email; weili-ict, issue #120&lt;/a&gt; ，修复&lt;code&gt;\proofname&lt;/code&gt;命令对2015年及更早&lt;code&gt;LaTeX&lt;/code&gt;编译器的兼容性问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-02-20&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/100"&gt;opt-gaobin, issue #100&lt;/a&gt; ，增加定理、定义、证明等数学环境。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/102"&gt;DRjy, issue #102&lt;/a&gt; ，调整&lt;code&gt;\mathcal&lt;/code&gt;字体样式。根据 [zike Liu, email] ，适当缩减目录列表的缩进。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/105"&gt;xiaoyaoE, issue #105&lt;/a&gt; ，使数字字体和英文字体一致。完善中文版和国际版之间的中英格式切换。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-01-10&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/57"&gt;mnpengjk, issue #57&lt;/a&gt; ， 将公式编号前加点纳入模版默认，更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt; 。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/95"&gt;yunyun2019, issue #95&lt;/a&gt; ，采用 &lt;a href="https://github.com/zepinglee"&gt;zepinglee&lt;/a&gt; 基于国标样式为&lt;code&gt;ucas&lt;/code&gt;所定制文献样式：&lt;a href="https://github.com/CTeX-org/gbt7714-bibtex-style/tree/ucas"&gt;ucas 样式分支&lt;/a&gt; ，文献样式更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%96%87%E7%8C%AE%E6%A0%B7%E5%BC%8F"&gt;文献样式&lt;/a&gt;。根据 [邵岳林, email] ，将附录复原为常规的排版设置，若需将附录置于参考文献后，请见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-03&lt;/code&gt; 根据国科大本科部陆晴老师和本科部学位办丁云云老师的复审审核建议再次修复一些样式细节问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-02&lt;/code&gt; 模板进行了重大更新，修复了样式、字体、格式等许多问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据国科大本科部陆晴老师的建议对模版样式进行了诸多拓展和修正，并完善对本科生论文元素的兼容性。&lt;/li&gt;
&lt;li&gt;在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下解决了如何多次调用&lt;code&gt;Times New Roman&lt;/code&gt;而不导致黑体调用错误的问题。根据 [twn1993, email]，修复默认黑体为微软雅黑而不是&lt;code&gt;SimHei&lt;/code&gt;的问题。&lt;/li&gt;
&lt;li&gt;繁复折腾测试后终于找出一个在&lt;code&gt;ctex&lt;/code&gt;默认黑体替换粗宋体设定环境内全局&lt;code&gt;AutoFakeBold&lt;/code&gt;失效状态下折衷特定字体库不全条件下生僻字显示和系统默认字重不全条件下粗宋体显示以及不同操作系统下如何平衡上述字库自重矛盾还有根据操作系统自动调用所带有的&lt;code&gt;Times&lt;/code&gt;字体的方案。&lt;/li&gt;
&lt;li&gt;设定论文封面据英文学位名如自动切换。密级据是否填写自动显示。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-22&lt;/code&gt; 演示表标题居表上，加粗图表标注，设置长图表标题悬挂缩进（由于&lt;code&gt;bicaption&lt;/code&gt;宏包无法正确接受&lt;code&gt;caption&lt;/code&gt;宏包的&lt;code&gt;margin&lt;/code&gt;选项，图表中英标题第一行无法正确同步缩进，从而放弃第一行的缩进），强调多图中子图标题的规范使用，通过摘要和符号列表演示标题不在目录中显示却仍在页眉中显示。根据 [赵永明, email]，设置双语图表标题和&lt;code&gt;bicaption&lt;/code&gt;不在图形列表和表格列表中显示英文标题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-21&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/42"&gt;zhanglinbo, issue #42&lt;/a&gt; ，使用 &lt;a href="https://github.com/xiaoyao9933/UCASthesis"&gt;xiaoyao9933&lt;/a&gt; 制作的&lt;code&gt;ucas_logo.pdf&lt;/code&gt;使学校&lt;code&gt;logo&lt;/code&gt;放大不失真。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/41"&gt;Starsky Wong, issue #41&lt;/a&gt; ，设置标题英文设为&lt;code&gt;Times New Roman&lt;/code&gt;。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/29"&gt;will0n, issue #29&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/26"&gt;Man-Ting-Fang, issue #26&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/12"&gt;diyiliaoya, issue #12&lt;/a&gt; ，和 [赵永明, email] ，矫正一些格式细节问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/30"&gt;tangjie1992, issue #30&lt;/a&gt; ，配置算法环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-02-04&lt;/code&gt; 在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下修复误用字体命令导致的粗宋体异常。然后，将模板兼容性进一步扩展为兼容操作系统&lt;code&gt;Windows&lt;/code&gt;，&lt;code&gt;Linux&lt;/code&gt;，&lt;code&gt;MacOS&lt;/code&gt;和&lt;code&gt;LaTeX &lt;/code&gt;编译引擎&lt;code&gt;pdflatex&lt;/code&gt;，&lt;code&gt;xelatex&lt;/code&gt;，&lt;code&gt;lualatex&lt;/code&gt;。移除&lt;code&gt;microtype&lt;/code&gt;宏包以提高编译效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-01-28&lt;/code&gt; 基于国科大&lt;code&gt;2018&lt;/code&gt;新版论文规范进行了重大修改，采用新的封面、声明、页眉页脚样式。展示标题中使用数学公式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2017-05-14&lt;/code&gt; 根据 [赵永明, email] ，增加&lt;code&gt;\citepns{}&lt;/code&gt;和&lt;code&gt;\citetns{}&lt;/code&gt;命令提供上标引用下混合非上标引用的需求。根据 [臧光明, email] ，添加设定论文为&lt;code&gt;thesis&lt;/code&gt;或&lt;code&gt;dissertation&lt;/code&gt;的命令。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mohuangrui</author><guid isPermaLink="false">https://github.com/mohuangrui/ucasthesis</guid><pubDate>Mon, 02 Dec 2019 00:08:00 GMT</pubDate></item><item><title>billryan/resume #9 in TeX, This week</title><link>https://github.com/billryan/resume</link><description>&lt;p&gt;&lt;i&gt;An elegant \LaTeX\ résumé template&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h1&gt;
&lt;p&gt;Hit branch &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; if you want a Simplified Chinese résumé.&lt;/p&gt;
&lt;p&gt;中文用户请前往 &lt;a href="https://github.com/billryan/resume/tree/zh_CN"&gt;zh_CN&lt;/a&gt; 分支。&lt;/p&gt;
&lt;p&gt;An elegant \LaTeX\ résumé template, compiled with \XeLaTeX. Inspired by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/zachscrivena/simple-resume-cv"&gt;zachscrivena/simple-resume-cv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ctan.org/pkg/res" rel="nofollow"&gt;res&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianxu.net/en/files/JianXu_CV.pdf" rel="nofollow"&gt;JianXu's CV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.berkeley.edu/~paciorek/computingTips/Latex_template_creating_CV_.html" rel="nofollow"&gt;paciorek's CV/Resume template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.sharelatex.com/blog/2011/03/27/how-to-write-a-latex-class-file-and-design-your-own-cv.html" rel="nofollow"&gt;How to write a LaTeX class file and design your own CV (Part 1) - ShareLaTeX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Easy to further customize or extend&lt;/li&gt;
&lt;li&gt;Full support for unicode characters (e.g. CJK) with \XeLaTeX\&lt;/li&gt;
&lt;li&gt;Perfect Simplified Chinese fonts supported with Adobefonts&lt;/li&gt;
&lt;li&gt;FontAwesome 4.6.3 support&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fork this repository&lt;/li&gt;
&lt;li&gt;Add information about you directly in GitHub&lt;/li&gt;
&lt;li&gt;Compile TeX file to PDF with &lt;a href="https://latexonline.cc/" rel="nofollow"&gt;LaTeX.Online&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Can also use Overleaf for online compilation (&lt;a href="https://www.overleaf.com/" rel="nofollow"&gt;https://www.overleaf.com/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sample-output" class="anchor" aria-hidden="true" href="#sample-output"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample Output&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409353-3fecfc00-b608-11e9-8e83-84962912c956.png" alt="English" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409351-3f546580-b608-11e9-9f6d-d232a68c5451.png" alt="English with photo" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1292567/62409352-3fecfc00-b608-11e9-8d9e-76243ca3052a.png" alt="简体中文" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463503/resume.pdf"&gt;English PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463501/resume_photo.pdf"&gt;English with photo PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/billryan/resume/files/3463502/resume-zh_CN.pdf"&gt;简体中文 PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Edit in ShareLaTeX online - &lt;a href="https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9" rel="nofollow"&gt;https://www.sharelatex.com/templates/556b27cf0d23e5a8117053d9&lt;/a&gt;, &lt;strong&gt;no TeX software install!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Compile tex on your Computer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you only need a résumé in English or have installed Adobe Simplified Chinese on your OS, &lt;strong&gt;It would be better to clone only the master branch,&lt;/strong&gt; since the Simplified Chinese fonts files are too large.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/billryan/resume.git --branch master --depth 1 --single-branch &amp;lt;folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://opensource.org/licenses/MIT" rel="nofollow"&gt;The MIT License (MIT)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyrighted fonts are not subjected to this License.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>billryan</author><guid isPermaLink="false">https://github.com/billryan/resume</guid><pubDate>Mon, 02 Dec 2019 00:09:00 GMT</pubDate></item><item><title>terryum/awesome-deep-learning-papers #10 in TeX, This week</title><link>https://github.com/terryum/awesome-deep-learning-papers</link><description>&lt;p&gt;&lt;i&gt;The most cited deep learning papers&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome---most-cited-deep-learning-papers" class="anchor" aria-hidden="true" href="#awesome---most-cited-deep-learning-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome - Most Cited Deep Learning Papers&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.&lt;/p&gt;
&lt;p&gt;A curated list of the most cited deep learning papers (2012-2016)&lt;/p&gt;
&lt;p&gt;We believe that there exist &lt;em&gt;classic&lt;/em&gt; deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a &lt;em&gt;curated list&lt;/em&gt; of the awesome deep learning papers which are considered as &lt;em&gt;must-reads&lt;/em&gt; in certain research domains.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-background" class="anchor" aria-hidden="true" href="#background"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Background&lt;/h2&gt;
&lt;p&gt;Before this list, there exist other &lt;em&gt;awesome deep learning lists&lt;/em&gt;, for example, &lt;a href="https://github.com/kjw0612/awesome-deep-vision"&gt;Deep Vision&lt;/a&gt; and &lt;a href="https://github.com/kjw0612/awesome-rnn"&gt;Awesome Recurrent Neural Networks&lt;/a&gt;. Also, after this list comes out, another awesome list for deep learning beginners, called &lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap"&gt;Deep Learning Papers Reading Roadmap&lt;/a&gt;, has been created and loved by many deep learning researchers.&lt;/p&gt;
&lt;p&gt;Although the &lt;em&gt;Roadmap List&lt;/em&gt; includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; here as a good starting point of overviewing deep learning researches.&lt;/p&gt;
&lt;p&gt;To get the news for newly released papers everyday, follow my &lt;a href="https://twitter.com/TerryUm_ML" rel="nofollow"&gt;twitter&lt;/a&gt; or &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;facebook page&lt;/a&gt;!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-awesome-list-criteria" class="anchor" aria-hidden="true" href="#awesome-list-criteria"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome list criteria&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A list of &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; published from 2012 to 2016 is suggested.&lt;/li&gt;
&lt;li&gt;If a paper is added to the list, another paper (usually from *More Papers from 2016" section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)&lt;/li&gt;
&lt;li&gt;Papers that are important, but failed to be included in the list, will be listed in &lt;em&gt;More than Top 100&lt;/em&gt; section.&lt;/li&gt;
&lt;li&gt;Please refer to &lt;em&gt;New Papers&lt;/em&gt; and &lt;em&gt;Old Papers&lt;/em&gt; sections for the papers published in recent 6 months or before 2012.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;(Citation criteria)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;lt; 6 months&lt;/strong&gt; : &lt;em&gt;New Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt; :  +60 citations or "More Papers from 2016"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt; :  +200 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt; :  +400 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2013&lt;/strong&gt; :  +600 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2012&lt;/strong&gt; :  +800 citations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~2012&lt;/strong&gt; : &lt;em&gt;Old Papers&lt;/em&gt; (by discussion)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We need your contributions!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request.
(Please read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; for further instructions, though just letting me know the title of papers can also be a big contribution to us.)&lt;/p&gt;
&lt;p&gt;(Update) You can download all top-100 papers with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/fetch_papers.py"&gt;this&lt;/a&gt; and collect all authors' names with &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/get_authors.py"&gt;this&lt;/a&gt;. Also, &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/top100papers.bib"&gt;bib file&lt;/a&gt; for all top-100 papers are available. Thanks, doodhwala, &lt;a href="https://github.com/sunshinemyson"&gt;Sven&lt;/a&gt; and &lt;a href="https://github.com/grepinsight"&gt;grepinsight&lt;/a&gt;!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#understanding--generalization--transfer"&gt;Understanding / Generalization / Transfer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimization--training-techniques"&gt;Optimization / Training Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unsupervised--generative-models"&gt;Unsupervised / Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#convolutional-neural-network-models"&gt;Convolutional Network Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image-segmentation--object-detection"&gt;Image Segmentation / Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#image--video--etc"&gt;Image / Video / Etc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-language-processing--rnns"&gt;Natural Language Processing / RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speech--other-domain"&gt;Speech / Other Domain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning--robotics"&gt;Reinforcement Learning / Robotics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#more-papers-from-2016"&gt;More Papers from 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(More than Top 100)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-papers"&gt;New Papers&lt;/a&gt; : Less than 6 months&lt;/li&gt;
&lt;li&gt;&lt;a href="#old-papers"&gt;Old Papers&lt;/a&gt; : Before 2012&lt;/li&gt;
&lt;li&gt;&lt;a href="#hw--sw--dataset"&gt;HW / SW / Dataset&lt;/a&gt; : Technical reports&lt;/li&gt;
&lt;li&gt;&lt;a href="#book--survey--review"&gt;Book / Survey / Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#video-lectures--tutorials--blogs"&gt;Video Lectures / Tutorials / Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#appendix-more-than-top-100"&gt;Appendix: More than Top 100&lt;/a&gt; : More papers not in the list&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-understanding--generalization--transfer" class="anchor" aria-hidden="true" href="#understanding--generalization--transfer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Understanding / Generalization / Transfer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt; (2015), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1503.02531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images&lt;/strong&gt; (2015), A. Nguyen et al. &lt;a href="http://arxiv.org/pdf/1412.1897" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How transferable are features in deep neural networks?&lt;/strong&gt; (2014), J. Yosinski et al. &lt;a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNN features off-the-Shelf: An astounding baseline for recognition&lt;/strong&gt; (2014), A. Razavian et al. &lt;a href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning and transferring mid-Level image representations using convolutional neural networks&lt;/strong&gt; (2014), M. Oquab et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visualizing and understanding convolutional networks&lt;/strong&gt; (2014), M. Zeiler and R. Fergus &lt;a href="http://arxiv.org/pdf/1311.2901" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decaf: A deep convolutional activation feature for generic visual recognition&lt;/strong&gt; (2014), J. Donahue et al. &lt;a href="http://arxiv.org/pdf/1310.1531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-optimization--training-techniques" class="anchor" aria-hidden="true" href="#optimization--training-techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimization / Training Techniques&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training very deep networks&lt;/strong&gt; (2015), R. Srivastava et al. &lt;a href="http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt; (2015), S. Loffe and C. Szegedy &lt;a href="http://arxiv.org/pdf/1502.03167" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/strong&gt; (2015), K. He et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dropout: A simple way to prevent neural networks from overfitting&lt;/strong&gt; (2014), N. Srivastava et al. &lt;a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt; (2014), D. Kingma and J. Ba &lt;a href="http://arxiv.org/pdf/1412.6980" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://arxiv.org/pdf/1207.0580.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random search for hyper-parameter optimization&lt;/strong&gt; (2012) J. Bergstra and Y. Bengio &lt;a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-unsupervised--generative-models" class="anchor" aria-hidden="true" href="#unsupervised--generative-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised / Generative Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="http://arxiv.org/pdf/1601.06759v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved techniques for training GANs&lt;/strong&gt; (2016), T. Salimans et al. &lt;a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt; (2015), A. Radford et al. &lt;a href="https://arxiv.org/pdf/1511.06434v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt; (2015), K. Gregor et al. &lt;a href="http://arxiv.org/pdf/1502.04623" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt; (2014), I. Goodfellow et al. &lt;a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-encoding variational Bayes&lt;/strong&gt; (2013), D. Kingma and M. Welling &lt;a href="http://arxiv.org/pdf/1312.6114" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt; (2013), Q. Le et al. &lt;a href="http://arxiv.org/pdf/1112.6209" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-convolutional-neural-network-models" class="anchor" aria-hidden="true" href="#convolutional-neural-network-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Network Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rethinking the inception architecture for computer vision&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inception-v4, inception-resnet and the impact of residual connections on learning&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href="http://arxiv.org/pdf/1602.07261" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt; (2016), K. He et al. &lt;a href="https://arxiv.org/pdf/1603.05027v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt; (2016), K. He et al. &lt;a href="http://arxiv.org/pdf/1512.03385" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial transformer network&lt;/strong&gt; (2015), M. Jaderberg et al., &lt;a href="http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt; (2015), C. Szegedy et al.  &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt; (2014), K. Simonyan and A. Zisserman &lt;a href="http://arxiv.org/pdf/1409.1556" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Return of the devil in the details: delving deep into convolutional nets&lt;/strong&gt; (2014), K. Chatfield et al. &lt;a href="http://arxiv.org/pdf/1405.3531" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OverFeat: Integrated recognition, localization and detection using convolutional networks&lt;/strong&gt; (2013), P. Sermanet et al. &lt;a href="http://arxiv.org/pdf/1312.6229" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxout networks&lt;/strong&gt; (2013), I. Goodfellow et al. &lt;a href="http://arxiv.org/pdf/1302.4389v4" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network in network&lt;/strong&gt; (2013), M. Lin et al. &lt;a href="http://arxiv.org/pdf/1312.4400" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageNet classification with deep convolutional neural networks&lt;/strong&gt; (2012), A. Krizhevsky et al. &lt;a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image-segmentation--object-detection" class="anchor" aria-hidden="true" href="#image-segmentation--object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image: Segmentation / Object Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt; (2016), J. Redmon et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt; (2015), J. Long et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/strong&gt; (2015), S. Ren et al. &lt;a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt; (2015), R. Girshick &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt; (2014), R. Girshick et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt; (2014), K. He et al. &lt;a href="http://arxiv.org/pdf/1406.4729" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/strong&gt;, L. Chen et al. &lt;a href="https://arxiv.org/pdf/1412.7062" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning hierarchical features for scene labeling&lt;/strong&gt; (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-image--video--etc" class="anchor" aria-hidden="true" href="#image--video--etc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image / Video / Etc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/strong&gt; (2016), C. Dong et al. &lt;a href="https://arxiv.org/pdf/1501.00092v3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt; (2015), L. Gatys et al. &lt;a href="https://arxiv.org/pdf/1508.06576" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt; (2015), A. Karpathy and L. Fei-Fei &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt; (2015), K. Xu et al. &lt;a href="http://arxiv.org/pdf/1502.03044" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt; (2015), O. Vinyals et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt; (2015), J. Donahue et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VQA: Visual question answering&lt;/strong&gt; (2015), S. Antol et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DeepFace: Closing the gap to human-level performance in face verification&lt;/strong&gt; (2014), Y. Taigman et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Large-scale video classification with convolutional neural networks&lt;/strong&gt; (2014), A. Karpathy et al. &lt;a href="http://vision.stanford.edu/pdf/karpathy14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two-stream convolutional networks for action recognition in videos&lt;/strong&gt; (2014), K. Simonyan et al. &lt;a href="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3D convolutional neural networks for human action recognition&lt;/strong&gt; (2013), S. Ji et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;&lt;a id="user-content-natural-language-processing--rnns" class="anchor" aria-hidden="true" href="#natural-language-processing--rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing / RNNs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Neural Architectures for Named Entity Recognition&lt;/strong&gt; (2016), G. Lample et al. &lt;a href="http://aclweb.org/anthology/N/N16/N16-1030.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exploring the limits of language modeling&lt;/strong&gt; (2016), R. Jozefowicz et al. &lt;a href="http://arxiv.org/pdf/1602.02410" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Teaching machines to read and comprehend&lt;/strong&gt; (2015), K. Hermann et al. &lt;a href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt; (2015), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1508.04025" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditional random fields as recurrent neural networks&lt;/strong&gt; (2015), S. Zheng and S. Jayasumana. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory networks&lt;/strong&gt; (2014), J. Weston et al. &lt;a href="https://arxiv.org/pdf/1410.3916" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural turing machines&lt;/strong&gt; (2014), A. Graves et al. &lt;a href="https://arxiv.org/pdf/1410.5401" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Neural machine translation by jointly learning to align and translate&lt;/strong&gt; (2014), D. Bahdanau et al. &lt;a href="http://arxiv.org/pdf/1409.0473" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt; (2014), I. Sutskever et al. &lt;a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt; (2014), K. Cho et al. &lt;a href="http://arxiv.org/pdf/1406.1078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A convolutional neural network for modeling sentences&lt;/strong&gt; (2014), N. Kalchbrenner et al. &lt;a href="http://arxiv.org/pdf/1404.2188v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional neural networks for sentence classification&lt;/strong&gt; (2014), Y. Kim &lt;a href="http://arxiv.org/pdf/1408.5882" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Glove: Global vectors for word representation&lt;/strong&gt; (2014), J. Pennington et al. &lt;a href="http://anthology.aclweb.org/D/D14/D14-1162.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of sentences and documents&lt;/strong&gt; (2014), Q. Le and T. Mikolov &lt;a href="http://arxiv.org/pdf/1405.4053" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient estimation of word representations in vector space&lt;/strong&gt; (2013), T. Mikolov et al.  &lt;a href="http://arxiv.org/pdf/1301.3781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive deep models for semantic compositionality over a sentiment treebank&lt;/strong&gt; (2013), R. Socher et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt; (2013), A. Graves. &lt;a href="https://arxiv.org/pdf/1308.0850" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-speech--other-domain" class="anchor" aria-hidden="true" href="#speech--other-domain"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech / Other Domain&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end attention-based large vocabulary speech recognition&lt;/strong&gt; (2016), D. Bahdanau et al. &lt;a href="https://arxiv.org/pdf/1508.04395" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in English and Mandarin&lt;/strong&gt; (2015), D. Amodei et al. &lt;a href="https://arxiv.org/pdf/1512.02595" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt; (2013), A. Graves &lt;a href="http://arxiv.org/pdf/1303.5778.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition&lt;/strong&gt; (2012) G. Dahl et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Acoustic modeling using deep belief networks&lt;/strong&gt; (2012), A. Mohamed et al. &lt;a href="http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-reinforcement-learning--robotics" class="anchor" aria-hidden="true" href="#reinforcement-learning--robotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning / Robotics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt; (2016), S. Levine et al. &lt;a href="https://arxiv.org/pdf/1603.02199" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt; (2016), V. Mnih et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/strong&gt; (2016), H. Hasselt et al. &lt;a href="https://arxiv.org/pdf/1509.06461.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt; (2016), D. Silver et al. &lt;a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt; (2015), T. Lillicrap et al. &lt;a href="https://arxiv.org/pdf/1509.02971" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt; (2015), V. Mnih et al. &lt;a href="http://www.davidqiu.com:8888/research/nature14236.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for detecting robotic grasps&lt;/strong&gt; (2015), I. Lenz et al. &lt;a href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt; (2013), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1312.5602.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;&lt;a id="user-content-more-papers-from-2016" class="anchor" aria-hidden="true" href="#more-papers-from-2016"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Papers from 2016&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt; (2016), J. Ba et al. &lt;a href="https://arxiv.org/pdf/1607.06450v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt; (2016), M. Andrychowicz et al. &lt;a href="http://arxiv.org/pdf/1606.04474v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain-adversarial training of neural networks&lt;/strong&gt; (2016), Y. Ganin et al. &lt;a href="http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WaveNet: A Generative Model for Raw Audio&lt;/strong&gt; (2016), A. Oord et al. &lt;a href="https://arxiv.org/pdf/1609.03499v2" rel="nofollow"&gt;[pdf]&lt;/a&gt; &lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Colorful image colorization&lt;/strong&gt; (2016), R. Zhang et al. &lt;a href="https://arxiv.org/pdf/1603.08511" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generative visual manipulation on the natural image manifold&lt;/strong&gt; (2016), J. Zhu et al. &lt;a href="https://arxiv.org/pdf/1609.03552" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Texture networks: Feed-forward synthesis of textures and stylized images&lt;/strong&gt; (2016), D Ulyanov et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD: Single shot multibox detector&lt;/strong&gt; (2016), W. Liu et al. &lt;a href="https://arxiv.org/pdf/1512.02325" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt; (2016), F. Iandola et al. &lt;a href="http://arxiv.org/pdf/1602.07360" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eie: Efficient inference engine on compressed deep neural network&lt;/strong&gt; (2016), S. Han et al. &lt;a href="http://arxiv.org/pdf/1602.01528" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1&lt;/strong&gt; (2016), M. Courbariaux et al. &lt;a href="https://arxiv.org/pdf/1602.02830" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic memory networks for visual and textual question answering&lt;/strong&gt; (2016), C. Xiong et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stacked attention networks for image question answering&lt;/strong&gt; (2016), Z. Yang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt; (2016), A. Graves et al. &lt;a href="https://www.gwern.net/docs/2016-graves.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google's neural machine translation system: Bridging the gap between human and machine translation&lt;/strong&gt; (2016), Y. Wu et al. &lt;a href="https://arxiv.org/pdf/1609.08144" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-new-papers" class="anchor" aria-hidden="true" href="#new-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Newly published papers (&amp;lt; 6 months) which are worth reading&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. &lt;a href="https://arxiv.org/pdf/1704.04861.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. &lt;a href="https://arxiv.org/pdf/1705.03122" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. &lt;a href="https://arxiv.org/pdf/1702.01932" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. &lt;a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. &lt;a href="https://arxiv.org/pdf/1703.10135.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Photo Style Transfer (2017), F. Luan et al. &lt;a href="http://arxiv.org/pdf/1703.07511v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. &lt;a href="http://arxiv.org/pdf/1703.03864v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deformable Convolutional Networks (2017), J. Dai et al. &lt;a href="http://arxiv.org/pdf/1703.06211v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mask R-CNN (2017), K. He et al. &lt;a href="https://128.84.21.199/pdf/1703.06870" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. &lt;a href="http://arxiv.org/pdf/1703.05192v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., &lt;a href="http://arxiv.org/pdf/1702.07825v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. &lt;a href="http://arxiv.org/pdf/1702.06506v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. &lt;a href="https://arxiv.org/abs/1702.03275" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wasserstein GAN (2017), M. Arjovsky et al. &lt;a href="https://arxiv.org/pdf/1701.07875v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. &lt;a href="https://arxiv.org/pdf/1611.03530" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Least squares generative adversarial networks (2016), X. Mao et al. &lt;a href="https://arxiv.org/abs/1611.04076v2" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-old-papers" class="anchor" aria-hidden="true" href="#old-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Old Papers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Classic papers published before 2012&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep sparse rectifier neural networks (2011), X. Glorot et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Natural language processing (almost) from scratch (2011), R. Collobert et al. &lt;a href="http://arxiv.org/pdf/1103.0398" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent neural network based language model (2010), T. Mikolov et al. &lt;a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning mid-level features for recognition (2010), Y. Boureau &lt;a href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A practical guide to training restricted boltzmann machines (2010), G. Hinton &lt;a href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning deep architectures for AI (2009), Y. Bengio. &lt;a href="http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Greedy layer-wise training of deep networks (2007), Y. Bengio et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. &lt;a href="http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A fast learning algorithm for deep belief nets (2006), G. Hinton et al. &lt;a href="http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gradient-based learning applied to document recognition (1998), Y. LeCun et al. &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. &lt;a href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-hw--sw--dataset" class="anchor" aria-hidden="true" href="#hw--sw--dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HW / SW / Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. &lt;a href="https://arxiv.org/pdf/1606.05250.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenAI gym (2016), G. Brockman et al. &lt;a href="https://arxiv.org/pdf/1606.01540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. &lt;a href="http://arxiv.org/pdf/1603.04467" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.&lt;/li&gt;
&lt;li&gt;Torch7: A matlab-like environment for machine learning, R. Collobert et al. &lt;a href="https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc &lt;a href="http://arxiv.org/pdf/1412.4564" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. &lt;a href="http://arxiv.org/pdf/1409.0575" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. &lt;a href="http://arxiv.org/pdf/1408.5093" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-book--survey--review" class="anchor" aria-hidden="true" href="#book--survey--review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Book / Survey / Review&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. &lt;a href="https://arxiv.org/pdf/1702.07800" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Learning: An Overview (2017), Y. Li, &lt;a href="http://arxiv.org/pdf/1701.07274v2.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. &lt;a href="http://arxiv.org/pdf/1703.01619v1.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. &lt;a href="http://neuralnetworksanddeeplearning.com/index.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (Book, 2016), Goodfellow et al. &lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSTM: A search space odyssey (2016), K. Greff et al. &lt;a href="https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;amp;utm_medium=social&amp;amp;utm_source=plus.google.com&amp;amp;utm_campaign=buffer" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tutorial on Variational Autoencoders (2016), C. Doersch. &lt;a href="https://arxiv.org/pdf/1606.05908" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton &lt;a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep learning in neural networks: An overview (2015), J. Schmidhuber &lt;a href="http://arxiv.org/pdf/1404.7828" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Representation learning: A review and new perspectives (2013), Y. Bengio et al. &lt;a href="http://arxiv.org/pdf/1206.5538" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-video-lectures--tutorials--blogs" class="anchor" aria-hidden="true" href="#video-lectures--tutorials--blogs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Lectures / Tutorials / Blogs&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(Lectures)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University &lt;a href="http://cs231n.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CS224d, Deep Learning for Natural Language Processing, Stanford University &lt;a href="http://cs224d.stanford.edu/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford &lt;a href="https://github.com/oxford-cs-deepnlp-2017/lectures"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Tutorials)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NIPS 2016 Tutorials, Long Beach &lt;a href="https://nips.cc/Conferences/2016/Schedule?type=Tutorial" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICML 2016 Tutorials, New York City &lt;a href="http://techtalks.tv/icml/2016/tutorials/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICLR 2016 Videos, San Juan &lt;a href="http://videolectures.net/iclr2016_san_juan/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Learning Summer School 2016, Montreal &lt;a href="http://videolectures.net/deeplearning2016_montreal/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bay Area Deep Learning School 2016, Stanford &lt;a href="https://www.bayareadlschool.org/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(Blogs)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI &lt;a href="https://www.openai.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Distill &lt;a href="http://distill.pub/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andrej Karpathy Blog &lt;a href="http://karpathy.github.io/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colah's Blog &lt;a href="http://colah.github.io/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WildML &lt;a href="http://www.wildml.com/" rel="nofollow"&gt;[Web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;FastML &lt;a href="http://www.fastml.com/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TheMorningPaper &lt;a href="https://blog.acolyer.org" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-appendix-more-than-top-100" class="anchor" aria-hidden="true" href="#appendix-more-than-top-100"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Appendix: More than Top 100&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(2016)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1603.06147" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. &lt;a href="http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html" rel="nofollow"&gt;[html]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. &lt;a href="https://arxiv.org/pdf/1503.00949" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. &lt;a href="https://arxiv.org/pdf/1505.03540" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. &lt;a href="https://arxiv.org/pdf/1610.09038" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adversarially learned inference (2016), V. Dumoulin et al. &lt;a href="https://ishmaelbelghazi.github.io/ALI/" rel="nofollow"&gt;[web]&lt;/a&gt;&lt;a href="https://arxiv.org/pdf/1606.00704v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding convolutional neural networks (2016), J. Koushik &lt;a href="https://arxiv.org/pdf/1605.09081v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. &lt;a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive computation time for recurrent neural networks (2016), A. Graves &lt;a href="http://arxiv.org/pdf/1603.08983" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Densely connected convolutional networks (2016), G. Huang et al. &lt;a href="https://arxiv.org/pdf/1608.06993v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.&lt;/li&gt;
&lt;li&gt;Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/gu16.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. &lt;a href="https://arxiv.org/pdf/1606.02858" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. &lt;a href="https://arxiv.org/pdf/1604.00788" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. &lt;a href="https://arxiv.org/pdf/1606.01781" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bag of tricks for efficient text classification (2016), A. Joulin et al. &lt;a href="https://arxiv.org/pdf/1607.01759" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning to compose neural networks for question answering (2016), J. Andreas et al. &lt;a href="https://arxiv.org/pdf/1601.01705" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. &lt;a href="https://arxiv.org/pdf/1603.08155" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. &lt;a href="http://arxiv.org/pdf/1412.1842" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;What makes for effective detection proposals? (2016), J. Hosang et al. &lt;a href="https://arxiv.org/pdf/1502.05082" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. &lt;a href="http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep networks with stochastic depth (2016), G. Huang et al., &lt;a href="https://arxiv.org/pdf/1603.09382" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. &lt;a href="http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(2015)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Exploring models and data for image question answering (2015), M. Ren et al. &lt;a href="http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. &lt;a href="http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mind's eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From captions to visual concepts and back (2015), H. Fang et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. &lt;a href="http://arxiv.org/pdf/1502.05698" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. &lt;a href="http://arxiv.org/pdf/1506.07285" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. &lt;a href="https://arxiv.org/pdf/1510.00149" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. &lt;a href="https://arxiv.org/pdf/1503.00075" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Character-aware neural language models (2015), Y. Kim et al. &lt;a href="https://arxiv.org/pdf/1508.06615" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Grammar as a foreign language (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Trust Region Policy Optimization (2015), J. Schulman et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Beyond short snippents: Deep networks for video classification (2015) &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. &lt;a href="https://arxiv.org/pdf/1505.04366v1" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Understanding neural networks through deep visualization (2015), J. Yosinski et al. &lt;a href="https://arxiv.org/pdf/1506.06579" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al.  &lt;a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep generative image models using a￼ laplacian pyramid of adversarial networks (2015), E.Denton et al. &lt;a href="http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. &lt;a href="http://www.jmlr.org/proceedings/papers/v37/chung15.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. &lt;a href="https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pointer networks (2015), O. Vinyals et al. &lt;a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. &lt;a href="https://arxiv.org/pdf/1506.02078" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Attention-based models for speech recognition (2015), J. Chorowski et al. &lt;a href="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;End-to-end memory networks (2015), S. Sukbaatar et al. &lt;a href="http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Describing videos by exploiting temporal structure (2015), L. Yao et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A neural conversational model (2015), O. Vinyals and Q. Le. &lt;a href="https://arxiv.org/pdf/1506.05869.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (&lt;a href="https://www.transacl.org/ojs/index.php/tacl/article/download/570/124" rel="nofollow"&gt;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. &lt;a href="http://aclweb.org/anthology/P/P15/P15-1033.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1041.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. &lt;a href="http://aclweb.org/anthology/D/D15/D15-1176.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;(~2014)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. &lt;a href="https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent models of visual attention (2014), V. Mnih et al. &lt;a href="http://arxiv.org/pdf/1406.6247.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. &lt;a href="https://arxiv.org/pdf/1412.3555" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Addressing the rare word problem in neural machine translation (2014), M. Luong et al. &lt;a href="https://arxiv.org/pdf/1410.8206" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.&lt;/li&gt;
&lt;li&gt;Recurrent neural network regularization (2014), W. Zaremba et al. &lt;a href="http://arxiv.org/pdf/1409.2329" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Intriguing properties of neural networks (2014), C. Szegedy et al. &lt;a href="https://arxiv.org/pdf/1312.6199.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. &lt;a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scalable object detection using deep neural networks (2014), D. Erhan et al. &lt;a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Regularization of neural networks using dropconnect (2013), L. Wan et al. &lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. &lt;a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. &lt;a href="http://www.aclweb.org/anthology/N13-1#page=784" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Large scale distributed deep networks (2012), J. Dean et al. &lt;a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. &lt;a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf" rel="nofollow"&gt;[pdf]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;Thank you for all your contributions. Please make sure to read the &lt;a href="https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md"&gt;contributing guide&lt;/a&gt; before you make a pull request.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/60561947585c982aee67ed3e3b25388184cc0aa3/687474703a2f2f6d6972726f72732e6372656174697665636f6d6d6f6e732e6f72672f70726573736b69742f627574746f6e732f38387833312f7376672f63632d7a65726f2e737667" alt="CC0" data-canonical-src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To the extent possible under law, &lt;a href="https://www.facebook.com/terryum.io/" rel="nofollow"&gt;Terry T. Um&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>terryum</author><guid isPermaLink="false">https://github.com/terryum/awesome-deep-learning-papers</guid><pubDate>Mon, 02 Dec 2019 00:10:00 GMT</pubDate></item><item><title>xueruini/thuthesis #11 in TeX, This week</title><link>https://github.com/xueruini/thuthesis</link><description>&lt;p&gt;&lt;i&gt;LaTeX Thesis Template for Tsinghua University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/xueruini/thuthesis" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5da8c22b95bd0e5eb940bc5fea16af8feb2ed401/68747470733a2f2f7472617669732d63692e6f72672f7875657275696e692f7468757468657369732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/xueruini/thuthesis.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/thuthesis/Lobby" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f93c05a42da86d653b4d5ad075031b2f4a9c60a1/68747470733a2f2f6261646765732e6769747465722e696d2f7468757468657369732f4c6f6262792e737667" alt="Join the chat at https://gitter.im/thuthesis/Lobby" data-canonical-src="https://badges.gitter.im/thuthesis/Lobby.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/releases"&gt;&lt;img src="https://camo.githubusercontent.com/495c01faee6697531e60e3f0ca1e66743582e890/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f7875657275696e692f7468757468657369732f746f74616c2e737667" alt="Github downloads" data-canonical-src="https://img.shields.io/github/downloads/xueruini/thuthesis/total.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/releases/latest"&gt;&lt;img src="https://camo.githubusercontent.com/a08197fcf9be7d3e34ac0526686304fbd4817146/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f7875657275696e692f7468757468657369732f616c6c2e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/github/release/xueruini/thuthesis/all.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/xueruini/thuthesis/commits/master"&gt;&lt;img src="https://camo.githubusercontent.com/13a6034e83188b5338130101b729835496b5110f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f7875657275696e692f7468757468657369732f6c61746573742e737667" alt="GitHub commits" data-canonical-src="https://img.shields.io/github/commits-since/xueruini/thuthesis/latest.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-whats-thuthesis" class="anchor" aria-hidden="true" href="#whats-thuthesis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's ThuThesis?&lt;/h1&gt;
&lt;p&gt;ThuThesis is an abbreviation of &lt;b&gt;T&lt;/b&gt;sing&lt;b&gt;h&lt;/b&gt;ua &lt;b&gt;U&lt;/b&gt;niversity &lt;b&gt;Thesis&lt;/b&gt; LaTeX Template.&lt;/p&gt;
&lt;p&gt;This package establishes a simple and easy-to-use LaTeX template for Tsinghua dissertations, including general undergraduate research papers, masters theses, doctoral theses, doctoral dissertations, and post-doc reports. Additional support for other formats (what else is there?) will be added continuously. An English translation of this README follows the Chinese below.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-thuthesis是什么" class="anchor" aria-hidden="true" href="#thuthesis是什么"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ThuThesis是什么？&lt;/h1&gt;
&lt;p&gt;ThuThesis为 &lt;b&gt;T&lt;/b&gt;sing&lt;b&gt;h&lt;/b&gt;ua &lt;b&gt;U&lt;/b&gt;niversity &lt;b&gt;Thesis&lt;/b&gt; LaTeX Template之缩写。&lt;/p&gt;
&lt;p&gt;此宏包旨在建立一个简单易用的清华大学学位论文LaTeX模板，包括本科综合论文训练、硕士论文、博士论文、博士哲学论文以及博士后出站报告。现在支持本科、硕士、博士论文、博士后出站报告格式，对其它格式（还有么？）的支持会陆续加入。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-文档" class="anchor" aria-hidden="true" href="#文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;文档&lt;/h1&gt;
&lt;p&gt;请&lt;a href="https://github.com/xueruini/thuthesis/releases"&gt;下载&lt;/a&gt;模板，里面包括具体使用说明以及示例文档：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模板使用说明 (thuthesis.pdf)&lt;/li&gt;
&lt;li&gt;示例文档 (main.pdf)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-下载" class="anchor" aria-hidden="true" href="#下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;发行版：&lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开发版：&lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-升级" class="anchor" aria-hidden="true" href="#升级"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;升级&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-自动更新" class="anchor" aria-hidden="true" href="#自动更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自动更新&lt;/h2&gt;
&lt;p&gt;通过 TeX 发行版工具自动从 &lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt; 更新。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-手动更新" class="anchor" aria-hidden="true" href="#手动更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;手动更新&lt;/h2&gt;
&lt;p&gt;从 &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt; 下载放入论文目录，执行命令（Windows 用户在文件夹空白处按&lt;code&gt;Shift+鼠标右键&lt;/code&gt;，点击“在此处打开命令行窗口”）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xetex thuthesis.ins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;即可得到 &lt;code&gt;thuthesis.cls&lt;/code&gt; 等模板文件。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-提问" class="anchor" aria-hidden="true" href="#提问"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;提问&lt;/h1&gt;
&lt;p&gt;按推荐顺序排序：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先到 &lt;a href="https://github.com/xueruini/thuthesis/wiki/FAQ"&gt;FAQ&lt;/a&gt; 看看常见问题&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/xueruini/thuthesis/issues"&gt;Github Issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.newsmth.net/nForum/#!board/TeX" rel="nofollow"&gt;TeX@newsmth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://groups.google.com/group/thuthesis" rel="nofollow"&gt;ThuThesis@Google Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-makefile的用法" class="anchor" aria-hidden="true" href="#makefile的用法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile的用法&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make [{all&lt;span class="pl-k"&gt;|&lt;/span&gt;thesis&lt;span class="pl-k"&gt;|&lt;/span&gt;shuji&lt;span class="pl-k"&gt;|&lt;/span&gt;doc&lt;span class="pl-k"&gt;|&lt;/span&gt;clean&lt;span class="pl-k"&gt;|&lt;/span&gt;cleanall&lt;span class="pl-k"&gt;|&lt;/span&gt;distclean}]&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-目标" class="anchor" aria-hidden="true" href="#目标"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目标&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make all&lt;/code&gt;       等于 &lt;code&gt;make thesis &amp;amp;&amp;amp; make shuji &amp;amp;&amp;amp; make doc&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cls&lt;/code&gt;       生成模板文件；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt;    生成论文 main.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make shuji&lt;/code&gt;     生成书脊 shuji.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make doc&lt;/code&gt;       生成使用说明书 thuthesis.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;     删除示例文件的中间文件（不含 main.pdf）；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt;  删除示例文件的中间文件和 main.pdf；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; 删除示例文件和模板的所有中间文件和 PDF。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;Download and unzip the template. Specific usage documentation and examples can be found in the files below. At present, these documents are &lt;b&gt;only available in Chinese&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Template usage (thuthesis.pdf)&lt;/li&gt;
&lt;li&gt;Template example (main.pdf)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-downloads" class="anchor" aria-hidden="true" href="#downloads"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloads&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Published version: &lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Developer version: &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-automatic" class="anchor" aria-hidden="true" href="#automatic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automatic&lt;/h2&gt;
&lt;p&gt;Get the most up-to-date published version of the TeX tools from &lt;a href="http://www.ctan.org/pkg/thuthesis" rel="nofollow"&gt;CTAN&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-manual" class="anchor" aria-hidden="true" href="#manual"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manual&lt;/h2&gt;
&lt;p&gt;Download the package from &lt;a href="https://github.com/xueruini/thuthesis"&gt;GitHub&lt;/a&gt; to the root directory of your thesis, then execute the command (Windows users &lt;code&gt;Shift + right click&lt;/code&gt; white area in the file window and click "Open command line window here from the popup menu"):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xetex thuthesis.ins
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You'll get &lt;code&gt;thuthesis.cls&lt;/code&gt; along with other template files.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reporting-issues" class="anchor" aria-hidden="true" href="#reporting-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reporting Issues&lt;/h1&gt;
&lt;p&gt;Please follow the procedure below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Check the  &lt;a href="https://github.com/xueruini/thuthesis/wiki/FAQ"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/xueruini/thuthesis/issues"&gt;Github Issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.newsmth.net/nForum/#!board/TeX" rel="nofollow"&gt;TeX@newsmth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://groups.google.com/group/thuthesis" rel="nofollow"&gt;ThuThesis@Google Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-makefile-usage" class="anchor" aria-hidden="true" href="#makefile-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Makefile Usage&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make [{all&lt;span class="pl-k"&gt;|&lt;/span&gt;thesis&lt;span class="pl-k"&gt;|&lt;/span&gt;shuji&lt;span class="pl-k"&gt;|&lt;/span&gt;doc&lt;span class="pl-k"&gt;|&lt;/span&gt;clean&lt;span class="pl-k"&gt;|&lt;/span&gt;cleanall&lt;span class="pl-k"&gt;|&lt;/span&gt;distclean}]&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-targets" class="anchor" aria-hidden="true" href="#targets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Targets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make all&lt;/code&gt;       same as &lt;code&gt;make thesis &amp;amp;&amp;amp; make shuji &amp;amp;&amp;amp; make doc&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cls&lt;/code&gt;       generate template file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make thesis&lt;/code&gt;    generate thesis main.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make shuji&lt;/code&gt;     generate book spine for printing shuji.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make doc&lt;/code&gt;       generate documentation thuthesis.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;     delete all examples' files (excluding main.pdf);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make cleanall&lt;/code&gt;  delete all examples' files and main.pdf;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt; delete all examples' and templates' files and PDFs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xueruini</author><guid isPermaLink="false">https://github.com/xueruini/thuthesis</guid><pubDate>Mon, 02 Dec 2019 00:11:00 GMT</pubDate></item><item><title>pingcap/docs-cn #12 in TeX, This week</title><link>https://github.com/pingcap/docs-cn</link><description>&lt;p&gt;&lt;i&gt;TiDB/TiKV/PD documents in Chinese.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table data-table-type="yaml-metadata"&gt;
  &lt;thead&gt;
  &lt;tr&gt;
  &lt;th&gt;draft&lt;/th&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
  &lt;tr&gt;
  &lt;td&gt;&lt;div&gt;true&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1&gt;&lt;a id="user-content-tidb-简介" class="anchor" aria-hidden="true" href="#tidb-简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TiDB 简介&lt;/h1&gt;
&lt;p&gt;TiDB 是 PingCAP 公司设计的开源分布式 HTAP (Hybrid Transactional and Analytical Processing) 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性。TiDB 兼容 MySQL，支持无限的水平扩展，具备强一致性和高可用性。TiDB 的目标是为 OLTP (Online Transactional Processing) 和 OLAP (Online Analytical Processing) 场景提供一站式的解决方案。&lt;/p&gt;
&lt;p&gt;TiDB 具备如下特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;高度兼容 MySQL&lt;/p&gt;
&lt;p&gt;&lt;a href="/v3.0/reference/mysql-compatibility.md"&gt;大多数情况下&lt;/a&gt;，无需修改代码即可从 MySQL 轻松迁移至 TiDB，分库分表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;水平弹性扩展&lt;/p&gt;
&lt;p&gt;通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分布式事务&lt;/p&gt;
&lt;p&gt;TiDB 100% 支持标准的 ACID 事务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;真正金融级高可用&lt;/p&gt;
&lt;p&gt;相比于传统主从 (M-S) 复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复 (auto-failover)，无需人工介入。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一站式 HTAP 解决方案&lt;/p&gt;
&lt;p&gt;TiDB 作为典型的 OLTP 行存数据库，同时兼具强大的 OLAP 性能，配合 TiSpark，可提供一站式 HTAP 解决方案，一份存储同时处理 OLTP &amp;amp; OLAP，无需传统繁琐的 ETL 过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;云原生 SQL 数据库&lt;/p&gt;
&lt;p&gt;TiDB 是为云而设计的数据库，支持公有云、私有云和混合云，使部署、配置和维护变得十分简单。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TiDB 的设计目标是 100% 的 OLTP 场景和 80% 的 OLAP 场景，更复杂的 OLAP 分析可以通过 &lt;a href="/v3.0/reference/tispark.md"&gt;TiSpark 项目&lt;/a&gt;来完成。&lt;/p&gt;
&lt;p&gt;TiDB 对业务没有任何侵入性，能优雅的替换传统的数据库中间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力。&lt;/p&gt;
&lt;p&gt;三篇文章了解 TiDB 技术内幕：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-1/" rel="nofollow"&gt;说存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-2/" rel="nofollow"&gt;说计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-3/" rel="nofollow"&gt;谈调度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pingcap</author><guid isPermaLink="false">https://github.com/pingcap/docs-cn</guid><pubDate>Mon, 02 Dec 2019 00:12:00 GMT</pubDate></item><item><title>wklchris/Note-by-LaTeX #13 in TeX, This week</title><link>https://github.com/wklchris/Note-by-LaTeX</link><description>&lt;p&gt;&lt;i&gt;中文 LaTeX 手册 - 简单粗暴 LaTeX。A LaTeX manual written by me to help more Chinese LaTeX learners: A concise handbook of LaTeX. &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="Readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-欢迎来到-latex-cn-项目welcome" class="anchor" aria-hidden="true" href="#欢迎来到-latex-cn-项目welcome"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;欢迎来到 LaTeX-cn 项目！Welcome!&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;== 所有PDF内容如果定稿，下载会发布在release（&lt;a href="https://github.com/wklchris/Note-by-LaTeX/releases"&gt;https://github.com/wklchris/Note-by-LaTeX/releases&lt;/a&gt;）中。PDF files can be accessed in the release page of this repo.  ==&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#incoming-update"&gt;&lt;strong&gt;Incoming Update 即将更新&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sub-repos"&gt;Sub-repos 仓库内容&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#latex-cn-chinese"&gt;LaTeX-cn 中文项目&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#latex-beamer-en-suspended"&gt;LaTeX-beamer 英文项目（挂起）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#outdate-projects-log"&gt;过往项目日志&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-incoming-update" class="anchor" aria-hidden="true" href="#incoming-update"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Incoming Update&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;本公告于2018年10月14日发送，当前版本 v1.6.3。This incoming announcement was leaked on Oct 14, 2018. Current version: v1.6.3.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本手册所有内容基本确定，对于普通 LaTeX 使用者而言已足够使用。TikZ 部分缓慢更新，不排除断更可能。作者目前忙于其他事务，因此请不要继续等待本手册的后续版本&lt;/strong&gt;。&lt;em&gt;&lt;strong&gt;All contents of this handbook has been determined, which is enough for normal LaTeX users. Please don't wait following version of it because the author is busy in many other things. As for TikZ chapter, it is SLOWLY updating and might be suspended in the future&lt;/strong&gt;.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;本手册可能出版；即使出版，本人也保证本仓库仍然维护与开源．&lt;em&gt;This handbook will probably be published. I guarantee readers that I will still maintain this repo and keep it open-source even if the book is published.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-sub-repos" class="anchor" aria-hidden="true" href="#sub-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sub-repos&lt;/h1&gt;
&lt;p&gt;本仓库用于存放我的个人学习笔记，全部使用 LaTeX 或者 Markdown 进行撰写。所有运行环境均为Windows系统。具体包括：&lt;em&gt;This repository is for the use of storing my learning notes (by LaTeX or Markdown). All running environments are &lt;strong&gt;Windows&lt;/strong&gt; system. Namely it  includes:&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-latex-cn-chinese" class="anchor" aria-hidden="true" href="#latex-cn-chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LaTeX-cn (Chinese)&lt;/h2&gt;
&lt;p&gt;LaTeX是一种排版系统，比通常使用的Microsoft Word字处理软件在排版方面更有优势。该一站式手册讲述了如何高效使用LaTeX。&lt;em&gt;LaTeX is a typesetting system, which outperforms Microsoft Word on typesetting. I write this all-in-one manual to tell how to use LaTeX efficiently.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最新版本v1.6.3，于2018年3月发布。&lt;em&gt;Latest version is v1.6.3 releaesd in Mar, 2018.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;正考虑将 Tikz 作为附加章节，届时可能发布带 Tikz与不带 Tikz 两版手册。&lt;em&gt;Considering adding Tikz as an additional chapter. And I might upload two versions of the manual (with/without Tikz respectively) at the next release.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-latex-beamer-en-suspended" class="anchor" aria-hidden="true" href="#latex-beamer-en-suspended"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LaTeX-beamer-en (suspended)&lt;/h2&gt;
&lt;p&gt;（挂起）关于beamer的基础使用，完全使用英文。&lt;em&gt;(Suspended) Basic beamer knowledge. Totally written in English.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-outdate-projects-log" class="anchor" aria-hidden="true" href="#outdate-projects-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Outdate Projects Log&lt;/h1&gt;
&lt;p&gt;过往项目已于2017年10月初取消跟进。读者可以回退到当时的版本进行查找。&lt;/p&gt;
&lt;p&gt;p.s. 在 Release 页面，python-cn 的第一版仍保留了下来。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LaTeX-cn&lt;/strong&gt;: ver 1.6 于2017年6月定稿。&lt;/p&gt;
&lt;p&gt;—— 正在思考如何改进至下一版。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;githubpush-cn&lt;/strong&gt;: 2016年8月24日定稿。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;python-cn&lt;/strong&gt;: 2016年7月22日定稿。&lt;/p&gt;
&lt;p&gt;—— 附：Python安装Lib的&lt;a href="http://www.lfd.uci.edu/~gohlke/pythonlibs" rel="nofollow"&gt;非官方Whl文件下载点&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wklchris</author><guid isPermaLink="false">https://github.com/wklchris/Note-by-LaTeX</guid><pubDate>Mon, 02 Dec 2019 00:13:00 GMT</pubDate></item><item><title>kourgeorge/arxiv-style #14 in TeX, This week</title><link>https://github.com/kourgeorge/arxiv-style</link><description>&lt;p&gt;&lt;i&gt;A Latex style and template for paper preprints (based on NIPS style)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description:&lt;/h2&gt;
&lt;p&gt;The project hosts an aesthetic an simple LaTeX style suitable for "preprint" publications such as arXiv and bio-arXiv, etc.
It is based on the &lt;a href="https://media.nips.cc/Conferences/NIPS2018/Styles/nips_2018.sty" rel="nofollow"&gt;&lt;strong&gt;nips_2018.sty&lt;/strong&gt;&lt;/a&gt; style.&lt;/p&gt;
&lt;p&gt;This styling maintains the esthetic of NIPS but adding and changing features to make it (IMO) even better and nore suitable for preprints.
The result looks fairly different from NIPS style so that readers won't get confused to think that the preprint was published in NIPS.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-why-nips" class="anchor" aria-hidden="true" href="#why-nips"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why NIPS?&lt;/h3&gt;
&lt;p&gt;Because the NIPS styling is a comfortable single column format that is very esthetic and convenient for reading.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Use Document class &lt;strong&gt;article&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Copy &lt;strong&gt;arxiv.sty&lt;/strong&gt; to the folder containing your tex file.&lt;/li&gt;
&lt;li&gt;add &lt;code&gt;\usepackage{arxiv}&lt;/code&gt; after &lt;code&gt;\documentclass{article}&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The only packages used in the style file are &lt;strong&gt;geometry&lt;/strong&gt; and &lt;strong&gt;fancyheader&lt;/strong&gt;. Do not reimport them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See &lt;strong&gt;template.tex&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-project-files" class="anchor" aria-hidden="true" href="#project-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project files:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;arxiv.sty&lt;/strong&gt; - the style file.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;template.tex&lt;/strong&gt; - a sample template that uses the &lt;strong&gt;arxiv style&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;references.bib&lt;/strong&gt; - the bibliography source file for template.tex.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;template.pdf&lt;/strong&gt; - a sample output of the template file that demonstrated the design provided by the arxiv style.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-handling-references-when-submitting-to-arxivorg" class="anchor" aria-hidden="true" href="#handling-references-when-submitting-to-arxivorg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Handling References when submitting to arXiv.org&lt;/h2&gt;
&lt;p&gt;The most convenient way to manage references is using an external BibTeX file and pointing to it from the main file.
However, this requires running the &lt;a href="http://www.bibtex.org/" rel="nofollow"&gt;bibtex&lt;/a&gt; tool to "compile" the &lt;code&gt;.bib&lt;/code&gt; file and create &lt;code&gt;.bbl&lt;/code&gt; file containing "bibitems" that can be directly inserted in the main tex file.
However, unfortunately the arXiv Tex environment (&lt;a href="https://www.tug.org/texlive/" rel="nofollow"&gt;Tex Live&lt;/a&gt;) do not do that.
So easiest way when submitting to arXiv is to create a single self-contained .tex file that contains the references.
This can be done by running the BibTeX command on your machine and insert the content of the generated &lt;code&gt;.bbl&lt;/code&gt; file into the &lt;code&gt;.tex&lt;/code&gt; file and commenting out the &lt;code&gt;\bibliography{references}&lt;/code&gt; that point to the external references file.&lt;/p&gt;
&lt;p&gt;Below are the commands that should be run in the project folder:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;$ latex template&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;$ bibtex template&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;template.bbl&lt;/code&gt; file will be generated (make sure it is there)&lt;/li&gt;
&lt;li&gt;Copy the &lt;code&gt;template.bbl&lt;/code&gt; file content to &lt;code&gt;template.tex&lt;/code&gt; into the &lt;code&gt;\begin{thebibliography}&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Comment out the &lt;code&gt;\bibliography{references}&lt;/code&gt; command in &lt;code&gt;template.tex&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;You ready to submit to arXiv.org.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-general-notes" class="anchor" aria-hidden="true" href="#general-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;General Notes:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;For help, comments, praises, bug reporting or change requests, you can contact the author at: kourgeorge/at/gmail.com.&lt;/li&gt;
&lt;li&gt;You can use, redistribute and do whatever with this project, however, the author takes no responsibility on whatever usage of this project.&lt;/li&gt;
&lt;li&gt;If you start another project based on this project, it would be nice to mention/link to this project.&lt;/li&gt;
&lt;li&gt;You are very welcome to contribute to this project.&lt;/li&gt;
&lt;li&gt;A good looking 2 column template can be found in &lt;a href="https://github.com/brenhinkeller/preprint-template.tex"&gt;https://github.com/brenhinkeller/preprint-template.tex&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>kourgeorge</author><guid isPermaLink="false">https://github.com/kourgeorge/arxiv-style</guid><pubDate>Mon, 02 Dec 2019 00:14:00 GMT</pubDate></item><item><title>rstudio/cheatsheets #15 in TeX, This week</title><link>https://github.com/rstudio/cheatsheets</link><description>&lt;p&gt;&lt;i&gt;RStudio Cheat Sheets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-rstudio-cheat-sheets" class="anchor" aria-hidden="true" href="#rstudio-cheat-sheets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RStudio Cheat Sheets&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="pngs/rstudio-ide.png"&gt;&lt;img src="pngs/rstudio-ide.png" width="364" height="288" align="right" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The cheat sheets make it easy to learn about and use some of our favorite packages. They are published in their respective PDF versions here: &lt;a href="https://www.rstudio.com/resources/cheatsheets/" rel="nofollow"&gt;https://www.rstudio.com/resources/cheatsheets/&lt;/a&gt;, some are also available in the RStudio IDE under Help-Cheatsheets.&lt;/p&gt;
&lt;p&gt;This repository contains the source Apple Keynote files or the current, archived and translated versions.&lt;/p&gt;
&lt;p&gt;The cheat sheets use the creative commons copyright. Please see the LICENSE document for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h2&gt;
&lt;p&gt;If you wish to contribute to this effort by translating a cheat sheet, please feel free to use the source Keynote file. To submit a translation, please use a Pull Request via GitHub or email it to us at &lt;a href="mailto:info@rstudio.com"&gt;info@rstudio.com&lt;/a&gt; with the subject "Translated Cheatsheet".&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tips-for-making-a-new-cheat-sheet" class="anchor" aria-hidden="true" href="#tips-for-making-a-new-cheat-sheet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips for making a new cheat sheet&lt;/h2&gt;
&lt;p&gt;Keep these tips in mind when creating a new cheat sheet:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;RStudio cheat sheets are hosted at &lt;a href="https://github.com/rstudio/cheatsheets"&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;. You can submit new cheat sheets to the repository with a pull request.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The files &lt;code&gt;keynotes/0-template.key&lt;/code&gt; and &lt;code&gt;powerpoints/0-template.ppt&lt;/code&gt; are official templates that contain some helpful tips.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip. You may find it easier to create a new cheat sheet by duplicating the most recent Keynote / Powerpoint cheat sheet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The cheat sheets are not meant to be text documents. Ideally, they are scannable visual aids that use layout and visual mnemonics to help people zoom into the functions they need. As an analogy, think of a cheat sheet as more like a well organized computer menu bar that leads you to a command than a manual that documents each command.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The cheat sheets use a cohesive, black and white printer friendly theme (which is what you see in the sheets), so please stay close to the appearance of the existing sheets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It's already baked into every cheat sheet and the template, but you should include a &lt;a href="https://creativecommons.org/" rel="nofollow"&gt;Creative Commons&lt;/a&gt; Copyright on each side of the sheet to make them easy to repurpose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Budget more time than you expect to make the sheets. So far, I've found this process to be the least time consuming:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the package web page and any vignettes to identify which functions to include (I try to include anything that doesn't seem trivial.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Organize the functions into meaningful, self-explanatory groups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Think about how to visualize the purpose of each function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Think about what key mental models, definitions, or explanations the cheat sheet should contain in addition to the functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sketch out several possible layouts for the sheet. Take care to put the more basic and/or pre-requisite content above and to the left of other content. Try to keep related content on the same side of the page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type out all of the explanations and function definitions. Lay them out. Verify that everything fits. White space is very important. Use it to make the sheet scannable, even if it means smaller text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Making the visuals is the most time consuming part, so I usually save them for last.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tweak until happy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pay attention to the details!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Final tip: Edit the text to be very concise - rely on diagrams where possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rstudio</author><guid isPermaLink="false">https://github.com/rstudio/cheatsheets</guid><pubDate>Mon, 02 Dec 2019 00:15:00 GMT</pubDate></item></channel></rss>