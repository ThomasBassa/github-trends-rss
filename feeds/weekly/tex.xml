<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, This week</title><link>https://github.com/trending/tex?since=weekly</link><description>The top repositories on GitHub for tex, measured weekly</description><pubDate>Thu, 26 Dec 2019 01:12:47 GMT</pubDate><lastBuildDate>Thu, 26 Dec 2019 01:12:47 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>hmemcpy/milewski-ctfp-pdf #1 in TeX, This week</title><link>https://github.com/hmemcpy/milewski-ctfp-pdf</link><description>&lt;p&gt;&lt;i&gt;Bartosz Milewski's 'Category Theory for Programmers' unofficial PDF and LaTeX source&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-category-theory-for-programmers" class="anchor" aria-hidden="true" href="#category-theory-for-programmers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Category Theory for Programmers&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/601206/43392303-f770d7be-93fb-11e8-8db8-b7e915b435ba.png"&gt;&lt;img src="https://user-images.githubusercontent.com/601206/43392303-f770d7be-93fb-11e8-8db8-b7e915b435ba.png" alt="image" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;b&gt;Direct link: &lt;a href="https://github.com/hmemcpy/milewski-ctfp-pdf/releases/download/v1.3.0/category-theory-for-programmers.pdf"&gt;category-theory-for-programmers.pdf&lt;/a&gt;&lt;/b&gt;&lt;br&gt;
(Latest release: v1.3.0, August 2019. See &lt;a href="https://github.com/hmemcpy/milewski-ctfp-pdf/releases"&gt;releases&lt;/a&gt; for additional formats and languages.)&lt;/p&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/hmemcpy/milewski-ctfp-pdf" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8695bdb18f91fa7028908a9172f0b5d051879160/68747470733a2f2f7472617669732d63692e6f72672f686d656d6370792f6d696c6577736b692d637466702d7064662e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/hmemcpy/milewski-ctfp-pdf.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href="https://s3.amazonaws.com/milewski-ctfp-pdf/category-theory-for-programmers.pdf" rel="nofollow"&gt;(latest CI build)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/601206/47271389-8eea0900-d581-11e8-8e81-5b932e336336.png"&gt;&lt;img src="https://user-images.githubusercontent.com/601206/47271389-8eea0900-d581-11e8-8e81-5b932e336336.png" alt="Buy Category Theory for Programmers" width="410" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;&lt;a href="https://www.blurb.com/b/9621951-category-theory-for-programmers-new-edition-hardco" rel="nofollow"&gt;Available in full-color hardcover print&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Publish date: 12 August, 2019. Based off release tag &lt;a href="https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0"&gt;v1.3.0&lt;/a&gt;. See &lt;a href="errata-1.3.0.md"&gt;errata-1.3.0&lt;/a&gt; for changes and fixes since print.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.blurb.com/b/9603882-category-theory-for-programmers-scala-edition-pape" rel="nofollow"&gt;Scala Edition is now available in paperback&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Publish date: 12 August, 2019. Based off release tag &lt;a href="https://github.com/hmemcpy/milewski-ctfp-pdf/releases/tag/v1.3.0"&gt;v1.3.0&lt;/a&gt;. See &lt;a href="errata-scala.md"&gt;errata-scala&lt;/a&gt; for changes and fixes since print.&lt;/p&gt;
&lt;p&gt;This is an &lt;em&gt;unofficial&lt;/em&gt; PDF version of "Category Theory for Programmers" by Bartosz Milewski, converted from his &lt;a href="https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/" rel="nofollow"&gt;blogpost series&lt;/a&gt; (with permission!)&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-building" class="anchor" aria-hidden="true" href="#building"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building&lt;/h2&gt;
&lt;p&gt;The best way to build the book is using the &lt;a href="https://nixos.org/nix/" rel="nofollow"&gt;Nix&lt;/a&gt; package manager. After installing Nix, type &lt;code&gt;nix-shell&lt;/code&gt; in the root directory of the project. This will download all the needed dependencies and tools to build the book (TeXLive, required fonts and packages, Pygments theme for syntax highligting, etc.)&lt;/p&gt;
&lt;p&gt;When the download is complete, and you're prompted with a shell, use the instructions below to build the book.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;src&lt;/code&gt; directory contains the LaTeX sources. To recompile the book, go there and enter:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To build the Scala edition, type:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make scala&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Upon successful compilation, the files will be placed in the &lt;code&gt;out&lt;/code&gt; directory next to &lt;code&gt;src&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The file &lt;code&gt;preamble.tex&lt;/code&gt; contains all the configuration and style declarations.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;PDF LaTeX source and the tools to create it are based on the work by Andres Raba et al., available here: &lt;a href="https://github.com/sarabander/sicp-pdf"&gt;https://github.com/sarabander/sicp-pdf&lt;/a&gt;.&lt;br&gt;
The book content is taken, with permission, from Bartosz Milewski's blogpost series, and adapted to the LaTeX format.&lt;/p&gt;
&lt;p&gt;Thanks to the following people for contributing corrections/conversions and misc:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oleg Rakitskiy&lt;/li&gt;
&lt;li&gt;Jared Weakly&lt;/li&gt;
&lt;li&gt;Paolo G. Giarrusso&lt;/li&gt;
&lt;li&gt;Adi Shavit&lt;/li&gt;
&lt;li&gt;Mico Loretan&lt;/li&gt;
&lt;li&gt;Marcello Seri&lt;/li&gt;
&lt;li&gt;Erwin Maruli Tua Pakpahan&lt;/li&gt;
&lt;li&gt;Markus Hauck&lt;/li&gt;
&lt;li&gt;Yevheniy Zelenskyy&lt;/li&gt;
&lt;li&gt;Ross Kirsling&lt;/li&gt;
&lt;li&gt;...and many others!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note from Bartosz: I really appreciate all your contributions. You made this book much better than I could have imagined. Thank you!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;The PDF book, &lt;code&gt;.tex&lt;/code&gt; files, and associated images and figures in directories &lt;code&gt;src/fig&lt;/code&gt; and &lt;code&gt;src/content&lt;/code&gt; are licensed under Creative Commons Attribution-ShareAlike 4.0 International License (&lt;a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="nofollow"&gt;cc by-sa&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The script files &lt;code&gt;scraper.py&lt;/code&gt; and others are licensed under GNU General Public License version 3 (for details, see &lt;a href="https://github.com/hmemcpy/milewski-ctfp-pdf/blob/master/LICENSE"&gt;LICENSE&lt;/a&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hmemcpy</author><guid isPermaLink="false">https://github.com/hmemcpy/milewski-ctfp-pdf</guid><pubDate>Thu, 26 Dec 2019 00:01:00 GMT</pubDate></item><item><title>Duan-JM/awesome-papers-fewshot #2 in TeX, This week</title><link>https://github.com/Duan-JM/awesome-papers-fewshot</link><description>&lt;p&gt;&lt;i&gt;Collection for Few-shot Learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;Awesome Papers Few-shot focus on collecting paper published on top conferences in Few-shot learning area,
hoping that this cut some time costing for beginners. Morever we also glad to see this repo can be a virtual online seminar,
which can be a home to all researchers who have the enthusiasm to exchange interesting ideas.&lt;/p&gt;
&lt;p&gt;Awesome Papers Few-shot 收集了近几年定会中与小样本学习相关的论文，并简单的进行了类别划分与整理。
一方面，我们希望这个仓库能够帮助广大希望入坑小样本学习的同胞减少入坑搜集论文的时间成本。另一方面，
我们也希望这里能称为研究小样本学习的同胞们互相交流有趣想法的一个小平台。&lt;/p&gt;
&lt;p&gt;The papers collected in this repo are manually selected by myself, I am hoping that more researchers interested in this area can maintain this repo together.&lt;/p&gt;
&lt;p&gt;仓库中收藏的论文均为我本人从历年顶会中手动挑选并阅读过和小样本学习相关的论文，也希望能有广大的同行来共同维护它。
（注意：部分深入解释 Meta-Learning 的论文并未收入到此仓库中，有兴趣的朋友可以发 issue 一起讨论）。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#image-classification"&gt;Image Classification&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#parameter-optimize-based-few-shot-learning"&gt;Parameter Optimize Based Few-shot Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#papers"&gt;Papers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#generative-based-few-shot-learning"&gt;Generative Based Few-shot Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#papers-1"&gt;Papers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#metric-based-few-shot-learning"&gt;Metric Based Few-shot Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#traditional"&gt;Traditional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#semi-supervised"&gt;Semi-Supervised&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#supervised"&gt;Supervised&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#special"&gt;Special&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#unsorted"&gt;Unsorted&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#external-memory"&gt;External Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#architecture"&gt;Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#task-representation-and-measure"&gt;Task Representation and Measure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#multi-label-image-classification"&gt;Multi Label Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#add-additional-informations"&gt;Add Additional Informations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#self-training"&gt;Self-training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#results-in-datasets"&gt;Results in Datasets&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#mini-imagenet"&gt;mini-Imagenet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#more-direction"&gt;More Direction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#object-detection"&gt;Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#segementation"&gt;Segementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#generative-model"&gt;Generative Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#domain-adaptation"&gt;Domain Adaptation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#visual-tracking"&gt;Visual Tracking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#others"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-awesome-resources"&gt;Other Awesome Resources&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#relevant-awesome-datasets-repo"&gt;Relevant Awesome Datasets Repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#relevant-awesome-few-shot-playground-repo"&gt;Relevant Awesome Few-shot PlayGround Repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#relevant-awesome-blogs"&gt;Relevant Awesome Blogs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-recommend-a-paper"&gt;How to recommend a paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#main-contributors"&gt;Main Contributors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;&lt;a id="user-content-image-classification" class="anchor" aria-hidden="true" href="#image-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Classification&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;[arXiv 2019] (&lt;a href="https://arxiv.org/pdf/1904.05046.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Generalizing from a Few Examples A Survey on Few-Shot Learning&lt;/li&gt;
&lt;li&gt;[ICLR 2019] (&lt;a href="https://arxiv.org/pdf/1904.04232" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/wyharveychen/CloserLookFewShot"&gt;code&lt;/a&gt;) A Closer Look At Few-shot Classification&lt;/li&gt;
&lt;li&gt;[arXiv 2019] (&lt;a href="https://arxiv.org/pdf/1909.02729.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) A Baseline for Few-shot Image Classification&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-parameter-optimize-based-few-shot-learning" class="anchor" aria-hidden="true" href="#parameter-optimize-based-few-shot-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parameter Optimize Based Few-shot Learning&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;One line descriptions:&lt;/strong&gt; Generate parameters for the classifier or finetune part of the models&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-papers" class="anchor" aria-hidden="true" href="#papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2017 Meta-learner LSTM Ravi] (&lt;a href="https://openreview.net/pdf?id=rJY0-Kcll&amp;amp;source=post_page---------------------------" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/twitter/meta-learning-lstm."&gt;code&lt;/a&gt;) Optimization as a Model for Few-shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use LSTM to generate classifier's parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[arXiv 2018 REPTILE] (&lt;a href="https://arxiv.org/pdf/1803.02999.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) On First-Order Meta-Learning Algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2018 SNAIL] (&lt;a href="https://arxiv.org/pdf/1707.03141.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) A Simple Neural Attentive Meta- Learner&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improve the Meta-Learner LSTM, by adding temporal convolution and caual attention to the network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2018] (&lt;a href="https://arxiv.org/pdf/1804.09458.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/gidariss/FewShotWithoutForgetting"&gt;code&lt;/a&gt;) Dynamic Few-Shot Visual Learning without Forgetting&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2018] (&lt;a href="https://arxiv.org/pdf/1712.07136.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Low-Shot Learning with Imprinted Weights&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Passing, generate weights for classifier. (I DONOT like it, the exp only compare to matching networks and "Generative + classifier")&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (RECOMMENDED!) (&lt;a href="https://arxiv.org/pdf/1710.06177.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Learning to Learn Image Classifiers with Visual Analogy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1903.05050.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Dense Classification and Implanting for Few-Shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2019] (&lt;a href="https://arxiv.org/pdf/1905.06331.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/likesiwell/LGM-Net/"&gt;code&lt;/a&gt;) LGM-Net: Learning to Generate Matching Networks for Few shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1904.05967.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ucbdrive/tafe-net"&gt;code&lt;/a&gt;) TAFE-Net- Task-Aware Feature Embeddings for Low Shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a meta-learner to generate parameters for the feature extractor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2019] (&lt;a href="https://arxiv.org/pdf/1810.07218.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/renmengye/inc-few-shot-attractor-public"&gt;code&lt;/a&gt;) Incremental Few-Shot Learning with Attention Attractor Networks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using normal way to pretrain the backbone on the base classes, then using the base class weights to fintune the classifier on the few-shot episodic network.&lt;/li&gt;
&lt;li&gt;Achieve the normal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2019 LEO Vinyals] (RECOMMENDED!) (&lt;a href="https://arxiv.org/pdf/1807.05960.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/deepmind/leo"&gt;code&lt;/a&gt;) Meta-learning with latent embedding optimization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High dimensional problem is hard to solve in the low-data circumstances, so this work try to bypass the limitations by learning a data-dependent latent low-dimensional latent space of model parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1905.01102.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/gidariss/wDAE_GNN_FewShot"&gt;code&lt;/a&gt;) Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Little better than LEO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2018] Delta-encoder: an effective sample synthesis method for few-shot object recognition&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2019] Meta-learning with differentiable closed-form solvers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Teach to use tradional machining learning methods&lt;/li&gt;
&lt;li&gt;Most likely no good than LEO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2019] Fast Context Adaptation via Meta-Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Update partial parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-generative-based-few-shot-learning" class="anchor" aria-hidden="true" href="#generative-based-few-shot-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Based Few-shot Learning&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;One line descriptions:&lt;/strong&gt; Generate features to expasion small datasets to large datasets, then fintune.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[NIPS 2018 Bengio] (&lt;a href="https://papers.nips.cc/paper/7504-metagan-an-adversarial-approach-to-few-shot-learning.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) MetaGAN An Adversarial Approach to Few-Shot Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-papers-1" class="anchor" aria-hidden="true" href="#papers-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;[ICCV 2017] (&lt;a href="https://arxiv.org/pdf/1606.02819.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/facebookresearch/low-shot-shrink-hallucinate"&gt;code&lt;/a&gt;) Low-shot Visual Recognition by Shrinking and Hallucinating Features&lt;/li&gt;
&lt;li&gt;[CVPR 2018] (&lt;a href="https://arxiv.org/pdf/1801.05401.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Low-Shot Learning from Imaginary Data&lt;/li&gt;
&lt;li&gt;[NIPS 2018] (&lt;a href="https://arxiv.org/pdf/1810.11730.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks&lt;/li&gt;
&lt;li&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1812.01784.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/edgarschnfld/CADA-VAE-PyTorc"&gt;code&lt;/a&gt;) Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders
&lt;ul&gt;
&lt;li&gt;Aiming at cross modal situations&lt;/li&gt;
&lt;li&gt;Using encode image x and class descriptor y, then pull the mean and variance of them together by a loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[CVPR 2019 IDeMe-Net] (&lt;a href="https://arxiv.org/pdf/1905.11641.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tankche1/IDeMe-Net."&gt;code&lt;/a&gt;) Image Deformation Meta-Networks for One-Shot Learning
&lt;ul&gt;
&lt;li&gt;This paper assumes that deformed images may not be visually realistic, they still maintain critical semantic information.&lt;/li&gt;
&lt;li&gt;Pretty good at one-shot on mini-imagenet(59.14%)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-metric-based-few-shot-learning" class="anchor" aria-hidden="true" href="#metric-based-few-shot-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metric Based Few-shot Learning&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;One line descriptions:&lt;/strong&gt; Compute the class representation, then use metric functions to measure the similarity between query sample and each class representaions.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-traditional" class="anchor" aria-hidden="true" href="#traditional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Traditional&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2012] One-Shot Learning with a Hierarchical Nonparametric Bayesian Model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using Hierarchical Bayesian Model after extracted features. Which is similar to build the category graph method in IJCAI 2019.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2015] Siamese Neural Networks for One-Shot Image Recognition&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2016] Matching Networks for One Shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This methods cast the problem of one-shot learning within the set-to-set framework&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2017]  (RECOMMENDED!) Prototypical Networks for Few-shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2018] Learning to Compare：Relation Network for Few-Shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change metric functions to CNNs&lt;/li&gt;
&lt;li&gt;Provide a clean framework that elegantly encompasses both few and zero-shot learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-semi-supervised" class="anchor" aria-hidden="true" href="#semi-supervised"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semi-Supervised&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2018 Ravi] (&lt;a href="https://arxiv.org/pdf/1803.00676.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/renmengye/few-shot-ssl-public"&gt;code&lt;/a&gt;) Meta-Learning for Semi-Supervised Few-Shot Classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using soft K-means to refine the prototypes, then using varient ways(training methods) to eliminate the outline points.&lt;/li&gt;
&lt;li&gt;Create new datasets - tiredImagenet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2018] Few-Shot Learning with Graph Neural Networks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2018] (RECOMMENDED!) Low-Shot Learning With Large-Scale Diffusion&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2019] (&lt;a href="https://arxiv.org/pdf/1805.10002.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Learning to Propagate Labels-transductive Propagation Network for Few-shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1905.01436.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Edge-Labeling Graph Neural Network for Few-shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-supervised" class="anchor" aria-hidden="true" href="#supervised"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2017] Few-Shot Learning Through an Information Retrieval Lens&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2018] (RECOMMENDED!) TADAM-Task dependent adaptive metric for improved few-shot learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In every task, use task representations to finetune the output of each Conv Blocks, like BN functionally.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ECCV 2018] (&lt;a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Dynamic Conditional Networks for FewShot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Basically same as TADAM(using task representation to finetune the backbones), it use a conditional feature to influence the output of conv layers (Linear combination).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1905.11116.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/Clarifai/few-shot-ctm."&gt;code&lt;/a&gt;) Finding Task-Relevant Features for Few-Shot Learning by Category Traversal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2019] (RECOMMENDED!) (&lt;a href="https://arxiv.org/pdf/1905.06549.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2019] (&lt;a href="https://arxiv.org/pdf/1902.04552.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) (RECOMMENDED!) Infinite Mixture Prototypes for Few-shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Point out that data distribution for one class are not uni-model (Verify in my experiments too).&lt;/li&gt;
&lt;li&gt;(Clustering methods) Semi-Supervised methods for prototypical networks. Show this methods even suit for unsupervised situations(protentially).&lt;/li&gt;
&lt;li&gt;Improve on Alphabets dataset, remain or improve on omniglot and mini-imagenet.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (&lt;a href="https://arxiv.org/pdf/1908.05257" rel="nofollow"&gt;paper&lt;/a&gt;) Few-Shot Learning with Global Class Representations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Synthesis new samples to elleviate the data imbalance problem between Base and Novel Classes.&lt;/li&gt;
&lt;li&gt;During training, compute two losses, one is the original losses, the other is the score for the whole classes including noval classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[IJCAI 2019] (&lt;a href="https://arxiv.org/pdf/1905.04042" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/liulu112601/PPN"&gt;code&lt;/a&gt;) Prototype Propagation Networks (PPN) for Weakly-supervised Few-shot Learning on Category Graph&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Maually build an category graph, then add parents label's class represention into the child class representations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (RECOMMENDED) (&lt;a href="https://arxiv.org/pdf/1904.08482.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/mibastro/VPE"&gt;code&lt;/a&gt;) Variational Prototyping-Encoder- One-Shot Learning with Prototypical Images&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use encoder to translate the real images to abstract prototypes, such as painted traffic signs, then compare query and sample in the prototypes latent space.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2019] (&lt;a href="https://arxiv.org/pdf/1902.07104.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Adaptive Cross-Modal Few-shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using texture information to enhance the performance, which reach a comparable result on mini-imagenet&lt;/li&gt;
&lt;li&gt;Perform well on 1-shot rather than 5-shot or 10-shot&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chu_Spot_and_Learn_A_Maximum-Entropy_Patch_Sampler_for_Few-Shot_Image_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Spot and Learn A Maximum-Entropy Patch Sampler for Few-Shot Image Classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample parts of the image to form the batch to represent the class.&lt;/li&gt;
&lt;li&gt;One-shot not pretty good(51%)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1906.01905.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Baby steps towards few-shot learning with multiple semantics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Show 4.5 years old baby perform 70% on 1-shot case, adult achieve 99%.&lt;/li&gt;
&lt;li&gt;Add multi-semantic into the task.&lt;/li&gt;
&lt;li&gt;However on 5-shot case LEO perform exceed both this paper and the paper above with no semantics information.&lt;/li&gt;
&lt;li&gt;For 1-shot case, this method achieve 67.2% +- 0.4% compare to 70% of human baby performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1904.08502" rel="nofollow"&gt;paper&lt;/a&gt;) Few-Shot Learning with Localization in Realistic Settings&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Locate the object in the images first, then classify them.&lt;/li&gt;
&lt;li&gt;Classify in real-world images, somehow not interesting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2019] (&lt;a href="https://arxiv.org/pdf/1910.07677.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Cross Attention Network for Few-shot Classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a attention(mask) to pay more attention on the part of the images&lt;/li&gt;
&lt;li&gt;Add transductive inference part&lt;/li&gt;
&lt;li&gt;Pretty good result on mini-imagenet 80.64 +- 0.35% under ResNet-12 (16 conv layers)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1903.12290.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calculating the similarity between query and class represent feature in feature level, rather than instance level. It seperate original feature in m part and then compute the similarity to the K-nearst class partial features.&lt;/li&gt;
&lt;li&gt;Good Result on mini-ImageNet 71.02 ± 0.64% with Conv4_64F.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] Large-Scale Few-Shot Learning- Knowledge Transfer With Class Hierarchy&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aiming at learning large-scale problem, not just on 5 novel class.&lt;/li&gt;
&lt;li&gt;Using the Class Names embeddings(text embedding) to form a class hierarchy.&lt;/li&gt;
&lt;li&gt;Get a pretter higher result than existing methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1812.02391v2.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Meta-Transfer Learning for Few-Shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not like it, for the results are not significant, nearly no improve on 5 way 5 shot on mini-ImageNet.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2018] (&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Temporal_Hallucinating_for_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Temporal Hallucinating for Action Recognition with Few Still Images&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attempt to recall cues from relevant action videos.&lt;/li&gt;
&lt;li&gt;Maybe good at one-shot, not worse than the baseline in 5-shot and 10-shot scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2019] Learning to Propagate for Graph Meta-Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learns to propagate messages between prototypes of different classes on the graph, so that learning the prototype of each class benefits from the data of other related classes.&lt;/li&gt;
&lt;li&gt;Attention mechanic.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Hao_Collect_and_Select_Semantic_Alignment_Metric_Learning_for_Few-Shot_Learning_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt; Collect and Select: Semantic Alignment Metric Learning for Few-Shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use attention to pick(Select) most relevant part to compare&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2019] Distribution Consistency based Covariance Metric Networks for Few Shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slight improve on 1-shot compare to Relation Network, however degenerate on 5-shot compare to Protoypical Network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2019] A Dual Attention Network with Semantic Embedding for Few-shot Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add spatial attention and task attention.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-special" class="anchor" aria-hidden="true" href="#special"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Special&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-unsorted" class="anchor" aria-hidden="true" href="#unsorted"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsorted&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[Nature 子刊 MI 2018] (&lt;a href="https://arxiv.org/pdf/1810.01256.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Continuous Learning of Context-dependent Processing in Neural Networks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During training a network consecutively for different tasks, OWNs weights are only allowed to be modified in the direction orthogonal to the subspace spanned by all inputs on which the network has been trained (termed input space hereafter). This ensures that new learning processes will not interfere with the learned tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (RECOMMANDED!) (&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Diversity with Cooperation: Ensemble Methods for Few-Shot Classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New way to solve few-shot learning problems without meta-learing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-external-memory" class="anchor" aria-hidden="true" href="#external-memory"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;External Memory&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2016] Meta-Learning with Memory-Augmented Neural Networks&lt;/p&gt;
&lt;p&gt;This work lead NTM into the image classification, technically, this work should not belong to the few-shot problems.
This method can identify the image labels, even the true label of current image are inputed along with the next image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2018] (&lt;a href="https://arxiv.org/pdf/1804.08281.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Memory Matching Networks for One-Shot Image Recognition&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICLR 2019] (&lt;a href="https://arxiv.org/pdf/1902.02527.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/cogentlabs/apl."&gt;code&lt;/a&gt;) Adaptive Posterior Learning-Few-Shot Learning with a Surprise-Based Memory Module&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-architecture" class="anchor" aria-hidden="true" href="#architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Architecture&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[ICML 2017] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1805.07722.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Task-Agnostic Meta-Learning for Few-shot Learning&lt;/p&gt;
&lt;p&gt;A training method force model to learn a unbiased initial model without over-performing on some particular tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-task-representation-and-measure" class="anchor" aria-hidden="true" href="#task-representation-and-measure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Task Representation and Measure&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;[ICCV 2019] (&lt;a href="https://arxiv.org/pdf/1902.03545.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) (RECOMMENDED!) TASK2VEC- Task Embedding for Meta-Learning
&lt;ul&gt;
&lt;li&gt;Use Fisher information matrix to judge which backbone is suitable for current task.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-multi-label-image-classification" class="anchor" aria-hidden="true" href="#multi-label-image-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi Label Image Classification&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;[CVPR 2019 oral] (&lt;a href="https://arxiv.org/pdf/1902.09811.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) LaSO-Label-Set Operations networks for multi-label few-shot learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-add-additional-informations" class="anchor" aria-hidden="true" href="#add-additional-informations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Add Additional Informations&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (&lt;a href="https://arxiv.org/pdf/1812.09213.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://sites.google.com/view/comprepr/home" rel="nofollow"&gt;code&lt;/a&gt;) Learning Compositional Representations for Few-Shot Recognition&lt;/p&gt;
&lt;p&gt;Add additional annotations to the classes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1904.03472.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Few-shot Learning via Saliency-guided Hallucination of Samples&lt;/p&gt;
&lt;p&gt;Form segmentations and mix up, aiming at eliminates the back ground noise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (&lt;a href="https://arxiv.org/pdf/1906.05186.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Boosting Few-Shot Visual Learning with Self-Supervision&lt;/p&gt;
&lt;p&gt;Self-supervision means to rotate itself, and compute two losses.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-self-training" class="anchor" aria-hidden="true" href="#self-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Self-training&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2019] (&lt;a href="https://arxiv.org/pdf/1906.00562.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Learning to Self-Train for Semi-Supervised Few-Shot Classification&lt;/p&gt;
&lt;p&gt;Label the query set for the first run, then retrain the model with the pesudo label for the second run. (Simple but effective)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-results-in-datasets" class="anchor" aria-hidden="true" href="#results-in-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results in Datasets&lt;/h2&gt;
&lt;p&gt;Basically, we use &lt;a href="https://github.com/brendenlake/omniglot"&gt;Omniglot&lt;/a&gt;, &lt;a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning" rel="nofollow"&gt;mini-Imagenet&lt;/a&gt;,
&lt;a href="https://arxiv.org/abs/1803.00676" rel="nofollow"&gt;tiredImagenet&lt;/a&gt;, &lt;a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html" rel="nofollow"&gt;CUB 2011&lt;/a&gt; and full &lt;a href="http://image-net.org" rel="nofollow"&gt;Imagenet&lt;/a&gt; for the datasets. We list the latest methods' performs in mini-Imagenet.
Welcome contributes to expand the tables of results.&lt;/p&gt;
&lt;p&gt;基本上在小样本图像分类领域，主流的数据集为 Omniglot，mini-Imagenet，tired-Imagenet，CUB 和完整的 ImageNet。在这里我们总结了当前已有的方法在 mini-ImageNet 上的表现。
非常欢迎大家来补充呀。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-mini-imagenet" class="anchor" aria-hidden="true" href="#mini-imagenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning" rel="nofollow"&gt;mini-Imagenet&lt;/a&gt;&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Years&lt;/th&gt;
&lt;th&gt;Methods&lt;/th&gt;
&lt;th&gt;Backbone&lt;/th&gt;
&lt;th&gt;5-way 1-shot&lt;/th&gt;
&lt;th&gt;5-way 5-shot&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td&gt;Matching Network&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;43.56 +- 0.84%&lt;/td&gt;
&lt;td&gt;55.31% +- 0.73%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;MAML&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;48.7% +- 1.84%&lt;/td&gt;
&lt;td&gt;63.15% +- 0.91%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;Prototypical Network&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;49.42% +- 0.78%&lt;/td&gt;
&lt;td&gt;68.20% +- 0.66%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;Relation Network&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;50.44% +- 0.82%&lt;/td&gt;
&lt;td&gt;65.32% +- 0.70%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;MetaGAN: An Adversarial Approach to Few-Shot Learning&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;46.13+-1.78%&lt;/td&gt;
&lt;td&gt;60.71+-0.89%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Incremental Few-Shot Learning with Attention Attractor Networks&lt;/td&gt;
&lt;td&gt;ResNet-10&lt;/td&gt;
&lt;td&gt;54.95+-0.30&lt;/td&gt;
&lt;td&gt;63.04+-0.30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Adaptive Cross-Modal Few-shot Learning&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;65.30 ±0.49%&lt;/td&gt;
&lt;td&gt;78.10 ± 0.36%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Cross Attention Network for Few-shot Classification&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;67.19 ± 0.55&lt;/td&gt;
&lt;td&gt;80.64 ± 0.35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Learning to Self-Train for Semi-Supervised Few-Shot Classification&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;70.1 ± 1.9&lt;/td&gt;
&lt;td&gt;78.7 ± 0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning&lt;/td&gt;
&lt;td&gt;Conv-64F&lt;/td&gt;
&lt;td&gt;51.24±0.74%&lt;/td&gt;
&lt;td&gt;71.02±0.64%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Few-Shot Learning with Localization in Realistic Settings&lt;/td&gt;
&lt;td&gt;ResNet-50&lt;/td&gt;
&lt;td&gt;49.64±.31%&lt;/td&gt;
&lt;td&gt;69.45±.28%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Baby steps towards few-shot learning with multiple semantics&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;67.2 ± 0.4%&lt;/td&gt;
&lt;td&gt;74.8 ± 0.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning&lt;/td&gt;
&lt;td&gt;WRN-28-10&lt;/td&gt;
&lt;td&gt;62.96+-0.15%&lt;/td&gt;
&lt;td&gt;78.85+-0.10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Spot and Learn: A Maximum-Entropy Patch Sampler for Few-Shot Image Classification&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;47.18+-0.83%&lt;/td&gt;
&lt;td&gt;66.41+-0.67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Meta-Transfer Learning for Few-Shot Learning&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;61.2+-1.8%&lt;/td&gt;
&lt;td&gt;75.5+-0.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Dense Classification and Implanting for Few-Shot Learning&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;62.53+-0.19%&lt;/td&gt;
&lt;td&gt;78.95+-0.13%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Edge-Labeling Graph Neural Network for Few-shot Learning&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;66.85%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Finding Task-Relevant Features for Few-Shot Learning by Category Traversal&lt;/td&gt;
&lt;td&gt;COnv4&lt;/td&gt;
&lt;td&gt;41.62%&lt;/td&gt;
&lt;td&gt;58.77%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Few-shot Learning via Saliency-guided Hallucination of Samples&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;65.30 ±0.49%&lt;/td&gt;
&lt;td&gt;78.10 ± 0.36%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;Memory Matching Networks for One-Shot Image Recognition&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;53.37+-0.48%&lt;/td&gt;
&lt;td&gt;66.97+-0.35%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;Dynamic Few-Shot Visual Learning without Forgetting&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;55.45+-0.89%&lt;/td&gt;
&lt;td&gt;70.13+-0.68%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;Few-Shot Learning with Global Class Representations&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;53.21+-0.40%&lt;/td&gt;
&lt;td&gt;72.34+-0.32%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;69.13+-0.35%&lt;/td&gt;
&lt;td&gt;72.28+-0.68%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning&lt;/td&gt;
&lt;td&gt;ResNet-12&lt;/td&gt;
&lt;td&gt;61.65 ± 0.15%&lt;/td&gt;
&lt;td&gt;76.36 ± 0.10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;META-LEARNING WITH LATENT EMBEDDING OPTIMIZATION&lt;/td&gt;
&lt;td&gt;WRN-28- 10&lt;/td&gt;
&lt;td&gt;61.76+-0.08%&lt;/td&gt;
&lt;td&gt;77.59+-0.12%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;55.51%&lt;/td&gt;
&lt;td&gt;69.86%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;META-LEARNING FOR SEMI-SUPERVISED FEW-SHOT CLASSIFICATION&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;50.09+-0.45%&lt;/td&gt;
&lt;td&gt;64.59+-0.28%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;A SIMPLE NEURAL ATTENTIVE META-LEARNER&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;55.71+-0.99%&lt;/td&gt;
&lt;td&gt;68.88+-0.92%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;43.44+-0.77%&lt;/td&gt;
&lt;td&gt;60.60+-0.71%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Centroid Networks for Few-Shot Clustering and Unsupervised Few-Shot Classification&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;62.6+-0.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;Infinite Mixture Prototypes for Few-Shot Learning&lt;/td&gt;
&lt;td&gt;Conv4&lt;/td&gt;
&lt;td&gt;49.6+-0.8%&lt;/td&gt;
&lt;td&gt;68.1+-0.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-more-direction" class="anchor" aria-hidden="true" href="#more-direction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Direction&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-object-detection" class="anchor" aria-hidden="true" href="#object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[CVPR 2017] (&lt;a href="https://arxiv.org/pdf/1612.02559.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/rkwitt/GuidedAugmentation"&gt;code&lt;/a&gt;) AGA - Attribute-Guided Augmentation
&lt;ul&gt;
&lt;li&gt;Using external depth and pose informations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/abs/1806.04728" rel="nofollow"&gt;paper&lt;/a&gt;) RepMet-Representative-based Metric Learning for Classification and Few-shot Object Detection&lt;/li&gt;
&lt;li&gt;[CVPR 2019] (&lt;a href="https://arxiv.org/pdf/1903.09372.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Few-shot Adaptive Faster R-CNN&lt;/li&gt;
&lt;li&gt;[CVPR 2019] Feature Selective Anchor-Free Module for Single-Shot Object Detection&lt;/li&gt;
&lt;li&gt;[ICCV 2019] (&lt;a href="https://arxiv.org/pdf/1812.01866" rel="nofollow"&gt;paper&lt;/a&gt;) Few-shot Object Detection via Feature Reweighting&lt;/li&gt;
&lt;li&gt;[NIPS 2019] (&lt;a href="https://arxiv.org/abs/1911.12529" rel="nofollow"&gt;papre&lt;/a&gt;)One-Shot Object Detection with Co-Attention and Co-Excitation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-segementation" class="anchor" aria-hidden="true" href="#segementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Segementation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] CANet- Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2019] (&lt;a href="https://www.researchgate.net/publication/335296764_Attention-Based_Multi-Context_Guiding_for_Few-Shot_Semantic_Segmentation" rel="nofollow"&gt;paper&lt;/a&gt;) Attention-based Multi-Context Guiding for Few-Shot Semantic Segmentation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Utilize the output of the different layers between query branch and support branch to gain more context informations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[arXiv 2019] AMP-Adaptive Masked Proxies for Few-Shot Segmentation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not sure result in this area.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2019] Unsupervised Meta-learning of Figure-Ground Segmentation via Imitating Visual Effects&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Differetiate the background from images.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generative-model" class="anchor" aria-hidden="true" href="#generative-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ICCV 2019] (&lt;a href="https://arxiv.org/pdf/1905.01723" rel="nofollow"&gt;paper&lt;/a&gt;) Few-Shot Unsupervised Image-to-Image Translation&lt;/li&gt;
&lt;li&gt;[ICCV 2019 best] (&lt;a href="https://arxiv.org/abs/1905.01164" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tamarott/SinGAN"&gt;code&lt;/a&gt;) SinGAN: Learning a Generative Model from a Single Natural Image&lt;/li&gt;
&lt;li&gt;[CVPR 2018] Multi-Content GAN for Few-Shot Font Style Transfer&lt;/li&gt;
&lt;li&gt;[NIPS 2019] ( &lt;a href="https://nvlabs.github.io/few-shot-vid2vid/main.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://nvlabs.github.io/few-shot-vid2vid/" rel="nofollow"&gt;code&lt;/a&gt; )Few-shot Video-to-Video Synthesis&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-domain-adaptation" class="anchor" aria-hidden="true" href="#domain-adaptation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Domain Adaptation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[NIPS 2017] Few-Shot Adversarial Domain Adaptation&lt;/li&gt;
&lt;li&gt;[ICCV 2019] Bidirectional One-Shot Unsupervised Domain Mapping&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reinforcement-learning" class="anchor" aria-hidden="true" href="#reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reinforcement Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ICML 2019] Few-Shot Intent Inference via Meta-Inverse Reinforcement Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-visual-tracking" class="anchor" aria-hidden="true" href="#visual-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Tracking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[ICCV 2019] Deep Meta Learning for Real-Time Target-Aware Visual Tracking&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-others" class="anchor" aria-hidden="true" href="#others"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[IJCAI 2019] Incremental Few-Shot Learning for Pedestrian Attribute Recognition&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2018] AffinityNet- Semi-supervised Few-shot Learning for Disease Type Prediction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use few-shot method to enhance oringal disease type prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[arXiv 2019] (&lt;a href="https://arxiv.org/pdf/1902.10482.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) Few-Shot Text Classification with Induction Network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduce dynamic routing to generate better class representations. One real industrial project.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NIPS 2018] Neural Voice Cloning with a Few Samples&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[IJCAI 2019] Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (&lt;a href="https://arxiv.org/pdf/1909.01205" rel="nofollow"&gt;paper&lt;/a&gt;) Few-Shot Generalization for Single-Image 3D Reconstruction via Priors&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (&lt;a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_ACMM_Aligned_Cross-Modal_Memory_for_Few-Shot_Image_and_Sentence_Matching_ICCV_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;) ACMM: Aligned Cross-Modal Memory for Few-Shot Image and Sentence Matching&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[ICCV 2019] (RECOMMANDED!) Task-Driven Modular Networks for Zero-Shot Compositional Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An interesting usage of a bunch of MLPs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[CVPR 2019] (&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Rostami_SAR_Image_Classification_Using_Few-Shot_Cross-Domain_Transfer_Learning_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/MSiam/AdaptiveMaskedProxies."&gt;code&lt;/a&gt;) SAR Image Classification Using Few-shot Cross-domain Transfer Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2019] Hybrid Attention-based Prototypical Networks for Noisy Few-Shot Relation Classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relation Classification with FewRel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2019] Few-Shot Image and Sentence Matching via Gated Visual-Semantic Embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image and Sentence Matching&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[AAAI 2018] Few Shot Transfer Learning BetweenWord Relatedness and Similarity Tasks Using A Gated Recurrent Siamese Network&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-other-awesome-resources" class="anchor" aria-hidden="true" href="#other-awesome-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other Awesome Resources&lt;/h1&gt;
&lt;p&gt;We collect some awesome code and blogs here.
(Note that if you are now writing a few-shot papers, feel free to checkout &lt;code&gt;resources&lt;/code&gt; file to get some bib there)&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-relevant-awesome-datasets-repo" class="anchor" aria-hidden="true" href="#relevant-awesome-datasets-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relevant Awesome Datasets Repo&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-relevant-awesome-few-shot-playground-repo" class="anchor" aria-hidden="true" href="#relevant-awesome-few-shot-playground-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relevant Awesome Few-shot PlayGround Repo&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-relevant-awesome-blogs" class="anchor" aria-hidden="true" href="#relevant-awesome-blogs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relevant Awesome Blogs&lt;/h3&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-recommend-a-paper" class="anchor" aria-hidden="true" href="#how-to-recommend-a-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to recommend a paper&lt;/h1&gt;
&lt;p&gt;You are highly welcome to recommend a paper to this repo.
The only thing you need to do is make a new issue with its name, conference name, years and some recommends words(no more than 400 words).&lt;/p&gt;
&lt;p&gt;非常欢迎大家来推荐相关论文呀，推荐论文的方式非常简单，只需要提交一个 Issue，并在 Issue 中写清楚论文的题目，发表的会议名称以及年份和一个不超过 400 字的推荐理由即可。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;EXAMPLE&lt;/p&gt;
&lt;p&gt;Title: [ICML 2019] TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning&lt;/p&gt;
&lt;p&gt;Recommend: First paper point out how to measure the backbone is bad or good for the current task(episode).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-main-contributors" class="anchor" aria-hidden="true" href="#main-contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Contributors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="www.github.com/Duan-JM"&gt;Duan-JM&lt;/a&gt; (Image Classification)&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Duan-JM</author><guid isPermaLink="false">https://github.com/Duan-JM/awesome-papers-fewshot</guid><pubDate>Thu, 26 Dec 2019 00:02:00 GMT</pubDate></item><item><title>TheNetAdmin/zjuthesis #3 in TeX, This week</title><link>https://github.com/TheNetAdmin/zjuthesis</link><description>&lt;p&gt;&lt;i&gt;Zhejiang University Graduation Thesis/Design LaTeX template.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-浙江大学毕业设计论文-latex-模板" class="anchor" aria-hidden="true" href="#浙江大学毕业设计论文-latex-模板"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;浙江大学毕业设计/论文 LaTeX 模板&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c848d901a7d7639fde1c3e45d0169452427a7bff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a6a757468657369732d6c617465782d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/c848d901a7d7639fde1c3e45d0169452427a7bff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7a6a757468657369732d6c617465782d626c75652e737667" alt="ZJUTHESIS" data-canonical-src="https://img.shields.io/badge/zjuthesis-latex-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15b3ad872462a85b94409993e150cb32acb16816/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f5468654e657441646d696e2f7a6a757468657369732e7376673f6c6162656c3d76657273696f6e267374796c653d706f706f7574"&gt;&lt;img src="https://camo.githubusercontent.com/15b3ad872462a85b94409993e150cb32acb16816/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f5468654e657441646d696e2f7a6a757468657369732e7376673f6c6162656c3d76657273696f6e267374796c653d706f706f7574" alt="GitHub release" data-canonical-src="https://img.shields.io/github/release/TheNetAdmin/zjuthesis.svg?label=version&amp;amp;style=popout" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b7c21de3ba81250381a9107cdd69ea0dc3cd133f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f7468656e657461646d696e2f7a6a757468657369732f746f74616c2e7376673f636f6c6f723d626c7565267374796c653d706f706f7574"&gt;&lt;img src="https://camo.githubusercontent.com/b7c21de3ba81250381a9107cdd69ea0dc3cd133f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f7468656e657461646d696e2f7a6a757468657369732f746f74616c2e7376673f636f6c6f723d626c7565267374796c653d706f706f7574" alt="GitHub All Releases" data-canonical-src="https://img.shields.io/github/downloads/thenetadmin/zjuthesis/total.svg?color=blue&amp;amp;style=popout" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TheNetAdmin/zjuthesis/workflows/Build%20Tests/badge.svg"&gt;&lt;img src="https://github.com/TheNetAdmin/zjuthesis/workflows/Build%20Tests/badge.svg" alt="GithubAction" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;本项目为浙江大学毕业设计/论文的 LaTeX 模板，包含本科生、硕士生与博士生模板，以及英文硕博士模板。&lt;/p&gt;
&lt;p&gt;This is a LaTeX template for Zhejiang University graduation thesis/design.
It comes with undergraduate and graduate (master and phd) template.
It also comes with an English template for graduate students.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.cc98.org/topic/4762356" rel="nofollow"&gt;CC98校内讨论贴&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-使用" class="anchor" aria-hidden="true" href="#使用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/TheNetAdmin/zjuthesis/releases"&gt;下载模板代码&lt;/a&gt;，
每个专业模板都有预览 pdf 文件，可以单独下载查看。
模板代码请下载 &lt;code&gt;zjuthesis-vx.x.x.zip&lt;/code&gt; 文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装 TeXLive 工具包，编译需要 XeTeX 引擎。安装所需的镜像文件可以选用浙江大学开源镜像站提供的&lt;a href="https://mirrors.zju.edu.cn/CTAN/systems/texlive/Images/" rel="nofollow"&gt;镜像&lt;/a&gt;以便在校内网下更快下载。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;code&gt;zjuthesis.tex&lt;/code&gt; 中 &lt;code&gt;\documentclass[]{zjuthesis}&lt;/code&gt; 部分填写个人信息，注意以下信息用于控制文档的生成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Degree&lt;/code&gt; 为 &lt;code&gt;undergraduate&lt;/code&gt; 时，编译本科生论文：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Type&lt;/th&gt;
&lt;th align="left"&gt;Period&lt;/th&gt;
&lt;th align="left"&gt;BlindReview&lt;/th&gt;
&lt;th align="left"&gt;MajorFormat&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;thesis: 论文类&lt;/td&gt;
&lt;td align="left"&gt;proposal: 开题报告&lt;/td&gt;
&lt;td align="left"&gt;true: 生成盲审用pdf（隐藏个人信息）&lt;/td&gt;
&lt;td align="left"&gt;默认: general&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;design: 设计类&lt;/td&gt;
&lt;td align="left"&gt;final: 最终论文/设计&lt;/td&gt;
&lt;td align="left"&gt;false: 生成提交用pdf&lt;/td&gt;
&lt;td align="left"&gt;与 &lt;code&gt;config/format/major/&lt;/code&gt; 下目录名相同&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Degree&lt;/code&gt; 为 &lt;code&gt;graduate&lt;/code&gt; 时，编译硕士生/博士生论文：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Type&lt;/th&gt;
&lt;th align="left"&gt;BlindReview&lt;/th&gt;
&lt;th align="left"&gt;MajorFormat&lt;/th&gt;
&lt;th align="left"&gt;GradLevel&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;thesis: 学术论文&lt;/td&gt;
&lt;td align="left"&gt;true: 生成盲审用pdf（隐藏个人信息）&lt;/td&gt;
&lt;td align="left"&gt;默认: general&lt;/td&gt;
&lt;td align="left"&gt;master: 硕士&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;design: 专业学术论文&lt;/td&gt;
&lt;td align="left"&gt;false: 生成提交用pdf&lt;/td&gt;
&lt;td align="left"&gt;与 &lt;code&gt;config/format/major/&lt;/code&gt; 下目录名相同&lt;/td&gt;
&lt;td align="left"&gt;doctor: 博士&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;equal: 同等学力论文&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;code&gt;body&lt;/code&gt; 目录下编写内容&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;code&gt;pages&lt;/code&gt; 目录下填写必要的内容，如审核评语等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;code&gt;figure&lt;/code&gt; 目录下保存图片，在 &lt;code&gt;body/ref.bib&lt;/code&gt; 内插入文献条目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在根目录下运行命令 &lt;code&gt;latexmk&lt;/code&gt;（或者 &lt;code&gt;latexmk -xelatex -outdir=out zjuthesis&lt;/code&gt;）即可编译 pdf 文件到 &lt;code&gt;out&lt;/code&gt; 目录（该目录不会被记录版本）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本模板已经兼容 TeXLive 2019，并且这个版本复制伪粗体文字不会产生乱码，建议单独使用 TeXLive 的同学尽快升级 2019 版本&lt;/li&gt;
&lt;li&gt;本模板默认情况下使用 &lt;code&gt;general&lt;/code&gt; 格式，如需使用其他专业格式，请修改 &lt;code&gt;zjuthesis.tex&lt;/code&gt; 中 &lt;code&gt;\documentclass&lt;/code&gt; 部分的 &lt;code&gt;MajorFormat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;计算机专业的部分页面与学校通用格式不同，如果你是计算机专业的同学，请使用计算机专业的模板&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-for-english-graduate-template-user" class="anchor" aria-hidden="true" href="#for-english-graduate-template-user"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;For English graduate template user&lt;/h2&gt;
&lt;p&gt;Set &lt;code&gt;\documentclass&lt;/code&gt; fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Degree = graduate&lt;/li&gt;
&lt;li&gt;Type = thesis/design&lt;/li&gt;
&lt;li&gt;MajorFormat = general&lt;/li&gt;
&lt;li&gt;Language = english&lt;/li&gt;
&lt;li&gt;GradLevel = master/doctor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This template only comes with English graduate student template,
if you are interested in English undergraduate template,
please open a new issue and discuss the details and requirements.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-扩展" class="anchor" aria-hidden="true" href="#扩展"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;扩展&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;针对每个专业的扩展格式编写请新建目录 &lt;code&gt;config/format/major/专业简称&lt;/code&gt; ，在该目录下固定新建文件 &lt;code&gt;format.tex&lt;/code&gt;，引入该目录下所有格式设置文件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;扩展格式的 &lt;code&gt;\usepackage{packagename}&lt;/code&gt; 尽量放在其所在子目录下的 &lt;code&gt;packages.tex&lt;/code&gt; 内，不要放在 &lt;code&gt;config/packages.tex&lt;/code&gt; 内。&lt;/p&gt;
&lt;p&gt;这样可以避免其他专业同学使用时产生 package 冲突或额外引入。
同时由于 XeTeX 编译速度很慢，减少不必要的 package 可以提高编译效率。样例模板使用了 Tikz package，如果大家写论文时不需要这个包，可以将其删除。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最后修改 &lt;code&gt;zjuthesis.tex&lt;/code&gt; 中 &lt;code&gt;\documentclass&lt;/code&gt; 部分的 &lt;code&gt;MajorFormat&lt;/code&gt; ，使用新格式的目录名即可&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;现在支持的专业模板如下&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;模板名称&lt;/th&gt;
&lt;th align="left"&gt;专业&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;general&lt;/td&gt;
&lt;td align="left"&gt;空模板&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;cs&lt;/td&gt;
&lt;td align="left"&gt;计算机科学与技术&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;isee&lt;/td&gt;
&lt;td align="left"&gt;信息电子&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;math&lt;/td&gt;
&lt;td align="left"&gt;数学&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;physics&lt;/td&gt;
&lt;td align="left"&gt;物理&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;经过两年的使用，本模板的 &lt;code&gt;general&lt;/code&gt; 模板可以满足大多数学院的要求。
目前只有&lt;strong&gt;计算机学院&lt;/strong&gt;的模板使用的封面和评分页与校级模板有所不同，
使用时请注意切换；
其他的专业模板只是提供了方便各专业使用的宏，没有额外的版面设置。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-slide-模板" class="anchor" aria-hidden="true" href="#slide-模板"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slide 模板&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;这是之前分享在浙大云盘的一个 Slide 模板，现在转移到 GitHub 方便同学们连同 LaTeX 模板一起下载:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/TheNetAdmin/zjuthesis/releases/tag/v2.1.1-slide"&gt;GitHub 下载链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitee.com/netadmin/zjuthesis/releases/v2.1.1-slide" rel="nofollow"&gt;Gitee （国内镜像仓库）下载链接&lt;/a&gt;（国内网络用这个链接下载比较快）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-高级使用教程" class="anchor" aria-hidden="true" href="#高级使用教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;高级使用教程&lt;/h2&gt;
&lt;p&gt;如果你熟悉 git 的用法，希望用 git 来管理自己的论文，并且保持最新的样式，可以采用如下方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;fork 本 repo，必要的话可以设置为 private
&lt;blockquote&gt;
&lt;p&gt;如果你不想使用 GitHub，可以直接 git clone 本 repo 并同步到其他的 git repo 中。
但一定要保持原有的 commit，并且设置好指向原始 repo 的 remote url，这样才能进行后续的样式更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;新开 branch，在这个 branch 上编写论文内容&lt;/li&gt;
&lt;li&gt;需要与最新样式同步时：
&lt;ol&gt;
&lt;li&gt;先 commit 论文分支&lt;/li&gt;
&lt;li&gt;切换到 master 分支，从原始 repo 执行 git pull&lt;/li&gt;
&lt;li&gt;然后切换到论文分支，将其 rebase 到 master 分支最新的 commit，并逐步修复 conflict&lt;/li&gt;
&lt;li&gt;git push 到你的 repo 中&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;然后在论文分支继续编写内容&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这样你的论文内容总是与样式分离，可以分别更新&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-q--a" class="anchor" aria-hidden="true" href="#q--a"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Q &amp;amp; A&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Q: 没有我所在专业的格式？&lt;/p&gt;
&lt;p&gt;A: 由于个人精力有限，难以查阅并编写各系具体要求的格式，如果同学们有相关需求，可以：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;在 GitHub 上提出 Pull Request，贡献你编写的代码&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;在 GitHub 上提出 issue，附上模板格式要求&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: latexmk 编译不通过？&lt;/p&gt;
&lt;p&gt;A: 可以先尝试执行 &lt;code&gt;tlmgr update --self --all&lt;/code&gt; 更新整个 TeXLive，然后重新编译。因为直接安装的 TeXLive 并不包含所有最新版本的宏包，一些旧版宏包的 BUG 可能会影响编译。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: 参考文献编译有问题？&lt;/p&gt;
&lt;p&gt;A: 如果你的参考文献没有出现在 pdf 中，请尝试使用 &lt;code&gt;latexmk&lt;/code&gt; 命令编译，或者使用 &lt;code&gt;biber&lt;/code&gt; 命令编译参考文献。
LaTeX 的参考文献需要额外编译一次，单独的 &lt;code&gt;xelatex&lt;/code&gt; 并不会编译参考文献。
不过 &lt;code&gt;latexmk&lt;/code&gt; 会追踪参考文献内容变化，并自动编译参考文献，所以建议大家使用 &lt;code&gt;latexmk&lt;/code&gt; 来编译文档。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: 如何配合查重？&lt;/p&gt;
&lt;p&gt;A: 详情见&lt;a href="https://github.com/TheNetAdmin/zjuthesis/issues/14"&gt;issue讨论&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;目前查重工具对 LaTeX 生成的 pdf 支持比较差，主要有两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;XeTeX 伪粗体会导致复制时得到乱码；&lt;/li&gt;
&lt;li&gt;LaTeX 生成的 pdf 默认采用了 UTF-8 的编码，而查重工具对这种编码支持不好，可能会认为是 GBK 之类的编码，无法读取正确的中文字符；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一点可以通过升级到 TeXLive 2019 解决。&lt;/p&gt;
&lt;p&gt;第二点暂时没有特别好的解决方法。
如果有同学有相关经验的话，可以 issue 留言或者邮件告知我，我会更新 README。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: pdf 文字复制得到乱码？&lt;/p&gt;
&lt;p&gt;A: 最新的解决方法是升级 TeXLive 2019 版本，此版本似乎不会产生中文复制乱码的问题&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果你在用 TeXLive 2018 及以前的版本：&lt;/p&gt;
&lt;p&gt;这是因为 Windows 自带的仿宋没有粗体，所以本模板使用了伪粗体：
&lt;code&gt;config/zjuthesis.cls&lt;/code&gt; =&amp;gt; &lt;code&gt;\PassOptionsToPackage{AutoFakeBold}{xeCJK}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果大家有对查重的要求，可以删除这一行，并手动指定粗体字体，比如使用楷体作为本模板的‘粗体’，这也是一种常见的解决方案。&lt;/p&gt;
&lt;p&gt;要想彻底解决这个问题，只能换用有真正粗体版本的字体，目前为止我并没有找到仿宋对应的粗体版本。
Office 对字体伪粗体问题有更好的解决方式，毕竟人家是收钱的嘛……&lt;/p&gt;
&lt;p&gt;相关讨论见：&lt;a href="https://github.com/CTeX-org/ctex-kit/issues/353"&gt;CTeX GitHub Issue&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: 某些 pdf 阅读器(如 Chrome )打开看不到中文，或者中文乱码&lt;/p&gt;
&lt;p&gt;A: 可能是 LaTeX 没有正确嵌入字体，最简单的解决方案是用没有乱码的 pdf 阅读器打开-&amp;gt;打印-&amp;gt;打印成 pdf ，然后尝试用有问题的阅读器打开，看是否仍有问题。&lt;/p&gt;
&lt;p&gt;如果需要了解具体发生了什么，请查阅 zjuthesis.log ，在文件内搜索 warning 和 error ，看一下是否有字体相关的报错。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: 怎么在 Overleaf 上使用？&lt;/p&gt;
&lt;p&gt;A: 下载本模板并在 Overleaf 上上传 .zip 文件，选择使用 XeLatex 编译器。&lt;/p&gt;
&lt;p&gt;上传后请删除 &lt;code&gt;.latexmkrc&lt;/code&gt; 文件，否则编译会失败而且不报任何错误。&lt;/p&gt;
&lt;p&gt;由于字体存在版权问题，还请大家自行上传字体，并修改中文字体设置（位于 &lt;code&gt;config/format/general/font.tex&lt;/code&gt; 以及 &lt;code&gt;config/foramt/major/.../font.tex&lt;/code&gt;）。修改过程可参考本仓库中的 &lt;code&gt;script/ci/setup.sh&lt;/code&gt; 脚本。&lt;/p&gt;
&lt;p&gt;如果编译超时（不显示 pdf 也不报错），请尝试注释中文字体设置（代码位置见上一段），然后重新编译。编译超时可能是缺失字体导致的，请大家自行上传字体并设置字体路径。&lt;/p&gt;
&lt;p&gt;我会不定期更新一下 &lt;a href="https://www.overleaf.com/latex/templates/zhe-jiang-da-xue-bi-ye-she-ji-slash-lun-wen-mo-ban-zjuthesis/kzcgmdyvkjxj" rel="nofollow"&gt;Overleaf 模板&lt;/a&gt;，但想用最新模板的同学请自行上传并设置。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意：Overleaf 还在使用 TeXLive 2017，但前文提到的乱码问题在 2019 版才得以解决，所以请同学们务必使用 TeXLive 2019 进行最终提交版的编译。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q: 怎么配合 vscode 使用？&lt;/p&gt;
&lt;p&gt;A: 有两种方式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用 vscode 打开根目录下的 zjuthesis.tex 文件，Ctrl+Shift+P 打开命令窗口，&lt;code&gt;LaTeX Workshop: Build with recipe&lt;/code&gt; =&amp;gt; &lt;code&gt;latexmk (latexmkrc)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;参见&lt;a href="https://github.com/TheNetAdmin/zjuthesis/issues/11"&gt;这里&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他问题请在 &lt;a href="https://github.com/TheNetAdmin/zjuthesis/issues/"&gt;GitHub issue&lt;/a&gt; 提出。&lt;/p&gt;
&lt;p&gt;请各位同学遇到问题尽量在 GitHub issue 里提出，这样方便有同样问题的同学查询。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-开源许可" class="anchor" aria-hidden="true" href="#开源许可"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;开源许可&lt;/h2&gt;
&lt;p&gt;本项目代码部分基于MIT协议开源&lt;/p&gt;
&lt;p&gt;学校标志与学校文件的版权归浙江大学所有&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>TheNetAdmin</author><guid isPermaLink="false">https://github.com/TheNetAdmin/zjuthesis</guid><pubDate>Thu, 26 Dec 2019 00:03:00 GMT</pubDate></item><item><title>dart-lang/language #4 in TeX, This week</title><link>https://github.com/dart-lang/language</link><description>&lt;p&gt;&lt;i&gt;Design of the Dart language&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dart-language-evolution" class="anchor" aria-hidden="true" href="#dart-language-evolution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dart language evolution&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/dart-lang/language" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/603c879b44e3c78bdd33b11d894682ba28e5fc1f/68747470733a2f2f7472617669732d63692e6f72672f646172742d6c616e672f6c616e67756167652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/dart-lang/language.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository is a place for the Dart language team to work on
language changes and features, and to solicit and accept feedback and requests.&lt;/p&gt;
&lt;p&gt;Issues and feature requests relevant to the language and the specification may
be filed &lt;a href="https://github.com/dart-lang/language/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-organization" class="anchor" aria-hidden="true" href="#organization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Organization&lt;/h1&gt;
&lt;p&gt;We follow &lt;a href="https://github.com/dart-lang/language/blob/master/doc/life_of_a_language_feature.md"&gt;this
process&lt;/a&gt;
for planning and rolling out language changes.&lt;/p&gt;
&lt;p&gt;Features currently being worked on are listed in the
&lt;a href="https://github.com/dart-lang/language/projects/1"&gt;language funnel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Notes from language design meetings can be
found &lt;a href="https://github.com/dart-lang/language/blob/master/minutes/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-dart-language" class="anchor" aria-hidden="true" href="#dart-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dart Language&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.dartlang.org" rel="nofollow"&gt;Dart&lt;/a&gt; is an open-source, scalable programming language, with robust
libraries and runtimes, for building web, server, and mobile apps.&lt;/p&gt;
&lt;p&gt;This repository tracks the Dart language specification
and changes to the Dart language.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Anyone can participate in the discussion about language changes
by participating on the dart language mailing list,
by replying to issues in this repository,
and by uploading documents, tests or other resources.&lt;/p&gt;
&lt;p&gt;When commenting on issues in this repository, keep in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;g-emoji class="g-emoji" alias="+1" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png"&gt;👍&lt;/g-emoji&gt; reactions are more useful than comments to show support.&lt;/li&gt;
&lt;li&gt;Motivating examples help us understand why you want new features more than
pointers to other languages which have them. We love hearing feedback about
your experiences with other languages, but we also want to know why they are
right for Dart in particular.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license--patents" class="anchor" aria-hidden="true" href="#license--patents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License &amp;amp; patents&lt;/h2&gt;
&lt;p&gt;See &lt;a href="https://github.com/dart-lang/language/blob/master/LICENSE"&gt;LICENSE&lt;/a&gt; and &lt;a href="https://github.com/dart-lang/language/blob/master/PATENTS"&gt;PATENTS&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dart-lang</author><guid isPermaLink="false">https://github.com/dart-lang/language</guid><pubDate>Thu, 26 Dec 2019 00:04:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #5 in TeX, This week</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-中文翻译" class="anchor" aria-hidden="true" href="#deep-learning-中文翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning 中文翻译&lt;/h1&gt;
&lt;p&gt;在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。
我们尽可能地保留了原书&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;中的意思并保留原书的语句。&lt;/p&gt;
&lt;p&gt;然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。&lt;/p&gt;
&lt;p&gt;大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。&lt;/p&gt;
&lt;p&gt;对应的翻译者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责&lt;/li&gt;
&lt;li&gt;第2、5、8、11、15、18章由 @liber145 负责&lt;/li&gt;
&lt;li&gt;第3、6、9章由 @KevinLee1110 负责&lt;/li&gt;
&lt;li&gt;第13、16、17、19章及第12.1至12.3节由 @futianfan 负责&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-面向的读者" class="anchor" aria-hidden="true" href="#面向的读者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向的读者&lt;/h2&gt;
&lt;p&gt;请直接下载&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;阅读。
不打算提供EPUB等格式，如有需要请自行修改。&lt;/p&gt;
&lt;p&gt;这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;原版&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-出版及开源原因" class="anchor" aria-hidden="true" href="#出版及开源原因"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;出版及开源原因&lt;/h2&gt;
&lt;p&gt;本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。
如果你觉得中文版不行，希望你能多提建议。非常感谢各位！
纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。&lt;/p&gt;
&lt;p&gt;纸质版目前在人民邮电出版社的异步社区出售，见&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;地址&lt;/a&gt;。
价格不低，但看了样本之后，我们认为物有所值。
注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。&lt;/p&gt;
&lt;p&gt;以下是开源的具体原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;捐赠情况&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意"泄露"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。&lt;/li&gt;
&lt;li&gt;深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。&lt;/li&gt;
&lt;li&gt;开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们有3个类别的校对人员。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负责人也就是对应的翻译者。&lt;/li&gt;
&lt;li&gt;简单阅读，对语句不通顺或难以理解的地方提出修改意见。&lt;/li&gt;
&lt;li&gt;中英对比，进行中英对应阅读，排除少翻错翻的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有校对建议都保存在各章的&lt;code&gt;annotations.txt&lt;/code&gt;文件中。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人&lt;/th&gt;
&lt;th&gt;简单阅读&lt;/th&gt;
&lt;th&gt;中英对比&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;第一章 前言&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;第二章 线性代数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;第三章 概率与信息论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;第四章 数值计算&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;第五章 机器学习基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;第六章 深度前馈网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;第七章 深度学习中的正则化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;第八章 深度模型中的优化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;第九章 卷积网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;第十章 序列建模：循环和递归网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;第十一章 实践方法论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;第十二章 应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;第十三章 线性因子模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;第十四章 自编码器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;第十五章 表示学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;第十六章 深度学习中的结构化概率模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;第十七章 蒙特卡罗方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;第十八章 面对配分函数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;第十九章 近似推断&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;第二十章 深度生成模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;参考文献&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！&lt;/p&gt;
&lt;p&gt;还有很多同学提出了不少建议，我们都列在此处。&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;如有遗漏，请务必通知我们，可以发邮件至&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;。
这是我们必须要感谢的，所以不要不好意思。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;排版&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-注意" class="anchor" aria-hidden="true" href="#注意"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;各种问题或者建议可以提issue，建议使用中文。&lt;/li&gt;
&lt;li&gt;由于版权问题，我们不能将图片和bib上传，请见谅。&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;可用于学习研究目的，不得用于任何商业行为。谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdown格式" class="anchor" aria-hidden="true" href="#markdown格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Markdown格式&lt;/h2&gt;
&lt;p&gt;这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;。
注意，这种转换没有把图放进去，也不会放图。目前使用单个&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;，基于latex文件转换，以后可能会更改但原则是不直接修改&lt;a href="docs/_posts"&gt;md文件&lt;/a&gt;。
需要的同学可以自行修改&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-html格式" class="anchor" aria-hidden="true" href="#html格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML格式&lt;/h2&gt;
&lt;p&gt;读者可以使用&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;进行转换，直接将PDF转换为HTML。&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Thu, 26 Dec 2019 00:05:00 GMT</pubDate></item><item><title>posquit0/Awesome-CV #6 in TeX, This week</title><link>https://github.com/posquit0/Awesome-CV</link><description>&lt;p&gt;&lt;i&gt;:page_facing_up: Awesome CV is LaTeX template for your outstanding job application&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-------------awesome-cv" class="anchor" aria-hidden="true" href="#------------awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
  &lt;a href="https://github.com/posquit0/Awesome-CV" title="AwesomeCV Documentation"&gt;
    &lt;img alt="AwesomeCV" src="https://github.com/posquit0/Awesome-CV/raw/master/icon.png" width="200px" height="200px" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;br&gt;
  Awesome CV
&lt;/h1&gt;
&lt;p align="center"&gt;
  LaTeX template for your outstanding job application
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://www.paypal.me/posquit0" rel="nofollow"&gt;
    &lt;img alt="Donate" src="https://camo.githubusercontent.com/abbdd7bf97ae7919db5962b255f40aded5189c4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d626c75652e737667" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-blue.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://circleci.com/gh/posquit0/Awesome-CV" rel="nofollow"&gt;
    &lt;img alt="CircleCI" src="https://camo.githubusercontent.com/d42593802854990d35ca42943e478dd35d6c64c9/68747470733a2f2f636972636c6563692e636f6d2f67682f706f7371756974302f417765736f6d652d43562e7376673f7374796c653d736869656c64" data-canonical-src="https://circleci.com/gh/posquit0/Awesome-CV.svg?style=shield" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Resume" src="https://camo.githubusercontent.com/836d3a9f44da3462e5c47b6c58bf066bffbaf739/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726573756d652d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/resume-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/cv.pdf" rel="nofollow"&gt;
    &lt;img alt="Example CV" src="https://camo.githubusercontent.com/8afab53a91bc30d0da18a9ea0cc70f2d0a1571df/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63762d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/cv-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;
    &lt;img alt="Example Coverletter" src="https://camo.githubusercontent.com/ce88ed0c1af9e5611df67818460447b69572ae9d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f7665726c65747465722d7064662d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/coverletter-pdf-green.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-awesome-cv" class="anchor" aria-hidden="true" href="#what-is-awesome-cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is Awesome CV?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Awesome CV&lt;/strong&gt; is LaTeX template for a &lt;strong&gt;CV(Curriculum Vitae)&lt;/strong&gt;, &lt;strong&gt;Résumé&lt;/strong&gt; or &lt;strong&gt;Cover Letter&lt;/strong&gt; inspired by &lt;a href="https://www.sharelatex.com/templates/cv-or-resume/fancy-cv" rel="nofollow"&gt;Fancy CV&lt;/a&gt;. It is easy to customize your own template, especially since it is really written by a clean, semantic markup.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-donate" class="anchor" aria-hidden="true" href="#donate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Donate&lt;/h2&gt;
&lt;p&gt;Please help keep this project alive! Donations are welcome and will go towards further development of this project.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PayPal: paypal.me/posquit0
BTC: 1Je3DxJVM2a9nTVPNo55SfQwpmxA6N2KKb
BCH: 1Mg1wG7PwHGrHYSWS67TsGSjo5GHEVbF16
ETH: 0x77ED9B4659F80205E9B9C9FB1E26EDB9904AFCC7
QTUM: QZT7D6m3QtTTqp7s4ZWAwLtGDsoHMMaM8E
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Thank you for your support!&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-preview" class="anchor" aria-hidden="true" href="#preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preview&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-résumé" class="anchor" aria-hidden="true" href="#résumé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Résumé&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Page. 1&lt;/th&gt;
&lt;th align="center"&gt;Page. 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-0.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-1.png" alt="Résumé" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-cover-letter" class="anchor" aria-hidden="true" href="#cover-letter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cover Letter&lt;/h4&gt;
&lt;p&gt;You can see &lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Without Sections&lt;/th&gt;
&lt;th align="center"&gt;With Sections&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-0.png" alt="Cover Letter(Traditional)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-1.png" alt="Cover Letter(Awesome)" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv/tvmzpvdjfqxp" rel="nofollow"&gt;&lt;strong&gt;Edit Résumé on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.overleaf.com/latex/templates/awesome-cv-cover-letter/pfzzjspkthbk" rel="nofollow"&gt;&lt;strong&gt;Edit Cover Letter on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:&lt;/em&gt; Above services do not guarantee up-to-date source code of Awesome CV&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h4&gt;
&lt;p&gt;A full TeX distribution is assumed.  &lt;a href="http://tex.stackexchange.com/q/55437" rel="nofollow"&gt;Various distributions for different operating systems (Windows, Mac, *nix) are available&lt;/a&gt; but TeX Live is recommended.
You can &lt;a href="http://tex.stackexchange.com/q/1092" rel="nofollow"&gt;install TeX from upstream&lt;/a&gt; (recommended; most up-to-date) or use &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; if you really want that.  (It's generally a few years behind.)&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h4&gt;
&lt;p&gt;At a command prompt, run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ xelatex {your-cv}.tex&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This should result in the creation of &lt;code&gt;{your-cv}.pdf&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-credit" class="anchor" aria-hidden="true" href="#credit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Credit&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.latex-project.org" rel="nofollow"&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; is a fantastic typesetting program that a lot of people use these days, especially the math and computer science people in academia.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/furl/latex-fontawesome"&gt;&lt;strong&gt;LaTeX FontAwesome&lt;/strong&gt;&lt;/a&gt; is bindings for FontAwesome icons to be used in XeLaTeX.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/roboto"&gt;&lt;strong&gt;Roboto&lt;/strong&gt;&lt;/a&gt; is the default font on Android and ChromeOS, and the recommended font for Google’s visual language, Material Design.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/adobe-fonts/source-sans-pro"&gt;&lt;strong&gt;Source Sans Pro&lt;/strong&gt;&lt;/a&gt; is a set of OpenType fonts that have been designed to work well in user interface (UI) environments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;You are free to take my &lt;code&gt;.tex&lt;/code&gt; file and modify it to create your own resume. Please don't use my resume for anything else without my permission, though!&lt;/p&gt;
&lt;p&gt;If you have any questions, feel free to join me at &lt;code&gt;#posquit0&lt;/code&gt; on Freenode and ask away. Click &lt;a href="https://kiwiirc.com/client/irc.freenode.net/posquit0" rel="nofollow"&gt;here&lt;/a&gt; to connect.&lt;/p&gt;
&lt;p&gt;Good luck!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;See Also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/posquit0/hugo-awesome-identity"&gt;Awesome Identity&lt;/a&gt; - A single-page Hugo theme to introduce yourself.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>posquit0</author><guid isPermaLink="false">https://github.com/posquit0/Awesome-CV</guid><pubDate>Thu, 26 Dec 2019 00:06:00 GMT</pubDate></item><item><title>lib-pku/libpku #7 in TeX, This week</title><link>https://github.com/lib-pku/libpku</link><description>&lt;p&gt;&lt;i&gt;贵校课程资料民间整理&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-libpku---贵校课程资料民间整理" class="anchor" aria-hidden="true" href="#libpku---贵校课程资料民间整理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;libpku - 贵校课程资料民间整理&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-preface" class="anchor" aria-hidden="true" href="#preface"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preface&lt;/h2&gt;
&lt;p&gt;（引用自 &lt;a href="https://github.com/QSCTech/zju-icicles"&gt;QSCTech/zju-icicles&lt;/a&gt; ）&lt;/p&gt;
&lt;p&gt;来到一所大学，从第一次接触许多课，直到一门一门完成，这个过程中我们时常收集起许多资料和情报。&lt;/p&gt;
&lt;p&gt;有些是需要在网上搜索的电子书，每次见到一门新课程，Google 一下教材名称，有的可以立即找到，有的却是要花费许多眼力；有些是历年试卷或者 A4 纸，前人精心收集制作，抱着能对他人有用的想法公开，却需要在各个群或者私下中摸索以至于从学长手中代代相传；有些是上完一门课才恍然领悟的技巧，原来这门课重点如此，当初本可以更轻松地完成得更好……&lt;/p&gt;
&lt;p&gt;我也曾很努力地收集各种课程资料，但到最后，某些重要信息的得到却往往依然是纯属偶然。这种状态时常令我感到后怕与不安。我也曾在课程结束后终于有了些许方法与总结，但这些想法无处诉说，最终只能把花费时间与精力才换来的经验耗散在了漫漫的遗忘之中。&lt;/p&gt;
&lt;p&gt;我为这一年一年，这么多人孤军奋战的重复劳动感到不平。&lt;/p&gt;
&lt;p&gt;我希望能够将这些隐晦的、不确定的、口口相传的资料和经验，变为公开的、易于获取的和大家能够共同完善、积累的共享资料。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我希望只要是前人走过的弯路，后人就不必再走。&lt;/strong&gt; 这是我的信念，也是我建立这个项目的原因。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;使用方法：访问 &lt;a href="https://lib-pku.github.io/" rel="nofollow"&gt;https://lib-pku.github.io/&lt;/a&gt; ，点击资料链接即可下载。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://minhaskamal.github.io/DownGit/#/home" rel="nofollow"&gt;https://minhaskamal.github.io/DownGit/#/home&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;——因为很重要所以说了三遍&lt;/p&gt;
&lt;p&gt;Issue、PR、纠错、资料、选课/考试攻略，完全欢迎！&lt;/p&gt;
&lt;p&gt;来自大家的关注、维护和贡献，才是让这个攻略继续存在的动力~&lt;/p&gt;
&lt;p&gt;对于课程的评价可写在对应课程文件夹的 &lt;code&gt;README.md&lt;/code&gt; 中。如果想上传课件（请确保无版权问题），推荐使用 PDF 格式，避免系统差。&lt;/p&gt;
&lt;p&gt;由于本项目体积很大，故推荐采用在 &lt;strong&gt;GitHub Web 端直接上传&lt;/strong&gt; 的方式，具体操作如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先 Fork 本项目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;上传文件到已有文件夹：打开对应文件夹，点击绿色 Download 按钮旁的 upload，上传你的文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传文件到新文件夹：打开任意文件夹，点击绿色 Download 按钮旁的 upload，&lt;strong&gt;把浏览器地址栏中文件夹名称改为你想要新建的文件夹名称，然后回车&lt;/strong&gt;，上传你的文件。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;提交 PR：Fork 本项目，然后在 GitHub 网页端点击 Upload File 上传文件，发起 PR 即可。留意一下项目的文件组织喔。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;或者也可以直接附加在 &lt;strong&gt;Issue&lt;/strong&gt; 中，由维护者进行添加。&lt;/p&gt;
&lt;p&gt;或者也可以发送邮件至 &lt;strong&gt;&lt;a href="mailto:libpku@protonmail.com"&gt;libpku@protonmail.com&lt;/a&gt;&lt;/strong&gt; ，由维护者进行添加。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;这不是北京大学图书馆。
我们也不对项目中信息的准确性或真实性做任何承诺。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果有侵权情况，麻烦您发送必要的信息至 &lt;a href="mailto:libpku@protonmail.com"&gt;libpku@protonmail.com&lt;/a&gt; ，带来不便还请您谅解。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;资料来自网络，相关权利由原作者所有，这个 repo 仅用于收集现有资料。&lt;/p&gt;
&lt;p&gt;当然，我们不会为收集到的资料收费，或是尝试收取捐赠。&lt;/p&gt;
&lt;p&gt;我们只是尝试为后来的同学节省一些时间。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-related-works" class="anchor" aria-hidden="true" href="#related-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related Works&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/QSCTech/zju-icicles"&gt;浙江大学课程攻略共享计划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/martinwu42/project-hover"&gt;气垫船计划——免费、去中心化的北京大学往年题资料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/EECS-PKU-XSB/Shared-learning-materials"&gt;北京大学信科学生会学术部资料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tongtzeho/PKUCourse"&gt;北大计算机课程大作业&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PKUanonym/REKCARC-TSC-UHT"&gt;清华大学计算机系课程攻略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zjdx1998/seucourseshare"&gt;东南大学课程共享计划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/USTC-Resource/USTC-Course"&gt;中国科学技术大学计算机学院课程资源&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CoolPhilChen/SJTU-Courses/"&gt;上海交通大学课程资料分享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sysuexam/SYSU-Exam"&gt;中山大学课程资料分享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/idealclover/NJU-Review-Materials"&gt;南京大学课程复习资料&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CooperNiu/ZZU-Courses-Resource"&gt;郑州大学课程复习资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(more to be added....)&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>lib-pku</author><guid isPermaLink="false">https://github.com/lib-pku/libpku</guid><pubDate>Thu, 26 Dec 2019 00:07:00 GMT</pubDate></item><item><title>mohuangrui/ucasthesis #8 in TeX, This week</title><link>https://github.com/mohuangrui/ucasthesis</link><description>&lt;p&gt;&lt;i&gt; [最新样式] 中国科学院大学学位论文 LaTeX 模板  LaTeX Thesis Template for the University of Chinese Academy of Sciences &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ucasthesis-国科大学位论文-latex-模板-最新样式" class="anchor" aria-hidden="true" href="#ucasthesis-国科大学位论文-latex-模板-最新样式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;ucasthesis&lt;/code&gt; 国科大学位论文 LaTeX 模板 [最新样式]&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-模板下载" class="anchor" aria-hidden="true" href="#模板下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板下载&lt;/h2&gt;
&lt;p&gt;请在页面右边点击：&lt;strong&gt;Clone or download -&amp;gt; Download Zip&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-重要建议" class="anchor" aria-hidden="true" href="#重要建议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要建议&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;关于 LaTeX 的知识性问题，请查阅 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki"&gt;LaTeX 知识小站&lt;/a&gt; 和 &lt;a href="https://en.wikibooks.org/wiki/LaTeX" rel="nofollow"&gt;LaTeX Wikibook&lt;/a&gt;，如发问需前往 &lt;a href="https://github.com/CTeX-org/forum"&gt;CTeX Forum&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;关于 ucasthesis 编译和设计的问题，请先读 &lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;，如发问需遵从&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"&gt;提问流程&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;使用邮件传播 ucasthesis 时，请先删除 &lt;code&gt;artratex.bat&lt;/code&gt; 以防范 Dos 脚本的潜在风险。&lt;/li&gt;
&lt;li&gt;开题报告请见：&lt;a href="https://github.com/mohuangrui/ucasproposal"&gt;ucasproposal: 中国科学院大学开题报告 LaTeX 模板&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;书脊制作请见：&lt;a href="https://github.com/mohuangrui/latexspine"&gt;latexspine: LaTeX 书脊模板&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-模板简介" class="anchor" aria-hidden="true" href="#模板简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板简介&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 为撰写中国科学院大学&lt;strong&gt;本&lt;/strong&gt;、&lt;strong&gt;硕&lt;/strong&gt;、&lt;strong&gt;博&lt;/strong&gt;学位论文和&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;&lt;strong&gt;博后&lt;/strong&gt;&lt;/a&gt;报告的 LaTeX 模版。ucasthesis 提供了简单明了的&lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;。无论你是否具有 LaTeX 使用经验，都可较为轻松地使用以完成学位论文的撰写和排版。谢谢大家的测试、反馈和支持，我们一起的努力让 ucasthesis 非常荣幸地得到了国科大本科部陆晴老师、本科部学位办丁云云老师和中科院数学与系统科学研究院吴凌云研究员的支持，并得到吴凌云学长在 &lt;a href="http://www.ctex.org/HomePage" rel="nofollow"&gt;CTEX&lt;/a&gt; 的发布。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;考虑到许多同学可能缺乏 LaTeX 使用经验，ucasthesis 将 LaTeX 的复杂性高度封装，开放出简单的接口，以便轻易使用。同时，对用 LaTeX 撰写论文的一些主要难题，如制图、制表、文献索引等，进行了详细说明，并提供了相应的代码样本，理解了上述问题后，对于初学者而言，使用此模板撰写学位论文将不存在实质性的困难。所以，如果你是初学者，请不要直接放弃，因为同样为初学者的我，十分明白让 LaTeX 简单易用的重要性，而这正是 ucasthesis 所追求和体现的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;此中国科学院大学学位论文模板 ucasthesis 基于中科院数学与系统科学研究院吴凌云研究员的 CASthesis 模板发展而来。当前 ucasthesis 模板满足最新的中国科学院大学学位论文撰写要求和封面设定。兼顾操作系统：Windows，Linux，MacOS 和 LaTeX 编译引擎：pdflatex，xelatex，lualatex。支持中文书签、中文渲染、中文粗体显示、拷贝 PDF 中的文本到其他文本编辑器等特性（&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE"&gt;Windows 系统 PDF 拷贝乱码的解决方案需见：字体配置&lt;/a&gt;）。此外，对模板的文档结构进行了精心设计，撰写了编译脚本提高模板的易用性和使用效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 的目标在于简化学位论文的撰写，利用 LaTeX 格式与内容分离的特征，模板将格式设计好后，作者可只需关注论文内容。 同时，ucasthesis 有着整洁一致的代码结构和扼要的注解，对文档的仔细阅读可为初学者提供一个学习 LaTeX 的窗口。此外，模板的架构十分注重通用性，事实上，ucasthesis 不仅是国科大学位论文模板，同时，通过少量修改即可成为使用 LaTeX 撰写中英文文章或书籍的通用模板，并为使用者的个性化设定提供了接口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-重要通知" class="anchor" aria-hidden="true" href="#重要通知"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要通知&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2019-10-21&lt;/code&gt; 模板样式进行了修改，请查看下面的修改描述，以决定是否需要更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-12-06&lt;/code&gt; 移除 commit 中的二进制文件，以极大减少 Fork 后的文件大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/198"&gt;huiwenzhang, issue #198&lt;/a&gt; 修复&lt;code&gt;mainmatter&lt;/code&gt;下&lt;code&gt;\chapter*&lt;/code&gt;的页眉错误。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/195"&gt;Fancy0609, muzimuzhi, issue #195&lt;/a&gt; 调整由&lt;code&gt;AutoFakeBold&lt;/code&gt;控制的伪粗体加粗程度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-11&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/190"&gt;Pantrick, issue #190&lt;/a&gt; 采用 &lt;a href="https://github.com/muzimuzhi"&gt;muzimuzhi&lt;/a&gt; 提供的方法实现&lt;code&gt;\advisor{}&lt;/code&gt;和&lt;code&gt;\institute{}&lt;/code&gt;的自动换行功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-08-01&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/183"&gt;vectorliu, issue #183&lt;/a&gt; 修改英文模式下的&lt;code&gt;plain&lt;/code&gt;选项为&lt;code&gt;scheme=plain&lt;/code&gt;以消除对&lt;code&gt;Algorithm&lt;/code&gt;样式的修改。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-15&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/177"&gt;HaorenWang, issue #177&lt;/a&gt; 调整矢量、矩阵、张量字体样式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-09&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/170"&gt;DRjy, issue #170&lt;/a&gt; 轻微缩减目录中编号与标题的间距；根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/174"&gt;e71828, issue #174&lt;/a&gt; 轻微增加页眉中编号与标题的间距。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-05-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/169"&gt;CDMA2019, issue #169&lt;/a&gt; 提供横排图表环境下页眉页脚的横排，具体使用见 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%A8%AA%E6%8E%92%E5%9B%BE%E8%A1%A8"&gt;横排图表&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-24&lt;/code&gt; 拓展模版兼容 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;博后报告&lt;/a&gt;。修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/156"&gt;gsp2014, issue #156&lt;/a&gt; 文献引用中的连字符的间断显示和上标引用中逗号下沉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-19&lt;/code&gt; 修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/117"&gt;nihaomiao, issue #117&lt;/a&gt;&lt;code&gt;\mathbf&lt;/code&gt;失效问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-16&lt;/code&gt; 修复国际生需要的&lt;code&gt;plain&lt;/code&gt;模式下无法改变英文章标题字体大小的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-09&lt;/code&gt; 对部分宏命令进行调整，无功能及样式上的修改。若需更新，建议参考 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%9B%B4%E6%96%B0%E6%8C%87%E5%8D%97"&gt;更新指南&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-04&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/134"&gt;liuy334, songchunlin, issue #134&lt;/a&gt; ，调整行距使&lt;code&gt;LaTeX&lt;/code&gt;版与&lt;code&gt;Word&lt;/code&gt;版的行数和每行字数相一致。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-28&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/49"&gt;zssasa, allenwoods, issue #49&lt;/a&gt; ，修复&lt;code&gt;bicaption&lt;/code&gt;对&lt;code&gt;longtable&lt;/code&gt;的兼容性。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/133"&gt;BowenHou, issue #133&lt;/a&gt; ，使下划线能对长标题自动换行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/127"&gt;DRjy, muzimuzhi, issue #127&lt;/a&gt; ，为&lt;code&gt;摘要&lt;/code&gt;等无需在目录中显示的结构元素建立书签。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/130"&gt;muzimuzhi, issue #130&lt;/a&gt; ，修正对&lt;code&gt;\voffset&lt;/code&gt;的使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-14&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/121"&gt;opt-gaobin, issue #121&lt;/a&gt; ，修正中文标点使下划线断掉的问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/120"&gt;Guoqiang Zhang, email; weili-ict, issue #120&lt;/a&gt; ，修复&lt;code&gt;\proofname&lt;/code&gt;命令对2015年及更早&lt;code&gt;LaTeX&lt;/code&gt;编译器的兼容性问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-02-20&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/100"&gt;opt-gaobin, issue #100&lt;/a&gt; ，增加定理、定义、证明等数学环境。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/102"&gt;DRjy, issue #102&lt;/a&gt; ，调整&lt;code&gt;\mathcal&lt;/code&gt;字体样式。根据 [zike Liu, email] ，适当缩减目录列表的缩进。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/105"&gt;xiaoyaoE, issue #105&lt;/a&gt; ，使数字字体和英文字体一致。完善中文版和国际版之间的中英格式切换。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-01-10&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/57"&gt;mnpengjk, issue #57&lt;/a&gt; ， 将公式编号前加点纳入模版默认，更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt; 。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/95"&gt;yunyun2019, issue #95&lt;/a&gt; ，采用 &lt;a href="https://github.com/zepinglee"&gt;zepinglee&lt;/a&gt; 基于国标样式为&lt;code&gt;ucas&lt;/code&gt;所定制文献样式：&lt;a href="https://github.com/CTeX-org/gbt7714-bibtex-style/tree/ucas"&gt;ucas 样式分支&lt;/a&gt; ，文献样式更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%96%87%E7%8C%AE%E6%A0%B7%E5%BC%8F"&gt;文献样式&lt;/a&gt;。根据 [邵岳林, email] ，将附录复原为常规的排版设置，若需将附录置于参考文献后，请见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-03&lt;/code&gt; 根据国科大本科部陆晴老师和本科部学位办丁云云老师的复审审核建议再次修复一些样式细节问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-02&lt;/code&gt; 模板进行了重大更新，修复了样式、字体、格式等许多问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据国科大本科部陆晴老师的建议对模版样式进行了诸多拓展和修正，并完善对本科生论文元素的兼容性。&lt;/li&gt;
&lt;li&gt;在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下解决了如何多次调用&lt;code&gt;Times New Roman&lt;/code&gt;而不导致黑体调用错误的问题。根据 [twn1993, email]，修复默认黑体为微软雅黑而不是&lt;code&gt;SimHei&lt;/code&gt;的问题。&lt;/li&gt;
&lt;li&gt;繁复折腾测试后终于找出一个在&lt;code&gt;ctex&lt;/code&gt;默认黑体替换粗宋体设定环境内全局&lt;code&gt;AutoFakeBold&lt;/code&gt;失效状态下折衷特定字体库不全条件下生僻字显示和系统默认字重不全条件下粗宋体显示以及不同操作系统下如何平衡上述字库自重矛盾还有根据操作系统自动调用所带有的&lt;code&gt;Times&lt;/code&gt;字体的方案。&lt;/li&gt;
&lt;li&gt;设定论文封面据英文学位名如自动切换。密级据是否填写自动显示。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-22&lt;/code&gt; 演示表标题居表上，加粗图表标注，设置长图表标题悬挂缩进（由于&lt;code&gt;bicaption&lt;/code&gt;宏包无法正确接受&lt;code&gt;caption&lt;/code&gt;宏包的&lt;code&gt;margin&lt;/code&gt;选项，图表中英标题第一行无法正确同步缩进，从而放弃第一行的缩进），强调多图中子图标题的规范使用，通过摘要和符号列表演示标题不在目录中显示却仍在页眉中显示。根据 [赵永明, email]，设置双语图表标题和&lt;code&gt;bicaption&lt;/code&gt;不在图形列表和表格列表中显示英文标题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-21&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/42"&gt;zhanglinbo, issue #42&lt;/a&gt; ，使用 &lt;a href="https://github.com/xiaoyao9933/UCASthesis"&gt;xiaoyao9933&lt;/a&gt; 制作的&lt;code&gt;ucas_logo.pdf&lt;/code&gt;使学校&lt;code&gt;logo&lt;/code&gt;放大不失真。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/41"&gt;Starsky Wong, issue #41&lt;/a&gt; ，设置标题英文设为&lt;code&gt;Times New Roman&lt;/code&gt;。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/29"&gt;will0n, issue #29&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/26"&gt;Man-Ting-Fang, issue #26&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/12"&gt;diyiliaoya, issue #12&lt;/a&gt; ，和 [赵永明, email] ，矫正一些格式细节问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/30"&gt;tangjie1992, issue #30&lt;/a&gt; ，配置算法环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-02-04&lt;/code&gt; 在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下修复误用字体命令导致的粗宋体异常。然后，将模板兼容性进一步扩展为兼容操作系统&lt;code&gt;Windows&lt;/code&gt;，&lt;code&gt;Linux&lt;/code&gt;，&lt;code&gt;MacOS&lt;/code&gt;和&lt;code&gt;LaTeX &lt;/code&gt;编译引擎&lt;code&gt;pdflatex&lt;/code&gt;，&lt;code&gt;xelatex&lt;/code&gt;，&lt;code&gt;lualatex&lt;/code&gt;。移除&lt;code&gt;microtype&lt;/code&gt;宏包以提高编译效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-01-28&lt;/code&gt; 基于国科大&lt;code&gt;2018&lt;/code&gt;新版论文规范进行了重大修改，采用新的封面、声明、页眉页脚样式。展示标题中使用数学公式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2017-05-14&lt;/code&gt; 根据 [赵永明, email] ，增加&lt;code&gt;\citepns{}&lt;/code&gt;和&lt;code&gt;\citetns{}&lt;/code&gt;命令提供上标引用下混合非上标引用的需求。根据 [臧光明, email] ，添加设定论文为&lt;code&gt;thesis&lt;/code&gt;或&lt;code&gt;dissertation&lt;/code&gt;的命令。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mohuangrui</author><guid isPermaLink="false">https://github.com/mohuangrui/ucasthesis</guid><pubDate>Thu, 26 Dec 2019 00:08:00 GMT</pubDate></item><item><title>DeathKing/LaTeX-Template-Cn #9 in TeX, This week</title><link>https://github.com/DeathKing/LaTeX-Template-Cn</link><description>&lt;p&gt;&lt;i&gt;\LaTeX 中文模版收集。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;注意！！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;此仓库已停止更新，推荐读者访问：&lt;a href="http://www.latexstudio.net/" rel="nofollow"&gt;LaTeX 开源小屋 &lt;/a&gt; 获取更多、更现代(xeLaTeX)、支持度更好的 \LaTeX 模板。&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-latex-template-cn" class="anchor" aria-hidden="true" href="#latex-template-cn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LaTeX-Template-Cn&lt;/h1&gt;
&lt;p&gt;收集互联网上各种实用 \LaTeX 中文模版，并适配至 CTeX 套件 2.9.2.164 版本。&lt;/p&gt;
&lt;p&gt;主要来源：&lt;a href="http://zzg34b.w3.c361.com/templet/graduateThesis.htm" rel="nofollow"&gt;LaTeX 编辑部 &lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-关于latex环境" class="anchor" aria-hidden="true" href="#关于latex环境"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;关于LaTeX环境&lt;/h2&gt;
&lt;p&gt;CTeX 项目主页：&lt;a href="http://www.ctex.org/HomePage" rel="nofollow"&gt;CTEX: 项目&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CTeX 中文套装是基于 Windows 下的 MiKTeX 系统，集成了编辑器 WinEdt 和 PostScript 处理软件 Ghostscript 和 GSview 等主要工具。 CTeX 中文套装在 MiKTeX 的基础上增加了对中文的完整支持。 CTeX 中文套装支持 CJK, xeCJK, CCT, TY 等多种中文 TeX 处理方式。&lt;/p&gt;
&lt;p&gt;本仓库中所有模版均已通过 CTeX 2.9.2.164 套件编译测试。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-仓库结构说明" class="anchor" aria-hidden="true" href="#仓库结构说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;仓库结构说明&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;├─Book        % 书籍模版
├─BUGFIX      % 解决 LaTeX/CTeX 中一些 BUG 的代码
│  └─gb      % 解决“\bibname undefined”错误 （见Wiki）
├─Paper       % 论文模版
│  └─CHN-PaperTemplate  %『强烈推荐』通用论文模版
├─Preview     % 效果图
├─Slide       % 幻灯片模版
└─Thesis      % 学位论文模版
    ├─HIT     % 哈尔滨工业大学学位论文模版
    ├─NUDT    % 国防科技大学学位论文模版
    ├─THU     % 清华大学学位论文模版
    └─WHU     % 武汉大学学位论文模版
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>DeathKing</author><guid isPermaLink="false">https://github.com/DeathKing/LaTeX-Template-Cn</guid><pubDate>Thu, 26 Dec 2019 00:09:00 GMT</pubDate></item></channel></rss>