<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: TeX, This week</title><link>https://github.com/trending/tex?since=weekly</link><description>The top repositories on GitHub for tex, measured weekly</description><pubDate>Sat, 02 Nov 2019 02:38:41 GMT</pubDate><lastBuildDate>Sat, 02 Nov 2019 02:38:41 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>mohuangrui/ucasthesis #1 in TeX, This week</title><link>https://github.com/mohuangrui/ucasthesis</link><description>&lt;p&gt;&lt;i&gt; [最新样式] 中国科学院大学学位论文 LaTeX 模板  LaTeX Thesis Template for the University of Chinese Academy of Sciences &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ucasthesis-国科大学位论文-latex-模板-最新样式" class="anchor" aria-hidden="true" href="#ucasthesis-国科大学位论文-latex-模板-最新样式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;ucasthesis&lt;/code&gt; 国科大学位论文 LaTeX 模板 [最新样式]&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-模板下载" class="anchor" aria-hidden="true" href="#模板下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板下载&lt;/h2&gt;
&lt;p&gt;请在页面右边点击：&lt;strong&gt;Clone or download -&amp;gt; Download Zip&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-重要建议" class="anchor" aria-hidden="true" href="#重要建议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要建议&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;关于 LaTeX 的知识性问题，请查阅 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki"&gt;ucasthesis 知识小站&lt;/a&gt; 和 &lt;a href="https://en.wikibooks.org/wiki/LaTeX" rel="nofollow"&gt;LaTeX Wikibook&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;关于模板编译和样式设计的问题，请先仔细阅读 &lt;strong&gt;模板使用说明.pdf&lt;/strong&gt; 并遵从提问流程。&lt;/li&gt;
&lt;li&gt;使用邮件传播 ucasthesis 时，请先删除 &lt;code&gt;artratex.bat&lt;/code&gt; 以防范 Dos 脚本的潜在风险。&lt;/li&gt;
&lt;li&gt;开题报告请见：&lt;a href="https://github.com/mohuangrui/ucasproposal"&gt;ucasproposal: 中国科学院大学开题报告 LaTeX 模板&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;书脊制作请见：&lt;a href="https://github.com/mohuangrui/latexspine"&gt;latexspine: LaTeX 书脊模板&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-模板简介" class="anchor" aria-hidden="true" href="#模板简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模板简介&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 为撰写中国科学院大学&lt;strong&gt;本&lt;/strong&gt;、&lt;strong&gt;硕&lt;/strong&gt;、&lt;strong&gt;博&lt;/strong&gt;学位论文和&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;&lt;strong&gt;博后&lt;/strong&gt;&lt;/a&gt;报告的 LaTeX 模版。ucasthesis 提供了简单明了的&lt;strong&gt;模板使用说明.pdf&lt;/strong&gt;。无论你是否具有 LaTeX 使用经验，都可较为轻松地使用以完成学位论文的撰写和排版。谢谢大家的测试、反馈和支持，我们一起的努力让 ucasthesis 非常荣幸地得到了国科大本科部陆晴老师、本科部学位办丁云云老师和中科院数学与系统科学研究院吴凌云研究员的支持，并得到吴凌云学长在 &lt;a href="http://www.ctex.org/HomePage" rel="nofollow"&gt;CTEX&lt;/a&gt; 的发布。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;考虑到许多同学可能缺乏 LaTeX 使用经验，ucasthesis 将 LaTeX 的复杂性高度封装，开放出简单的接口，以便轻易使用。同时，对用 LaTeX 撰写论文的一些主要难题，如制图、制表、文献索引等，进行了详细说明，并提供了相应的代码样本，理解了上述问题后，对于初学者而言，使用此模板撰写学位论文将不存在实质性的困难。所以，如果你是初学者，请不要直接放弃，因为同样为初学者的我，十分明白让 LaTeX 简单易用的重要性，而这正是 ucasthesis 所追求和体现的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;此中国科学院大学学位论文模板 ucasthesis 基于中科院数学与系统科学研究院吴凌云研究员的 CASthesis 模板发展而来。当前 ucasthesis 模板满足最新的中国科学院大学学位论文撰写要求和封面设定。兼顾操作系统：Windows，Linux，MacOS 和 LaTeX 编译引擎：pdflatex，xelatex，lualatex。支持中文书签、中文渲染、中文粗体显示、拷贝 PDF 中的文本到其他文本编辑器等特性（&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%AD%97%E4%BD%93%E9%85%8D%E7%BD%AE"&gt;Windows 系统 PDF 拷贝乱码的解决方案需见：字体配置&lt;/a&gt;）。此外，对模板的文档结构进行了精心设计，撰写了编译脚本提高模板的易用性和使用效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ucasthesis 的目标在于简化学位论文的撰写，利用 LaTeX 格式与内容分离的特征，模板将格式设计好后，作者可只需关注论文内容。 同时，ucasthesis 有着整洁一致的代码结构和扼要的注解，对文档的仔细阅读可为初学者提供一个学习 LaTeX 的窗口。此外，模板的架构十分注重通用性，事实上，ucasthesis 不仅是国科大学位论文模板，同时，通过少量修改即可成为使用 LaTeX 撰写中英文文章或书籍的通用模板，并为使用者的个性化设定提供了接口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-重要通知" class="anchor" aria-hidden="true" href="#重要通知"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;重要通知&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2019-10-21&lt;/code&gt; 模板样式进行了修改，请查看下面的修改描述，以决定是否需要更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-更新记录" class="anchor" aria-hidden="true" href="#更新记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;更新记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/198"&gt;huiwenzhang, issue #198&lt;/a&gt; 修复&lt;code&gt;mainmatter&lt;/code&gt;下&lt;code&gt;\chapter*&lt;/code&gt;的页眉错误。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-12&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/195"&gt;Fancy0609, muzimuzhi, issue #195&lt;/a&gt; 调整由&lt;code&gt;AutoFakeBold&lt;/code&gt;控制的伪粗体加粗程度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-10-11&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/190"&gt;Pantrick, issue #190&lt;/a&gt; 采用 &lt;a href="https://github.com/muzimuzhi"&gt;muzimuzhi&lt;/a&gt; 提供的方法实现&lt;code&gt;\advisor{}&lt;/code&gt;和&lt;code&gt;\institute{}&lt;/code&gt;的自动换行功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-08-01&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/183"&gt;vectorliu, issue #183&lt;/a&gt; 修改英文模式下的&lt;code&gt;plain&lt;/code&gt;选项为&lt;code&gt;scheme=plain&lt;/code&gt;以消除对&lt;code&gt;Algorithm&lt;/code&gt;样式的修改。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-15&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/177"&gt;HaorenWang, issue #177&lt;/a&gt; 调整矢量、矩阵、张量字体样式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-06-09&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/170"&gt;DRjy, issue #170&lt;/a&gt; 轻微缩减目录中编号与标题的间距；根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/174"&gt;e71828, issue #174&lt;/a&gt; 轻微增加页眉中编号与标题的间距。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-05-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/169"&gt;CDMA2019, issue #169&lt;/a&gt; 提供横排图表环境下页眉页脚的横排，具体使用见 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%A8%AA%E6%8E%92%E5%9B%BE%E8%A1%A8"&gt;横排图表&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-24&lt;/code&gt; 拓展模版兼容 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E5%A6%82%E4%BD%95%E5%A1%AB%E5%86%99%E5%8D%9A%E5%A3%AB%E5%90%8E%E7%9A%84-frontinfotex-"&gt;博后报告&lt;/a&gt;。修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/156"&gt;gsp2014, issue #156&lt;/a&gt; 文献引用中的连字符的间断显示和上标引用中逗号下沉。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-19&lt;/code&gt; 修复 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/117"&gt;nihaomiao, issue #117&lt;/a&gt;&lt;code&gt;\mathbf&lt;/code&gt;失效问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-16&lt;/code&gt; 修复国际生需要的&lt;code&gt;plain&lt;/code&gt;模式下无法改变英文章标题字体大小的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-09&lt;/code&gt; 对部分宏命令进行调整，无功能及样式上的修改。若需更新，建议参考 &lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%9B%B4%E6%96%B0%E6%8C%87%E5%8D%97"&gt;更新指南&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-04-04&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/134"&gt;liuy334, songchunlin, issue #134&lt;/a&gt; ，调整行距使&lt;code&gt;LaTeX&lt;/code&gt;版与&lt;code&gt;Word&lt;/code&gt;版的行数和每行字数相一致。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-28&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/49"&gt;zssasa, allenwoods, issue #49&lt;/a&gt; ，修复&lt;code&gt;bicaption&lt;/code&gt;对&lt;code&gt;longtable&lt;/code&gt;的兼容性。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/133"&gt;BowenHou, issue #133&lt;/a&gt; ，使下划线能对长标题自动换行。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-25&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/127"&gt;DRjy, muzimuzhi, issue #127&lt;/a&gt; ，为&lt;code&gt;摘要&lt;/code&gt;等无需在目录中显示的结构元素建立书签。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/130"&gt;muzimuzhi, issue #130&lt;/a&gt; ，修正对&lt;code&gt;\voffset&lt;/code&gt;的使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-03-14&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/121"&gt;opt-gaobin, issue #121&lt;/a&gt; ，修正中文标点使下划线断掉的问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/120"&gt;Guoqiang Zhang, email; weili-ict, issue #120&lt;/a&gt; ，修复&lt;code&gt;\proofname&lt;/code&gt;命令对2015年及更早&lt;code&gt;LaTeX&lt;/code&gt;编译器的兼容性问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-02-20&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/100"&gt;opt-gaobin, issue #100&lt;/a&gt; ，增加定理、定义、证明等数学环境。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/102"&gt;DRjy, issue #102&lt;/a&gt; ，调整&lt;code&gt;\mathcal&lt;/code&gt;字体样式。根据 [zike Liu, email] ，适当缩减目录列表的缩进。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/105"&gt;xiaoyaoE, issue #105&lt;/a&gt; ，使数字字体和英文字体一致。完善中文版和国际版之间的中英格式切换。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2019-01-10&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/57"&gt;mnpengjk, issue #57&lt;/a&gt; ， 将公式编号前加点纳入模版默认，更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt; 。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/95"&gt;yunyun2019, issue #95&lt;/a&gt; ，采用 &lt;a href="https://github.com/zepinglee"&gt;zepinglee&lt;/a&gt; 基于国标样式为&lt;code&gt;ucas&lt;/code&gt;所定制文献样式：&lt;a href="https://github.com/CTeX-org/gbt7714-bibtex-style/tree/ucas"&gt;ucas 样式分支&lt;/a&gt; ，文献样式更多讨论可见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E6%96%87%E7%8C%AE%E6%A0%B7%E5%BC%8F"&gt;文献样式&lt;/a&gt;。根据 [邵岳林, email] ，将附录复原为常规的排版设置，若需将附录置于参考文献后，请见：&lt;a href="https://github.com/mohuangrui/ucasthesis/wiki/%E7%90%90%E5%B1%91%E7%BB%86%E8%8A%82"&gt;琐屑细节&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-03&lt;/code&gt; 根据国科大本科部陆晴老师和本科部学位办丁云云老师的复审审核建议再次修复一些样式细节问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-04-02&lt;/code&gt; 模板进行了重大更新，修复了样式、字体、格式等许多问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据国科大本科部陆晴老师的建议对模版样式进行了诸多拓展和修正，并完善对本科生论文元素的兼容性。&lt;/li&gt;
&lt;li&gt;在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下解决了如何多次调用&lt;code&gt;Times New Roman&lt;/code&gt;而不导致黑体调用错误的问题。根据 [twn1993, email]，修复默认黑体为微软雅黑而不是&lt;code&gt;SimHei&lt;/code&gt;的问题。&lt;/li&gt;
&lt;li&gt;繁复折腾测试后终于找出一个在&lt;code&gt;ctex&lt;/code&gt;默认黑体替换粗宋体设定环境内全局&lt;code&gt;AutoFakeBold&lt;/code&gt;失效状态下折衷特定字体库不全条件下生僻字显示和系统默认字重不全条件下粗宋体显示以及不同操作系统下如何平衡上述字库自重矛盾还有根据操作系统自动调用所带有的&lt;code&gt;Times&lt;/code&gt;字体的方案。&lt;/li&gt;
&lt;li&gt;设定论文封面据英文学位名如自动切换。密级据是否填写自动显示。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-22&lt;/code&gt; 演示表标题居表上，加粗图表标注，设置长图表标题悬挂缩进（由于&lt;code&gt;bicaption&lt;/code&gt;宏包无法正确接受&lt;code&gt;caption&lt;/code&gt;宏包的&lt;code&gt;margin&lt;/code&gt;选项，图表中英标题第一行无法正确同步缩进，从而放弃第一行的缩进），强调多图中子图标题的规范使用，通过摘要和符号列表演示标题不在目录中显示却仍在页眉中显示。根据 [赵永明, email]，设置双语图表标题和&lt;code&gt;bicaption&lt;/code&gt;不在图形列表和表格列表中显示英文标题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-03-21&lt;/code&gt; 根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/42"&gt;zhanglinbo, issue #42&lt;/a&gt; ，使用 &lt;a href="https://github.com/xiaoyao9933/UCASthesis"&gt;xiaoyao9933&lt;/a&gt; 制作的&lt;code&gt;ucas_logo.pdf&lt;/code&gt;使学校&lt;code&gt;logo&lt;/code&gt;放大不失真。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/41"&gt;Starsky Wong, issue #41&lt;/a&gt; ，设置标题英文设为&lt;code&gt;Times New Roman&lt;/code&gt;。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/29"&gt;will0n, issue #29&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/26"&gt;Man-Ting-Fang, issue #26&lt;/a&gt; ，&lt;a href="https://github.com/mohuangrui/ucasthesis/issues/12"&gt;diyiliaoya, issue #12&lt;/a&gt; ，和 [赵永明, email] ，矫正一些格式细节问题。根据 &lt;a href="https://github.com/mohuangrui/ucasthesis/issues/30"&gt;tangjie1992, issue #30&lt;/a&gt; ，配置算法环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-02-04&lt;/code&gt; 在 &lt;a href="https://github.com/CTeX-org/ctex-kit"&gt;ctex&lt;/a&gt; 开发者的帮助下修复误用字体命令导致的粗宋体异常。然后，将模板兼容性进一步扩展为兼容操作系统&lt;code&gt;Windows&lt;/code&gt;，&lt;code&gt;Linux&lt;/code&gt;，&lt;code&gt;MacOS&lt;/code&gt;和&lt;code&gt;LaTeX &lt;/code&gt;编译引擎&lt;code&gt;pdflatex&lt;/code&gt;，&lt;code&gt;xelatex&lt;/code&gt;，&lt;code&gt;lualatex&lt;/code&gt;。移除&lt;code&gt;microtype&lt;/code&gt;宏包以提高编译效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2018-01-28&lt;/code&gt; 基于国科大&lt;code&gt;2018&lt;/code&gt;新版论文规范进行了重大修改，采用新的封面、声明、页眉页脚样式。展示标题中使用数学公式。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;2017-05-14&lt;/code&gt; 根据 [赵永明, email] ，增加&lt;code&gt;\citepns{}&lt;/code&gt;和&lt;code&gt;\citetns{}&lt;/code&gt;命令提供上标引用下混合非上标引用的需求。根据 [臧光明, email] ，添加设定论文为&lt;code&gt;thesis&lt;/code&gt;或&lt;code&gt;dissertation&lt;/code&gt;的命令。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mohuangrui</author><guid isPermaLink="false">https://github.com/mohuangrui/ucasthesis</guid><pubDate>Sat, 02 Nov 2019 00:01:00 GMT</pubDate></item><item><title>pingcap/docs-cn #2 in TeX, This week</title><link>https://github.com/pingcap/docs-cn</link><description>&lt;p&gt;&lt;i&gt;TiDB/TiKV/PD documents in Chinese.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table data-table-type="yaml-metadata"&gt;
  &lt;thead&gt;
  &lt;tr&gt;
  &lt;th&gt;draft&lt;/th&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
  &lt;tr&gt;
  &lt;td&gt;&lt;div&gt;true&lt;/div&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1&gt;&lt;a id="user-content-tidb-简介" class="anchor" aria-hidden="true" href="#tidb-简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TiDB 简介&lt;/h1&gt;
&lt;p&gt;TiDB 是 PingCAP 公司设计的开源分布式 HTAP (Hybrid Transactional and Analytical Processing) 数据库，结合了传统的 RDBMS 和 NoSQL 的最佳特性。TiDB 兼容 MySQL，支持无限的水平扩展，具备强一致性和高可用性。TiDB 的目标是为 OLTP (Online Transactional Processing) 和 OLAP (Online Analytical Processing) 场景提供一站式的解决方案。&lt;/p&gt;
&lt;p&gt;TiDB 具备如下特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;高度兼容 MySQL&lt;/p&gt;
&lt;p&gt;&lt;a href="/v3.0/reference/mysql-compatibility.md"&gt;大多数情况下&lt;/a&gt;，无需修改代码即可从 MySQL 轻松迁移至 TiDB，分库分表后的 MySQL 集群亦可通过 TiDB 工具进行实时迁移。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;水平弹性扩展&lt;/p&gt;
&lt;p&gt;通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分布式事务&lt;/p&gt;
&lt;p&gt;TiDB 100% 支持标准的 ACID 事务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;真正金融级高可用&lt;/p&gt;
&lt;p&gt;相比于传统主从 (M-S) 复制方案，基于 Raft 的多数派选举协议可以提供金融级的 100% 数据强一致性保证，且在不丢失大多数副本的前提下，可以实现故障的自动恢复 (auto-failover)，无需人工介入。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一站式 HTAP 解决方案&lt;/p&gt;
&lt;p&gt;TiDB 作为典型的 OLTP 行存数据库，同时兼具强大的 OLAP 性能，配合 TiSpark，可提供一站式 HTAP 解决方案，一份存储同时处理 OLTP &amp;amp; OLAP，无需传统繁琐的 ETL 过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;云原生 SQL 数据库&lt;/p&gt;
&lt;p&gt;TiDB 是为云而设计的数据库，支持公有云、私有云和混合云，使部署、配置和维护变得十分简单。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TiDB 的设计目标是 100% 的 OLTP 场景和 80% 的 OLAP 场景，更复杂的 OLAP 分析可以通过 &lt;a href="/v3.0/reference/tispark.md"&gt;TiSpark 项目&lt;/a&gt;来完成。&lt;/p&gt;
&lt;p&gt;TiDB 对业务没有任何侵入性，能优雅的替换传统的数据库中间件、数据库分库分表等 Sharding 方案。同时它也让开发运维人员不用关注数据库 Scale 的细节问题，专注于业务开发，极大的提升研发的生产力。&lt;/p&gt;
&lt;p&gt;三篇文章了解 TiDB 技术内幕：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-1/" rel="nofollow"&gt;说存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-2/" rel="nofollow"&gt;说计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pingcap.com/blog-cn/tidb-internal-3/" rel="nofollow"&gt;谈调度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pingcap</author><guid isPermaLink="false">https://github.com/pingcap/docs-cn</guid><pubDate>Sat, 02 Nov 2019 00:02:00 GMT</pubDate></item><item><title>rstudio/cheatsheets #3 in TeX, This week</title><link>https://github.com/rstudio/cheatsheets</link><description>&lt;p&gt;&lt;i&gt;RStudio Cheat Sheets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-rstudio-cheat-sheets" class="anchor" aria-hidden="true" href="#rstudio-cheat-sheets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RStudio Cheat Sheets&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="pngs/rstudio-ide.png"&gt;&lt;img src="pngs/rstudio-ide.png" width="364" height="288" align="right" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The cheat sheets make it easy to learn about and use some of our favorite packages. They are published in their respective PDF versions here: &lt;a href="https://www.rstudio.com/resources/cheatsheets/" rel="nofollow"&gt;https://www.rstudio.com/resources/cheatsheets/&lt;/a&gt;, some are also available in the RStudio IDE under Help-Cheatsheets.&lt;/p&gt;
&lt;p&gt;This repository contains the source Apple Keynote files or the current, archived and translated versions.&lt;/p&gt;
&lt;p&gt;The cheat sheets use the creative commons copyright. Please see the LICENSE document for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h2&gt;
&lt;p&gt;If you wish to contribute to this effort by translating a cheat sheet, please feel free to use the source Keynote file. To submit a translation, please use a Pull Request via GitHub or email it to us at &lt;a href="mailto:info@rstudio.com"&gt;info@rstudio.com&lt;/a&gt; with the subject "Translated Cheatsheet".&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tips-for-making-a-new-cheat-sheet" class="anchor" aria-hidden="true" href="#tips-for-making-a-new-cheat-sheet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips for making a new cheat sheet&lt;/h2&gt;
&lt;p&gt;Keep these tips in mind when creating a new cheat sheet:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;RStudio cheat sheets are hosted at &lt;a href="https://github.com/rstudio/cheatsheets"&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;. You can submit new cheat sheets to the repository with a pull request.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The files &lt;code&gt;keynotes/0-template.key&lt;/code&gt; and &lt;code&gt;powerpoints/0-template.ppt&lt;/code&gt; are official templates that contain some helpful tips.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip. You may find it easier to create a new cheat sheet by duplicating the most recent Keynote / Powerpoint cheat sheet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The cheat sheets are not meant to be text documents. Ideally, they are scannable visual aids that use layout and visual mnemonics to help people zoom into the functions they need. As an analogy, think of a cheat sheet as more like a well organized computer menu bar that leads you to a command than a manual that documents each command.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The cheat sheets use a cohesive, black and white printer friendly theme (which is what you see in the sheets), so please stay close to the appearance of the existing sheets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It's already baked into every cheat sheet and the template, but you should include a &lt;a href="https://creativecommons.org/" rel="nofollow"&gt;Creative Commons&lt;/a&gt; Copyright on each side of the sheet to make them easy to repurpose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Budget more time than you expect to make the sheets. So far, I've found this process to be the least time consuming:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the package web page and any vignettes to identify which functions to include (I try to include anything that doesn't seem trivial.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Organize the functions into meaningful, self-explanatory groups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Think about how to visualize the purpose of each function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Think about what key mental models, definitions, or explanations the cheat sheet should contain in addition to the functions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sketch out several possible layouts for the sheet. Take care to put the more basic and/or pre-requisite content above and to the left of other content. Try to keep related content on the same side of the page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type out all of the explanations and function definitions. Lay them out. Verify that everything fits. White space is very important. Use it to make the sheet scannable, even if it means smaller text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Making the visuals is the most time consuming part, so I usually save them for last.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tweak until happy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pay attention to the details!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Final tip: Edit the text to be very concise - rely on diagrams where possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rstudio</author><guid isPermaLink="false">https://github.com/rstudio/cheatsheets</guid><pubDate>Sat, 02 Nov 2019 00:03:00 GMT</pubDate></item><item><title>thunlp/NRLPapers #4 in TeX, This week</title><link>https://github.com/thunlp/NRLPapers</link><description>&lt;p&gt;&lt;i&gt;Must-read papers on network representation learning (NRL) / network embedding (NE)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-must-read-papers-on-nrlne" class="anchor" aria-hidden="true" href="#must-read-papers-on-nrlne"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Must-read papers on NRL/NE.&lt;/h2&gt;
&lt;p&gt;NRL: network representation learning. NE: network embedding.&lt;/p&gt;
&lt;p&gt;Contributed by &lt;a href="http://thunlp.org/~tcc/" rel="nofollow"&gt;Cunchao Tu&lt;/a&gt;, Yuan Yao, Zhengyan Zhang, GanquCui, Hao Wang (BUPT), Changxin Tian (BUPT), Jie Zhou and Cheng Yang (BUPT).&lt;/p&gt;
&lt;p&gt;We release &lt;a href="https://github.com/thunlp/openne"&gt;OpenNE&lt;/a&gt;, an open source toolkit for NE/NRL. This repository provides a standard NE/NRL(Network Representation Learning）training and testing framework. Currently, the implemented models in OpenNE include DeepWalk, LINE, node2vec, GraRep, TADW and GCN.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-content" class="anchor" aria-hidden="true" href="#content"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#survey-papers"&gt;Survey Papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#models"&gt;Models&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#bacis-models"&gt;Bacis Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#attributed-network"&gt;Attributed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dynamic-network"&gt;Dynamic Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#heterogeneous-information-network"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipartite-network"&gt;Bipartite Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#directed-network"&gt;Directed Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-models"&gt;Other Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#applications"&gt;Applications&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#natural-language-processing"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#knowledge-graph"&gt;Knowledge Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#social-network"&gt;Social Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#graph-clustering"&gt;Graph Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#community-detection"&gt;Community Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#recommendation"&gt;Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-applications"&gt;Other Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-survey-papers" class="anchor" aria-hidden="true" href="#survey-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Survey Papers&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning on Graphs: Methods and Applications.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; IEEE Data(base) Engineering Bulletin 2017. &lt;a href="https://arxiv.org/pdf/1709.05584.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Embedding Techniques, Applications, and Performance: A Survey.&lt;/strong&gt;
&lt;em&gt;Palash Goyal, Emilio Ferrara.&lt;/em&gt; Knowledge Based Systems 2017. &lt;a href="https://arxiv.org/pdf/1705.02801.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications.&lt;/strong&gt;
&lt;em&gt;Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang.&lt;/em&gt; TKDE 2017. &lt;a href="https://arxiv.org/pdf/1709.07604.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: A Survey.&lt;/strong&gt;
&lt;em&gt;Daokun Zhang, Jie Yin, Xingquan Zhu, Chengqi Zhang.&lt;/em&gt; IEEE Transactions on Big Data 2018. &lt;a href="https://arxiv.org/pdf/1801.05852.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Tutorial on Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1808.02590.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning: An Overview.(In Chinese)&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Cheng Yang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; 2017. &lt;a href="http://engine.scichina.com/publisher/scp/journal/SSI/47/8/10.1360/N112017-00145" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive biases, deep learning, and graph networks.&lt;/strong&gt;
&lt;em&gt;Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1806.01261.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Models&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-bacis-models" class="anchor" aria-hidden="true" href="#bacis-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bacis Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SepNE: Bringing Separability to Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziyao Li, Liang Zhang, Guojie Song.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4333" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust Negative Sampling for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Mohammadreza Armandpour, Patrick Ding, Jianhua Huang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.stat.tamu.edu/~armand/R-NS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Structure and Transfer Behaviors Embedding via Deep Prediction Model.&lt;/strong&gt;
&lt;em&gt;Xin Sun, Zenghui Song, Junyu Dong, Yongbo Yu, Claudia Plant, Christian Böhm.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4436" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Simplifying Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/wu19e/wu19e.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GMNN: Graph Markov Neural Networks.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Yoshua Bengio, Jian Tang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/qu19a/qu19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Blockmodels meet Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Nikhil Mehta, Lawrence Carin Duke, Piyush Rai.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/mehta19a/mehta19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Disentangled Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, Wenwu Zhu.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/ma19a/ma19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position-aware Graph Neural Networks.&lt;/strong&gt;
&lt;em&gt;Jiaxuan You, Rex Ying, Jure Leskovec.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/you19b/you19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing.&lt;/strong&gt;
&lt;em&gt;Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, Aram Galstyan.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/abu-el-haija19a/abu-el-haija19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph U-Nets.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Shuiwang Ji.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/gao19a/gao19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Attention Graph Pooling.&lt;/strong&gt;
&lt;em&gt;Junhyun Lee, Inyeop Lee, Jaewoo Kang.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/lee19c/lee19c.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking.&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1707.03815.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.&lt;/strong&gt;
&lt;em&gt;Jie Chen, Tengfei Ma, Cao Xiao.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1801.10247.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Attention Networks.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio.&lt;/em&gt; ICLR 2018. &lt;a href="https://arxiv.org/pdf/1710.10903.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stochastic Training of Graph Convolutional Networks with Variance Reduction.&lt;/strong&gt;
&lt;em&gt;Jianfei Chen, Jun Zhu, Le Song.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1710.10568.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarially Regularized Graph Autoencoder for Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0362.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discrete Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiaobo Shen, Shirui Pan, Weiwei Liu, Yew-Soon Ong, Quan-Sen Sun.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0493.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Hashing for Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Qixiang Wang, Shanfeng Wang, Maoguo Gong, Yue Wu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0390.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Inductive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed.&lt;/em&gt; WWW 2018. &lt;a href="http://ryanrossi.com/pubs/rossi-et-al-WWW18-BigNet.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Active Discriminative Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Li Gao, Hong Yang, Chuan Zhou, Jia Wu, Shirui Pan, Yue Hu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0296.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MILE: A Multi-Level Framework for Scalable Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Saket Gurukar, Srinivasan Parthasarathy.&lt;/em&gt; arxiv 2018. &lt;a href="https://arxiv.org/pdf/1802.09612.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Out-of-sample extension of graph adjacency spectral embedding.&lt;/strong&gt;
&lt;em&gt;Keith Levin, Farbod Roosta-Khorasani, Michael W. Mahoney, Carey E. Priebe.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1802.06307.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepWalk: Online Learning of Social Representations.&lt;/strong&gt;
&lt;em&gt;Bryan Perozzi, Rami Al-Rfou, Steven Skiena.&lt;/em&gt; KDD 2014. &lt;a href="https://arxiv.org/pdf/1403.6652" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/phanein/deepwalk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non-transitive Hashing with Latent Similarity Componets.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Fei Wang, Jun Wang, Wenwu Zhu.&lt;/em&gt; KDD 2015. &lt;a href="http://cuip.thumedialab.com/papers/KDD-NonTransitiveHashing.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraRep: Learning Graph Representations with Global Structural Information.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Qiongkai Xu.&lt;/em&gt; CIKM 2015. &lt;a href="https://www.researchgate.net/profile/Qiongkai_Xu/publication/301417811_GraRep/links/5847ecdb08ae8e63e633b5f2/GraRep.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/GraRep"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LINE: Large-scale Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Me.&lt;/em&gt; WWW 2015. &lt;a href="https://arxiv.org/pdf/1503.03578.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tangjianpku/LINE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Neural Networks for Learning Graph Representations.&lt;/strong&gt;
&lt;em&gt;Shaosheng Cao, Wei Lu, Xiongkai Xu.&lt;/em&gt; AAAI 2016. &lt;a href="https://pdfs.semanticscholar.org/1a37/f07606d60df365d74752857e8ce909f700b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/ShelsonCao/DNGR"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Revisiting Semi-supervised Learning with Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov.&lt;/em&gt; ICML 2016. &lt;a href="http://www.jmlr.org/proceedings/papers/v48/yanga16.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Max-Margin DeepWalk: Discriminative Learning of Network Representation.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2016. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2016_mmdw.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/mmdw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Discriminative Deep RandomWalk for Network Classification.&lt;/strong&gt;
&lt;em&gt;Juzheng Li, Jun Zhu, Bo Zhang.&lt;/em&gt; ACL 2016. &lt;a href="http://www.aclweb.org/anthology/P16-1095" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Network Embedding.&lt;/strong&gt;
&lt;em&gt;Daixin Wang, Peng Cui, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/SDNE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Neighborhood Based Classification of Nodes in a Network.&lt;/strong&gt;
&lt;em&gt;Sharad Nandanwar, M. N. Murty.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/Paper_679.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Preserving Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, Shiqiang Yang.&lt;/em&gt; AAAI 2017. &lt;a href="http://cuip.thumedialab.com/papers/NE-Community.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised Classification with Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Thomas N. Kipf, Max Welling.&lt;/em&gt; ICLR 2017. &lt;a href="https://arxiv.org/pdf/1609.02907.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tkipf/gcn"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fast Network Embedding Enhancement via High Order Proximity Approximation.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Maosong Sun, Zhiyuan Liu, Cunchao Tu.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_neu.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/neu"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CANE: Context-Aware Network Embedding for Relation Modeling.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Han Liu, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; ACL 2017. &lt;a href="http://thunlp.org/~tcc/publications/acl2017_cane.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/cane"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A General View for Network Embedding as Matrix Factorization.&lt;/strong&gt;
&lt;em&gt;Xin Liu, Tsuyoshi Murata, Kyoung-Sook Kim, Chatchawan Kotarasu, Chenyi Zhuang.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?doid=3289600.3291029" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Embedding Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Zaiqiao Meng, Shangsong Liang, Xiangliang Zhang, Hongyan Bao.&lt;/em&gt; WSDM 2019. &lt;a href="https://mine.kaust.edu.sa/Documents/papers/WSDM19attribute.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhanced Network Embeddings via Exploiting Edge Labels.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Xiaofei Sun, Yingtao Tian, Bryan Perozzi, Muhao Chen, Steven Skiena.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1809.05124.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Improve Network Embeddings with Regularization.&lt;/strong&gt;
&lt;em&gt;Yi Zhang, Jianguo Lu, Ofer Shai.&lt;/em&gt; CIKM 2018. &lt;a href="https://jlu.myweb.cs.uwindsor.ca/n2v.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modeling Multi-way Relations with Hypergraph Embedding.&lt;/strong&gt;
&lt;em&gt;Chia-An Yu, Ching-Lun Tai, Tak-Shing Chan, Yi-Hsuan Yang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269274" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chia-an/HGE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;REGAL: Representation Learning-based Graph Alignment.&lt;/strong&gt;
&lt;em&gt;Mark Heimann, Haoming Shen, Tara Safavi, Danai Koutra.&lt;/em&gt; CIKM 2018. &lt;a href="https://arxiv.org/pdf/1802.06257.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai, Qiang Li, Jian Tang, Dan Wang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.07838.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/sachinbiradar9/Adverserial-Inductive-Deep-Walk"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli Embeddings for Graphs.&lt;/strong&gt;
&lt;em&gt;Vinith Misra, Sumit Bhatia.&lt;/em&gt; AAAI 2018. &lt;a href="http://sumitbhatia.net/papers/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GraphGAN: Graph Representation Learning with Generative Adversarial Nets.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, jia Wang, jialin Wang, MIAO ZHAO, Weinan Zhang, Fuzheng Zhang, Xie Xing, Minyi Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.08267.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HARP: Hierarchical Representation Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Haochen Chen, Bryan Perozzi, Yifan Hu, Steven Skiena.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1706.07845.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/GTmac/HARP"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Social Rank Regulated Large-scale Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yupeng Gu, Yizhou Sun, Yanen Li, Yang Yang.&lt;/em&gt; WWW 2018. &lt;a href="http://yangy.org/works/ge/rare.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Latent Network Summarization: Bridging Network Embedding and Summarization.&lt;/strong&gt;
&lt;em&gt;Di Jin,Ryan Rossi,Danai Koutra,Eunyee Koh,Sungchul Kim,Anup Rao&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1811.04461.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching.&lt;/strong&gt;
&lt;em&gt;Dingqi Yang,Paolo Rosso,Bin Li,Philippe Cudre-Mauroux.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330951/p1162-yang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ProGAN: Network Embedding via Proximity Generative Adversarial Network.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao,Jian Pei,Heng Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330866/p1308-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Global Alignment Graph Kernel Using Random Features: From Node Embedding to Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Lingfei Wu,Ian En-Hsu Yen,Zhen Zhang,Kun Xu,Liang Zhao,Xi Peng,Yinglong Xia,Charu Aggarwal.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330918/p1418-wu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Graph Embeddings via Sparse Transpose Proximities.&lt;/strong&gt;
&lt;em&gt;Yuan Yin,Zhewei Wei.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330860/p1429-yin.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AutoNRL: Hyperparameter Optimization for Massive Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Ke Tu,Jianxin Ma,Peng Cui,Jian Pei,Wenwu Zhu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330848/p216-tu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Representation Learning via Hard and Channel-Wise Attention Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao,Shuiwang Ji.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330897/p741-gao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Training Methods for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Quanyu Dai,Xiao Shen,Liang Zhang,Qiang Li,Dan Wang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1908.11514.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-relational Network Embeddings with Relational Proximity and Node Attributes.&lt;/strong&gt;
&lt;em&gt;Ming-Han Feng,Chin-Chi Hsu,Cheng-Te Li,Mi-Yen Yeh,Shou-De Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://pdfs.semanticscholar.org/6274/3cbebc142897c6c005f3c12c00b9202ca43f.pdf?_ga=2.108748866.1527570260.1569422306-1231101604.1568798295" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sampled in Pairs and Driven by Text: A New Graph Embedding Framework.&lt;/strong&gt;
&lt;em&gt;Liheng Chen,Yanru Qu,Zhenghui Wang,Weinan Zhang,Ken Chen,Shaodian Zhang,Yong Yu.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313520#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DDGK: Learning Graph Representations via Deep Divergence Graph Kernels.&lt;/strong&gt;
&lt;em&gt;Rami Al-Rfou,Dustin Zelle,Bryan Perozzi.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1904.09671.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tag2Vec: Learning Tag Representations in Tag Networks.&lt;/strong&gt;
&lt;em&gt;Junshan Wang,Zhicong Lu,Guojia Song,Yue Fan,Lun Du,Wei Lin.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1905.03041.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;struc2vec: Learning Node Representations from Structural Identity.&lt;/strong&gt;
&lt;em&gt;Leonardo F. R. Ribeiro, Pedro H. P. Saverese, Daniel R. Figueiredo.&lt;/em&gt; KDD 2017. &lt;a href="https://arxiv.org/pdf/1704.03165.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inductive Representation Learning on Large Graphs.&lt;/strong&gt;
&lt;em&gt;William L. Hamilton, Rex Ying, Jure Leskovec.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1706.02216.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Graph Embeddings with Embedding Propagation.&lt;/strong&gt;
&lt;em&gt;Alberto Garcia Duran, Mathias Niepert.&lt;/em&gt; NIPS 2017. &lt;a href="https://arxiv.org/pdf/1710.03059.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhancing the Network Embedding Quality with Structural Similarity.&lt;/strong&gt;
&lt;em&gt;Tianshu Lyu, Yuan Zhang, Yan Zhang.&lt;/em&gt; CIKM 2017. &lt;a href="https://pdfs.semanticscholar.org/e54a/374d7e24260450e2081b93005a491d1b9116.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;An Attention-based Collaboration Framework for Multi-View Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, Jiawei Han.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1709.06636.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Embedding Uncertain Graphs.&lt;/strong&gt;
&lt;em&gt;Jiafeng Hu, Reynold Cheng, Zhipeng Huang, Yixang Fang, Siqiang Luo.&lt;/em&gt; CIKM 2017. &lt;a href="https://i.cs.hku.hk/~zphuang/pub/CIKM17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec.&lt;/strong&gt;
&lt;em&gt;Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1710.02971.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conditional Network Embeddings.&lt;/strong&gt;
&lt;em&gt;Bo Kang, Jefrey Lijffijt, Tijl De Bie.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1805.07544" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Infomax.&lt;/strong&gt;
&lt;em&gt;Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm.&lt;/em&gt; ICLR 2019. &lt;a href="https://arxiv.org/abs/1809.10341" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Anonymous Walk Embeddings.&lt;/strong&gt;
&lt;em&gt;Sergey Ivanov, Evgeny Burnaev.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1805.11921.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fairwalk: Towards Fair Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Tahleen Rahman, Bartlomiej Surma, Michael Backes, Yang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://yangzhangalmo.github.io/papers/IJCAI19.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph and Autoencoder Based Feature Extraction for Zero-shot Learning.&lt;/strong&gt;
&lt;em&gt;Yang Liu, Deyan Xie, Quanxue Gao, Jungong Han, Shujian Wang, Xinbo Gao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0421.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Space Embedding.&lt;/strong&gt;
&lt;em&gt;João Pereira, Evgeni Levin, Erik Stroes, Albert Groen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1907.13443" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Arbitrary-Order Proximity Preserved Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-ArbitraryProximity.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Variational Network Embedding in Wasserstein Space.&lt;/strong&gt;
&lt;em&gt;Dingyuan Zhu, Peng Cui, Daixin Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-DeepVariational.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEGAN: A Generative Adversarial Network for Multi-View Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiwei Sun, Suhang Wang, Tsung-Yu Hsieh, Xianfeng Tang, Vasant Honavar.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1909.01084" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding under Partial Monitoring for Evolving Networks&lt;/strong&gt;
&lt;em&gt;Yu Han, Jie Tang, Qian Chen.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0342.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Embedding with Dual Generation Tasks.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Na Li, Zhicheng He.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0709.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Triplet Enhanced AutoEncoder: Model-free Discriminative Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yao Yang, Haoran Chen, Junming Shao.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0745.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Recursive Network Embedding with Regular Equivalence.&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, Philip S. Yu, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="http://cuip.thumedialab.com/papers/NE-RegularEquivalence.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Structural Node Embeddings via Diffusion Wavelets.&lt;/strong&gt;
&lt;em&gt;Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1710.10321.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Paced Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://par.nsf.gov/servlets/purl/10074506" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Deep Network Representations with Adversarially Regularized Autoencoders.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Cheng Zheng, Wei Cheng, Charu Aggarwal, Dongjin Song, Bo Zong, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="https://sites.cs.ucsb.edu/~bzong/doc/kdd-18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Learnable Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Hongyang Gao, Zhengyang Wang, Shuiwang Ji.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1808.03965" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-attributed-network" class="anchor" aria-hidden="true" href="#attributed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Attributed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Outlier Aware Network Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sambaran Bandyopadhyay, N. Lokesh, M. N. Murty.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3763" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large-Scale Heterogeneous Feature Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Fan Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4276" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Bayesian Optimization on Attributed Graphs.&lt;/strong&gt;
&lt;em&gt;Jiaxu Cui, Bo Yang, Xia Hu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/pdf/1905.13403.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficient Attributed Network Embedding via Recursive Randomized Hashing.&lt;/strong&gt;
&lt;em&gt;Wei Wu, Bin Li, Ling Chen, Chengqi Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0397.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Hongchang Gao, Heng Huang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0467.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ANRL: Attributed Network Representation Learning via Deep Neural Networks.&lt;/strong&gt;
&lt;em&gt;Zhen Zhang, Hongxia Yang, Jiajun Bu, Sheng Zhou, Pinggang Yu, Jianwei Zhang, Martin Ester, Can Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0438.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrative Network Embedding via Deep Joint Reconstruction.&lt;/strong&gt;
&lt;em&gt;Di Jin, Meng Ge, Liang Yang, Dongxiao He, Longbiao Wang, Weixiong Zhang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0473.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;node2vec: Scalable Feature Learning for Networks.&lt;/strong&gt;
&lt;em&gt;Aditya Grover, Jure Leskovec.&lt;/em&gt; KDD 2016. &lt;a href="http://www.kdd.org/kdd2016/papers/files/rfp0218-groverA.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/aditya-grover/node2vec"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Representation Learning with Rich Text Information.&lt;/strong&gt;
&lt;em&gt;Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, Edward Y. Chang.&lt;/em&gt; IJCAI 2015. &lt;a href="http://thunlp.org/~yangcheng/publications/ijcai15.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/tadw"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tri-Party Deep Network Representation.&lt;/strong&gt;
&lt;em&gt;Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, Yang Wang.&lt;/em&gt; IJCAI 2016. &lt;a href="https://www.ijcai.org/Proceedings/16/Papers/271.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransNet: Translation-Based Network Representation Learning for Social Relation Extraction.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun.&lt;/em&gt; IJCAI 2017. &lt;a href="http://thunlp.org/~tcc/publications/ijcai2017_transnet.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/thunlp/transnet"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PRRE: Personalized Relation Ranking Embedding for Attributed Networks.&lt;/strong&gt;
&lt;em&gt;Sheng Zhou, Hongxia Yang, Xin Wang, Jiajun Bu, Martin Ester, Pinggang Yu, Jianwei Zhang, Can Wang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271741" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhoushengisnoob/PRRE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RSDNE: Exploring Relaxed Similarity and Dissimilarity from Completely-imbalanced Labels for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zheng Wang, Xiaojun Ye, Chaokun Wang, YueXin Wu, Changping Wang, Kaiwen Liang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16062/15722" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/zhengwang100/RSDNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised embedding in attributed networks with outliers.&lt;/strong&gt;
&lt;em&gt;Jiongqian Liang, Peter Jacobs, Jiankai Sun, and Srinivasan Parthasarathy.&lt;/em&gt; SDM 2018. &lt;a href="https://arxiv.org/pdf/1703.08100.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="http://jiongqianliang.com/SEANO/" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Representation Learning Framework for Property Graphs.&lt;/strong&gt;
&lt;em&gt;Yifan Hou,Hongzhi Chen,Changji Li,James Cheng,Ming-Chang Yang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330948/p65-hou.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning from Labeled and Unlabeled Vertices in Networks.&lt;/strong&gt;
&lt;em&gt;Wei Ye, Linfei Zhou, Dominik Mautz, Claudia Plant, Christian B?hm.&lt;/em&gt; KDD 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3098142" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Label Informed Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; WSDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM17_LANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accelerated Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Jundong Li, Xia Hu.&lt;/em&gt; SDM 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/SDM17_AANE.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variation Autoencoder Based Network Representation Learning for Classification.&lt;/strong&gt;
&lt;em&gt;Hang Li, Haozheng Wang, Zhenglu Yang, Masato Odagaki.&lt;/em&gt; ACL 2017. &lt;a href="https://aclweb.org/anthology/P17-3010" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Signed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Suhang Wang, Charu Aggarwal, Jiliang Tang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://suhangwang.ist.psu.edu/publications/SNEA.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;From Properties to Links: Deep Network Embedding on Incomplete Graphs.&lt;/strong&gt;
&lt;em&gt;Dejian Yang, Senzhang Wang, Chaozhuo Li, Xiaoming Zhang, Zhoujun Li.&lt;/em&gt; CIKM 2017. &lt;a href="https://dl.acm.org/citation.cfm?id=3132847.3132975" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exploring Expert Cognition for Attributed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Huang, Qingquan Song, Jundong Li, Xia Ben Hu.&lt;/em&gt; WSDM 2018. &lt;a href="http://www.public.asu.edu/~jundongl/paper/WSDM18_NEEC.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Taxonomy Aware Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Xiao Wang, Wenwu Zhu.&lt;/em&gt; KDD 2018. &lt;a href="https://jianxinma.github.io/assets/NetHiex.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network-Specific Variational Auto-Encoder for Embedding in Attribute Networks.&lt;/strong&gt;
&lt;em&gt;Di Jin, Bingyi Li, Pengfei Jiao, Dongxiao He, Weixiong Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0370.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPINE: Structural Identity Preserved Inductive Network Embedding.&lt;/strong&gt;
&lt;em&gt;Junliang Guo, Linli Xu, Jingchang Liu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0333.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Content to Node: Self-translation Network Embedding.&lt;/strong&gt;
&lt;em&gt;Jie Liu, Zhicheng He, Lai Wei, Yalou Huang.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3219988" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-dynamic-network" class="anchor" aria-hidden="true" href="#dynamic-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Dynamic Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding : An Extended Approach for Skip-gram based Network Embedding.&lt;/strong&gt;
&lt;em&gt;Lun Du, Yun Wang, Guojie Song, Zhicong Lu, Junshan Wang.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0288.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Network Embedding by Modeling Triadic Closure Process.&lt;/strong&gt;
&lt;em&gt;Lekui Zhou, Yang Yang, Xiang Ren, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="http://yangy.org/works/dynamictriad/dynamic_triad.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DepthLGP: Learning Embeddings of Out-of-Sample Nodes in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Jianxin Ma, Peng Cui, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://jianxinma.github.io/assets/DepthLGP.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TIMERS: Error-Bounded SVD Restart on Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Ziwei Zhang, Peng Cui, Jian Pei, Xiao Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.09541.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks.&lt;/strong&gt;
&lt;em&gt;Srijan Kumar,Xikun Zhang,Jure Leskovec.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330895/p1269-kumar.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attributed Network Embedding for Learning in a Dynamic Environment.&lt;/strong&gt;
&lt;em&gt;Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, Huan Liu.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1706.01860.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DyRep: Learning Representations over Dynamic Graphs.&lt;/strong&gt;
&lt;em&gt;Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, Hongyuan Zha.&lt;/em&gt; ICLR 2019. &lt;a href="https://openreview.net/forum?id=HyePrhR5KX" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding Temporal Network via Neighborhood Formation.&lt;/strong&gt;
&lt;em&gt;Yuan Zuo, Guannan Liu, Hao Lin, Jia Guo, Xiaoqian Hu, Junjie Wu.&lt;/em&gt; KDD 2018. &lt;a href="https://zuoyuan.github.io/files/htne_kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Node Embedding over Temporal Graphs.&lt;/strong&gt;
&lt;em&gt;Uriel Singer, Ido Guy, Kira Radinsky.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0640.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Embeddings for User Profiling in Twitter.&lt;/strong&gt;
&lt;em&gt;Shangsong Liang, Xiangliang Zhang, Zhaochun Ren, Evangelos Kanoulas.&lt;/em&gt; KDD 2018. &lt;a href="https://repository.kaust.edu.sa/bitstream/handle/10754/628781/p1764-liang.pdf?sequence=1&amp;amp;isAllowed=y" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetWalk: A Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Networks.&lt;/strong&gt;
&lt;em&gt;Wenchao Yu, Wei Cheng, Charu Aggarwal, Kai Zhang, Haifeng Chen, Wei Wang.&lt;/em&gt; KDD 2018. &lt;a href="http://www.shichuan.org/hin/topic/Embedding/2018.KDD%202018%20NetWalk_A%20Flexible%20Deep%20Embedding%20Approach%20for%20Anomaly%20Detection%20in%20Dynamic%20Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Optimization for Embedding Highly-Dynamic and Recency-Sensitive Data.&lt;/strong&gt;
&lt;em&gt;Xumin Chen, Peng Cui, Shiqiang Yang.&lt;/em&gt; KDD 2018. &lt;a href="http://pengcui.thumedialab.com/papers/NE-ScalableOptimization.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-heterogeneous-information-network" class="anchor" aria-hidden="true" href="#heterogeneous-information-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Heterogeneous Information Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relation Structure-Aware Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yuanfu Lu, Chuan Shi, Linmei Hu, Zhiyuan Liu.&lt;/em&gt; AAAI 2019. &lt;a href="https://arxiv.org/abs/1905.08027" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hyperbolic Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xiao Wang, Yiding Zhang, Chuan Shi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/65.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Latent Representations of Nodes for Classifying in Heterogeneous Social Networks.&lt;/strong&gt;
&lt;em&gt;Yann Jacob, Ludovic Denoyer, Patrick Gallinar.&lt;/em&gt; WSDM 2014. &lt;a href="http://webia.lip6.fr/~gallinar/gallinari/uploads/Teaching/WSDM2014-jacob.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Heterogeneous Network Embedding via Deep Architectures.&lt;/strong&gt;
&lt;em&gt;Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, Thomas S. Huang.&lt;/em&gt; KDD 2015. &lt;a href="http://www.ifp.illinois.edu/~chang87/papers/kdd_2015.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;metapath2vec: Scalable Representation Learning for Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Yuxiao Dong, Nitesh V. Chawla, Ananthram Swami.&lt;/em&gt; KDD 2017. &lt;a href="https://www3.nd.edu/~dial/publications/dong2017metapath2vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://ericdongyx.github.io/metapath2vec/m2v.html" rel="nofollow"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang, Ananthram Swami, Nitesh V. Chawla.&lt;/em&gt; WSDM 2019. &lt;a href="https://dl.acm.org/citation.cfm?id=3291001" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/chuxuzhang/WSDM2019_SHNE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Are Meta-Paths Necessary?: Revisiting Heterogeneous Graph Embeddings.&lt;/strong&gt;
&lt;em&gt;Rana Hussein, Dingqi Yang, Philippe Cudré-Mauroux.&lt;/em&gt; CIKM 2018. &lt;a href="https://exascale.info/assets/pdf/hussein2018cikm.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Abnormal Event Detection via Heterogeneous Information Network Embedding.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Chuan Shi, Xiao Wang.&lt;/em&gt; CIKM 2018. &lt;a href="http://shichuan.org/doc/62.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multidimensional Network Embedding with Hierarchical Structures.&lt;/strong&gt;
&lt;em&gt;Yao Ma, Zhaochun Ren, Ziheng Jiang, Jiliang Tang, Dawei Yin.&lt;/em&gt; WSDM 2018. &lt;a href="http://cse.msu.edu/~mayao4/downloads/Multidimensional_Network_Embedding_with_Hierarchical_Structure.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Curriculum Learning for Heterogeneous Star Network Embedding via Deep Reinforcement Learning.&lt;/strong&gt;
&lt;em&gt;Meng Qu, Jian Tang, Jiawei Han.&lt;/em&gt; WSDM 2018. &lt;a href="http://delivery.acm.org/10.1145/3160000/3159711/p468-qu.pdf?ip=203.205.141.123&amp;amp;id=3159711&amp;amp;acc=ACTIVE%20SERVICE&amp;amp;key=39FCDE838982416F%2E39FCDE838982416F%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;amp;__acm__=1519788484_7383495a5c522cbe124e62e4d768f8cc" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative Adversarial Network based Heterogeneous Bibliographic Network Representation for Personalized Citation Recommendation.&lt;/strong&gt;
&lt;em&gt;J. Han, Xiaoyan Cai, Libin Yang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/1596/d6487012696ba400fb69904a2c372a08a2be.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distance-aware DAG Embedding for Proximity Search on Heterogeneous Graphs.&lt;/strong&gt;
&lt;em&gt;Zemin Liu, Vincent W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin Chen-Chuan Chang, Minghui Wu, Jing Ying.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/b1cc/127a65c40e71121106d0c663f9b5baf9d6f9.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Attributed Multiplex Heterogeneous Network.&lt;/strong&gt;
&lt;em&gt;Yukuo Cen,Xu Zou,Jianwei Zhang,Hongxia Yang,Jingren Zhou,Jie Tang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330964/p1358-cen.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning on Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Binbin Hu,Yuan Fang,Chuan Shi&lt;/em&gt; KDD 2019 &lt;a href="http://delivery.acm.org/10.1145/3340000/3330970/p120-hu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HetGNN: Heterogeneous Graph Neural Network.&lt;/strong&gt;
&lt;em&gt;Chuxu Zhang,Dongjin Song,Chao Huang,Ananthram Swami,Nitesh V. Chawla.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330961/p793-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;IntentGC: a Scalable Graph Convolution Framework Fusing Heterogeneous Information for Recommendation&lt;/strong&gt;
&lt;em&gt;Jun Zhao, Zhou Zhou, Ziyu Guan, Wei Zhao, Ning Wei, Guang Qiu and Xiaofei He.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330686/p2347-zhao.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation.&lt;/strong&gt;
&lt;em&gt;Shaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma and Yongliang Li.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330673/p2478-fan.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Your Style Your Identity: Leveraging Writing and Photography Styles for Drug Trafficker Identification in Darknet Markets over Attributed Heterogeneous Information Network.&lt;/strong&gt;
&lt;em&gt;Yiming Zhang, Yujie Fan,Wei Song, Shifu HouYanfang Ye, Xin Li,Liang Zhao,Chuan Shi,Jiabin Wang, Qi Xiong.&lt;/em&gt; WWW 2019. &lt;a href="https://www.gwern.net/docs/sr/2019-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning.&lt;/strong&gt;
&lt;em&gt;Tao-yang Fu, Wang-Chien Lee, Zhen Lei.&lt;/em&gt; CIKM 2017. &lt;a href="http://shichuan.org/hin/topic/Embedding/2017.%20CIKM%20HIN2Vec.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu.&lt;/em&gt; WSDM 2018. &lt;a href="https://arxiv.org/pdf/1712.00732.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ActiveHNE: Active Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Xia Chen, Guoxian Yu, Jun Wang, Carlotta Domeniconi, Zhao Li, Xiangliang Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0294.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unified Embedding Model over Heterogeneous Information Network for Personalized Recommendation.&lt;/strong&gt;
&lt;em&gt;Zekai Wang, Hongzhi Liu, Yingpeng Du, Zhonghai Wu, Xing Zhang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0529.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Yu Shi, Qi Zhu, Fang Guo, Chao Zhang, Jiawei Han.&lt;/em&gt; KDD 2018. &lt;a href="https://yu-shi-homepage.github.io/kdd18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction.&lt;/strong&gt;
&lt;em&gt;Hongxu Chen, Hongzhi Yin, Weiqing Wang, Hao Wang, Quoc Viet Hung Nguyen, Xue Li.&lt;/em&gt; KDD 2018. &lt;a href="http://net.pku.edu.cn/daim/hongzhi.yin/papers/KDD18-Hongxu.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-bipartite-network" class="anchor" aria-hidden="true" href="#bipartite-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Bipartite Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collaborative Similarity Embedding for Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Chih-Ming Chen,Chuan-Ju Wang,Ming-Feng Tsai,Yi-Hsuan Yang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1902.06188.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Node Embeddings in Interaction Graphs.&lt;/strong&gt;
&lt;em&gt;Yao Zhang, Yun Xiong, Xiangnan Kong, Yangyong Zhu.&lt;/em&gt; CIKM 2017. &lt;a href="https://web.cs.wpi.edu/~xkong/publications/papers/cikm17.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Representation Learning for Bipartite Graphs.&lt;/strong&gt;
&lt;em&gt;Chong Li, Kunyang Jia, Dan Shen, C.J. Richard Shi, Hongxia Yang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0398.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-directed-network" class="anchor" aria-hidden="true" href="#directed-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Directed Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation.&lt;/strong&gt;
&lt;em&gt;Jiankai Sun, Bortik Bandyopadhyay, Armin Bashizade, Jiongqian Liang, P. Sadayappan, Srinivasan Parthasarathy.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/3794" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Asymmetric Transitivity Preserving Graph Embedding.&lt;/strong&gt;
&lt;em&gt;Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, Wenwu Zhu.&lt;/em&gt; KDD 2016. &lt;a href="http://cuip.thumedialab.com/papers/hoppe.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;"Bridge": Enhanced Signed Directed Network Embedding.&lt;/strong&gt;
&lt;em&gt;Yiqi Chen, Tieyun Qian, Huan Liu, Ke Sun.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271738" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SIDE: Representation Learning in Signed Directed Networks.&lt;/strong&gt;
&lt;em&gt;Junghwan Kim, Haekyu Park, Ji-Eun Lee, U Kang.&lt;/em&gt; WWW 2018. &lt;a href="https://datalab.snu.ac.kr/side/resources/side.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-models" class="anchor" aria-hidden="true" href="#other-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalable Multiplex Network Embedding. （Multiplex Network)&lt;/strong&gt;
&lt;em&gt;Hongming Zhang, Liwei Qiu, Lingling Yi, Yangqiu Song.&lt;/em&gt; IJCAI 2018. &lt;a href="http://www.cse.ust.hk/~yqsong/papers/2018-IJCAI-MultiplexNetworkEmbedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structural Deep Embedding for Hyper-Networks. (Hyper-Network)&lt;/strong&gt;
&lt;em&gt;Ke Tu, Peng Cui, Xiao Wang, fei Wang, Wenwu Zhu.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10146.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Representation Learning for Scale-free Networks. (Scale-free Network)&lt;/strong&gt;
&lt;em&gt;Rui Feng, Yang Yang, Wenjie Hu, Fei Wu, Yueting Zhuang.&lt;/em&gt; AAAI 2018. &lt;a href="https://arxiv.org/pdf/1711.10755.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Co-Regularized Deep Multi-Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Jingchao Ni, Shiyu Chang, Xiao Liu, Wei Cheng, Haifeng Chen, Dongkuan Xu, Xiang Zhang.&lt;/em&gt; WWW 2018. &lt;a href="https://nijingchao.github.io/paper/www18_dmne.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Joint Link Prediction and Network Alignment via Cross-graph Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Xingbo Du, Junchi Yan, Hongyuan Zha.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0312.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DANE: Domain Adaptive Network Embedding. (Multi-Network)&lt;/strong&gt;
&lt;em&gt;Yizhou Zhang, Guojie Song, Lun Du, Shuwen Yang, Yilun Jin.&lt;/em&gt; IJCAI 2019. &lt;a href="https://arxiv.org/abs/1906.00684" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SPARC: Self-Paced Network Representation for Few-Shot Rare Category Characterization. (Few-Shot Learning)&lt;/strong&gt;
&lt;em&gt;Dawei Zhou, Jingrui He, Hongxia Yang, Wei Fan.&lt;/em&gt; KDD 2018. &lt;a href="https://dl.acm.org/authorize?N665885" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-applications" class="anchor" aria-hidden="true" href="#applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Applications&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Natural Language Processing&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Personalized Question Routing via Heterogeneous Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zeyu Li, Jyun-Yu Jiang, Yizhou Sun, Wei Wang.&lt;/em&gt; AAAI 2019. &lt;a href="http://web.cs.ucla.edu/~yzsun/papers/2019_AAAI_QR.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks.&lt;/strong&gt;
&lt;em&gt;Jian Tang, Meng Qu, Qiaozhu Mei.&lt;/em&gt; KDD 2015. &lt;a href="https://arxiv.org/pdf/1508.00200.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/mnqu/PTE"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-knowledge-graph" class="anchor" aria-hidden="true" href="#knowledge-graph"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Knowledge Graph&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interaction Embeddings for Prediction and Explanation in Knowledge Graphs.&lt;/strong&gt;
&lt;em&gt;Wen Zhang, Bibek Paudel, Wei Zhang, Abraham Bernstein, Huajun Chen.&lt;/em&gt; WSDM 2019. &lt;a href="https://arxiv.org/pdf/1903.04750.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shared Embedding Based Neural Networks for Knowledge Graph Completion.&lt;/strong&gt;
&lt;em&gt;Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, Xueqi Cheng.&lt;/em&gt; CIKM 2018 &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Re-evaluating Embedding-Based Knowledge Graph Completion Methods.&lt;/strong&gt;
&lt;em&gt;Farahnaz Akrami, Lingbing Guo, Wei Hu, Chengkai Li.&lt;/em&gt; CIKM 2018. &lt;a href="http://ranger.uta.edu/~cli/pubs/2018/kgcompletion-cikm18short-akrami.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-social-network" class="anchor" aria-hidden="true" href="#social-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Social Network&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Learning for Weakly-Supervised Social Network Alignment.&lt;/strong&gt;
&lt;em&gt;Chaozhuo Li, Senzhang Wang, Yukun Wang, Philip Yu, Yanbo Liang, Yun Liu, Zhoujun Li.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/3889" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TransConv: Relationship Embedding in Social Networks.&lt;/strong&gt;
&lt;em&gt;Yi-Yu Lai, Jennifer Neville, Dan Goldwasser.&lt;/em&gt; AAAI 2019. &lt;a href="https://aaai.org/ojs/index.php/AAAI/article/view/4314" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semi-supervised User Geolocation via Graph Convolutional Networks.&lt;/strong&gt;
&lt;em&gt;Afshin Rahimi, Trevor Cohn, Timothy Baldwin.&lt;/em&gt; ACL 2018. &lt;a href="https://arxiv.org/pdf/1804.08049.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MASTER: across Multiple social networks, integrate Attribute and STructure Embedding for Reconciliation.&lt;/strong&gt;
&lt;em&gt;Sen Su, Li Sun, Zhongbao Zhang, Gen Li, Jielun Qu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://www.ijcai.org/proceedings/2018/0537.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MEgo2Vec: Embedding Matched Ego Networks for User Alignment Across Social Networks.&lt;/strong&gt;
&lt;em&gt;Jing Zhang, Bo Chen, Xianming Wang, Hong Chen, Cuiping Li, Fengmei Jin, Guojie Song, Yutao Zhang.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3271705" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Link Prediction via Subgraph Embedding-Based Convex Matrix Completion.&lt;/strong&gt;
&lt;em&gt;Zhu Cao, Linlin Wang, Gerard De melo.&lt;/em&gt; AAAI 2018. &lt;a href="http://iiis.tsinghua.edu.cn/~weblt/papers/link-prediction-subgraphembeddings.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;On Exploring Semantic Meanings of Links for Embedding Social Networks.&lt;/strong&gt;
&lt;em&gt;Linchuan Xu, Xiaokai Wei, Jiannong Cao, Philip S Yu.&lt;/em&gt; WWW 2018. &lt;a href="https://pdfs.semanticscholar.org/ccd3/ede78393628b5f0256ebfccbb4ac293394de.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MCNE: An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network.&lt;/strong&gt;
&lt;em&gt;Hao Wang,Tong Xu,Qi Liu,Defu Lian,Enhong Chen,Dongfang Du,Han Wu,Wen Su.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330931/p1064-wang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Feature Selection in Signed Social Networks.&lt;/strong&gt;
&lt;em&gt;Kewei Cheng, Jundong Li, Huan Liu.&lt;/em&gt; KDD 2017. &lt;a href="http://www.public.asu.edu/~jundongl/paper/KDD17_SignedFS.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network Embedding with Community Structural Information.&lt;/strong&gt;
&lt;em&gt;Yu Li, Ying Wang, Tingting Zhang, Jiawei Zhang, Yi Chang.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0407.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-graph-clustering" class="anchor" aria-hidden="true" href="#graph-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Graph Clustering&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spectral Clustering in Heterogeneous Information Networks.&lt;/strong&gt;
&lt;em&gt;Xiang Li , Ben Kao, Zhaochun Ren, Dawei Yin.&lt;/em&gt; AAAI 2019. &lt;a href="https://www.researchgate.net/profile/Xiang_Li238/publication/332606853_Spectral_Clustering_in_Heterogeneous_Information_Networks/links/5cc035e892851c8d2200aa29/Spectral-Clustering-in-Heterogeneous-Information-Networks.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-view Clustering with Graph Embedding for Connectome Analysis.&lt;/strong&gt;
&lt;em&gt;Guixiang Ma, Lifang He, Chun-Ta Lu, Weixiang Shao, Philip S Yu, Alex D Leow, Ann B Ragin.&lt;/em&gt; CIKM 2017. &lt;a href="https://www.cs.uic.edu/~clu/doc/cikm17_mcge.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Graph Embedding for Ensemble Clustering.&lt;/strong&gt;
&lt;em&gt;Zhiqiang Tao, Hongfu Liu, Jun Li, Zhaowen Wang, Yun Fu.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0494.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Variational Graph Embedding and Clustering with Laplacian Eigenmaps.&lt;/strong&gt;
&lt;em&gt;Zitai Chen, Chuan Chen, Zong Zhang, Zibin Zheng, Qingsong Zou.&lt;/em&gt; IJCAI 2019. &lt;a href="https://www.ijcai.org/proceedings/2019/0297.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-community-detection" class="anchor" aria-hidden="true" href="#community-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Community Detection&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incorporating Network Embedding into Markov Random Field for Better Community Detection.&lt;/strong&gt;
&lt;em&gt;Di Jin, Xinxin You, Weihao Li, Dongxiao He, Peng Cui, Francoise Fogelman-Soulie, Tanmoy Chakraborty.&lt;/em&gt; AAAI 2019. &lt;a href="http://pengcui.thumedialab.com/papers/NE-MRF.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Unified Framework for Community Detection and Network Representation Learning.&lt;/strong&gt;
&lt;em&gt;Cunchao Tu, Xiangkai Zeng, Hao Wang, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun, Bo Zhang, Leyu Lin.&lt;/em&gt; TKDE 2018. &lt;a href="https://ieeexplore.ieee.org/abstract/document/8403293/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COSINE: Community-Preserving Social Network Embedding from Information Diffusion Cascades.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang, Tianshu Lyu, Yan Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://pdfs.semanticscholar.org/fec8/24c51b59063ba92b66bb7404010954ced5ac.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-facet Network Embedding: Beyond the General Solution of Detection and Representation.&lt;/strong&gt;
&lt;em&gt;Liang Yang, Xiaochun Cao, Yuanfang Guo.&lt;/em&gt; AAAI 2018. &lt;a href="https://yangliang.github.io/pdf/aaai18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Community Detection in Attributed Graphs: An Embedding Approach.&lt;/strong&gt;
&lt;em&gt;Ye Li, Chaofeng Sha, Xin Huang, Yanchun Zhang.&lt;/em&gt; AAAI 2018. &lt;a href="https://www.comp.hkbu.edu.hk/~xinhuang/publications/pdfs/AAAI18.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preserving Proximity and Global Ranking for Node Embedding.&lt;/strong&gt;
&lt;em&gt;Yi-An Lai, Chin-Chi Hsu, Wenhao Chen, Mi-Yen Yeh, Shou-De Lin.&lt;/em&gt; NIPS 2017. &lt;a href="https://pdfs.semanticscholar.org/b692/c82115889115ef3e63fb7e6b23c8eb9c85b3.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Community Embedding with Community Detection and Node Embedding on Graphs.&lt;/strong&gt;
&lt;em&gt;Sandro Cavallari, Vincent W. Zheng, Hongyun Cai, Kevin ChenChuan Chang, Erik Cambria.&lt;/em&gt; CIKM 2017. &lt;a href="https://sentic.net/community-embedding.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-recommendation" class="anchor" aria-hidden="true" href="#recommendation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Recommendation&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Convolutional Neural Networks for Web-Scale Recommender Systems.&lt;/strong&gt;
&lt;em&gt;Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec.&lt;/em&gt; KDD 2018. &lt;a href="https://arxiv.org/pdf/1806.01973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Is a Single Vector Enough? Exploring Node Polysemy for Network Embedding.&lt;/strong&gt;
&lt;em&gt;Ninghao Liu,Qiaoyu Tan,Yuening Li,Hongxia Yang,Jingren Zhou,Xia Hu.&lt;/em&gt; KDD 2019. &lt;a href="https://arxiv.org/pdf/1905.10668.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender System.&lt;/strong&gt;
&lt;em&gt;Qitian Wu,Hengrui Zhang,Xiaofeng Gao,Peng He,Paul Weng,Han Gao,Guihai Chen.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1903.10433.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-other-applications" class="anchor" aria-hidden="true" href="#other-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#content"&gt;Other Applications&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cash-out User Detection based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism. (Finance)&lt;/strong&gt;
&lt;em&gt;Binbin Hu, Zhiqiang Zhang, Chuan Shi, Jun Zhou, Xiaolong Li, Yuan Qi.&lt;/em&gt; AAAI 2019. &lt;a href="http://shichuan.org/doc/64.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building Causal Graphs from Medical Literature and Electronic Medical Records. (Medicine)&lt;/strong&gt;
&lt;em&gt;Galia Nordon, Gideon Koren, Varda Shalev, Benny Kimelfeld, Uri Shalit, Kira Radinsky.&lt;/em&gt; AAAI 2019. &lt;a href="http://www.kiraradinsky.com/files/aaai-building-causal.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Attacks on Node Embeddings via Graph Poisoning. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Stephan Günnemann.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bojchevski19a/bojchevski19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compositional Fairness Constraints for Graph Embeddings. (Adversarial)&lt;/strong&gt;
&lt;em&gt;Avishek Bose, William Hamilton.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/bose19a/bose19a.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gromov-Wasserstein Learning for Graph Matching and Node Embedding. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Hongteng Xu, Dixin Luo, Hongyuan Zha, Lawrence Carin Duke.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/xu19b/xu19b.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects. (Graph Matching)&lt;/strong&gt;
&lt;em&gt;Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli.&lt;/em&gt; ICML 2019. &lt;a href="http://proceedings.mlr.press/v97/li19d/li19d.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MolGAN: An implicit generative model for small molecular graphs. (Molecular Generation)&lt;/strong&gt;
&lt;em&gt;Nicola De Cao, Thomas Kipf.&lt;/em&gt; ICML Workshop 2018. &lt;a href="https://arxiv.org/pdf/1805.11973.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational recurrent neural networks. (Relational Reasoning)&lt;/strong&gt;
&lt;em&gt;Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap.&lt;/em&gt; NeurIPS 2018. &lt;a href="https://arxiv.org/pdf/1806.01822.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constructing Narrative Event Evolutionary Graph for Script Event Prediction. (Script Event Prediction)&lt;/strong&gt;
&lt;em&gt;Zhongyang Li, Xiao Ding, Ting Liu.&lt;/em&gt; IJCAI 2018. &lt;a href="https://arxiv.org/abs/1805.05081" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/eecrazy/ConstructingNEEG_IJCAI_2018"&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A Network-embedding Based Method for Author Disambiguation. (Author Disambiguation)&lt;/strong&gt;
&lt;em&gt;Jun Xu, Siqi Shen, Dongsheng Li, Yongquan Fu.&lt;/em&gt; CIKM 2018. &lt;a href="https://dl.acm.org/citation.cfm?id=3269272" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deep Graph Embedding for Ranking Optimization in E-commerce.(E-commerce)&lt;/strong&gt;
&lt;em&gt;Chen Chu, Zhao Li, Beibei Xin, Fengchao Peng, Chuanren Liu, Remo Rohs, Qiong Luo, Jingren Zhou.&lt;/em&gt; CIKM 2018. &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6330176/" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning Network-to-Network Model for Content-rich Network Embedding.&lt;/strong&gt;
&lt;em&gt;Zhicheng He,Jie Liu,Na Li,Yalou Huang.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330924/p1037-he.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unifying Inter-region Autocorrelation and Intra-region Structures for Spatial Embedding via Collective Adversarial Learning.&lt;/strong&gt;
&lt;em&gt;Yunchao Zhang,Pengyang Wang,Xiaolin Li,Yu Zheng,Yanjie Fu.&lt;/em&gt; KDD 2019. &lt;a href="http://delivery.acm.org/10.1145/3340000/3330972/p1700-zhang.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural IR Meets Graph Embedding: A Ranking Model for Product Search.&lt;/strong&gt;
&lt;em&gt;Yuan Zhang,Dong Wang,Yan Zhang.&lt;/em&gt; WWW 2019. &lt;a href="https://arxiv.org/pdf/1901.08286.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Network Embedding for Multi-Network Alignment.&lt;/strong&gt;
&lt;em&gt;Xiaokai Chu,Xinxin Fan,Di Yao,Zhihua Zhu,Jianhui Huang,Jingping Bi.&lt;/em&gt; WWW 2019. &lt;a href="https://sci-hub.tw/10.1145/3308558.3313499#" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Name Disambiguation in Anonymized Graphs using Network Embedding. (Name Disambiguation)&lt;/strong&gt;
&lt;em&gt;Baichuan Zhang, Mohammad Al Hasan.&lt;/em&gt; CIKM 2017. &lt;a href="https://arxiv.org/pdf/1702.02287.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NetGAN: Generating Graphs via Random Walks. (Graph Generation)&lt;/strong&gt;
&lt;em&gt;Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zügner, Stephan Günnemann.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1803.00816" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Graph Networks as Learnable Physics Engines for Inference and Control. (Physics)&lt;/strong&gt;
&lt;em&gt;Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia.&lt;/em&gt; ICML 2018. &lt;a href="https://arxiv.org/pdf/1806.01242.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relational inductive bias for physical construction in humans and machines. (Human physical reasoning)&lt;/strong&gt;
&lt;em&gt;Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R. McKee, Joshua B. Tenenbaum, Peter W. Battaglia.&lt;/em&gt; CogSci 2018. &lt;a href="https://arxiv.org/pdf/1806.01203.pdf" rel="nofollow"&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>thunlp</author><guid isPermaLink="false">https://github.com/thunlp/NRLPapers</guid><pubDate>Sat, 02 Nov 2019 00:04:00 GMT</pubDate></item><item><title>exacity/deeplearningbook-chinese #5 in TeX, This week</title><link>https://github.com/exacity/deeplearningbook-chinese</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Book Chinese Translation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-中文翻译" class="anchor" aria-hidden="true" href="#deep-learning-中文翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning 中文翻译&lt;/h1&gt;
&lt;p&gt;在众多网友的帮助和校对下，中文版终于出版了。尽管还有很多问题，但至少90%的内容是可读的，并且是准确的。
我们尽可能地保留了原书&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;Deep Learning&lt;/a&gt;中的意思并保留原书的语句。&lt;/p&gt;
&lt;p&gt;然而我们水平有限，我们无法消除众多读者的方差。我们仍需要大家的建议和帮助，一起减小翻译的偏差。&lt;/p&gt;
&lt;p&gt;大家所要做的就是阅读，然后汇总你的建议，提issue（最好不要一个一个地提）。如果你确定你的建议不需要商量，可以直接发起PR。&lt;/p&gt;
&lt;p&gt;对应的翻译者：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第1、4、7、10、14、20章及第12.4、12.5节由 @swordyork 负责&lt;/li&gt;
&lt;li&gt;第2、5、8、11、15、18章由 @liber145 负责&lt;/li&gt;
&lt;li&gt;第3、6、9章由 @KevinLee1110 负责&lt;/li&gt;
&lt;li&gt;第13、16、17、19章及第12.1至12.3节由 @futianfan 负责&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-面向的读者" class="anchor" aria-hidden="true" href="#面向的读者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向的读者&lt;/h2&gt;
&lt;p&gt;请直接下载&lt;a href="https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf"&gt;PDF&lt;/a&gt;阅读。
不打算提供EPUB等格式，如有需要请自行修改。&lt;/p&gt;
&lt;p&gt;这一版准确性已经有所提高，读者可以以中文版为主、英文版为辅来阅读学习，但我们仍建议研究者阅读&lt;a href="http://www.deeplearningbook.org/" rel="nofollow"&gt;原版&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-出版及开源原因" class="anchor" aria-hidden="true" href="#出版及开源原因"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;出版及开源原因&lt;/h2&gt;
&lt;p&gt;本书由人民邮电出版社出版，如果你觉得中文版PDF对你有所帮助，希望你能支持下纸质正版书籍。
如果你觉得中文版不行，希望你能多提建议。非常感谢各位！
纸质版也会进一步更新，需要大家更多的建议和意见，一起完善中文版。&lt;/p&gt;
&lt;p&gt;纸质版目前在人民邮电出版社的异步社区出售，见&lt;a href="http://www.epubit.com.cn/book/details/4278" rel="nofollow"&gt;地址&lt;/a&gt;。
价格不低，但看了样本之后，我们认为物有所值。
注意，我们不会通过媒体进行宣传，希望大家先看电子版内容，再判断是否购买纸质版。&lt;/p&gt;
&lt;p&gt;以下是开源的具体原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不是文学工作者，不专职翻译。单靠我们，无法给出今天的翻译，众多网友都给我们提出了宝贵的建议，因此开源帮了很大的忙。出版社会给我们稿费（我们也不知道多少，可能2万左右），我们也不好意思自己用，商量之后觉得捐出是最合适的，以所有贡献过的网友的名义（我们把稿费捐给了杉树公益，用于4名贵州高中生三年的生活费，见&lt;a href="https://github.com/exacity/deeplearningbook-chinese/blob/master/donation.pdf"&gt;捐赠情况&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;PDF电子版对于技术类书籍来说是很重要的，随时需要查询，拿着纸质版到处走显然不合适。国外很多技术书籍都有对应的电子版（虽然不一定是正版），而国内的几乎没有。个人认为这是出版社或者作者认为国民素质还没有高到主动为知识付费的境界，所以不愿意"泄露"电子版。时代在进步，我们也需要改变。特别是翻译作品普遍质量不高的情况下，要敢为天下先。&lt;/li&gt;
&lt;li&gt;深度学习发展太快，日新月异，所以我们希望大家更早地学到相关的知识。我觉得原作者开放PDF电子版也有类似的考虑，也就是先阅读后付费。我们认为中国人口素质已经足够高，懂得为知识付费。当然这不是付给我们的，是付给出版社的，出版社再付给原作者。我们不希望中文版的销量因PDF电子版的存在而下滑。出版社只有值回了版权才能在以后引进更多的优秀书籍。我们这个开源翻译先例也不会成为一个反面案例，以后才会有更多的PDF电子版。&lt;/li&gt;
&lt;li&gt;开源也涉及版权问题，出于版权原因，我们不再更新此初版PDF文件，请大家以最终的纸质版为准。（但源码会一直更新）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;p&gt;我们有3个类别的校对人员。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负责人也就是对应的翻译者。&lt;/li&gt;
&lt;li&gt;简单阅读，对语句不通顺或难以理解的地方提出修改意见。&lt;/li&gt;
&lt;li&gt;中英对比，进行中英对应阅读，排除少翻错翻的情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有校对建议都保存在各章的&lt;code&gt;annotations.txt&lt;/code&gt;文件中。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;章节&lt;/th&gt;
&lt;th&gt;负责人&lt;/th&gt;
&lt;th&gt;简单阅读&lt;/th&gt;
&lt;th&gt;中英对比&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/" rel="nofollow"&gt;第一章 前言&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt;
&lt;td&gt;@linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/" rel="nofollow"&gt;第二章 线性代数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt;
&lt;td&gt;@badpoem&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/" rel="nofollow"&gt;第三章 概率与信息论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@SiriusXDJ&lt;/td&gt;
&lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/" rel="nofollow"&gt;第四章 数值计算&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;@zhangyafeikimi&lt;/td&gt;
&lt;td&gt;@hengqujushi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/" rel="nofollow"&gt;第五章 机器学习基础&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt;
&lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/" rel="nofollow"&gt;第六章 深度前馈网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/" rel="nofollow"&gt;第七章 深度学习中的正则化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@NBZCC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/" rel="nofollow"&gt;第八章 深度模型中的优化&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt;
&lt;td&gt;@huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/" rel="nofollow"&gt;第九章 卷积网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@KevinLee1110&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt;
&lt;td&gt;@zhiding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/" rel="nofollow"&gt;第十章 序列建模：循环和递归网络&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;lc&lt;/td&gt;
&lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/" rel="nofollow"&gt;第十一章 实践方法论&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/" rel="nofollow"&gt;第十二章 应用&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork, @futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@corenel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/" rel="nofollow"&gt;第十三章 线性因子模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;@cloudygoose&lt;/td&gt;
&lt;td&gt;@ZhiweiYang&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/" rel="nofollow"&gt;第十四章 自编码器&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/" rel="nofollow"&gt;第十五章 表示学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;@cnscottzheng&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/" rel="nofollow"&gt;第十六章 深度学习中的结构化概率模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/" rel="nofollow"&gt;第十七章 蒙特卡罗方法&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/" rel="nofollow"&gt;第十八章 面对配分函数&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@liber145&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@tankeco&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/" rel="nofollow"&gt;第十九章 近似推断&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@futianfan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/" rel="nofollow"&gt;第二十章 深度生成模型&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;@swordyork&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;参考文献&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;@pkuwwt&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们会在纸质版正式出版的时候，在书中致谢，正式感谢各位作出贡献的同学！&lt;/p&gt;
&lt;p&gt;还有很多同学提出了不少建议，我们都列在此处。&lt;/p&gt;
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz
@weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc
@bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp
@endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu  @zhangyafeikimi @showgood163 @gump88
@kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY
FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC
@zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool
@xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt;
&lt;p&gt;如有遗漏，请务必通知我们，可以发邮件至&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;。
这是我们必须要感谢的，所以不要不好意思。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-todo" class="anchor" aria-hidden="true" href="#todo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TODO&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;排版&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-注意" class="anchor" aria-hidden="true" href="#注意"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;注意&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;各种问题或者建议可以提issue，建议使用中文。&lt;/li&gt;
&lt;li&gt;由于版权问题，我们不能将图片和bib上传，请见谅。&lt;/li&gt;
&lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt;
&lt;li&gt;可用于学习研究目的，不得用于任何商业行为。谢谢！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-markdown格式" class="anchor" aria-hidden="true" href="#markdown格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Markdown格式&lt;/h2&gt;
&lt;p&gt;这种格式确实比较重要，方便查阅，也方便索引。初步转换后，生成网页，具体见&lt;a href="https://exacity.github.io/deeplearningbook-chinese" rel="nofollow"&gt;deeplearningbook-chinese&lt;/a&gt;。
注意，这种转换没有把图放进去，也不会放图。目前使用单个&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;，基于latex文件转换，以后可能会更改但原则是不直接修改&lt;a href="docs/_posts"&gt;md文件&lt;/a&gt;。
需要的同学可以自行修改&lt;a href="scripts/convert2md.sh"&gt;脚本&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-html格式" class="anchor" aria-hidden="true" href="#html格式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HTML格式&lt;/h2&gt;
&lt;p&gt;读者可以使用&lt;a href="https://github.com/coolwanglu/pdf2htmlEX"&gt;pdf2htmlEX&lt;/a&gt;进行转换，直接将PDF转换为HTML。&lt;/p&gt;
&lt;p&gt;Updating.....&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>exacity</author><guid isPermaLink="false">https://github.com/exacity/deeplearningbook-chinese</guid><pubDate>Sat, 02 Nov 2019 00:05:00 GMT</pubDate></item><item><title>wzpan/BeamerStyleSlides #6 in TeX, This week</title><link>https://github.com/wzpan/BeamerStyleSlides</link><description>&lt;p&gt;&lt;i&gt;🌈Beamer风格的幻灯片模板集。包含了PowerPoint和Keynote两套格式。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-beamerstyleslides" class="anchor" aria-hidden="true" href="#beamerstyleslides"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BeamerStyleSlides&lt;/h1&gt;
&lt;p&gt;Beamer风格的幻灯片模板集。包含了PowerPoint和Keynote两套格式。尤其适用于晋升述职、技术分享和学术汇报。&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a id="" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/wzpan/BeamerStyleSlides/master/preview/tile.png"&gt;&lt;img width="100%" src="https://raw.githubusercontent.com/wzpan/BeamerStyleSlides/master/preview/tile.png" alt="teaser" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#beamerstyleslides"&gt;BeamerStyleSlides&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%E5%85%8B%E9%9A%86%E6%96%B9%E6%B3%95"&gt;克隆方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7"&gt;使用技巧&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E4%B8%BB%E9%A2%98"&gt;主题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E7%AC%AC%E4%B8%89%E6%96%B9%E4%B8%BB%E9%A2%98"&gt;第三方主题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benchmark"&gt;Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%A6%82%E4%BD%95%E8%B4%A1%E7%8C%AE"&gt;如何贡献&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E8%87%B4%E8%B0%A2"&gt;致谢&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;a id="user-content-克隆方法" class="anchor" aria-hidden="true" href="#克隆方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;克隆方法&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;安装 git-lfs。为了节省仓库大小，项目使用 git-lfs 来对 *.key 和 *.pptx 进行版本控制。所以，在克隆前，要求先 &lt;a href="https://git-lfs.github.com/"&gt;安装好 git-lfs&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;安装完成后再执行 git clone 操作：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/wzpan/BeamerStyleSlides.git&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果仅仅只是为了使用，也可以在 Github 中直接点击喜欢的模板，下载单个文件（View raw）。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-使用技巧" class="anchor" aria-hidden="true" href="#使用技巧"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用技巧&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;首先可以在 &lt;a href="https://hartwork.org/beamer-theme-matrix/" rel="nofollow"&gt;beamer-theme-matrix&lt;/a&gt; 中找找看有没有心仪的样式；&lt;/li&gt;
&lt;li&gt;在上面的主题列表中看看对应的主题是否已经完成了复刻。&lt;/li&gt;
&lt;li&gt;如果有，可以点击查看预览图，看看效果是否满意。&lt;/li&gt;
&lt;li&gt;一些底部带有包含作者信息（author）、标题（title）字段的主题，可以通过编辑母板修改对应字段的值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-主题" class="anchor" aria-hidden="true" href="#主题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;主题&lt;/h2&gt;
&lt;p&gt;所有主题的风格尽力与 Beamer 所提供的风格保持一致。可以在 &lt;a href="https://hartwork.org/beamer-theme-matrix/" rel="nofollow"&gt;beamer-theme-matrix&lt;/a&gt; 中查看对应的主题样式。&lt;/p&gt;
&lt;p&gt;不过，我做了一些取舍：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;去掉了深色背景主题。个人认为深色背景主题在处理白底的图片时很不美观，所以我没有制作深色背景的 slides 的动力。当然，我愿意接受深色背景主题的pull request。&lt;/li&gt;
&lt;li&gt;PowerPoint 和 Keynote 并没有提供类似 Beamer 那样酷炫的导航栏功能，所以带导航栏的主题都不会进行复刻。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;目前完成的主题如下表所示。其中，单元格内容不为 &lt;code&gt;TBD&lt;/code&gt; 的主题说明已经完成了复刻，为 &lt;code&gt;TBD&lt;/code&gt; 表示正在复刻中（&lt;code&gt;To Be Done&lt;/code&gt;）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;collection&lt;/th&gt;
&lt;th&gt;default&lt;/th&gt;
&lt;th&gt;beaver&lt;/th&gt;
&lt;th&gt;crane&lt;/th&gt;
&lt;th&gt;dolphin&lt;/th&gt;
&lt;th&gt;dove&lt;/th&gt;
&lt;th&gt;lily&lt;/th&gt;
&lt;th&gt;orchid&lt;/th&gt;
&lt;th&gt;rose&lt;/th&gt;
&lt;th&gt;seagull&lt;/th&gt;
&lt;th&gt;seahorse&lt;/th&gt;
&lt;th&gt;whale&lt;/th&gt;
&lt;th&gt;wolverine&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-default.jpg"&gt;default-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-beaver.jpg"&gt;default-beaver&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-crane.jpg"&gt;default-crane&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-dolphin.jpg"&gt;default-dolphin&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-dove.jpg"&gt;default-dove&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-lily.jpg"&gt;default-lily&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-orchid.jpg"&gt;default-orchid&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-rose.jpg"&gt;default-rose&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-seagull.jpg"&gt;default-seagull&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-seahorse.jpg"&gt;default-seahorse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-whale.jpg"&gt;default-whale&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/default-wolverine.jpg"&gt;default-wolverine&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AnnArbor&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-default.jpg"&gt;AnnArbor-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-beaver.jpg"&gt;AnnArbor-beaver&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-crane.jpg"&gt;AnnArbor-crane&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-dolphin.jpg"&gt;AnnArbor-dolphin&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-dove.jpg"&gt;AnnArbor-dove&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-lily.jpg"&gt;AnnArbor-lily&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-orchid.jpg"&gt;AnnArbor-orchid&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-rose.jpg"&gt;annArbor-rose&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-seagull.jpg"&gt;AnnArbor-seagull&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-seahorse.jpg"&gt;AnnArbor-seahorse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-whale.jpg"&gt;AnnArbor-whale&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/AnnArbor-wolverine.jpg"&gt;AnnArbor-wolverine&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Antibes&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-default.jpg"&gt;Antibes-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-beaver.jpg"&gt;Antibes-beaver&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-crane.jpg"&gt;Antibes-crane&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-dolphin.jpg"&gt;Antibes-dolphin&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-dove.jpg"&gt;Antibes-dove&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-lily.jpg"&gt;Antibes-lily&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-orchid.jpg"&gt;Antibes-orchid&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Antibes-rose.jpg"&gt;Antibes-rose&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Berlin&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Berlin-default.jpg"&gt;Berlin-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Boadilla&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Boaddilla-default.jpg"&gt;Boadilla-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CambridgeUS&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/CambridgeUS-default.jpg"&gt;CambridgeUS-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Copenhagen&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Copenhagen-default.jpg"&gt;Copenhagen-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Darmstadt&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Darmstadt-default.jpg"&gt;darmstadt-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Darmstadt-beaver.jpg"&gt;darmstadt-beaver&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Darmstadt-crane.jpg"&gt;darmstadt-crane&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Darmstadt-seahorse.jpg"&gt;darmstadt-seahorse&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dresden&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Dresden-default.jpg"&gt;Dresden-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Frankfurt&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Frankfurt-default.jpg"&gt;Frankfurt-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ilmenau&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Ilmenau-default.jpg"&gt;Ilmenau-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;JuanLesPins&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/JuanLesPins-default.jpg"&gt;JuanLesPins-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Luebeck&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Luebeck-default.jpg"&gt;Luebeck-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Madrid&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Madrid-default.jpg"&gt;Madrid-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Malmoe&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Malmoe-default.jpg"&gt;Malmoe-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Montpellier&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Montpellier-default.jpg"&gt;Montpellier-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pittsburgh&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Pittsburgh-default.jpg"&gt;Pittsburgh-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rochester&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Rochester-default.jpg"&gt;Rochester-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Singapore&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Singapore-default.jpg"&gt;singapore-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Szegeb&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Szegeb-default.jpg"&gt;Szegeb-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Warsaw&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Warsaw-default.jpg"&gt;Warsaw-default&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;td&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-第三方主题" class="anchor" aria-hidden="true" href="#第三方主题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三方主题&lt;/h2&gt;
&lt;p&gt;请按顺序在下面添加你贡献的主题（不要插队）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主题名&lt;/th&gt;
&lt;th&gt;贡献者&lt;/th&gt;
&lt;th&gt;预览&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;fb-tao&lt;/td&gt;
&lt;td&gt;&lt;a href="http://github.com/wzpan/"&gt;wzpan&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/blob/master/preview/Contrib-fb-tao.jpg"&gt;Contrib-fb-tao&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-benchmark" class="anchor" aria-hidden="true" href="#benchmark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/wzpan/BeamerStyleSlides/tree/master/benchmark"&gt;benchmark&lt;/a&gt; 目录中包含了一份 LaTeX + Beamer 制作的 slides ，作为整套模板制作的参照。&lt;/p&gt;
&lt;p&gt;要注意的是模板并没有做到 100% 遵循原始的效果，毕竟 PowerPoint、Keynote 和 Beamer 各自的支持的能力有差异。&lt;/p&gt;
&lt;p&gt;我更希望做到的是在保留 Beamer 原有的风格的基础上因地制宜地做些调整。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-如何贡献" class="anchor" aria-hidden="true" href="#如何贡献"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;如何贡献&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;欢迎 fork 这个项目，并通过 pull request 的形式贡献你的主题！
&lt;ol&gt;
&lt;li&gt;第三方模板请分别保存到 Keynote / PowerPoint 目录里的 Contrib 目录中；&lt;/li&gt;
&lt;li&gt;比较省事的制作方式是先制作一个 PowerPoint 的模板，然后在 Keynote 里导入 PowerPoint 模板再另存为 Keynote 模板；&lt;/li&gt;
&lt;li&gt;完成后别忘了在 &lt;a href="#%E7%AC%AC%E4%B8%89%E6%96%B9%E4%B8%BB%E9%A2%98"&gt;第三方主题&lt;/a&gt; 里添加你的大作。&lt;/li&gt;
&lt;li&gt;主题的存放必须遵循按照现有的目录形式：&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;[-] ~/Documents/projects/BeamerStyleSlides/
 |-[-] Keynote
 |  |-[-] Contrib
 |  |  |-  mycustom.key  # 你的主题名
 |  |  ‘-  ...
 |  ‘-  ...
 |-[-] PowerPoint
 |  |-[-] Contrib
 |  |  |-  mycustom.pptx # 你的主题名
 |  |  ‘-  ...
 |  ‘-  ...
 |-[-] preview
 |  |-  ...
 |  |-  Contrib-mycustom-cover.jpg  # 你的主题封面图
 |  |-  Contrib-mycustom.jpg        # 你的主题预览图
 |  ‘-  ...
 ‘-  README.md  # 完成后编辑这个README，在第三方主题中插入你的主题说明。
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;如果喜欢这个项目，不妨给它加一星，或给我打赏：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;支付宝&lt;/th&gt;
&lt;th&gt;微信支付&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/425bcf255f29f94ae842ef056e1f893b917facda/687474703a2f2f68616861636b2e636f6d2f696d616765732f6d6973632f616c697061792e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/425bcf255f29f94ae842ef056e1f893b917facda/687474703a2f2f68616861636b2e636f6d2f696d616765732f6d6973632f616c697061792e706e67" height="248px" width="164px" title="支付宝" data-canonical-src="http://hahack.com/images/misc/alipay.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/8d4d2336b4d0cbaa4b51cb716290ec8ae0fadc41/687474703a2f2f68616861636b2e636f6d2f696d616765732f6d6973632f7765636861747061792e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/8d4d2336b4d0cbaa4b51cb716290ec8ae0fadc41/687474703a2f2f68616861636b2e636f6d2f696d616765732f6d6973632f7765636861747061792e6a706567" height="248px" width="164px" title="微信支付" data-canonical-src="http://hahack.com/images/misc/wechatpay.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-致谢" class="anchor" aria-hidden="true" href="#致谢"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;致谢&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;整套模板主题主要参考自 &lt;a href="https://ctan.org/pkg/beamer/" rel="nofollow"&gt;LaTeX Beamer&lt;/a&gt; 。&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wzpan</author><guid isPermaLink="false">https://github.com/wzpan/BeamerStyleSlides</guid><pubDate>Sat, 02 Nov 2019 00:06:00 GMT</pubDate></item><item><title>jikexueyuanwiki/tensorflow-zh #7 in TeX, This week</title><link>https://github.com/jikexueyuanwiki/tensorflow-zh</link><description>&lt;p&gt;&lt;i&gt;谷歌全新开源人工智能系统TensorFlow官方文档中文版&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-官方文档中文版" class="anchor" aria-hidden="true" href="#tensorflow-官方文档中文版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 官方文档中文版&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/TensorFlow.jpg"&gt;&lt;img src="SOURCE/images/TensorFlow.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-你正在阅读的项目可能会比-android-系统更加深远地影响着世界" class="anchor" aria-hidden="true" href="#你正在阅读的项目可能会比-android-系统更加深远地影响着世界"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;你正在阅读的项目可能会比 Android 系统更加深远地影响着世界！&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-缘起" class="anchor" aria-hidden="true" href="#缘起"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;缘起&lt;/h2&gt;
&lt;p&gt;2015年11月9日，Google发布人工智能系统TensorFlow并宣布开源，同日，极客学院组织在线TensorFlow中文文档翻译。&lt;/p&gt;
&lt;p&gt;机器学习作为人工智能的一种类型，可以让软件根据大量的数据来对未来的情况进行阐述或预判。如今，领先的科技巨头无不在机器学习下予以极大投入。Facebook、苹果、微软，甚至国内的百度。Google 自然也在其中。「TensorFlow」是 Google 多年以来内部的机器学习系统。如今，Google 正在将此系统成为开源系统，并将此系统的参数公布给业界工程师、学者和拥有大量编程能力的技术人员，这意味着什么呢？&lt;/p&gt;
&lt;p&gt;打个不太恰当的比喻，如今 Google 对待 TensorFlow 系统，有点类似于该公司对待旗下移动操作系统 Android。如果更多的数据科学家开始使用 Google 的系统来从事机器学习方面的研究，那么这将有利于 Google 对日益发展的机器学习行业拥有更多的主导权。&lt;/p&gt;
&lt;p&gt;为了让国内的技术人员在最短的时间内迅速掌握这一世界领先的 AI 系统，极客学院 Wiki 团队发起对 TensorFlow 官方文档的中文协同翻译，一周之内，全部翻译认领完成，一个月后，全部30章节翻译校对完成，上线极客学院Wiki平台并提供下载。&lt;/p&gt;
&lt;p&gt;Google TensorFlow项目负责人Jeff Dean为该中文翻译项目回信称："&lt;em&gt;看到能够将TensorFlow翻译成中文我非常激动，我们将TensorFlow开源的主要原因之一是为了让全世界的人们能够从机器学习与人工智能中获益，类似这样的协作翻译能够让更多的人更容易地接触到TensorFlow项目，很期待接下来该项目在全球范围内的应用!&lt;/em&gt;"&lt;/p&gt;
&lt;p&gt;Jeff回信原文：&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="SOURCE/images/jeff.png"&gt;&lt;img src="SOURCE/images/jeff.png" alt="jeff" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;再次衷心感谢每一位为该翻译项目做出贡献的同学，我们会持续关注TensorFlow、AI领域以及其它最新技术的发展、持续维护该协作翻译、持续提供更多更优质的内容，为广大IT学习者们服务！&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-内容来源" class="anchor" aria-hidden="true" href="#内容来源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容来源&lt;/h2&gt;
&lt;p&gt;英文官方网站：&lt;br&gt;
&lt;a href="http://tensorflow.org/" rel="nofollow"&gt;http://tensorflow.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方GitHub仓库：&lt;br&gt;
&lt;a href="https://github.com/tensorflow/tensorflow"&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文版 GitHub 仓库：&lt;br&gt;
&lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-参与者按认领章节排序" class="anchor" aria-hidden="true" href="#参与者按认领章节排序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参与者（按认领章节排序）&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-翻译" class="anchor" aria-hidden="true" href="#翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;翻译&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/PFZheng"&gt;@PFZheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/linbojin"&gt;@Tony Jin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenweican"&gt;@chenweican&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bingjin"&gt;@bingjin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/oskycar"&gt;@oskycar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/btpeter"&gt;@btpeter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Warln"&gt;@Warln&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangaicc"&gt;@wangaicc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thylaco1eo"&gt;@thylaco1eo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhangkom"&gt;@zhangkom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/derekshang"&gt;@derekshang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lianghyv"&gt;@lianghyv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nb312"&gt;@nb312&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Jim-Zenn"&gt;@Jim-Zenn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andyiac"&gt;@andyiac&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/TerenceCooper"&gt;@Terence Cooper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/leege100"&gt;@leege100&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-校对" class="anchor" aria-hidden="true" href="#校对"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;校对&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sstruct"&gt;@yangtze&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericxk"&gt;@ericxk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/LichAmnesia"&gt;@LichAmnesia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhyhooo"&gt;@zhyhooo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/waiwaizheng"&gt;@waiwaizheng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/WangHong-yang"&gt;@HongyangWang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorfly"&gt;@tensorfly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lonlonago"&gt;@lonlonago&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jishaoming"&gt;@jishaoming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lucky521"&gt;@lucky521&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/allensummer"&gt;@allensummer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/volvet"&gt;@volvet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ZHNathanielLee"&gt;@ZHNathanielLee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PengFoo"&gt;@pengfoo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/qiaohaijun"&gt;@qiaohaijun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SeikaScarlet"&gt;@Seika&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-进度记录" class="anchor" aria-hidden="true" href="#进度记录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;进度记录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2015-11-10, 谷歌发布全新人工智能系统TensorFlow并宣布开源, 极客学院Wiki启动协同翻译，创建 GitHub 仓库，制定协同规范&lt;/li&gt;
&lt;li&gt;2015-11-18, 所有章节认领完毕，翻译完成18章，校对认领7章，Star数361，fork数100，协同翻译QQ群及技术交流群的TF爱好者将近300人，GitHub搜索TensorFlow排名第二&lt;/li&gt;
&lt;li&gt;2015-12-10, Star数超过500&lt;/li&gt;
&lt;li&gt;2015-12-15, 项目正式上线&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-花絮" class="anchor" aria-hidden="true" href="#花絮"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;花絮&lt;/h2&gt;
&lt;p&gt;在组织翻译的过程中，有些事情令人印象深刻，记录下来，希望以后来学习文档的同学能够明了到手中这份文档的由来：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参加翻译的有学生，也有老师；有专门研究AI/ML的，也有对此感兴趣的；有国内的，也有远在纽约的；有工程技术人员也有博士、专家&lt;/li&gt;
&lt;li&gt;其中一位，&lt;a href="http://www.longmotto.com" rel="nofollow"&gt;恩泽&lt;/a&gt;同学，为了翻译一篇文档，在前一天没有睡觉的情况下坚持翻完，20个小时没有合眼&lt;/li&gt;
&lt;li&gt;还有一位老师，刚从讲台上讲完课，就立即给我们的翻译提修改意见&lt;/li&gt;
&lt;li&gt;很多同学自发的将搭建环境中遇到的问题总结到FAQ里帮助他人&lt;/li&gt;
&lt;li&gt;为了一个翻译细节，经常是来回几次，和其他人讨论完善&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-持续改进" class="anchor" aria-hidden="true" href="#持续改进"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;持续改进&lt;/h2&gt;
&lt;p&gt;这样的一个高技术领域的文档，我们在翻译的过程中，难免会有不完善的地方，希望请大家一起帮助我们持续改进文档的翻译质量，帮助更多的人，方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在GitHub上提Issue或Pull Request，地址为: &lt;a href="https://github.com/jikexueyuanwiki/tensorflow-zh"&gt;https://github.com/jikexueyuanwiki/tensorflow-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;加入TensorFlow技术交流群，与TensorFlower们一起研究交流技术干货--TensorFlow技术交流群：782484288&lt;/li&gt;
&lt;li&gt;对翻译感兴趣？加入协同翻译群：248320884，与翻译大神一道研究TensorFlow的本地化&lt;/li&gt;
&lt;li&gt;给我们写邮件： &lt;a href="mailto:wiki@jikexueyuan.com"&gt;wiki@jikexueyuan.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-感谢支持" class="anchor" aria-hidden="true" href="#感谢支持"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢支持&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://wiki.jikexueyuan.com" rel="nofollow"&gt;极客学院 Wiki&lt;/a&gt; 提供图文教程托管服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-离线版本" class="anchor" aria-hidden="true" href="#离线版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;离线版本&lt;/h2&gt;
&lt;p&gt;目前，离线版本(PDF、ePub)可正常下载、使用&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tex-pdf-修订版" class="anchor" aria-hidden="true" href="#tex-pdf-修订版"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tex-PDF 修订版&lt;/h2&gt;
&lt;p&gt;&lt;a href="tex_pdf"&gt;Tex-PDF 修订版&lt;/a&gt; 目前正在编订中，欢迎加入进来一起修订。您可以在此查看&lt;a href="tex_pdf/tensorflow_manual_cn.pdf"&gt;预览版&lt;/a&gt;目前最新状态。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jikexueyuanwiki</author><guid isPermaLink="false">https://github.com/jikexueyuanwiki/tensorflow-zh</guid><pubDate>Sat, 02 Nov 2019 00:07:00 GMT</pubDate></item><item><title>jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction #8 in TeX, This week</title><link>https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction</link><description>&lt;p&gt;&lt;i&gt;A selection of state-of-the-art research materials on trajectory prediction&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome-interaction-aware-behavior-and-trajectory-prediction" class="anchor" aria-hidden="true" href="#awesome-interaction-aware-behavior-and-trajectory-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome Interaction-aware Behavior and Trajectory Prediction&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/7db523b8b68f5a19da519ca6f7649ced6ce5cbf0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302d6666363962342e737667"&gt;&lt;img src="https://camo.githubusercontent.com/7db523b8b68f5a19da519ca6f7649ced6ce5cbf0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302d6666363962342e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-1.0-ff69b4.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/43ad2024e165bc7030449a7da48a278fbb73ecbe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c617374557064617465642d323031392e31302d6c69676874677265792e737667"&gt;&lt;img src="https://camo.githubusercontent.com/43ad2024e165bc7030449a7da48a278fbb73ecbe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c617374557064617465642d323031392e31302d6c69676874677265792e737667" alt="LastUpdated" data-canonical-src="https://img.shields.io/badge/LastUpdated-2019.10-lightgrey.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c169f833febcd19dc5eae6f3e11fd5eea0d5e261/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f546f7069632d6265686176696f72287472616a6563746f7279292d2d70726564696374696f6e2d79656c6c6f772e7376673f6c6f676f3d676974687562"&gt;&lt;img src="https://camo.githubusercontent.com/c169f833febcd19dc5eae6f3e11fd5eea0d5e261/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f546f7069632d6265686176696f72287472616a6563746f7279292d2d70726564696374696f6e2d79656c6c6f772e7376673f6c6f676f3d676974687562" alt="Topic" data-canonical-src="https://img.shields.io/badge/Topic-behavior(trajectory)--prediction-yellow.svg?logo=github" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="http://hits.dwyl.io/jiachenli94/Interaction-aware-Trajectory-Prediction" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e774f641c9fb064f3315f18f39b9a25f2def6103/687474703a2f2f686974732e6477796c2e696f2f6a69616368656e6c6939342f496e746572616374696f6e2d61776172652d5472616a6563746f72792d50726564696374696f6e2e737667" alt="HitCount" data-canonical-src="http://hits.dwyl.io/jiachenli94/Interaction-aware-Trajectory-Prediction.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a checklist of state-of-the-art research materials (datasets, blogs, papers and public codes) related to trajectory prediction. Wish it could be helpful for both academia and industry. (Still updating)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maintainers&lt;/strong&gt;: &lt;a href="https://jiachenli94.github.io" rel="nofollow"&gt;&lt;strong&gt;Jiachen Li&lt;/strong&gt;&lt;/a&gt;, Hengbo Ma, Jinning Li (University of California, Berkeley)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Emails&lt;/strong&gt;: {jiachen_li, hengbo_ma, jinning_li}@berkeley.edu&lt;/p&gt;
&lt;p&gt;Please feel free to pull request to add new resources or send emails to us for questions, discussion and collaborations.&lt;/p&gt;
&lt;p&gt;Also welcome to check the current research in our &lt;a href="https://msc.berkeley.edu/research/autonomous-vehicle.html" rel="nofollow"&gt;&lt;strong&gt;MSC Lab&lt;/strong&gt;&lt;/a&gt; at UC Berkeley.&lt;/p&gt;
&lt;p&gt;Please read &lt;a href="https://jiachenli94.github.io/Research_Intern_Opportunities_at_UC_Berkeley.pdf" rel="nofollow"&gt;&lt;strong&gt;this&lt;/strong&gt;&lt;/a&gt; if you want to apply for &lt;strong&gt;research intern opportunities&lt;/strong&gt; in our group.&lt;/p&gt;
&lt;p&gt;Please cite our work if you found this useful:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{Jiachen_IROS19,
  title={Conditional Generative Neural System for Probabilistic Trajectory Prediction},
  author={Li, Jiachen and Ma, Hengbo and Tomizuka, Masayoshi},
  booktitle={in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2019},
  organization={IEEE}
}

@inproceedings{Jiachen_ICRA19,
  title={Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning},
  author={Li, Jiachen and Ma, Hengbo and Tomizuka, Masayoshi},
  booktitle={2019 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2019},
  organization={IEEE}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="#datasets"&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#vehicles-and-traffic"&gt;Vehicles and Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pedestrians"&gt;Pedestrians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sport-players"&gt;Sport Players&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#literature-and-codes"&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#survey-papers"&gt;Survey Papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#physics-systems-with-interaction"&gt;Physics Systems with Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#intelligent-vehicles-traffic"&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mobile-robots"&gt;Mobile Robots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pedestrians"&gt;Pedestrians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#vehicle-pedestrians-interaction"&gt;Vehicle-Pedestrians Interaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sport-players"&gt;Sport Players&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benchmark-and-evaluation-metrics"&gt;Benchmark and Evaluation Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#others"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-vehicles-and-traffic" class="anchor" aria-hidden="true" href="#vehicles-and-traffic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vehicles and Traffic&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.interaction-dataset.com/" rel="nofollow"&gt;INTERACTION&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Roundabout / intersection&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/" rel="nofollow"&gt;KITTI&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Highway / rural areas&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.highd-dataset.com/" rel="nofollow"&gt;HighD&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Highway&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm" rel="nofollow"&gt;NGSIM&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Highway&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Tsinghua-Daimler_Cyclist_Detec/tsinghua-daimler_cyclist_detec.html" rel="nofollow"&gt;Cyclists&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Cyclists&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.nuscenes.org/" rel="nofollow"&gt;nuScenes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR / RADAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://bdd-data.berkeley.edu/" rel="nofollow"&gt;BDD100k&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists / people&lt;/td&gt;
&lt;td align="center"&gt;Highway / urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://apolloscape.auto/?source=post_page---------------------------" rel="nofollow"&gt;Apolloscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/udacity/self-driving-car/tree/master/datasets"&gt;Udacity&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.argoverse.org/" rel="nofollow"&gt;Argoverse&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://gamma.umd.edu/researchdirections/autonomousdriving/trafdataset" rel="nofollow"&gt;TRAF&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/buses/cyclists/bikes / people/animals&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-pedestrians" class="anchor" aria-hidden="true" href="#pedestrians"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pedestrians&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data" rel="nofollow"&gt;UCY&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Zara / students&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.vision.ee.ethz.ch/en/datasets/" rel="nofollow"&gt;ETH&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.viratdata.org/" rel="nofollow"&gt;VIRAT&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People / vehicles&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.cvlibs.net/datasets/kitti/" rel="nofollow"&gt;KITTI&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Highway / rural areas&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://irc.atr.jp/crest2010_HRI/ATC_dataset/" rel="nofollow"&gt;ATC&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Shopping center&lt;/td&gt;
&lt;td align="center"&gt;Range sensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html" rel="nofollow"&gt;Daimler&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;From moving vehicle&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.ee.cuhk.edu.hk/~xgwang/grandcentral.html" rel="nofollow"&gt;Central Station&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Inside station&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.html#datasets" rel="nofollow"&gt;Town Center&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban street&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/" rel="nofollow"&gt;Edinburgh&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.cityscapes-dataset.com/login/" rel="nofollow"&gt;Cityscapes&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://www.argoverse.org/" rel="nofollow"&gt;Argoverse&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera / LiDAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Vehicles / cyclists/ people&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="http://trajnet.stanford.edu/" rel="nofollow"&gt;TrajNet&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Urban&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-sport-players" class="anchor" aria-hidden="true" href="#sport-players"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sport Players&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Agents&lt;/th&gt;
&lt;th align="center"&gt;Scenarios&lt;/th&gt;
&lt;th align="center"&gt;Sensors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://datahub.io/collections/football" rel="nofollow"&gt;Football&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;People&lt;/td&gt;
&lt;td align="center"&gt;Football field&lt;/td&gt;
&lt;td align="center"&gt;Camera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-literature-and-codes" class="anchor" aria-hidden="true" href="#literature-and-codes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Literature and Codes&lt;/strong&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-survey-papers" class="anchor" aria-hidden="true" href="#survey-papers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Survey Papers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Human Motion Trajectory Prediction: A Survey, 2019 [&lt;a href="https://arxiv.org/abs/1905.06113" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A literature review on the prediction of pedestrian behavior in urban scenarios, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8569415" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Survey on Vision-Based Path Prediction. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-91131-1_4" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autonomous vehicles that interact with pedestrians: A survey of theory and practice. [&lt;a href="https://arxiv.org/abs/1805.11773" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory data mining: an overview. [&lt;a href="https://dl.acm.org/citation.cfm?id=2743025" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A survey on motion prediction and risk assessment for intelligent vehicles. [&lt;a href="https://robomechjournal.springeropen.com/articles/10.1186/s40648-014-0001-z" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-physics-systems-with-interaction" class="anchor" aria-hidden="true" href="#physics-systems-with-interaction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Physics Systems with Interaction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Factorised Neural Relational Inference for Multi-Interaction Systems, ICML workshop 2019. [&lt;a href="https://arxiv.org/abs/1905.08721v1" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/ekwebb/fNRI"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video, 2019. [&lt;a href="https://arxiv.org/pdf/1905.11169v1.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Neural Relational Inference for Interacting Systems, ICML 2018. [&lt;a href="https://arxiv.org/abs/1802.04687v2" rel="nofollow"&gt;paper&lt;/a&gt;] [&lt;a href="https://github.com/ethanfetaya/NRI"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Unsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks, UAI 2018. [&lt;a href="http://arxiv.org/abs/1807.09244v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Relational inductive biases, deep learning, and graph networks, 2018. [&lt;a href="https://arxiv.org/abs/1806.01261v3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions, ICLR 2018. [&lt;a href="http://arxiv.org/abs/1802.10353v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Graph networks as learnable physics engines for inference and control, ICML 2018. [&lt;a href="http://arxiv.org/abs/1806.01242v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Flexible Neural Representation for Physics Prediction, 2018. [&lt;a href="http://arxiv.org/abs/1806.08047v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple neural network module for relational reasoning, 2017. [&lt;a href="http://arxiv.org/abs/1706.01427v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VAIN: Attentional Multi-agent Predictive Modeling, NIPS 2017. [&lt;a href="https://arxiv.org/pdf/1706.06122.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Visual Interaction Networks, 2017. [&lt;a href="http://arxiv.org/abs/1706.01433v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Compositional Object-Based Approach to Learning Physical Dynamics, ICLR 2017. [&lt;a href="http://arxiv.org/abs/1612.00341v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction Networks for Learning about Objects, Relations and Physics, 2016. [&lt;a href="https://arxiv.org/abs/1612.00222" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/higgsfield/interaction_network_pytorch"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-intelligent-vehicles--traffic" class="anchor" aria-hidden="true" href="#intelligent-vehicles--traffic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Intelligent Vehicles &amp;amp; Traffic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Conditional generative neural system for probabilistic trajectory prediction, IROS 2019. [&lt;a href="https://arxiv.org/abs/1905.01631" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Interaction-aware multi-agent tracking and probabilistic behavior prediction via adversarial learning, ICRA 2019. [&lt;a href="https://arxiv.org/abs/1904.02390" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving, IEEE Trans. Intell. Transport. Systems, 2019. [&lt;a href="https://www.researchgate.net/publication/334560415_Generic_Tracking_and_Probabilistic_Prediction_Framework_and_Its_Application_in_Autonomous_Driving" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Coordination and trajectory prediction for vehicle interactions via bayesian generative modeling, IV 2019. [&lt;a href="https://arxiv.org/abs/1905.00587" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Wasserstein generative learning with kinematic constraints for probabilistic interactive driving behavior prediction, IV 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8813783" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GRIP: Graph-based Interaction-aware Trajectory Prediction, ITSC 2019. [&lt;a href="https://arxiv.org/abs/1907.07792" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;AGen: Adaptable Generative Prediction Networks for Autonomous Driving, IV 2019. [&lt;a href="http://www.cs.cmu.edu/~cliu6/files/iv19-1.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/rohanchandra30/TrackNPred"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks, CVPR 2019. [&lt;a href="https://arxiv.org/pdf/1812.09395.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Argoverse: 3D Tracking and Forecasting With Rich Maps, CVPR 2019 [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Robust Aleatoric Modeling for Future Vehicle Localization, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Hudnell_Robust_Aleatoric_Modeling_for_Future_Vehicle_Localization_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian occupancy prediction for autonomous vehicles, IRC 2019. [paper]&lt;/li&gt;
&lt;li&gt;Context-based path prediction for targets with switching dynamics, 2019.[&lt;a href="https://link.springer.com/article/10.1007/s11263-018-1104-4" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Imitative Models for Flexible Inference, Planning, and Control, 2019. [&lt;a href="https://arxiv.org/abs/1810.06544" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Infer: Intermediate representations for future prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.10641" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/talsperre/INFER"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-agent tensor fusion for contextual trajectory prediction, 2019. [&lt;a href="https://arxiv.org/abs/1904.04776" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-Aware Pedestrian Motion Prediction In Urban Intersections, 2018. [&lt;a href="https://arxiv.org/abs/1806.09453" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic probabilistic interactive situation recognition and prediction: From virtual to real, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8569780" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generic vehicle tracking framework capable of handling occlusions based on modified mixture particle filter, IV 2018. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/8500626" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multi-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs, 2018. [&lt;a href="https://arxiv.org/abs/1805.05499" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sequence-to-sequence prediction of vehicle trajectory via lstm encoder-decoder architecture, 2018. [&lt;a href="https://arxiv.org/abs/1802.06338" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;R2P2: A ReparameteRized Pushforward Policy for diverse, precise generative path forecasting, ECCV 2018. [&lt;a href="https://www.cs.cmu.edu/~nrhineha/R2P2.html" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting trajectories of vehicles using large-scale motion priors, IV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500604" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Vehicle trajectory prediction by integrating physics-and maneuver based approaches using interactive multiple models, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8186191" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, 2018. [&lt;a href="https://arxiv.org/abs/1808.05819v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative multi-agent behavioral cloning, 2018. [&lt;a href="https://www.semanticscholar.org/paper/Generative-Multi-Agent-Behavioral-Cloning-Zhan-Zheng/ccc196ada6ec9cad1e418d7321b0cd6813d9b261" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Sequence Learning with Auxiliary Information for Traffic Prediction, KDD 2018. [&lt;a href="https://arxiv.org/pdf/1806.07380.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/JingqingZ/BaiduTraffic"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction, 2017. [&lt;a href="https://link.springer.com/article/10.1007/s10514-017-9619-z" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic long-term prediction for autonomous vehicles, IV 2017. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7995726" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network, ITSC 2017. [&lt;a href="https://ieeexplore.ieee.org/document/6632960" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Desire: Distant future prediction in dynamic scenes with interacting agents, CVPR 2017. [&lt;a href="https://arxiv.org/abs/1704.04394" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/yadrimz/DESIRE"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Imitating driver behavior with generative adversarial networks, 2017. [&lt;a href="https://arxiv.org/abs/1701.06699" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/sisl/gail-driver"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Infogail: Interpretable imitation learning from visual demonstrations, 2017. [&lt;a href="https://arxiv.org/abs/1703.08840" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/YunzhuLi/InfoGAIL"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-term planning by short-term prediction, 2017. [&lt;a href="https://arxiv.org/abs/1602.01580" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-term path prediction in urban scenarios using circular distributions, 2017. [&lt;a href="https://www.sciencedirect.com/science/article/pii/S0262885617301853" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep learning driven visual path prediction from a single image, 2016. [&lt;a href="https://arxiv.org/abs/1601.07265" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Understanding interactions between traffic participants based on learned behaviors, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7535554" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Visual path prediction in complex scenes with crowded moving objects, CVPR 2016. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7780661/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A game-theoretic approach to replanning-aware interactive scene prediction and planning, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7353203" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intention-aware online pomdp planning for autonomous driving in a crowd, ICRA 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7139219" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856480" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Patch to the future: Unsupervised visual prediction, CVPR 2014. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/6909818/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Mobile agent trajectory prediction using bayesian nonparametric reachability trees, 2011. [&lt;a href="https://dspace.mit.edu/handle/1721.1/114899" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-mobile-robots" class="anchor" aria-hidden="true" href="#mobile-robots"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mobile Robots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multimodal probabilistic model-based planning for human-robot interaction, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1710.09483" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/StanfordASL/TrafficWeavingCVAE"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning, ICRA 2017. [&lt;a href="https://arxiv.org/abs/1609.07845" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Augmented dictionary learning for motion prediction, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487407" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting future agent motions for dynamic environments, ICMLA 2016. [&lt;a href="https://www.semanticscholar.org/paper/Predicting-Future-Agent-Motions-for-Dynamic-Previtali-Bordallo/2df8179ac7b819bad556b6d185fc2030c40f98fa" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bayesian intention inference for trajectory prediction with an unknown goal destination, IROS 2015. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/7354203/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning to predict trajectories of cooperatively navigating agents, ICRA 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6907442" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pedestrians-1" class="anchor" aria-hidden="true" href="#pedestrians-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pedestrians&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model, CVWW 2019. [&lt;a href="https://arxiv.org/pdf/1902.05437.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Path predictions using object attributes and semantic environment, VISIGRAPP 2019. [&lt;a href="http://mprg.jp/data/MPRG/C_group/C20190225_minoura.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic Path Planning using Obstacle Trajectory Prediction, CoDS-COMAD 2019. [&lt;a href="https://dl.acm.org/citation.cfm?id=3297006" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human Trajectory Prediction using Adversarial Loss, 2019. [&lt;a href="http://www.strc.ch/2019/Kothari_Alahi.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs, CVPR 2019. [&lt;a href="https://sites.google.com/view/ieeecvf-cvpr2019-precognition" rel="nofollow"&gt;&lt;em&gt;Precognition Workshop&lt;/em&gt;&lt;/a&gt;], [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Amirian_Social_Ways_Learning_Multi-Modal_Distributions_of_Pedestrian_Trajectories_With_GANs_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/amiryanj/socialways"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Peeking into the Future: Predicting Future Person Activities and Locations in Videos, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liang_Peeking_Into_the_Future_Predicting_Future_Person_Activities_and_Locations_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/google/next-prediction"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning to Infer Relations for Future Trajectory Forecast, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Choi_Learning_to_Infer_Relations_for_Future_Trajectory_Forecast_CVPRW_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Which Way Are You Going? Imitative Decision Learning for Path Forecasting in Dynamic Scenes, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Which_Way_Are_You_Going_Imitative_Decision_Learning_for_Path_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Overcoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction, CVPR 2019.  [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Makansi_Overcoming_Limitations_of_Mixture_Density_Networks_A_Sampling_and_Fitting_CVPR_2019_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sophie: An attentive gan for predicting paths compliant to social and physical constraints, CVPR 2019. [&lt;a href="https://arxiv.org/abs/1806.01482" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/hindupuravinash/the-gan-zoo/blob/master/README.md"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian path, pose, and intention prediction through gaussian process dynamical models and pedestrian activity recognition, 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8370119/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing, 2019. [&lt;a href="https://arxiv.org/abs/1808.06887" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;The simpler the better: Constant velocity for pedestrian motion prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.07933" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian trajectory prediction in extremely crowded scenarios, 2019. [&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/30862018" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Srlstm: State refinement for lstm towards pedestrian trajectory prediction, 2019. [&lt;a href="https://arxiv.org/abs/1903.02793" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Location-velocity attention for pedestrian trajectory prediction, WACV 2019. [&lt;a href="https://ieeexplore.ieee.org/document/8659060" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian Trajectory Prediction in Extremely Crowded Scenarios, Sensors, 2019. [&lt;a href="https://www.mdpi.com/1424-8220/19/5/1223/pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A data-driven model for interaction-aware pedestrian motion prediction in object cluttered environments, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1709.08528" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Move, Attend and Predict: An attention-based neural model for people’s movement prediction, Pattern Recognition Letters 2018. [&lt;a href="https://reader.elsevier.com/reader/sd/pii/S016786551830182X?token=1EF2B664B70D2B0C3ECDD07B6D8B664F5113AEA7533CE5F0B564EF9F4EE90D3CC228CDEB348F79FEB4E8CDCD74D4BA31" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds, ACCV 2018, [&lt;a href="https://arxiv.org/pdf/1812.07667.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://www.youtube.com/watch?v=7cCIC_JIfms" rel="nofollow"&gt;demo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ss-lstm: a hierarchical lstm model for pedestrian trajectory prediction, WACV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8354239" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social Attention: Modeling Attention in Human Crowds, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1710.04689" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/TNTant/social_lstm"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian prediction by planning using deep neural networks, ICRA 2018. [&lt;a href="https://arxiv.org/abs/1706.05904" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Joint long-term prediction of human motion using a planning-based social force approach, ICRA 2018. [&lt;a href="https://iliad-project.eu/publications/2018-2/joint-long-term-prediction-of-human-motion-using-a-planning-based-social-force-approach/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human motion prediction under social grouping constraints, IROS 2018. [&lt;a href="http://iliad-project.eu/publications/2018-2/human-motion-prediction-under-social-grouping-constraints/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks, CVPR 2018. [&lt;a href="https://arxiv.org/abs/1803.10892" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/agrimgupta92/sgan"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Group LSTM: Group Trajectory Prediction in Crowded Scenarios, ECCV 2018. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-030-11015-4_18" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Mx-lstm: mixing tracklets and vislets to jointly forecast trajectories and head poses, CVPR 2018. [&lt;a href="https://arxiv.org/abs/1805.00652" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intent prediction of pedestrians via motion trajectories using stacked recurrent neural networks, 2018. [&lt;a href="http://ieeexplore.ieee.org/document/8481390/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Transferable pedestrian motion prediction models at intersections, 2018. [&lt;a href="https://arxiv.org/abs/1804.00495" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Probabilistic map-based pedestrian motion prediction taking traffic participants into consideration, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500562" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Computationally Efficient Model for Pedestrian Motion Prediction, ECC 2018. [&lt;a href="https://arxiv.org/abs/1803.04702" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-aware trajectory prediction, ICPR 2018. [&lt;a href="https://arxiv.org/abs/1705.02503" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Set-based prediction of pedestrians in urban environments considering formalized traffic rules, ITSC 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8569434" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Building prior knowledge: A markov based pedestrian prediction model using urban environmental data, ICARCV 2018. [&lt;a href="https://arxiv.org/abs/1809.06045" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Depth Information Guided Crowd Counting for Complex Crowd Scenes, 2018. [&lt;a href="https://arxiv.org/abs/1803.02256" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Tracking by Prediction: A Deep Generative Model for Mutli-Person Localisation and Tracking, WACV 2018. [&lt;a href="https://arxiv.org/abs/1803.03347" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;“Seeing is Believing”: Pedestrian Trajectory Forecasting Using Visual Frustum of Attention, WACV 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8354238" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty, CVPR 2018. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/apratimbhattacharyya18/onboard_long_term_prediction"&gt;code+data&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Encoding Crowd Interaction with Deep Neural Network for Pedestrian Trajectory Prediction, CVPR 2018. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;], [&lt;a href="https://github.com/ShanghaiTechCVDL/CIDNN"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Walking Ahead: The Headed Social Force Model, 2017. [&lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169734" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Real-time certified probabilistic pedestrian forecasting, 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7959047" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A multiple-predictor approach to human motion prediction, ICRA 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7989265" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecasting interactive dynamics of pedestrians with fictitious play, CVPR 2017. [&lt;a href="https://arxiv.org/abs/1604.01431" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecast the plausible paths in crowd scenes, IJCAI 2017. [&lt;a href="https://www.ijcai.org/proceedings/2017/386" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bi-prediction: pedestrian trajectory prediction based on bidirectional lstm classification, DICTA 2017. [&lt;a href="https://ieeexplore.ieee.org/document/8227412/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Aggressive, Tense or Shy? Identifying Personality Traits from Crowd Videos, IJCAI 2017. [&lt;a href="https://www.ijcai.org/proceedings/2017/17" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Natural vision based method for predicting pedestrian behaviour in urban environments, ITSC 2017. [&lt;a href="http://ieeexplore.ieee.org/document/8317848/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Human Trajectory Prediction using Spatially aware Deep Attention Models, 2017. [&lt;a href="https://arxiv.org/pdf/1705.09436.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection, 2017. [&lt;a href="https://arxiv.org/pdf/1702.05552.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Forecasting Interactive Dynamics of Pedestrians with Fictitious Play, CVPR 2017. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ma_Forecasting_Interactive_Dynamics_CVPR_2017_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Social LSTM: Human trajectory prediction in crowded spaces, CVPR 2016. [&lt;a href="http://openaccess.thecvf.com/content_cvpr_2016/html/Alahi_Social_LSTM_Human_CVPR_2016_paper.html" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/quancore/social-lstm"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Comparison and evaluation of pedestrian motion models for vehicle safety systems, ITSC 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7795912" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Age and Group-driven Pedestrian Behaviour: from Observations to Simulations, 2016. [&lt;a href="https://collective-dynamics.eu/index.php/cod/article/view/A3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Structural-RNN: Deep learning on spatio-temporal graphs, CVPR 2016. [&lt;a href="https://arxiv.org/abs/1511.05298" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/asheshjain399/RNNexp"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Intent-aware long-term prediction of pedestrian motion, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487409" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-based detection of pedestrian crossing intention for autonomous driving in urban environments, IROS 2016. [&lt;a href="https://ieeexplore.ieee.org/abstract/document/7759351/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Novel planning-based algorithms for human motion prediction, ICRA 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7487505" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning social etiquette: Human trajectory understanding in crowded scenes, ECCV 2016. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_33" rel="nofollow"&gt;paper&lt;/a&gt;][&lt;a href="https://github.com/SajjadMzf/Pedestrian_Datasets_VIS"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;GLMP-realtime pedestrian path prediction using global and local movement patterns, ICRA 2016. [&lt;a href="http://ieeexplore.ieee.org/document/7487768/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Knowledge transfer for scene-specific motion prediction, ECCV 2016. [&lt;a href="https://arxiv.org/abs/1603.06987" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;STF-RNN: Space Time Features-based Recurrent Neural Network for predicting People Next Location, SSCI 2016. [&lt;a href="https://github.com/mhjabreel/STF-RNN"&gt;code&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Goal-directed pedestrian prediction, ICCV 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7406377" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory analysis and prediction for improved pedestrian safety: Integrated framework and evaluations, 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7225707" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting and recognizing human interactions in public spaces, 2015. [&lt;a href="https://link.springer.com/article/10.1007/s11554-014-0428-8" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning collective crowd behaviors with dynamic pedestrian-agents, 2015. [&lt;a href="https://link.springer.com/article/10.1007/s11263-014-0735-3" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Modeling spatial-temporal dynamics of human movements for predicting future trajectories, AAAI 2015. [&lt;a href="https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10126" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Unsupervised robot learning to predict person motion, ICRA 2015. [&lt;a href="https://ieeexplore.ieee.org/document/7139254" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A controlled interactive multiple model filter for combined pedestrian intention recognition and path prediction, ITSC 2015. [&lt;a href="http://ieeexplore.ieee.org/abstract/document/7313129/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Real-Time Predictive Modeling and Robust Avoidance of Pedestrians with Uncertain, Changing Intentions, 2014. [&lt;a href="https://arxiv.org/abs/1405.5581" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Behavior estimation for a complete framework for human motion prediction in crowded environments, ICRA 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6907734" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian’s trajectory forecast in public traffic with artificial neural network, ICPR 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6977417" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Will the pedestrian cross? A study on pedestrian path prediction, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6632960" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;BRVO: Predicting pedestrian trajectories using velocity-space reasoning, 2014. [&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/0278364914555543" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Context-based pedestrian path prediction, ECCV 2014. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-10599-4_40" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Pedestrian path prediction using body language traits, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856498/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Online maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [&lt;a href="https://ieeexplore.ieee.org/document/6856480" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning intentions for improved human motion prediction, 2013. [&lt;a href="https://ieeexplore.ieee.org/document/6766565" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-vehicle-pedestrians-interaction" class="anchor" aria-hidden="true" href="#vehicle-pedestrians-interaction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vehicle-Pedestrians Interaction&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-sport-players-1" class="anchor" aria-hidden="true" href="#sport-players-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sport Players&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Diverse Generation for Multi-Agent Sports Games, CVPR 2019. [&lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Yeh_Diverse_Generation_for_Multi-Agent_Sports_Games_CVPR_2019_paper.html" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Stochastic Prediction of Multi-Agent Interactions from Partial Observations, ICLR 2019. [&lt;a href="http://arxiv.org/abs/1902.09641v1" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generating Multi-Agent Trajectories using Programmatic Weak Supervision, ICLR 2019. [&lt;a href="http://arxiv.org/abs/1803.07612v6" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Multi-Agent Behavioral Cloning, ICML 2018. [&lt;a href="http://www.stephanzheng.com/pdf/Zhan_Zheng_Lucey_Yue_Generative_Multi_Agent_Behavioral_Cloning.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Where Will They Go? Predicting Fine-Grained Adversarial Multi-Agent Motion using Conditional Variational Autoencoders, ECCV 2018. [&lt;a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Panna_Felsen_Where_Will_They_ECCV_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Coordinated Multi-Agent Imitation Learning, ICML 2017. [&lt;a href="http://arxiv.org/abs/1703.03121v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generating long-term trajectories using deep hierarchical networks, 2017. [&lt;a href="https://arxiv.org/abs/1706.07138" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Fine-Grained Spatial Models for Dynamic Sports Play Prediction, ICDM 2014. [&lt;a href="https://ieeexplore.ieee.org/document/7023384/footnotes#footnotes" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Modeling of Multimodal Multi-Human Behavior, 2018. [&lt;a href="https://arxiv.org/pdf/1803.02015.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-benchmark-and-evaluation-metrics" class="anchor" aria-hidden="true" href="#benchmark-and-evaluation-metrics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark and Evaluation Metrics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Towards a fatality-aware benchmark of probabilistic reaction prediction in highly interactive driving scenarios, ITSC 2018. [&lt;a href="https://arxiv.org/abs/1809.03478" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;How good is my prediction? Finding a similarity measure for trajectory prediction evaluation, ITSC 2017. [&lt;a href="http://ieeexplore.ieee.org/document/8317825/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajnet: Towards a benchmark for human trajectory prediction. [&lt;a href="http://trajnet.epfl.ch/" rel="nofollow"&gt;website&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-others" class="anchor" aria-hidden="true" href="#others"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cyclist trajectory prediction using bidirectional recurrent neural networks, AI 2018. [&lt;a href="https://link.springer.com/chapter/10.1007/978-3-030-03991-2_28" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Road infrastructure indicators for trajectory prediction, 2018. [&lt;a href="https://ieeexplore.ieee.org/document/8500678" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using road topology to improve cyclist path prediction, 2017. [&lt;a href="https://ieeexplore.ieee.org/document/7995734/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Trajectory prediction of cyclists using a physical model and an artificial neural network, 2016. [&lt;a href="https://ieeexplore.ieee.org/document/7535484/" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jiachenli94</author><guid isPermaLink="false">https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction</guid><pubDate>Sat, 02 Nov 2019 00:08:00 GMT</pubDate></item><item><title>vdumoulin/conv_arithmetic #9 in TeX, This week</title><link>https://github.com/vdumoulin/conv_arithmetic</link><description>&lt;p&gt;&lt;i&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-convolution-arithmetic" class="anchor" aria-hidden="true" href="#convolution-arithmetic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution arithmetic&lt;/h1&gt;
&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning.&lt;/p&gt;
&lt;p&gt;The code and the images of this tutorial are free to use as regulated by the
licence and subject to proper attribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[1] Vincent Dumoulin, Francesco Visin - &lt;a href="https://arxiv.org/abs/1603.07285" rel="nofollow"&gt;A guide to convolution arithmetic
for deep learning&lt;/a&gt;
(&lt;a href="https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolution-animations" class="anchor" aria-hidden="true" href="#convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides&lt;/td&gt;
    &lt;td&gt;Half padding, no strides&lt;/td&gt;
    &lt;td&gt;Full padding, no strides&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides.gif"&gt;&lt;img width="150px" src="gif/padding_strides.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides&lt;/td&gt;
    &lt;td&gt;Padding, strides (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-transposed-convolution-animations" class="anchor" aria-hidden="true" href="#transposed-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transposed convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/arbitrary_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/arbitrary_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/same_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/same_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/full_padding_no_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/full_padding_no_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Half padding, no strides, transposed&lt;/td&gt;
    &lt;td&gt;Full padding, no strides, transposed&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/no_padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/no_padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/padding_strides_odd_transposed.gif"&gt;&lt;img width="150px" src="gif/padding_strides_odd_transposed.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed&lt;/td&gt;
    &lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-dilated-convolution-animations" class="anchor" aria-hidden="true" href="#dilated-convolution-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dilated convolution animations&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="gif/dilation.gif"&gt;&lt;img width="150px" src="gif/dilation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;No padding, no stride, dilation&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-makefile" class="anchor" aria-hidden="true" href="#generating-the-makefile"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the Makefile&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ ./bin/generate_makefile&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-generating-the-animations" class="anchor" aria-hidden="true" href="#generating-the-animations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generating the animations&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make all_animations&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The animations will be output to the &lt;code&gt;gif&lt;/code&gt; directory. Individual animation steps
will be output in PDF format to the &lt;code&gt;pdf&lt;/code&gt; directory and in PNG format to the
&lt;code&gt;png&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compiling-the-document" class="anchor" aria-hidden="true" href="#compiling-the-document"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compiling the document&lt;/h2&gt;
&lt;p&gt;From the repository's root directory:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ make&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>vdumoulin</author><guid isPermaLink="false">https://github.com/vdumoulin/conv_arithmetic</guid><pubDate>Sat, 02 Nov 2019 00:09:00 GMT</pubDate></item><item><title>jacobeisenstein/gt-nlp-class #10 in TeX, This week</title><link>https://github.com/jacobeisenstein/gt-nlp-class</link><description>&lt;p&gt;&lt;i&gt;Course materials for Georgia Tech CS 4650 and 7650, "Natural Language"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cs-4650-and-7650" class="anchor" aria-hidden="true" href="#cs-4650-and-7650"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CS 4650 and 7650&lt;/h1&gt;
&lt;p&gt;(&lt;strong&gt;Note about registration&lt;/strong&gt;: registration is currently restricted to students pursuing CS degrees for which this course is an essential requirement. Unfortunately, the enrollment is already at the limit of the classroom space, so this restriction is unlikely to be lifted.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Course&lt;/strong&gt;: Natural Language Understanding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instructor&lt;/strong&gt;: Jacob Eisenstein&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Semester&lt;/strong&gt;: Spring 2018&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Time&lt;/strong&gt;: Mondays and Wednesdays, 3:00-4:15pm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TAs&lt;/strong&gt;: Murali Raghu Babu, James Mullenbach, Yuval Pinter, Zhewei Sun&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1BuvRjPhfHmy7XAfpc5KoygdfqI3Cue3bbmiO6yYuX_E/edit?usp=sharing" rel="nofollow"&gt;Schedule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/document/d/1loefqZhmOaF2mP8yQPEx91jZ7BHylWixVtYlFhpIlGM/edit?usp=sharing" rel="nofollow"&gt;Recaps&lt;/a&gt; from previous classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which
are especially relevant to natural language processing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#readings"&gt;Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grading"&gt;Grading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#help"&gt;Help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#policies"&gt;Policies&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-learning-goals" class="anchor" aria-hidden="true" href="#learning-goals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning goals&lt;/h1&gt;
&lt;a name="user-content-learning"&gt;
&lt;ul&gt;
&lt;li&gt;Acquire the fundamental linguistic concepts that are relevant to language technology. This goal will be assessed in the short homework assignments and the exams.&lt;/li&gt;
&lt;li&gt;Analyze and understand state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the exams and the assigned projects.&lt;/li&gt;
&lt;li&gt;Implement state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the assigned projects.&lt;/li&gt;
&lt;li&gt;Adapt and apply state-of-the-art language technology to new problems and settings. This goal will be assessed in assigned projects.&lt;/li&gt;
&lt;li&gt;(7650 only) Read and understand current research on natural language processing. This goal will be assessed in assigned projects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-readings" class="anchor" aria-hidden="true" href="#readings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Readings&lt;/h1&gt;
&lt;/a&gt;&lt;a name="user-content-readings"&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-readings"&gt;Readings will be drawn mainly from my &lt;/a&gt;&lt;a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes"&gt;notes&lt;/a&gt;. Additional readings may be assigned from published papers, blogposts, and tutorials.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-supplemental-textbooks" class="anchor" aria-hidden="true" href="#supplemental-textbooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supplemental textbooks&lt;/h2&gt;
&lt;p&gt;These are completely optional, but might deepen your understanding of the material.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/" rel="nofollow"&gt;Speech and Language Processing&lt;/a&gt; is the textbook most often used in NLP courses. It's a great reference for both the linguistics and algorithms we'll encounter in this course. Several chapters from the upcoming &lt;a href="https://web.stanford.edu/~jurafsky/slp3/" rel="nofollow"&gt;third edition&lt;/a&gt; are free online.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Natural-Language-Processing-Python-Steven/dp/0596516495" rel="nofollow"&gt;Natural Language Processing with Python&lt;/a&gt;
shows how to do hands-on work with Python's Natural Language Toolkit (NLTK), and also brings a strong linguistic perspective.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.com/Schaums-Outline-Probability-Statistics-Edition/dp/007179557X/ref=pd_sim_b_1?ie=UTF8&amp;amp;refRID=1R57HWNCW6EEWD1ZRH4C" rel="nofollow"&gt;Schaum's Outline of Probability and Statistics&lt;/a&gt; can help you review the probability and statistics that we use in this course.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-grading" class="anchor" aria-hidden="true" href="#grading"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grading&lt;/h1&gt;
&lt;a name="user-content-grading"&gt;
&lt;p&gt;The graded material for the course will consist of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Seven short homework assignments, of which you must do six. Most of these involve performing linguistic annotation on some text of your choice. The purpose is to get a basic understanding of key linguistic concepts. Each assignment should take less than an hour. Each homework is worth 2 points (12 total). (Many of these homeworks are implemented at &lt;strong&gt;quizzes&lt;/strong&gt; on Canvas.)&lt;/li&gt;
&lt;li&gt;Four assigned problem sets. These involve building and using NLP techniques which are at or near the state-of-the-art. The purpose is to learn how to implement natural language processing software, and to have fun. These assignments must be done individually. Each problem set is worth ten points (48 total). Students enrolled in CS 7650 will have an additional, research-oriented component to the problem sets.&lt;/li&gt;
&lt;li&gt;An in-class midterm exam, worth 20 points, and a final exam, worth 20 points. The purpose of these exams is to assess understanding of the core theoretical concepts, and to encourage you to review and synthesize your understanding of these concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-grading"&gt;Barring a personal emergency or an institute-approved absence, you must take each exam on the day indicated in the schedule. Job interviews and travel plans are generally not a reason for an institute-approved absence. See &lt;/a&gt;&lt;a href="https://registrar.gatech.edu/info/institute-approved-absence-form-for-students" rel="nofollow"&gt;here&lt;/a&gt; for more information on GT policy about absences.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-late-policy" class="anchor" aria-hidden="true" href="#late-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Late policy&lt;/h2&gt;
&lt;p&gt;Problem sets will be accepted up to 72 hours late, at a penalty of 2 points per 24 hours. (Maximum score after missing the deadline: 10/12; maximum score 24 hours after the deadline: 8/12, etc.)  It is usually best just to turn in what you have at the due date. Late homeworks will not be accepted. This late policy is intended to ensure fair and timely evaluation.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-getting-help" class="anchor" aria-hidden="true" href="#getting-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting help&lt;/h1&gt;
&lt;a name="user-content-help"&gt;
&lt;h2&gt;&lt;a id="user-content-office-hours" class="anchor" aria-hidden="true" href="#office-hours"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Office hours&lt;/h2&gt;
&lt;p&gt;My office hours follow Wednesday classes (4:15-5:15PM) and take place in class when available.&lt;/p&gt;
&lt;p&gt;TA office hours are in CCB commons (1st floor) unless otherwise announced on Piazza.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Murali: Friday   10AM-11AM&lt;/li&gt;
&lt;li&gt;James:  Thursday 11AM-12PM&lt;/li&gt;
&lt;li&gt;Yuval:  Tuesday  3PM-4PM&lt;/li&gt;
&lt;li&gt;Zhewei: Monday   1PM-2PM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-online-help" class="anchor" aria-hidden="true" href="#online-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online help&lt;/h2&gt;
&lt;p&gt;Please use Piazza rather than personal email to ask questions. This helps other students, who may have the same question. Personal emails may not be answered. If you cannot make it to office hours, please use Piazza to make an appointment. It is unlikely that I will be able to chat if you make an unscheduled visit to my office. The same is true for the TAs.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-class-policies" class="anchor" aria-hidden="true" href="#class-policies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Class policies&lt;/h1&gt;
&lt;/a&gt;&lt;a name="user-content-policies"&gt;
&lt;p&gt;Attendance will not be taken, but &lt;strong&gt;you are responsible for knowing what happens in every class&lt;/strong&gt;. If you cannot attend class, make sure you check up with someone who was there.&lt;/p&gt;
&lt;p&gt;Respect your classmates and your instructor by preventing distractions. This means be on time, turn off your cellphone, and save side conversations for after class. If you can't read something I wrote on the board, or if you think I made a mistake in a derivation, please raise your hand and tell me!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using a laptop in class is likely to reduce your education attainment&lt;/strong&gt;. This has been documented by multiple studies, which are nicely summarized in the following article:&lt;/p&gt;
&lt;/a&gt;&lt;ul&gt;&lt;a name="user-content-policies"&gt;
&lt;/a&gt;&lt;li&gt;&lt;a name="user-content-policies"&gt;&lt;/a&gt;&lt;a href="https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html" rel="nofollow"&gt;https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am not going to ban laptops, as long as they are not a distraction to anyone but the user. But I suggest you try pen and paper for a few weeks, and see if it helps.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;a name="user-content-prerequisites"&gt;
&lt;p&gt;The official prerequisite for CS 4650 is CS 3510/3511, "Design and Analysis of Algorithms." This prerequisite is essential because understanding natural language processing algorithms requires familiarity with dynamic programming, as well as automata and formal language theory: finite-state and context-free languages, NP-completeness, etc. While course prerequisites are not enforced for graduate students, prior exposure to analysis of algorithms is very strongly recommended.&lt;/p&gt;
&lt;p&gt;Furthermore, this course assumes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good coding ability, corresponding to at least a third or fourth-year undergraduate CS major. Assignments will be in Python.&lt;/li&gt;
&lt;li&gt;Background in basic probability, linear algebra, and calculus.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;People sometimes want to take the course without having all of these
prerequisites. Frequent cases are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Junior CS students with strong programming skills but limited theoretical and mathematical background,&lt;/li&gt;
&lt;li&gt;Non-CS students with strong mathematical background but limited programming experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Students in the first group suffer in the exam and don't understand the lectures, and students in the second group suffer in the problem sets. My advice is to get the background material first, and
then take this course.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-collaboration-policy" class="anchor" aria-hidden="true" href="#collaboration-policy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Collaboration policy&lt;/h2&gt;
&lt;p&gt;One of the goals of the assigned work is to assess your individual progress in meeting the learning objectives of the course. You may discuss the homework and projects with other students, but your work must be your own -- particularly all coding and writing. For example:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-examples-of-acceptable-collaboration" class="anchor" aria-hidden="true" href="#examples-of-acceptable-collaboration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples of acceptable collaboration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Alice and Bob discuss alternatives for storing large, sparse vectors of feature counts, as required by a problem set.&lt;/li&gt;
&lt;li&gt;Bob is confused about how to implement the Viterbi algorithm, and asks Alice for a conceptual description of her strategy.&lt;/li&gt;
&lt;li&gt;Alice asks Bob if he encountered a failure condition at a "sanity check" in a coding assignment, and Bob explains at a conceptual level how he overcame that failure condition.&lt;/li&gt;
&lt;li&gt;Alice is having trouble getting adequate performance from her part-of-speech tagger. She finds a blog page or research paper that gives her some new ideas, which she implements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-examples-of-unacceptable-collaboration" class="anchor" aria-hidden="true" href="#examples-of-unacceptable-collaboration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples of unacceptable collaboration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Alice and Bob work together to write code for storing feature counts.&lt;/li&gt;
&lt;li&gt;Alice and Bob divide the assignment into parts, and each write the code for their part, and then share their solutions with each other to complete the assignment.&lt;/li&gt;
&lt;li&gt;Alice or Bob obtain a solution to a previous year's assignment or to a related assignment in another class, and use it as the starting point for their own solutions.&lt;/li&gt;
&lt;li&gt;Bob is having trouble getting adequate performance from his part-of-speech tagger. He finds source code online, and copies it into his own submission.&lt;/li&gt;
&lt;li&gt;Alice wants to win the Kaggle competition for a problem set. She finds the test set online, and customizes her submission to do well on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some assignments will involve written responses. Using other people’s text or figures without attribution is plagiarism, and is never acceptable.&lt;/p&gt;
&lt;/a&gt;&lt;p&gt;&lt;a name="user-content-prerequisites"&gt;Suspected cases of academic misconduct will be (and have been!) referred to the Honor Advisory Council. For any questions involving these or any other Academic Honor Code issues, please consult me, my teaching assistants, or &lt;/a&gt;&lt;a href="http://www.honor.gatech.edu" rel="nofollow"&gt;http://www.honor.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jacobeisenstein</author><guid isPermaLink="false">https://github.com/jacobeisenstein/gt-nlp-class</guid><pubDate>Sat, 02 Nov 2019 00:10:00 GMT</pubDate></item></channel></rss>