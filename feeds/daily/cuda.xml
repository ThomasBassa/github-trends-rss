<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Cuda, Today</title><link>https://github.com/trending/cuda?since=daily</link><description>The top repositories on GitHub for cuda, measured daily</description><pubDate>Fri, 24 Jan 2020 01:09:52 GMT</pubDate><lastBuildDate>Fri, 24 Jan 2020 01:09:52 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>CharlesShang/DCNv2 #1 in Cuda, Today</title><link>https://github.com/CharlesShang/DCNv2</link><description>&lt;p&gt;&lt;i&gt;Deformable Convolutional Networks v2 with Pytorch&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-deformable-convolutional-networks-v2-with-pytorch-10" class="anchor" aria-hidden="true" href="#deformable-convolutional-networks-v2-with-pytorch-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deformable Convolutional Networks V2 with Pytorch 1.0&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-build" class="anchor" aria-hidden="true" href="#build"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;    ./make.sh         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; build&lt;/span&gt;
    python test.py    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; run examples and gradient check &lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-an-example" class="anchor" aria-hidden="true" href="#an-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;An Example&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;deformable conv&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;from&lt;/span&gt; dcn_v2 &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;DCN&lt;/span&gt;
    &lt;span class="pl-c1"&gt;input&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.randn(&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-c1"&gt;128&lt;/span&gt;).cuda()
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; wrap all things (offset and mask) in DCN&lt;/span&gt;
    dcn &lt;span class="pl-k"&gt;=&lt;/span&gt; DCN(&lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-v"&gt;kernel_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;(&lt;span class="pl-c1"&gt;3&lt;/span&gt;,&lt;span class="pl-c1"&gt;3&lt;/span&gt;), &lt;span class="pl-v"&gt;stride&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-v"&gt;padding&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-v"&gt;deformable_groups&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;).cuda()
    output &lt;span class="pl-k"&gt;=&lt;/span&gt; dcn(&lt;span class="pl-c1"&gt;input&lt;/span&gt;)
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(output.shape)&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;deformable roi pooling&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;from&lt;/span&gt; dcn_v2 &lt;span class="pl-k"&gt;import&lt;/span&gt; DCNPooling
    &lt;span class="pl-c1"&gt;input&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.randn(&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;32&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;, &lt;span class="pl-c1"&gt;64&lt;/span&gt;).cuda()
    batch_inds &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.randint(&lt;span class="pl-c1"&gt;2&lt;/span&gt;, (&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;)).cuda().float()
    x &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.randint(&lt;span class="pl-c1"&gt;256&lt;/span&gt;, (&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;)).cuda().float()
    y &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.randint(&lt;span class="pl-c1"&gt;256&lt;/span&gt;, (&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;)).cuda().float()
    w &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.randint(&lt;span class="pl-c1"&gt;64&lt;/span&gt;, (&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;)).cuda().float()
    h &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.randint(&lt;span class="pl-c1"&gt;64&lt;/span&gt;, (&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;)).cuda().float()
    rois &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.cat((batch_inds, x, y, x &lt;span class="pl-k"&gt;+&lt;/span&gt; w, y &lt;span class="pl-k"&gt;+&lt;/span&gt; h), &lt;span class="pl-v"&gt;dim&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; mdformable pooling (V2)&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; wrap all things (offset and mask) in DCNPooling&lt;/span&gt;
    dpooling &lt;span class="pl-k"&gt;=&lt;/span&gt; DCNPooling(&lt;span class="pl-v"&gt;spatial_scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1.0&lt;/span&gt; &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;4&lt;/span&gt;,
                         &lt;span class="pl-v"&gt;pooled_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;,
                         &lt;span class="pl-v"&gt;output_dim&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;,
                         &lt;span class="pl-v"&gt;no_trans&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;,
                         &lt;span class="pl-v"&gt;group_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;,
                         &lt;span class="pl-v"&gt;trans_std&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.1&lt;/span&gt;).cuda()

    dout &lt;span class="pl-k"&gt;=&lt;/span&gt; dpooling(&lt;span class="pl-c1"&gt;input&lt;/span&gt;, rois)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-note" class="anchor" aria-hidden="true" href="#note"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note&lt;/h3&gt;
&lt;p&gt;Now the master branch is for pytorch 1.0 (new ATen API), you can switch back to pytorch 0.4 with,&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git checkout pytorch_0.4&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known Issues:&lt;/h3&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Gradient check w.r.t offset (solved)&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Backward is not reentrant (minor)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an adaption of the official &lt;a href="https://github.com/msracver/Deformable-ConvNets/tree/master/DCNv2_op"&gt;Deformable-ConvNets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;s&gt;I have ran the gradient check for many times with DOUBLE type. Every tensor &lt;strong&gt;except offset&lt;/strong&gt; passes.
However, when I set the offset to 0.5, it passes. I'm still wondering what cause this problem. Is it because some
non-differential points? &lt;/s&gt;&lt;/p&gt;
&lt;p&gt;Update: all gradient check passes with double precision.&lt;/p&gt;
&lt;p&gt;Another issue is that it raises &lt;code&gt;RuntimeError: Backward is not reentrant&lt;/code&gt;. However, the error is very small (&lt;code&gt;&amp;lt;1e-7&lt;/code&gt; for
float &lt;code&gt;&amp;lt;1e-15&lt;/code&gt; for double),
so it may not be a serious problem (?)&lt;/p&gt;
&lt;p&gt;Please post an issue or PR if you have any comments.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CharlesShang</author><guid isPermaLink="false">https://github.com/CharlesShang/DCNv2</guid><pubDate>Fri, 24 Jan 2020 00:01:00 GMT</pubDate></item><item><title>hangg7/deformable-kernels #2 in Cuda, Today</title><link>https://github.com/hangg7/deformable-kernels</link><description>&lt;p&gt;&lt;i&gt;Deforming kernels to adapt towards object deformation. To appear in ICLR 2020.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deformable-kernels-iclr-2020" class="anchor" aria-hidden="true" href="#deformable-kernels-iclr-2020"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;b&gt;Deformable Kernels&lt;/b&gt; &lt;a href="https://arxiv.org/abs/1910.02940" rel="nofollow"&gt;[ICLR 2020]&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/35777bdcf8791a68b34669de5e0567994540d90a/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e68616e67672f70726f6a656374732f6465666f726d61626c652d6b65726e656c732f7265736f75726365732f6572665f76697375616c697a6174696f6e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/35777bdcf8791a68b34669de5e0567994540d90a/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e68616e67672f70726f6a656374732f6465666f726d61626c652d6b65726e656c732f7265736f75726365732f6572665f76697375616c697a6174696f6e2e706e67" width="1024" data-canonical-src="http://people.eecs.berkeley.edu/~hangg/projects/deformable-kernels/resources/erf_visualization.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation&lt;/strong&gt; &lt;br&gt;
&lt;a href="http://people.eecs.berkeley.edu/~hangg/" rel="nofollow"&gt;Hang Gao&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;, Xizhou
Zhu&lt;sup&gt;*&lt;/sup&gt;, Steve Lin, &lt;a href="https://jifengdai.org/" rel="nofollow"&gt;Jifeng Dai&lt;/a&gt;. &lt;br&gt;
In &lt;a href="https://arxiv.org/abs/1910.02940" rel="nofollow"&gt;ICLR, 2020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This repository contains official implementation of deformable kernels. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#1-customized-operators"&gt;Customized operators for deformable kernels, along with its variants.&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#2-quickstart"&gt;Instructions to use our operators.&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3-results--pretrained-models"&gt;Results on ImageNet &amp;amp; COCO benchmarks, with pretrained models for
reproduction.&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#4-training--evaluation-code"&gt;Training and evaluation code.&lt;/a&gt;&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-0-getting-started" class="anchor" aria-hidden="true" href="#0-getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(0) Getting started&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-pytorch" class="anchor" aria-hidden="true" href="#pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Get &lt;a href="https://developer.nvidia.com/cuda-10.1-download-archive-base" rel="nofollow"&gt;CUDA 10.1&lt;/a&gt;
installed on your machine.&lt;/li&gt;
&lt;li&gt;Install PyTorch (&lt;a href="http://pytorch.org" rel="nofollow"&gt;pytorch.org&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;conda env create -f environment.yml&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-apex" class="anchor" aria-hidden="true" href="#apex"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apex&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install &lt;a href="https://github.com/NVIDIA/apex/"&gt;Apex&lt;/a&gt; from its official repo. This
will require CUDA 10.1 to work with the latest pytorch version (which is
&lt;code&gt;pytorch=1.3.1&lt;/code&gt; as being tested against). It is used for fast mix-precision
inference and should work out of the box.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-compile-our-operators" class="anchor" aria-hidden="true" href="#compile-our-operators"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compile our operators&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; assume at project root&lt;/span&gt;
(
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; deformable_kernels/ops/deform_kernel&lt;span class="pl-k"&gt;;&lt;/span&gt;
pip install -e &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;span class="pl-k"&gt;;&lt;/span&gt;
)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-1-customized-operators" class="anchor" aria-hidden="true" href="#1-customized-operators"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(1) Customized operators&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5d2882868724c090f853c05c6d296fffd42dcde1/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e68616e67672f70726f6a656374732f6465666f726d61626c652d6b65726e656c732f7265736f75726365732f646b5f666f72776172642e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/5d2882868724c090f853c05c6d296fffd42dcde1/687474703a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e68616e67672f70726f6a656374732f6465666f726d61626c652d6b65726e656c732f7265736f75726365732f646b5f666f72776172642e706e67" width="768" data-canonical-src="http://people.eecs.berkeley.edu/~hangg/projects/deformable-kernels/resources/dk_forward.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repo includes all deformable kernel variants described in our paper, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Global Deformable Kernels;&lt;/li&gt;
&lt;li&gt;Local Deformable Kernels;&lt;/li&gt;
&lt;li&gt;Local Deformable Kernels integrating with Deformable Convolutions;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Instead of learning offsets on image space, we propose to deform and resample
on kernel space. This enables powerful dynamic inference capacity. For more
technical details, please refer to their
&lt;a href="deformable_kernels/modules/deform_kernel.py"&gt;definitions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We also provide implementations on our rivalries, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.06211" rel="nofollow"&gt;Deformable Convolutions&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1904.04971" rel="nofollow"&gt;Soft Conditional Computation&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please refer to their module definitions under &lt;code&gt;deformable_kernels/modules&lt;/code&gt; folder.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-2-quickstart" class="anchor" aria-hidden="true" href="#2-quickstart"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(2) Quickstart&lt;/h2&gt;
&lt;p&gt;The following snippet constructs the deformable kernels we used for our experiments&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; deformable_kernels.modules &lt;span class="pl-k"&gt;import&lt;/span&gt; (
    GlobalDeformKernel2d,
    DeformKernel2d,
    DeformKernelConv2d,
)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; global DK with scope size 2, kernel size 1, stride 1, padding 0, depthwise convolution.&lt;/span&gt;
gdk &lt;span class="pl-k"&gt;=&lt;/span&gt; GlobalDeformKernel2d((&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;), [inplanes], [inplanes], &lt;span class="pl-v"&gt;groups&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[inplanes])
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (local) DK with scope size 4, kernel size 3, stride 1, padding 1, depthwise convolution.&lt;/span&gt;
dk &lt;span class="pl-k"&gt;=&lt;/span&gt; DeformKernel2d((&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;), [inplanes], [inplanes], &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-v"&gt;groups&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[inplanes])
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (local) DK integrating with dcn, with kernel &amp;amp; image offsets separately learnt.&lt;/span&gt;
dkc &lt;span class="pl-k"&gt;=&lt;/span&gt; DeformKernelConv2d((&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;), [inplanes], [inplanes], &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-v"&gt;groups&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[inplanes]).&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that all of our customized operators only support depthwise convolutions
now, mainly because that efficiently resampling kernels at runtime is
extremely slow if we orthogonally compute over each channel. We are trying to
loose this requirement by iterating our CUDA implementation. Any contribuitions
are welcome!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-3-results--pretrained-models" class="anchor" aria-hidden="true" href="#3-results--pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(3) Results &amp;amp; pretrained models&lt;/h2&gt;
&lt;p&gt;Under construction.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-4-training--evaluation-code" class="anchor" aria-hidden="true" href="#4-training--evaluation-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(4) Training &amp;amp; evaluation code&lt;/h2&gt;
&lt;p&gt;Under construction.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-license" class="anchor" aria-hidden="true" href="#a-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(A) License&lt;/h2&gt;
&lt;p&gt;This project is released under the &lt;a href="LICENSE"&gt;MIT license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-b-citation--contact" class="anchor" aria-hidden="true" href="#b-citation--contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(B) Citation &amp;amp; Contact&lt;/h2&gt;
&lt;p&gt;If you find this repo useful for your research, please consider citing this
bibtex:&lt;/p&gt;
&lt;div class="highlight highlight-text-tex-latex"&gt;&lt;pre&gt;@article{gao2019deformable,
  title={Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation},
  author={Gao, Hang and Zhu, Xizhou and Lin, Steve and Dai, Jifeng},
  journal={arXiv preprint arXiv:1910.02940},
  year={2019}
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please contact Hang Gao &lt;code&gt;&amp;lt;hangg AT eecs DOT berkeley DOT com&amp;gt;&lt;/code&gt; and Xizhou Zhu
&lt;code&gt;&amp;lt;ezra0408 AT mail.ustc DOT edu DOT cn&amp;gt;&lt;/code&gt; with any comments or feedback.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hangg7</author><guid isPermaLink="false">https://github.com/hangg7/deformable-kernels</guid><pubDate>Fri, 24 Jan 2020 00:02:00 GMT</pubDate></item><item><title>ilovepose/fast-human-pose-estimation.pytorch #3 in Cuda, Today</title><link>https://github.com/ilovepose/fast-human-pose-estimation.pytorch</link><description>&lt;p&gt;&lt;i&gt;Official pytorch Code for CVPR2019 paper "Fast Human Pose Estimation" https://arxiv.org/abs/1811.05419&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-fast-human-pose-estimation-cvpr2019" class="anchor" aria-hidden="true" href="#fast-human-pose-estimation-cvpr2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fast Human Pose Estimation CVPR2019&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is an official pytorch implementation of &lt;a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.html" rel="nofollow"&gt;&lt;em&gt;Fast Human Pose Estimation&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this work, we focus on the two problems&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to reduce the model size and computation using a model-agnostic method.&lt;/li&gt;
&lt;li&gt;How to improve the performance of the reduced model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In our paper&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We reduce the model size and computation through reducing the width and depth of a network.&lt;/li&gt;
&lt;li&gt;Propose the fast pose distillation (&lt;strong&gt;FPD&lt;/strong&gt;) to improve the performance of the reduced model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results on the MPII dataset demonstrate the effectiveness of our approach. We re-implemented the FPD using the HRNet codebase and provided extra evaluation on the COCO dataset.  Our method (FPD) can work without ground-truth labels, it can utilize unlabeled images.
&lt;a target="_blank" rel="noopener noreferrer" href="/figures/pose_kd.jpg"&gt;&lt;img src="/figures/pose_kd.jpg" alt="Illustrating the architecture of the proposed HRNet" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For the MPII dataset&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We first trained a teacher model (hourglass model, stacks=8, num_features=256, 90.520@MPII PCKh@0.5) and a student model (hourglass model, stacks=4, num_features=128, 89.040@MPII PCKh@0.5).&lt;/li&gt;
&lt;li&gt;We then used the teacher model's prediction and the ground-truth label to co-supervisie the student model (hourglass model, stacks=4, num_features=128, 87.934@MPII PCKh@0.5).&lt;/li&gt;
&lt;li&gt;Our experiment shows &lt;strong&gt;1.106%&lt;/strong&gt; gain from FPD.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;For the COCO dataset&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We first trained a teacher model (HRNet-W48, input size=256x192, 75.0@COCO-Valid-Set AP) and a student model (HRNet-W32, input size=256x192, 74.4@COCO-Valid-Set AP).&lt;/li&gt;
&lt;li&gt;We then used the teacher model's prediction and the ground-truth label to co-supervisie the student model (HRNet-W32, input size=256x192, 75.1@COCO-Valid-Set AP).&lt;/li&gt;
&lt;li&gt;Our experiment shows &lt;strong&gt;0.7%&lt;/strong&gt; gain from FPD.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;If you want to further improve the performance of the student model.You can remove the supervision of ground-truth label in the FPD when there are unlabeled images.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-main-results" class="anchor" aria-hidden="true" href="#main-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Results&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-results-on-mpii-val" class="anchor" aria-hidden="true" href="#results-on-mpii-val"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results on MPII val&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Arch&lt;/th&gt;
&lt;th&gt;Head&lt;/th&gt;
&lt;th&gt;Shoulder&lt;/th&gt;
&lt;th&gt;Elbow&lt;/th&gt;
&lt;th&gt;Wrist&lt;/th&gt;
&lt;th&gt;Hip&lt;/th&gt;
&lt;th&gt;Knee&lt;/th&gt;
&lt;th&gt;Ankle&lt;/th&gt;
&lt;th&gt;Mean&lt;/th&gt;
&lt;th&gt;Mean@0.1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;hourglass_teacher&lt;/td&gt;
&lt;td&gt;97.169&lt;/td&gt;
&lt;td&gt;96.382&lt;/td&gt;
&lt;td&gt;90.830&lt;/td&gt;
&lt;td&gt;86.466&lt;/td&gt;
&lt;td&gt;90.012&lt;/td&gt;
&lt;td&gt;86.802&lt;/td&gt;
&lt;td&gt;82.664&lt;/td&gt;
&lt;td&gt;90.520&lt;/td&gt;
&lt;td&gt;38.275&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hourglass_student&lt;/td&gt;
&lt;td&gt;96.828&lt;/td&gt;
&lt;td&gt;95.194&lt;/td&gt;
&lt;td&gt;87.728&lt;/td&gt;
&lt;td&gt;82.919&lt;/td&gt;
&lt;td&gt;87.900&lt;/td&gt;
&lt;td&gt;82.551&lt;/td&gt;
&lt;td&gt;78.270&lt;/td&gt;
&lt;td&gt;87.934&lt;/td&gt;
&lt;td&gt;34.634&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hourglass_student_FPD*&lt;/td&gt;
&lt;td&gt;96.385&lt;/td&gt;
&lt;td&gt;94.905&lt;/td&gt;
&lt;td&gt;87.847&lt;/td&gt;
&lt;td&gt;81.875&lt;/td&gt;
&lt;td&gt;87.225&lt;/td&gt;
&lt;td&gt;81.906&lt;/td&gt;
&lt;td&gt;78.955&lt;/td&gt;
&lt;td&gt;87.598&lt;/td&gt;
&lt;td&gt;34.359&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;hourglass_student_FPD&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;96.930&lt;/td&gt;
&lt;td&gt;95.550&lt;/td&gt;
&lt;td&gt;89.040&lt;/td&gt;
&lt;td&gt;84.444&lt;/td&gt;
&lt;td&gt;88.939&lt;/td&gt;
&lt;td&gt;84.021&lt;/td&gt;
&lt;td&gt;80.703&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;89.040&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;36.144&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flip test is used.&lt;/li&gt;
&lt;li&gt;Input size is 256x256.&lt;/li&gt;
&lt;li&gt;hourglass_student_FPD* means not using pretrained students.&lt;/li&gt;
&lt;li&gt;Not using multi-scale test.&lt;/li&gt;
&lt;li&gt;Batch size is 4.&lt;/li&gt;
&lt;li&gt;The PCKh metric implemented in the HRNet codebase for MPII dataset is slightly different from that in our paper.&lt;/li&gt;
&lt;li&gt;The performance of hourglass implemented using pytorch is lower than that implemented using torch(paper).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-results-on-coco-val2017-with-detector-having-human-ap-of-564-on-coco-val2017-dataset" class="anchor" aria-hidden="true" href="#results-on-coco-val2017-with-detector-having-human-ap-of-564-on-coco-val2017-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results on COCO val2017 with detector having human AP of 56.4 on COCO val2017 dataset&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Arch&lt;/th&gt;
&lt;th&gt;Input size&lt;/th&gt;
&lt;th&gt;#Params&lt;/th&gt;
&lt;th&gt;GFLOPs&lt;/th&gt;
&lt;th&gt;AP&lt;/th&gt;
&lt;th&gt;Ap .5&lt;/th&gt;
&lt;th&gt;AP .75&lt;/th&gt;
&lt;th&gt;AP (M)&lt;/th&gt;
&lt;th&gt;AP (L)&lt;/th&gt;
&lt;th&gt;AR&lt;/th&gt;
&lt;th&gt;AR .5&lt;/th&gt;
&lt;th&gt;AR .75&lt;/th&gt;
&lt;th&gt;AR (M)&lt;/th&gt;
&lt;th&gt;AR (L)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pose_hrnet_w48_teacher&lt;/td&gt;
&lt;td&gt;256x192&lt;/td&gt;
&lt;td&gt;63.6M&lt;/td&gt;
&lt;td&gt;14.6&lt;/td&gt;
&lt;td&gt;0.750&lt;/td&gt;
&lt;td&gt;0.906&lt;/td&gt;
&lt;td&gt;0.824&lt;/td&gt;
&lt;td&gt;0.713&lt;/td&gt;
&lt;td&gt;0.819&lt;/td&gt;
&lt;td&gt;0.803&lt;/td&gt;
&lt;td&gt;0.941&lt;/td&gt;
&lt;td&gt;0.867&lt;/td&gt;
&lt;td&gt;0.760&lt;/td&gt;
&lt;td&gt;0.866&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pose_hrnet_w32_student&lt;/td&gt;
&lt;td&gt;256x192&lt;/td&gt;
&lt;td&gt;28.5M&lt;/td&gt;
&lt;td&gt;7.1&lt;/td&gt;
&lt;td&gt;0.744&lt;/td&gt;
&lt;td&gt;0.905&lt;/td&gt;
&lt;td&gt;0.819&lt;/td&gt;
&lt;td&gt;0.708&lt;/td&gt;
&lt;td&gt;0.810&lt;/td&gt;
&lt;td&gt;0.798&lt;/td&gt;
&lt;td&gt;0.942&lt;/td&gt;
&lt;td&gt;0.865&lt;/td&gt;
&lt;td&gt;0.757&lt;/td&gt;
&lt;td&gt;0.858&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;pose_hrnet_w32_student_FPD&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;256x192&lt;/td&gt;
&lt;td&gt;28.5M&lt;/td&gt;
&lt;td&gt;7.1&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.751&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.906&lt;/td&gt;
&lt;td&gt;0.823&lt;/td&gt;
&lt;td&gt;0.714&lt;/td&gt;
&lt;td&gt;0.820&lt;/td&gt;
&lt;td&gt;0.804&lt;/td&gt;
&lt;td&gt;0.943&lt;/td&gt;
&lt;td&gt;0.869&lt;/td&gt;
&lt;td&gt;0.762&lt;/td&gt;
&lt;td&gt;0.865&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flip test is used.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/drive/folders/1fRUDNUDxe9fjqcRZ2bnF_TKMlO0nB_dk?usp=sharing" rel="nofollow"&gt;Person detector has person AP of 56.4 on COCO val2017 dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;GFLOPs is for convolution and linear layers only.&lt;/li&gt;
&lt;li&gt;Batch Size is 24.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development-environment" class="anchor" aria-hidden="true" href="#development-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development environment&lt;/h2&gt;
&lt;p&gt;The code is developed using python 3.5 on Ubuntu 16.04. NVIDIA GPUs are needed. The code is developed and tested using 4 TITAN XP GPU cards. Other platforms or GPU cards are not fully tested.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1-preparation" class="anchor" aria-hidden="true" href="#1-preparation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Preparation&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-11-prepare-the-dataset" class="anchor" aria-hidden="true" href="#11-prepare-the-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.1 Prepare the dataset&lt;/h4&gt;
&lt;p&gt;For the MPII dataset, the original annotation files are in matlab format. We have converted them into json format, you also need to download them from &lt;a href="https://1drv.ms/f/s!AhIXJn_J-blW00SqrairNetmeVu4" rel="nofollow"&gt;OneDrive&lt;/a&gt; or &lt;a href="https://drive.google.com/drive/folders/1En_VqmStnsXMdldXA6qpqEyDQulnmS3a?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;.
Extract them under &lt;code&gt;{POSE_ROOT}/data&lt;/code&gt;, your directory tree should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;${POSE_ROOT}/data/mpii
├── images
└── mpii_human_pose_v1_u12_1.mat
|—— annot
|   |—— gt_valid.mat
└── |—— test.json
    |   |—— train.json
    |   |—— trainval.json
    |   |—— valid.json
    └── images
        |—— 000001163.jpg
        |—— 000003072.jpg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the COCO dataset, your directory tree should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;${POSE_ROOT}/data/coco
├── annotations
├── images
│   ├── test2017
│   ├── train2017
│   └── val2017
└── person_detection_results
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-12-prepare-the-pretrained-models" class="anchor" aria-hidden="true" href="#12-prepare-the-pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.2 Prepare the pretrained models&lt;/h3&gt;
&lt;p&gt;Your directory tree should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$HOME/models
├── pytorch
│   ├── imagenet
│   │   ├── hrnet_w32-36af842e.pth
│   │   ├── hrnet_w48-8ef0771d.pth
│   │   └── resnet50-19c8e357.pth
│   ├── pose_coco
│   │   ├── pose_hrnet_w32_256x192.pth
│   │   └── pose_hrnet_w48_256x192.pth
│   └── pose_mpii
│       ├── bs4_hourglass_128_4_1_16_0.00025_0_140_87.934_model_best.pth
│       ├── bs4_hourglass_256_8_1_16_0.00025_0_140_90.520_model_best.pth
│       ├── pose_hrnet_w32_256x256.pth
│       └── pose_hrnet_w48_256x256.pth
└── student_FPD
    ├── hourglass_student_FPD*.pth
    ├── hourglass_student_FPD.pth
    └── pose_hrnet_w32_student_FPD.pth
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-13-prepare-the-environment" class="anchor" aria-hidden="true" href="#13-prepare-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.3 Prepare the environment&lt;/h3&gt;
&lt;p&gt;Setting the parameters in the file &lt;code&gt;prepare_env.sh&lt;/code&gt; as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; DATASET_ROOT=$HOME/datasets&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; COCO_ROOT=${DATASET_ROOT}/MSCOCO&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; MPII_ROOT=${DATASET_ROOT}/MPII&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; MODELS_ROOT=${DATASET_ROOT}/models&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then execute:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash prepare_env.sh&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you like, you can &lt;a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch"&gt;&lt;strong&gt;prepare the environment step by step&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-2-how-to-train-the-model" class="anchor" aria-hidden="true" href="#2-how-to-train-the-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. How to train the model&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-21-download-the-pretrained-models-and-place-them-like-the-section-12" class="anchor" aria-hidden="true" href="#21-download-the-pretrained-models-and-place-them-like-the-section-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.1 Download the pretrained models and place them like the section 1.2&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;For MPII dataset&lt;/strong&gt;:  &lt;a href="https://drive.google.com/open?id=1jxL-O5TowVRCZ_xjO-PcmS7juZTYe74T" rel="nofollow"&gt;[GoogleDrive]&lt;/a&gt; &lt;a href="https://pan.baidu.com/s/1Mm1E1G1pYDJVBW2MJTQAUw" rel="nofollow"&gt;[BaiduDrive]&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;hourglass student model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hourglass teacher model&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;For COCO dataset&lt;/strong&gt;:  &lt;a href="https://drive.google.com/open?id=1q09w7iDj_mmIVcXb-pOeLKd-n3Y8g_kA" rel="nofollow"&gt;[GoogleDrive]&lt;/a&gt; &lt;a href="https://pan.baidu.com/s/1Mm1E1G1pYDJVBW2MJTQAUw" rel="nofollow"&gt;[BaiduDrive]&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;HRNet-W32 student model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HRNet-W48 teacher model&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-22-start-training" class="anchor" aria-hidden="true" href="#22-start-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.2 Start training&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; COCO dataset training&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; scripts/fpd_coco
bash run_train_hrnet.sh

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; MPII dataset training&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; scripts/fpd_mpii
bash run_train_hrnet.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using hrnet model&lt;/span&gt;
bash run_train_hg.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using hourglass model&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; General training methods, we also provide script shell&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; scripts/mpii
bash run_train_hrnet.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using hrnet model&lt;/span&gt;
bash run_train_hg.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using hourglass model&lt;/span&gt;
bash run_train_resnet.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using resnet model&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; scripts/coco
bash run_train_hrnet.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using hrnet model&lt;/span&gt;
bash run_train_hg.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using hourglass model&lt;/span&gt;
bash run_train_resnet.sh &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; using resnet model&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-3-how-to-test-the-model" class="anchor" aria-hidden="true" href="#3-how-to-test-the-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. How to test the model&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-31-download-the-trained-student-models-and-place-them-like-section-12" class="anchor" aria-hidden="true" href="#31-download-the-trained-student-models-and-place-them-like-section-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.1 Download the trained student models and place them like section 1.2&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://drive.google.com/open?id=1LRn-yEluOg4l4xjeUkslyOXYh8ljeSwP" rel="nofollow"&gt;[GoogleDrive]&lt;/a&gt; &lt;a href="https://pan.baidu.com/s/1Mm1E1G1pYDJVBW2MJTQAUw" rel="nofollow"&gt;[BaiduDrive]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For MPII dataset:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;hourglass student FPD model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For COCO dataset:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;HRNet-W32 student FPD model&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-32-fpd-training-results-and-logs" class="anchor" aria-hidden="true" href="#32-fpd-training-results-and-logs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.2 FPD training results and logs&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://drive.google.com/open?id=1FJcXP_V9IQb_sRc3bc1Kjd82OCzSJ_-n" rel="nofollow"&gt;[GoogleDrive]&lt;/a&gt; &lt;a href="https://pan.baidu.com/s/1Mm1E1G1pYDJVBW2MJTQAUw" rel="nofollow"&gt;[BaiduDrive]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;coco_hrnet_w48_fpd_w32_256x256: pose_hrnet_w32_student_FPD model training resutls.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;mpii_hourglass_8_256_fpd_hg_4_128_not_pretrained: hourglass_student_FPD* model training resutls.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;mpii_hourglass_8_256_fpd_hg_4_128_pretrained: hourglass_student_FPD model training resutls.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you use our code or models in your research, please cite with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@InProceedings{Zhang_2019_CVPR,
author = {Zhang, Feng and Zhu, Xiatian and Ye, Mao},
title = {Fast Human Pose Estimation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-discussion-forum" class="anchor" aria-hidden="true" href="#discussion-forum"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Discussion forum&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://www.ilovepose.cn" rel="nofollow"&gt;ILovePose&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-unoffical-implementations" class="anchor" aria-hidden="true" href="#unoffical-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unoffical implementations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/yuanyuanli85/Fast_Human_Pose_Estimation_Pytorch"&gt;Fast_Human_Pose_Estimation_Pytorch&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;Thanks for the open-source HRNet&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/"&gt;Deep High-Resolution Representation Learning for Human Pose Estimation, Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ilovepose</author><guid isPermaLink="false">https://github.com/ilovepose/fast-human-pose-estimation.pytorch</guid><pubDate>Fri, 24 Jan 2020 00:03:00 GMT</pubDate></item><item><title>baidu-research/warp-ctc #4 in Cuda, Today</title><link>https://github.com/baidu-research/warp-ctc</link><description>&lt;p&gt;&lt;i&gt;Fast parallel CTC.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/doc/baidu-research-logo-small.png"&gt;&lt;img src="/doc/baidu-research-logo-small.png" alt="Baidu Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="README.zh_cn.md"&gt;In Chinese 中文版&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-warp-ctc" class="anchor" aria-hidden="true" href="#warp-ctc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;warp-ctc&lt;/h1&gt;
&lt;p&gt;A fast parallel implementation of CTC, on both CPU and GPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.cs.toronto.edu/~graves/icml_2006.pdf" rel="nofollow"&gt;Connectionist Temporal Classification&lt;/a&gt;
is a loss function useful for performing supervised learning on sequence data,
without needing an alignment between input data and labels.  For example, CTC
can be used to train
&lt;a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" rel="nofollow"&gt;end-to-end&lt;/a&gt;
&lt;a href="http://arxiv.org/pdf/1408.2873v2.pdf" rel="nofollow"&gt;systems&lt;/a&gt; for
&lt;a href="http://arxiv.org/abs/1512.02595" rel="nofollow"&gt;speech recognition&lt;/a&gt;,
which is how we have been using it at Baidu's Silicon Valley AI Lab.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/doc/deep-speech-ctc-small.png"&gt;&lt;img src="/doc/deep-speech-ctc-small.png" alt="DSCTC" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The illustration above shows CTC computing the probability of an output
sequence "THE CAT ", as a sum over all possible alignments of input sequences
that could map to "THE CAT ", taking into account that labels may be duplicated
because they may stretch over several time steps of the input data (represented by
the spectrogram at the bottom of the image).
Computing the sum of all such probabilities explicitly would be prohibitively costly due to the
combinatorics involved, but CTC uses dynamic programming to dramatically
reduce the complexity of the computation. Because CTC is a differentiable function,
it can be used during standard SGD training of deep neural networks.&lt;/p&gt;
&lt;p&gt;In our lab, we focus on scaling up recurrent neural networks, and CTC loss is an
important component. To make our system efficient, we parallelized the CTC
algorithm, as described in &lt;a href="http://arxiv.org/abs/1512.02595" rel="nofollow"&gt;this paper&lt;/a&gt;.
This project contains our high performance CPU and CUDA versions of the CTC loss,
along with bindings for &lt;a href="http://torch.ch/" rel="nofollow"&gt;Torch&lt;/a&gt;.
The library provides a simple C interface, so that it is easy to
integrate into deep learning frameworks.&lt;/p&gt;
&lt;p&gt;This implementation has improved training scalability beyond the
performance improvement from a faster parallel CTC implementation. For
GPU-focused training pipelines, the ability to keep all data local to
GPU memory allows us to spend interconnect bandwidth on increased data
parallelism.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-performance" class="anchor" aria-hidden="true" href="#performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Performance&lt;/h2&gt;
&lt;p&gt;Our CTC implementation is efficient compared with many of the other publicly available implementations.  It is
also written to be as numerically stable as possible.  The algorithm is numerically sensitive and we have observed
catastrophic underflow even in double precision with the standard calculation - the result of division of
two numbers on the order of 1e-324 which should have been approximately one, instead become infinity
when the denominator underflowed to 0.  Instead, by performing the calculation in log space, it is numerically
stable even in single precision floating point at the cost of significantly more expensive operations.  Instead of
one machine instruction, addition requires the evaluation of multiple transcendental functions.  Because of this,
the speed of CTC implementations can only be fairly compared if they are both performing the calculation the same
way.&lt;/p&gt;
&lt;p&gt;We compare our performance with &lt;a href="https://github.com/srvk/eesen/commit/68f2bc2d46a5513cce3c232a645292632a1b08f9"&gt;Eesen&lt;/a&gt;,
a CTC implementation built on
&lt;a href="https://github.com/mohammadpz/CTC-Connectionist-Temporal-Classification/commit/904e8c72e15334887609d399254cf05a591d570f"&gt;Theano&lt;/a&gt;,
and a Cython CPU only implementation &lt;a href="https://github.com/amaas/stanford-ctc/commit/c8859897336a349b6c561d2bf2d179fae90b4d67"&gt;Stanford-CTC&lt;/a&gt;.
We benchmark the Theano implementation operating on 32-bit floating-point numbers and doing the calculation in log-space,
in order to match the other implementations we compare against.  Stanford-CTC was modified to perform the calculation
in log-space as it did not support it natively.  It also does not support minibatches larger than 1, so would require
an awkward memory layout to use in a real training pipeline, we assume linear increase in cost with minibatch size.&lt;/p&gt;
&lt;p&gt;We show results on two problem sizes relevant to our English and Mandarin end-to-end models, respectively, where &lt;em&gt;T&lt;/em&gt; represents the number of timesteps in the input to CTC, &lt;em&gt;L&lt;/em&gt; represents the length of the labels for each example, and &lt;em&gt;A&lt;/em&gt; represents the alphabet size.&lt;/p&gt;
&lt;p&gt;On the GPU, our performance at a minibatch of 64 examples ranges from 7x faster to 155x faster than Eesen, and 46x to 68x faster than the Theano implementation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gpu-performance" class="anchor" aria-hidden="true" href="#gpu-performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPU Performance&lt;/h3&gt;
&lt;p&gt;Benchmarked on a single NVIDIA Titan X GPU.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;T&lt;/em&gt;=150, &lt;em&gt;L&lt;/em&gt;=40, &lt;em&gt;A&lt;/em&gt;=28&lt;/th&gt;
&lt;th&gt;warp-ctc&lt;/th&gt;
&lt;th&gt;Eesen&lt;/th&gt;
&lt;th&gt;Theano&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=1&lt;/td&gt;
&lt;td&gt;3.1 ms&lt;/td&gt;
&lt;td&gt;.5 ms&lt;/td&gt;
&lt;td&gt;67 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=16&lt;/td&gt;
&lt;td&gt;3.2 ms&lt;/td&gt;
&lt;td&gt;6  ms&lt;/td&gt;
&lt;td&gt;94 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=32&lt;/td&gt;
&lt;td&gt;3.2 ms&lt;/td&gt;
&lt;td&gt;12 ms&lt;/td&gt;
&lt;td&gt;119 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=64&lt;/td&gt;
&lt;td&gt;3.3 ms&lt;/td&gt;
&lt;td&gt;24 ms&lt;/td&gt;
&lt;td&gt;153 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=128&lt;/td&gt;
&lt;td&gt;3.5 ms&lt;/td&gt;
&lt;td&gt;49 ms&lt;/td&gt;
&lt;td&gt;231 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;T&lt;/em&gt;=150, &lt;em&gt;L&lt;/em&gt;=20, &lt;em&gt;A&lt;/em&gt;=5000&lt;/th&gt;
&lt;th&gt;warp-ctc&lt;/th&gt;
&lt;th&gt;Eesen&lt;/th&gt;
&lt;th&gt;Theano&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=1&lt;/td&gt;
&lt;td&gt;7 ms&lt;/td&gt;
&lt;td&gt;40   ms&lt;/td&gt;
&lt;td&gt;120 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=16&lt;/td&gt;
&lt;td&gt;9 ms&lt;/td&gt;
&lt;td&gt;619  ms&lt;/td&gt;
&lt;td&gt;385 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=32&lt;/td&gt;
&lt;td&gt;11 ms&lt;/td&gt;
&lt;td&gt;1238 ms&lt;/td&gt;
&lt;td&gt;665 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=64&lt;/td&gt;
&lt;td&gt;16 ms&lt;/td&gt;
&lt;td&gt;2475 ms&lt;/td&gt;
&lt;td&gt;1100 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=128&lt;/td&gt;
&lt;td&gt;23 ms&lt;/td&gt;
&lt;td&gt;4950 ms&lt;/td&gt;
&lt;td&gt;2100 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-cpu-performance" class="anchor" aria-hidden="true" href="#cpu-performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CPU Performance&lt;/h3&gt;
&lt;p&gt;Benchmarked on a dual-socket machine with two Intel E5-2660 v3
processors - warp-ctc used 40 threads to maximally take advantage of the CPU resources.
Eesen doesn't provide a CPU implementation. We noticed that the Theano implementation was not
parallelizing computation across multiple threads.  Stanford-CTC provides no mechanism
for parallelization across threads.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;T&lt;/em&gt;=150, &lt;em&gt;L&lt;/em&gt;=40, &lt;em&gt;A&lt;/em&gt;=28&lt;/th&gt;
&lt;th&gt;warp-ctc&lt;/th&gt;
&lt;th&gt;Stanford-CTC&lt;/th&gt;
&lt;th&gt;Theano&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=1&lt;/td&gt;
&lt;td&gt;2.6 ms&lt;/td&gt;
&lt;td&gt;13 ms&lt;/td&gt;
&lt;td&gt;15 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=16&lt;/td&gt;
&lt;td&gt;3.4 ms&lt;/td&gt;
&lt;td&gt;208 ms&lt;/td&gt;
&lt;td&gt;180 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=32&lt;/td&gt;
&lt;td&gt;3.9 ms&lt;/td&gt;
&lt;td&gt;416 ms&lt;/td&gt;
&lt;td&gt;375 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=64&lt;/td&gt;
&lt;td&gt;6.6 ms&lt;/td&gt;
&lt;td&gt;832 ms&lt;/td&gt;
&lt;td&gt;700 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=128&lt;/td&gt;
&lt;td&gt;12.2 ms&lt;/td&gt;
&lt;td&gt;1684 ms&lt;/td&gt;
&lt;td&gt;1340 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;em&gt;T&lt;/em&gt;=150, &lt;em&gt;L&lt;/em&gt;=20, &lt;em&gt;A&lt;/em&gt;=5000&lt;/th&gt;
&lt;th&gt;warp-ctc&lt;/th&gt;
&lt;th&gt;Stanford-CTC&lt;/th&gt;
&lt;th&gt;Theano&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=1&lt;/td&gt;
&lt;td&gt;21 ms&lt;/td&gt;
&lt;td&gt;31 ms&lt;/td&gt;
&lt;td&gt;850 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=16&lt;/td&gt;
&lt;td&gt;37 ms&lt;/td&gt;
&lt;td&gt;496 ms&lt;/td&gt;
&lt;td&gt;10800 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=32&lt;/td&gt;
&lt;td&gt;54 ms&lt;/td&gt;
&lt;td&gt;992 ms&lt;/td&gt;
&lt;td&gt;22000 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=64&lt;/td&gt;
&lt;td&gt;101 ms&lt;/td&gt;
&lt;td&gt;1984 ms&lt;/td&gt;
&lt;td&gt;42000 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;N&lt;/em&gt;=128&lt;/td&gt;
&lt;td&gt;184 ms&lt;/td&gt;
&lt;td&gt;3968 ms&lt;/td&gt;
&lt;td&gt;86000 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-interface" class="anchor" aria-hidden="true" href="#interface"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interface&lt;/h2&gt;
&lt;p&gt;The interface is in &lt;a href="include/ctc.h"&gt;&lt;code&gt;include/ctc.h&lt;/code&gt;&lt;/a&gt;.
It supports CPU or GPU execution, and you can specify OpenMP parallelism
if running on the CPU, or the CUDA stream if running on the GPU. We
took care to ensure that the library does not perform memory
allocation internally, in order to avoid synchronizations and
overheads caused by memory allocation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-compilation" class="anchor" aria-hidden="true" href="#compilation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Compilation&lt;/h2&gt;
&lt;p&gt;warp-ctc has been tested on Ubuntu 14.04 and OSX 10.10.  Windows is not supported
at this time.&lt;/p&gt;
&lt;p&gt;First get the code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/baidu-research/warp-ctc.git
cd warp-ctc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;create a build directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir build
cd build
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if you have a non standard CUDA install &lt;code&gt;export CUDA_BIN_PATH=/path_to_cuda&lt;/code&gt; so that CMake detects CUDA and
to ensure Torch is detected, make sure &lt;code&gt;th&lt;/code&gt; is in &lt;code&gt;$PATH&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;run cmake and build:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cmake ../
make
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The C library and torch shared libraries should now be built along with test
executables.  If CUDA was detected, then &lt;code&gt;test_gpu&lt;/code&gt; will be built; &lt;code&gt;test_cpu&lt;/code&gt;
will always be built.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h2&gt;
&lt;p&gt;To run the tests, make sure the CUDA libraries are in &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; (&lt;code&gt;DYLD_LIBRARY_PATH&lt;/code&gt; for OSX).&lt;/p&gt;
&lt;p&gt;The Torch tests must be run from the &lt;code&gt;torch_binding/tests/&lt;/code&gt; directory.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-torch-installation" class="anchor" aria-hidden="true" href="#torch-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Torch Installation&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;luarocks make torch_binding/rocks/warp-ctc-scm-1.rockspec&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can also install without cloning the repository using&lt;/p&gt;
&lt;p&gt;&lt;code&gt;luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;There is a Torch CTC &lt;a href="torch_binding/TUTORIAL.md"&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We welcome improvements from the community, please feel free to submit pull
requests.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues---limitations" class="anchor" aria-hidden="true" href="#known-issues---limitations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known Issues  / Limitations&lt;/h2&gt;
&lt;p&gt;The CUDA implementation requires a device of at least compute capability 3.0.&lt;/p&gt;
&lt;p&gt;The CUDA implementation supports a maximum label length of 639 (timesteps are
unlimited).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>baidu-research</author><guid isPermaLink="false">https://github.com/baidu-research/warp-ctc</guid><pubDate>Fri, 24 Jan 2020 00:04:00 GMT</pubDate></item></channel></rss>