<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Cuda, Today</title><link>https://github.com/trending/cuda?since=daily</link><description>The top repositories on GitHub for cuda, measured daily</description><pubDate>Mon, 20 Jan 2020 01:06:24 GMT</pubDate><lastBuildDate>Mon, 20 Jan 2020 01:06:24 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>CannyLab/tsne-cuda #1 in Cuda, Today</title><link>https://github.com/CannyLab/tsne-cuda</link><description>&lt;p&gt;&lt;i&gt;GPU Accelerated t-SNE for CUDA with Python bindings&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tsne-cuda" class="anchor" aria-hidden="true" href="#tsne-cuda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TSNE-CUDA&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/CannyLab/tsne-cuda" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ebcec8c3c030e4b1ef0d77bbcf780a904df1ed9e/68747470733a2f2f7472617669732d63692e6f72672f43616e6e794c61622f74736e652d637564612e7376673f6272616e63683d646f636b65722d6275696c64" alt="Build Status" data-canonical-src="https://travis-ci.org/CannyLab/tsne-cuda.svg?branch=docker-build" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repo is an optimized CUDA version of &lt;a href="https://github.com/KlugerLab/FIt-SNE"&gt;FIt-SNE algorithm&lt;/a&gt; with associated python modules. We find that our implementation of t-SNE can be up to 1200x faster than Sklearn, or up to 50x faster than Multicore-TSNE when used with the right GPU. The paper describing our approach, as well as the results below, is available at &lt;a href="https://arxiv.org/abs/1807.11824" rel="nofollow"&gt;https://arxiv.org/abs/1807.11824&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can install binaries with anaconda for CUDA versions 9.0, 9.2, 10.0, and 10.1 using &lt;code&gt;conda install cuda&amp;lt;major&amp;gt;&amp;lt;minor&amp;gt; tsnecuda -c cannylab&lt;/code&gt;. For more details or to install from source, check out our wiki: &lt;a href="https://github.com/CannyLab/tsne-cuda/wiki/"&gt;https://github.com/CannyLab/tsne-cuda/wiki/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-benchmarks" class="anchor" aria-hidden="true" href="#benchmarks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarks&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-simulated-data" class="anchor" aria-hidden="true" href="#simulated-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Simulated Data&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/simulated_speedup.png"&gt;&lt;img src="docs/simulated_speedup.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Time taken compared to other state of the art algorithms on synthetic datasets with 50 dimensions and four clusters for varying numbers of points. Note the log scale on both the points and time axis, and that the scale of the x-axis is in thousands of points (thus, the values on the x-axis range from 1K to 10M points. Dashed lines on SkLearn, BH-TSNE, and MULTICORE-4 represent projected times. Projected scaling assumes an O(nlog(n)) implementation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-mnist" class="anchor" aria-hidden="true" href="#mnist"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MNIST&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/mnist_speedup.png"&gt;&lt;img src="docs/mnist_speedup.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The performance of t-SNE-CUDA compared to other state-of-the-art implementations on the MNIST dataset. t-SNE-CUDA runs on the raw pixels of the MNIST dataset (60000 images x 768 dimensions) in under 7 seconds.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-cifar" class="anchor" aria-hidden="true" href="#cifar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CIFAR&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/cifar_speedup.png"&gt;&lt;img src="docs/cifar_speedup.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The performance of t-SNE-CUDA compared to other state-of-the-art implementations on the CIFAR-10 dataset. t-SNE-CUDA runs on the output of a classifier on the CIFAR-10 training set (50000 images x 1024 dimensions) in under 6 seconds. While we can run on the full pixel set in under 12 seconds, Euclidean distance is a poor metric in raw pixel space leading to poor quality embeddings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-comparison-of-embedding-quality" class="anchor" aria-hidden="true" href="#comparison-of-embedding-quality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Comparison of Embedding Quality&lt;/h3&gt;
&lt;p&gt;The quality of the embeddings produced by t-SNE-CUDA do not differ significantly from the state of the art implementations. See below for a comparison of MNIST cluster outputs.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/mnist_comparison.jpg"&gt;&lt;img src="docs/mnist_comparison.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Left: MULTICORE-4 (501s), Middle: BH-TSNE (1156s), Right: t-SNE-CUDA (Ours, 6.98s).&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;To install our library, follow the instructions in the &lt;a href="https://github.com/CannyLab/tsne-cuda/wiki/Installation"&gt;installation section&lt;/a&gt; of the wiki.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run" class="anchor" aria-hidden="true" href="#run"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run&lt;/h3&gt;
&lt;p&gt;Like many of the libraries available, the python wrappers subscribe to the same API as &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="nofollow"&gt;sklearn.manifold.TSNE&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can run it as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from tsnecuda import TSNE
X_embedded = TSNE(n_components=2, perplexity=15, learning_rate=10).fit_transform(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We only support &lt;code&gt;n_components=2&lt;/code&gt;. We currently have no plans to support more dimensions as this requires significant changes to the code to accomodate.&lt;/p&gt;
&lt;p&gt;For more information on running the library, or using it as a C++ library, see the &lt;a href="https://github.com/CannyLab/tsne-cuda/wiki/Basic-Usage:-Python"&gt;Python usage&lt;/a&gt; or &lt;a href="https://github.com/CannyLab/tsne-cuda/wiki/Basic-Usage:-Cxx"&gt;C++ Usage&lt;/a&gt; sections of the wiki.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;Please cite the corresponding paper if it was useful for your research:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{chan2019gpu,
  title={GPU accelerated t-distributed stochastic neighbor embedding},
  author={Chan, David M and Rao, Roshan and Huang, Forrest and Canny, John F},
  journal={Journal of Parallel and Distributed Computing},
  volume={131},
  pages={1--13},
  year={2019},
  publisher={Elsevier}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This library is built on top of the following technology, without this tech, none of this would be possible!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf" rel="nofollow"&gt;L. Van der Maaten's paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/KlugerLab/FIt-SNE"&gt;FIt-SNE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/DmitryUlyanov/Multicore-TSNE"&gt;Multicore-TSNE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/lvdmaaten/bhtsne/"&gt;BHTSNE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/OrangeOwlSolutions"&gt;CUDA Utilities/Pairwise Distance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://iss.ices.utexas.edu/?p=projects/galois/lonestargpu" rel="nofollow"&gt;LONESTAR-GPU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/facebookresearch/faiss"&gt;FAISS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/googletest"&gt;GTest&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jarro2783/cxxopts"&gt;CXXopts&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;Our code is built using components from FAISS, the Lonestar GPU library, GTest, CXXopts, and OrangeOwl's CUDA utilities. Each portion of the code is governed by their respective licenses - however our code is governed by the BSD-3 license found in LICENSE.txt&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CannyLab</author><guid isPermaLink="false">https://github.com/CannyLab/tsne-cuda</guid><pubDate>Mon, 20 Jan 2020 00:01:00 GMT</pubDate></item><item><title>hujie-frank/SENet #2 in Cuda, Today</title><link>https://github.com/hujie-frank/SENet</link><description>&lt;p&gt;&lt;i&gt;Squeeze-and-Excitation Networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-squeeze-and-excitation-networks-paper" class="anchor" aria-hidden="true" href="#squeeze-and-excitation-networks-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Squeeze-and-Excitation Networks &lt;sub&gt;(&lt;a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" rel="nofollow"&gt;paper&lt;/a&gt;)&lt;/sub&gt;&lt;/h1&gt;
&lt;p&gt;By Jie Hu&lt;sup&gt;[1]&lt;/sup&gt;, Li Shen&lt;sup&gt;[2]&lt;/sup&gt;, Gang Sun&lt;sup&gt;[1]&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://momenta.ai/" rel="nofollow"&gt;Momenta&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; and &lt;a href="http://www.robots.ox.ac.uk/~vgg/" rel="nofollow"&gt;University of Oxford&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-approach" class="anchor" aria-hidden="true" href="#approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Approach&lt;/h2&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-pipeline.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-pipeline.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align="center"&gt;
  Figure 1: Diagram of a Squeeze-and-Excitation building block.
&lt;/p&gt;
&lt;div align="center"&gt;
   &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-Inception-module.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-Inception-module.jpg" width="420" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet/blob/master/figures/SE-ResNet-module.jpg"&gt;&lt;img src="https://github.com/hujie-frank/SENet/raw/master/figures/SE-ResNet-module.jpg" width="420" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align="center"&gt;
  Figure 2: Schema of SE-Inception and SE-ResNet modules. We set r=16 in all our models.
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-implementation" class="anchor" aria-hidden="true" href="#implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementation&lt;/h2&gt;
&lt;p&gt;In this repository, Squeeze-and-Excitation Networks are implemented by &lt;a href="https://github.com/BVLC/caffe"&gt;Caffe&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-augmentation" class="anchor" aria-hidden="true" href="#augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Augmentation&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Method&lt;/th&gt;
&lt;th align="center"&gt;Settings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Mirror&lt;/td&gt;
&lt;td align="center"&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Crop&lt;/td&gt;
&lt;td align="center"&gt;8% ~ 100%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Aspect Ratio&lt;/td&gt;
&lt;td align="center"&gt;3/4 ~ 4/3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Random Rotation&lt;/td&gt;
&lt;td align="center"&gt;-10° ~ 10°&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Pixel Jitter&lt;/td&gt;
&lt;td align="center"&gt;-20 ~ 20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-note" class="anchor" aria-hidden="true" href="#note"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To achieve efficient training and testing, we combine the consecutive operations &lt;em&gt;&lt;strong&gt;channel-wise scale&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;element-wise summation&lt;/strong&gt;&lt;/em&gt; into a single layer &lt;strong&gt;"Axpy"&lt;/strong&gt; in the architectures with skip-connections, resulting in a considerable reduction in memory cost and computational burden.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, we found that the implementation for &lt;em&gt;&lt;strong&gt;global average pooling&lt;/strong&gt;&lt;/em&gt; on GPU supported by cuDNN and BVLC/caffe is less efficient. In this regard, we re-implement the operation which achieves significant acceleration.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-trained-models" class="anchor" aria-hidden="true" href="#trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trained Models&lt;/h2&gt;
&lt;p&gt;Table 1. Single crop validation error on ImageNet-1k (center 224x224 crop from resized image with shorter side = 256). The SENet-154 is one of our superior models used in &lt;a href="http://image-net.org/challenges/LSVRC/2017/index" rel="nofollow"&gt;ILSVRC 2017 Image Classification Challenge&lt;/a&gt; where we won the 1st place (Team name: &lt;a href="http://image-net.org/challenges/LSVRC/2017/results" rel="nofollow"&gt;WMW&lt;/a&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;Top-1&lt;/th&gt;
&lt;th align="center"&gt;Top-5&lt;/th&gt;
&lt;th align="center"&gt;Size&lt;/th&gt;
&lt;th align="center"&gt;Caffe Model&lt;/th&gt;
&lt;th align="center"&gt;Caffe Model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-BN-Inception&lt;/td&gt;
&lt;td align="center"&gt;23.62&lt;/td&gt;
&lt;td align="center"&gt;7.04&lt;/td&gt;
&lt;td align="center"&gt;46 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlTWRRbDZYbVB2WWc/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1qYoPdak" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-50&lt;/td&gt;
&lt;td align="center"&gt;22.37&lt;/td&gt;
&lt;td align="center"&gt;6.36&lt;/td&gt;
&lt;td align="center"&gt;107 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlS2QwZHFzM3RjNzg/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1gf5wsLl" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-101&lt;/td&gt;
&lt;td align="center"&gt;21.75&lt;/td&gt;
&lt;td align="center"&gt;5.72&lt;/td&gt;
&lt;td align="center"&gt;189 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlTEg4YmcwQ0FoZFU/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1c1FvCWg" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNet-152&lt;/td&gt;
&lt;td align="center"&gt;21.34&lt;/td&gt;
&lt;td align="center"&gt;5.54&lt;/td&gt;
&lt;td align="center"&gt;256 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlcFE0Q2NTcWl3WUE/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1dFEnSzR" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNeXt-50 (32 x 4d)&lt;/td&gt;
&lt;td align="center"&gt;20.97&lt;/td&gt;
&lt;td align="center"&gt;5.54&lt;/td&gt;
&lt;td align="center"&gt;105 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlQ2Z0Q204V1RITjA/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1dFbEmbv" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SE-ResNeXt-101 (32 x 4d)&lt;/td&gt;
&lt;td align="center"&gt;19.81&lt;/td&gt;
&lt;td align="center"&gt;4.96&lt;/td&gt;
&lt;td align="center"&gt;187 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWleklsNzBiZlprblk/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1qY2wjt6" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;SENet-154&lt;/td&gt;
&lt;td align="center"&gt;18.68&lt;/td&gt;
&lt;td align="center"&gt;4.47&lt;/td&gt;
&lt;td align="center"&gt;440 M&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://drive.google.com/file/d/0BwHV3BlNKkWlbTFZbzFTSXBUTUE/view?usp=sharing" rel="nofollow"&gt;GoogleDrive&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="https://pan.baidu.com/s/1o7HdfAE" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here we obtain better performance than those reported in the paper.
We re-train the SENets described in the paper on a single GPU server with 8 NVIDIA Titan X cards, using a mini-batch of 256 and a initial learning rate of 0.1 with more epoches.
In contrast, the results reported in the paper were obtained by training the networks with a larger batch size (1024) and learning rate (0.6) across 4 servers.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-third-party-re-implementations" class="anchor" aria-hidden="true" href="#third-party-re-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Third-party re-implementations&lt;/h2&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Caffe. SE-mudolues are integrated with a modificated ResNet-50 using a stride 2 in the 3x3 convolution instead of the first 1x1 convolution which obtains better performance: &lt;a href="https://github.com/shicai/SENet-Caffe"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;TensorFlow. SE-modules are integrated with a pre-activation ResNet-50 which follows the setup in &lt;a href="https://github.com/facebook/fb.resnet.torch"&gt;fb.resnet.torch&lt;/a&gt;: &lt;a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;TensorFlow. Simple Tensorflow implementation of SENets using Cifar10: &lt;a href="https://github.com/taki0112/SENet-Tensorflow"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MatConvNet. All the released SENets are imported into &lt;a href="https://github.com/vlfeat/matconvnet"&gt;MatConvNet&lt;/a&gt;: &lt;a href="https://github.com/albanie/mcnSENets"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MXNet. SE-modules are integrated with the ResNeXt and more architectures are coming soon: &lt;a href="https://github.com/bruinxiong/SENet.mxnet"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;PyTorch. Implementation of SENets by PyTorch: &lt;a href="https://github.com/moskomule/senet.pytorch"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Chainer. Implementation of SENets by Chainer: &lt;a href="https://github.com/nutszebra/SENets"&gt;Repository&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use Squeeze-and-Excitation Networks in your research, please cite the paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{hu2018senet,
  title={Squeeze-and-Excitation Networks},
  author={Jie Hu and Li Shen and Gang Sun},
  journal={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hujie-frank</author><guid isPermaLink="false">https://github.com/hujie-frank/SENet</guid><pubDate>Mon, 20 Jan 2020 00:02:00 GMT</pubDate></item></channel></rss>