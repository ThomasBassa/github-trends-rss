<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, Today</title><link>https://github.com/trending/jupyter-notebook?since=daily</link><description>The top repositories on GitHub for jupyter-notebook, measured daily</description><pubDate>Sun, 05 Jan 2020 01:06:55 GMT</pubDate><lastBuildDate>Sun, 05 Jan 2020 01:06:55 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>Pierian-Data/Complete-Python-3-Bootcamp #1 in Jupyter Notebook, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Sun, 05 Jan 2020 00:01:00 GMT</pubDate></item><item><title>DataScienceResearchPeru/OpenSource-RoadMap-DataScience #2 in Jupyter Notebook, Today</title><link>https://github.com/DataScienceResearchPeru/OpenSource-RoadMap-DataScience</link><description>&lt;p&gt;&lt;i&gt;춰Camino a una educaci칩n autodidacta en Ciencia de Datos!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt; 
&lt;a target="_blank" rel="noopener noreferrer" href="images/foto-github.png"&gt;&lt;img src="images/foto-github.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-open-source-road-map-data-science" class="anchor" aria-hidden="true" href="#open-source-road-map-data-science"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Open Source Road Map Data Science&lt;/h3&gt;
&lt;p align="center"&gt;
  &lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;游늵&lt;/g-emoji&gt; 춰Camino a una educaci칩n autodidacta en &lt;strong&gt;Data Science&lt;/strong&gt;!
  &lt;br&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-contenido" class="anchor" aria-hidden="true" href="#star-contenido"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Contenido&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#C%C3%B3mo-contribuir"&gt;C칩mo contribuir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#De-qu%C3%A9-trata-esto"&gt;De qu칠 trata esto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conviertete-en-un-estudiante-de-data-science-research-per%C3%BA"&gt;Conviertete en un estudiante de Data Science Research Per칰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Motivaci%C3%B3n-y-Preparaci%C3%B3n"&gt;Motivaci칩n y Preparaci칩n&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#curr%C3%ADcula"&gt;Curr칤cula&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#c%C3%B3mo-usar-est%C3%A1-gu%C3%ADa"&gt;C칩mo usar est치 gu칤a&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-star-c칩mo-contribuir" class="anchor" aria-hidden="true" href="#star-c칩mo-contribuir"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; C칩mo contribuir&lt;/h2&gt;
&lt;p&gt;El objetivo de este repositorio es contribuir a la formaci칩n de los profesionales interesados en Ciencia de Datos e Inteligencia Artificial.
Esto ayudar치 a incrementar los profesionales peruanos e hispanohablantes y as칤 tener mas Data Scientist, Data Engineer, Machine Learning Engineer, Data Architects y dem치s perfiles existentes.
Puede hacer un Pull Request y agregar m치s contenido que crea necesario.
Aqu칤 un &lt;a href="https://blog.desdelinux.net/tutorial-simple-primer-pr-pull-request/" rel="nofollow"&gt;Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-de-qu칠-trata-esto" class="anchor" aria-hidden="true" href="#star-de-qu칠-trata-esto"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; De qu칠 trata esto&lt;/h2&gt;
&lt;p&gt;Esto es un camino s칩lido para aquellos que desean completar un curso de Ciencia de datos en su propio tiempo,
con cursos de las &lt;strong&gt;mejores universidades&lt;/strong&gt; en el mundo. En nuestro plan de estudios, damos preferencia a los
cursos de estilo MOOC (Massive Open Online Course) porque estos cursos se crearon teniendo en cuenta nuestro
estilo de aprendizaje.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-conviertete-en-un-estudiante-de-data-science-research-per칰" class="anchor" aria-hidden="true" href="#star-conviertete-en-un-estudiante-de-data-science-research-per칰"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Conviertete en un estudiante de Data Science Research Per칰&lt;/h2&gt;
&lt;p&gt;Pueden enviarnos sugerencias y unirse a nuestros grupos de WhatsApp mediante el siguiente &lt;a href="https://wa.me/51931534817" rel="nofollow"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[Pendiente]&lt;/strong&gt; Para inscribirse oficialmente en este curso, debe crear un perfil en nuestra &lt;a href="https://datascience.pe" rel="nofollow"&gt;web&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-motivaci칩n-y-preparaci칩n" class="anchor" aria-hidden="true" href="#star-motivaci칩n-y-preparaci칩n"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Motivaci칩n y Preparaci칩n&lt;/h2&gt;
&lt;p&gt;Aqu칤 hay dos enlaces interesantes que pueden marcar &lt;strong&gt;toda&lt;/strong&gt; la diferencia en su viaje.&lt;/p&gt;
&lt;p&gt;El primero es un video motivacional que muestra a un chico que pas칩 por el "Desaf칤o MIT",
que consiste en aprender todo el curr칤culo MIT &lt;strong&gt;de 4 a침os&lt;/strong&gt; para Ciencias de la Computaci칩n en &lt;strong&gt;1 a침o&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.scotthyoung.com/blog/myprojects/mit-challenge-2/" rel="nofollow"&gt;MIT Challenge&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El segundo enlace es un MOOC que le ense침ar치 t칠cnicas de aprendizaje utilizadas por expertos en arte, m칰sica,
literatura, matem치ticas, ciencias, deportes y muchas otras disciplinas. Estas son &lt;strong&gt;habilidades fundamentales&lt;/strong&gt;
para tener 칠xito.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/learning-how-to-learn" rel="nofollow"&gt;Learning How to Learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;쮼stas listo para empezar?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-curr칤cula" class="anchor" aria-hidden="true" href="#star-curr칤cula"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Curr칤cula&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#algebra-lineal"&gt;Algebra Lineal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#calculo"&gt;C치lculo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#calculo-multivariable"&gt;C치lculo multivariable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#base-de-datos"&gt;Base de Datos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilidad-y-estadistica"&gt;Probabilidad y Estad칤stica&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#introduccion-a-la-ciencia-de-datos"&gt;Introducci칩n a la Ciencia de Datos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#machine-learning"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimizacion-convexa"&gt;Optimizaci칩n Convexa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#manipulacion-y-recuperacion-de-datos"&gt;Manipulaci칩n y recuperaci칩n de datos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#big-data"&gt;Big Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-language-processing"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#especializacion"&gt;Especializaci칩n&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-star-algebra-lineal" class="anchor" aria-hidden="true" href="#star-algebra-lineal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Algebra Lineal&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/linear-algebra-foundations-to-frontiers" rel="nofollow"&gt;Algebra Lineal - Fundamentos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;15 semanas&lt;/td&gt;
&lt;td align="center"&gt;8 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/advanced-linear-algebra-foundations-to-frontiers" rel="nofollow"&gt;Algebra Lineal - Avanzado&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;15 semanas&lt;/td&gt;
&lt;td align="center"&gt;8 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/applications-linear-algebra-part-1-davidsonx-d003x-1" rel="nofollow"&gt;Aplicaciones de 츼lgebra Lineal Part 1&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;4 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/applications-linear-algebra-part-2-davidsonx-d003x-2" rel="nofollow"&gt;Aplicaciones de 츼lgebra Lineal Part 2&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 semanas&lt;/td&gt;
&lt;td align="center"&gt;5 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-c치lculo" class="anchor" aria-hidden="true" href="#star-c치lculo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; C치lculo&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/calculus-1a-differentiation-mitx-18-01-1x" rel="nofollow"&gt;Calculus 1A: Diferenciaci칩n&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;13 semanas&lt;/td&gt;
&lt;td align="center"&gt;6-10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/calculus-1b-integration-mitx-18-01-2x" rel="nofollow"&gt;Calculus 1B: Integraci칩n&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;13 semanas&lt;/td&gt;
&lt;td align="center"&gt;5-10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/calculus-1c-coordinate-systems-infinite-mitx-18-01-3x" rel="nofollow"&gt;Calculus 1C: Sistemas de coordenadas y series infinitas&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;13 semanas&lt;/td&gt;
&lt;td align="center"&gt;6-10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-c치lculo-multivariable" class="anchor" aria-hidden="true" href="#star-c치lculo-multivariable"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; C치lculo multivariable&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/index.htm" rel="nofollow"&gt;MIT C치lculo multivariable&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;15 semanas&lt;/td&gt;
&lt;td align="center"&gt;8 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;MIT&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-base-de-datos" class="anchor" aria-hidden="true" href="#star-base-de-datos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Base de Datos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about" rel="nofollow"&gt;Curso Base de Datos de Stanford&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;8-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Stanford&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-probabilidad-y-estad칤stica" class="anchor" aria-hidden="true" href="#star-probabilidad-y-estad칤stica"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Probabilidad y Estad칤stica&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-probability-science-mitx-6-041x-1#.U3yb762SzIo" rel="nofollow"&gt;Introducci칩n a la Probabilidad&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;16 semanas&lt;/td&gt;
&lt;td align="center"&gt;12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/OLI/StatReasoning/Open/about" rel="nofollow"&gt;Razonamiento Estad칤stico&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Standford&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-statistics-descriptive-uc-berkeleyx-stat2-1x" rel="nofollow"&gt;Introducci칩n a la Estad칤stica: Descriptiva&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-statistics-probability-uc-berkeleyx-stat2-2x" rel="nofollow"&gt;Introducci칩n a la Estad칤stica: Probabil칤stica&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-statistics-inference-uc-berkeleyx-stat2-3x" rel="nofollow"&gt;Introducci칩n a la Estad칤stica: Inferencia&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-python" class="anchor" aria-hidden="true" href="#star-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Python&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-7" rel="nofollow"&gt;Introducci칩n a Computer Science y Programaci칩n usando Python&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;9 semanas&lt;/td&gt;
&lt;td align="center"&gt;15 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-computational-thinking-data-mitx-6-00-2x-3" rel="nofollow"&gt;Introducci칩n al Pensamiento Computacional y Ciencia de Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;10 semanas&lt;/td&gt;
&lt;td align="center"&gt;15 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://prod-edx-mktg-edit.edx.org/course/introduction-python-data-science-microsoft-dat208x-1" rel="nofollow"&gt;Introducci칩n a Python para Ciencia&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;6 semanas&lt;/td&gt;
&lt;td align="center"&gt;2-4 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/programming-python-data-science-microsoft-dat210x" rel="nofollow"&gt;Programaci칩n con Python para Ciencia de Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;6 semanas&lt;/td&gt;
&lt;td align="center"&gt;3-4 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-r" class="anchor" aria-hidden="true" href="#star-r"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; R&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/specializations/data-science-foundations-r" rel="nofollow"&gt;Programa especializado Data Science: Foundations using R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 meses&lt;/td&gt;
&lt;td align="center"&gt;6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/learn/r-programming" rel="nofollow"&gt;Programaci칩n R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;2 semanas&lt;/td&gt;
&lt;td align="center"&gt;10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/specializations/statistics" rel="nofollow"&gt;Programa especializado Statistics with R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;7 meses&lt;/td&gt;
&lt;td align="center"&gt;5 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/learn/intro-data-science-programacion-estadistica-r" rel="nofollow"&gt;Introducci칩n a Data Science: Programaci칩n Estad칤stica con R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 semanas&lt;/td&gt;
&lt;td align="center"&gt;3-5 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-introducci칩n-a-la-ciencia-de-datos" class="anchor" aria-hidden="true" href="#star-introducci칩n-a-la-ciencia-de-datos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Introducci칩n a la Ciencia de Datos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/course/datasci" rel="nofollow"&gt;Introducci칩n a la Ciencia de Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;8 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://cs109.github.io/2015/" rel="nofollow"&gt;Ciencia de Datos - CS109 de Harvard&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;12 semanas&lt;/td&gt;
&lt;td align="center"&gt;5-6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Harvard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/analytics-edge-mitx-15-071x-2" rel="nofollow"&gt;La Ventaja de Anal칤tica&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;12 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-15 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-machine-learning" class="anchor" aria-hidden="true" href="#star-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Machine Learning&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/learning-data-introductory-machine-caltechx-cs1156x" rel="nofollow"&gt;Introducci칩n a Machine Learning&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;10 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-20 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://work.caltech.edu/lectures.html" rel="nofollow"&gt;Aprendiendo de los Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;10 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-20 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;California Institute of Technology&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about" rel="nofollow"&gt;Aprendizaje estad칤stico&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;3 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Standford&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;Curso Machine Learning de Stanford&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;8-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-optimizaci칩n-convexa" class="anchor" aria-hidden="true" href="#star-optimizaci칩n-convexa"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Optimizaci칩n Convexa&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about" rel="nofollow"&gt;Optimizaci칩n Convexa&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;9 semanas&lt;/td&gt;
&lt;td align="center"&gt;10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Standford&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-manipulaci칩n-y-recuperaci칩n-de-datos" class="anchor" aria-hidden="true" href="#star-manipulaci칩n-y-recuperaci칩n-de-datos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Manipulaci칩n y recuperaci칩n de datos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/data-wrangling-with-mongodb--ud032" rel="nofollow"&gt;Manipulaci칩n y recuperaci칩n de datos con MongoDB&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;8 semanas&lt;/td&gt;
&lt;td align="center"&gt;10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-big-data" class="anchor" aria-hidden="true" href="#star-big-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Big Data&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617" rel="nofollow"&gt;Introducci칩n a Hadoop y MapReduce&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 semanas&lt;/td&gt;
&lt;td align="center"&gt;6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/deploying-a-hadoop-cluster--ud1000" rel="nofollow"&gt;Despliegue a Hadoop Cluster&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;3 semanas&lt;/td&gt;
&lt;td align="center"&gt;6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-natural-language-processing" class="anchor" aria-hidden="true" href="#star-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Natural Language Processing&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://cs224d.stanford.edu/" rel="nofollow"&gt;Deep Learning for Natural Language Processing&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Stanford&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-deep-learning" class="anchor" aria-hidden="true" href="#star-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Deep Learning&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duraci칩n&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/deep-learning--ud730" rel="nofollow"&gt;Deep Learning&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;12 semanas&lt;/td&gt;
&lt;td align="center"&gt;8-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-especializaci칩n" class="anchor" aria-hidden="true" href="#star-especializaci칩n"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; Especializaci칩n&lt;/h3&gt;
&lt;p&gt;Despu칠s de terminar los cursos anteriores, comience sus especializaciones en los temas que le interesan m치s.
Puede ver una lista de especializaciones disponibles. &lt;a href="https://github.com/DataScienceResearchPeru/OpenSource-RoadMap-DataScience/tree/master/especializacion"&gt;Aqu칤&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15a5e1c20a0a4a22f1db4c752e6629c81614ab7e/687474703a2f2f692e696d6775722e636f6d2f5245514b3056552e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/15a5e1c20a0a4a22f1db4c752e6629c81614ab7e/687474703a2f2f692e696d6775722e636f6d2f5245514b3056552e6a7067" alt="keep learning" data-canonical-src="http://i.imgur.com/REQK0VU.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-c칩mo-usar-est치-gu칤a" class="anchor" aria-hidden="true" href="#star-c칩mo-usar-est치-gu칤a"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;救넖잺&lt;/g-emoji&gt; C칩mo usar est치 gu칤a&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-orden-de-las-clases" class="anchor" aria-hidden="true" href="#orden-de-las-clases"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Orden de las clases&lt;/h3&gt;
&lt;p&gt;Esta gu칤a fue desarrollada para ser consumida en un enfoque lineal. 쯈u칠 significa esto? Que debes completar un curso a la vez.&lt;/p&gt;
&lt;p&gt;Los cursos ya est치n en el orden en que debe completarse.
Simplemente comience en la secci칩n &lt;a href="#%C3%A1lgebra-lineal"&gt;츼lgebra lineal&lt;/a&gt; y despu칠s de terminar el primer curso, comience el siguiente.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Si el curso no est치 abierto, h치galo de todos modos con los recursos de la clase anterior.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-debo-tomar-todos-los-cursos" class="anchor" aria-hidden="true" href="#debo-tomar-todos-los-cursos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;쮻ebo tomar todos los cursos?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Si!&lt;/strong&gt; 춰La intenci칩n es concluir &lt;strong&gt;todos&lt;/strong&gt; los cursos listados aqu칤!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-duraci칩n" class="anchor" aria-hidden="true" href="#duraci칩n"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Duraci칩n&lt;/h3&gt;
&lt;p&gt;춰Puede llevar m치s tiempo completar todas las clases en comparaci칩n con un curso regular de Ciencias de Datos, pero podemos &lt;strong&gt;garantizar&lt;/strong&gt; que su &lt;strong&gt;recompensa&lt;/strong&gt; ser치 proporcional a &lt;strong&gt;su motivaci칩n / dedicaci칩n&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Debes concentrarte en tu &lt;strong&gt;h치bito&lt;/strong&gt; y &lt;strong&gt;olvidarte&lt;/strong&gt; de los objetivos. Intenta invertir 1 ~ 2 horas &lt;strong&gt;todos los d칤as&lt;/strong&gt; estudiando este plan de estudios. Si haces esto, &lt;strong&gt;inevitablemente&lt;/strong&gt; terminar치s este plan de estudios.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cr칠ditos" class="anchor" aria-hidden="true" href="#cr칠ditos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cr칠ditos&lt;/h2&gt;
&lt;p&gt;El repositorio que nos sirvi칩 de inspiraci칩n: &lt;a href="https://github.com/ossu/data-science"&gt;OSSU &lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>DataScienceResearchPeru</author><guid isPermaLink="false">https://github.com/DataScienceResearchPeru/OpenSource-RoadMap-DataScience</guid><pubDate>Sun, 05 Jan 2020 00:02:00 GMT</pubDate></item><item><title>rasbt/deeplearning-models #3 in Jupyter Notebook, Today</title><link>https://github.com/rasbt/deeplearning-models</link><description>&lt;p&gt;&lt;i&gt;A collection of various deep learning architectures, models, and tips&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.7" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Models&lt;/h1&gt;
&lt;p&gt;A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-traditional-machine-learning" class="anchor" aria-hidden="true" href="#traditional-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Traditional Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perceptron&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression (Multinomial Logistic Regression)&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression with MLxtend's plot_decision_regions on Iris&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multilayer-perceptrons" class="anchor" aria-hidden="true" href="#multilayer-perceptrons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multilayer Perceptrons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multilayer Perceptron&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Dropout&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Batch Normalization&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Backpropagation from Scratch&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-basic" class="anchor" aria-hidden="true" href="#basic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network with He Initialization&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-he-init.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-he-init.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-concepts" class="anchor" aria-hidden="true" href="#concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concepts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Replacing Fully-Connnected by Equivalent Convolutional Layers&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/fc-to-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/fc-to-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-fully-convolutional" class="anchor" aria-hidden="true" href="#fully-convolutional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully Convolutional&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fully Convolutional Neural Network&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-allconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-allconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-lenet" class="anchor" aria-hidden="true" href="#lenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LeNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5 on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on CIFAR-10&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on QuickDraw&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-alexnet" class="anchor" aria-hidden="true" href="#alexnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlexNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AlexNet on CIFAR-10&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-vgg" class="anchor" aria-hidden="true" href="#vgg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VGG&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network VGG-16&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VGG-16 Gender Classifier Trained on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network VGG-19&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg19.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg19.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-densenet" class="anchor" aria-hidden="true" href="#densenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DenseNet-121 Digit Classifier Trained on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DenseNet-121 Image Classifier Trained on CIFAR-10&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-resnet" class="anchor" aria-hidden="true" href="#resnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ResNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ResNet and Residual Blocks&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/resnet-ex-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/resnet-ex-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Digit Classifier Trained on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Gender Classifier Trained on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Digit Classifier Trained on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Object Classifier Trained on QuickDraw&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Gender Classifier Trained on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Digit Classifier Trained on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Gender Classifier Trained on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Gender Classifier Trained on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Trained on CIFAR-10&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-152 Gender Classifier Trained on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-network-in-network" class="anchor" aria-hidden="true" href="#network-in-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network in Network&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Network in Network CIFAR-10 Classifier&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-normalization-layers" class="anchor" aria-hidden="true" href="#normalization-layers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Normalization Layers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BatchNorm before and after Activation for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Filter Response Normalization for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-metric-learning" class="anchor" aria-hidden="true" href="#metric-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metric Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Siamese Network with Multilayer Perceptrons&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/metric/siamese-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/metric/siamese-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-autoencoders" class="anchor" aria-hidden="true" href="#autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autoencoders&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-fully-connected-autoencoders" class="anchor" aria-hidden="true" href="#fully-connected-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully-connected Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoder (MNIST)&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autoencoder (MNIST) + Scikit-Learn Random Forest Classifier&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-convolutional-autoencoders" class="anchor" aria-hidden="true" href="#convolutional-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions / Transposed Convolutions&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions and Continuous Jaccard Distance&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions (without pooling operations)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on Quickdraw&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-variational-autoencoders" class="anchor" aria-hidden="true" href="#variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Variational Autoencoder&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Variational Autoencoder&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-conditional-variational-autoencoders" class="anchor" aria-hidden="true" href="#conditional-variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fully Connected GAN on MNIST&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Fully Connected Wasserstein GAN on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gan/wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST with Label Smoothing&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Wasserstein GAN on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gan/dc-wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/dc-wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph-neural-networks-gnns" class="anchor" aria-hidden="true" href="#graph-neural-networks-gnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph Neural Networks (GNNs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most Basic Graph Neural Network with Gaussian Filter on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Edge Prediction on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Spectral Graph Convolution on MNIST&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-recurrent-neural-networks-rnns" class="anchor" aria-hidden="true" href="#recurrent-neural-networks-rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-one-sentiment-analysis--classification" class="anchor" aria-hidden="true" href="#many-to-one-sentiment-analysis--classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-one: Sentiment Analysis / Classification&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple single-layer RNN (IMDB)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple single-layer RNN with packed sequences to ignore padding characters (IMDB)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB) and pre-trained GloVe word vectors&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells and Own Dataset in CSV Format (IMDB)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with GRU cells (IMDB)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer bi-directional RNN (IMDB)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (AG News)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Yelp Review Polarity)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Amazon Review Polarity)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-many--sequence-to-sequence" class="anchor" aria-hidden="true" href="#many-to-many--sequence-to-sequence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-Many / Sequence-to-Sequence&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple character RNN to generate new text (Charles Dickens)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ordinal-regression" class="anchor" aria-hidden="true" href="#ordinal-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ordinal Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ordinal Regression CNN -- CORAL w. ResNet34 on AFAD-Lite&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Niu et al. 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Beckham and Pal 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cyclical Learning Rate&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/tricks/cyclical-learning-rate.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cyclical-learning-rate.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Annealing with Increasing the Batch Size (w. CIFAR-10 &amp;amp; AlexNet)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Gradient Clipping (w. MLP on MNIST)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/tricks/gradclipping_mlp.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/gradclipping_mlp.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-transfer-learning" class="anchor" aria-hidden="true" href="#transfer-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transfer Learning Example (VGG16 pre-trained on ImageNet for Cifar-10)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;먝 [PyTorch: &lt;a href="pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#pytorch-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Custom Data Loader Example for PNG Files&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD)&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing" class="anchor" aria-hidden="true" href="#training-and-preprocessing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generating Validation Set Splits&lt;br&gt;
[PyTorch]: &lt;a href="pytorch_ipynb/mechanics/validation-splits.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/validation-splits.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Dataloading with Pinned Memory&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Standardizing Images&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-standardized.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-standardized.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Image Transformation Examples&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Char-RNN with Own Text File&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sentiment Classification RNN with Own CSV File&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-parallel-computing" class="anchor" aria-hidden="true" href="#parallel-computing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parallel Computing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using Multiple GPUs with DataParallel -- VGG-16 Gender Classifier on CelebA&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sequential API and hooks&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/mlp-sequential.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/mlp-sequential.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Weight Sharing Within a Layer&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Plotting Live Training Performance in Jupyter Notebooks with just Matplotlib&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-autograd" class="anchor" aria-hidden="true" href="#autograd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autograd&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Getting Gradients of an Intermediate Variable in PyTorch&lt;br&gt;
먝 [PyTorch: &lt;a href="pytorch_ipynb/mechanics/manual-gradients.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/manual-gradients.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tensorflow-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#tensorflow-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets-1" class="anchor" aria-hidden="true" href="#custom-datasets-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Storing an Image Dataset for Minibatch Training using HDF5&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Input Pipelines to Read Data from TFRecords Files&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/tfrecords.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/tfrecords.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Queue Runners to Feed Images Directly from Disk&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/file-queues.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/file-queues.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using TensorFlow's Dataset API&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/dataset-api.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/dataset-api.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing-1" class="anchor" aria-hidden="true" href="#training-and-preprocessing-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Saving and Loading Trained Models -- from TensorFlow Checkpoint Files and NumPy NPZ Archives&lt;br&gt;
먝 [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/deeplearning-models</guid><pubDate>Sun, 05 Jan 2020 00:03:00 GMT</pubDate></item><item><title>TrickyGo/Dive-into-DL-TensorFlow2.0 #4 in Jupyter Notebook, Today</title><link>https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0</link><description>&lt;p&gt;&lt;i&gt;燎설몿眠쉰쌙귄궏곎駱뷙임擥뷘솑達먺(Dive into Deep Learning)達뷗쟔眄ㄽXNet丹륂꿣賚좫쟠TensorFlow 2.0丹륂꿣庸껄몿眠쉰쒼쭝걇療뀑쑳辣걶뚟낿&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="docs/README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="img/cover.png"&gt;&lt;img width="700" src="img/cover.png" alt="卵咐" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="https://TrickyGo.github.io/Dive-into-DL-TensorFlow2.0" rel="nofollow"&gt;燎설몿眠&lt;/a&gt;卵&lt;a href="http://zh.d2l.ai/" rel="nofollow"&gt;귄궏곎駱뷙임擥뷘솑達먺&lt;/a&gt; 達뷗쟔MXNet坍滅丹륂꿣賚좫쟠TensorFlow2.0丹륂꿣某謗갬眄亂쩐잳得뻗식療뀑쑳辣걾쨃謗뙋쟑孚좮띭眄丹륁뒇剌쒼쭝걇療뀑쑳辣걶뚟낿達뷗쪺庸뛸雷孚췅欖먺療뀑쑳곑꼬 C. 茗孚啖뛴꾽欄쐔뱒 J. 雷꿞긜坍봰끩윇즸命쮠꺜榜뫦꺑庸GitHub喇썬庸&lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;蔞벇졵眄&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;疸&lt;/a&gt;&lt;a href="https://d2l.ai/" rel="nofollow"&gt;김&lt;/a&gt;곋燎선솆喇뻓啖疸꽨낿庸껁랿孚좮띭疸魃蜂건싳蔞벇졵眄疸쇉둖곋謗發TensorFlow2.0꽩룫낁邏뒲쨃燎설몿眠쉯끡啖亂좭솏達뷗쟔雷곋謗發PyTorch꽩룫眄孚좮띭&lt;a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch"&gt;Dive-into-DL-PyTorch&lt;/a&gt;庸껀랻蔞벋몯命쥕方뮊&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;꿣剌쒽띲雷썬걇蓂선쇊멆쨃깨某쇉띲雷썫쟔&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-酩坍" class="anchor" aria-hidden="true" href="#酩坍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;酩坍&lt;/h2&gt;
&lt;p&gt;燎섟즵擥疸魃깬냚code得깏ocs疸벇쟑雷坍윈뱣庸건뱄먻啖賴썭꼹駱떥쮠랻data疸쇒쨀윇쟔code雷坍윈뱣卵쐕땶鏤茗먾띶쓹upyter notebook坍滅庸건맄啖꾒ensorFlow2.0庸괦쨑docs雷坍윈뱣卵쐕땶markdown蓼쩐쨆眄귄궏곎駱뷙임擥뷘솑達먺達뷗쟔眄眠전丹좶쨃윈냁걀覓&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;卵母놸못雷遼뻖쒼걇GitHub Pages疸귎쨃覓쐓줉達뷗覓뻖뚟僚슔XNet遼瞭윒쨃坍벿ocs丹좬낊쫷잹達뷚돈燎괛잸낿庸꺿쪥僚賴얷쪱丹좭땶疸蓼眄縷뮏쯽亂좭랿孚좮띭뛴쥗야꺑갰끯죨ssue&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-咐뮌냄啖쥖쭧" class="anchor" aria-hidden="true" href="#咐뮌냄啖쥖쭧"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;咐뮌냄啖쥖쭧&lt;/h2&gt;
&lt;p&gt;燎설몿眠쉴록냄亂좭임擥뷘솑達먽얻웎庸껀쌳윉땶擔覓TensorFlow2.0謗發껁임擥뷘솑達먾뚟茗봴룲燎설몿眠쉰좄疸꽫부劉擔먽랠坍擔됁임擥뷘솑達먽갰燎쥔똗駱뷗먾뚟껁똞麵봳싅庸꺿먼낅否啖倣癩쥖眄賴썬솑得껂쨌溟庸껀북癩쥖眄暮坍賴썪籃쉰갡得껁북꾾庸꺿진끩癩쥖眄ㅀython模뒪뺆&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-富覓뻕뒃柳" class="anchor" aria-hidden="true" href="#富覓뻕뒃柳"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;富覓뻕뒃柳&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-雷좭씟疸" class="anchor" aria-hidden="true" href="#雷좭씟疸"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;雷좭씟疸&lt;/h3&gt;
&lt;p&gt;燎섟즵擥깬냚疸啖뜳atex선쨆庸꺿쪥github眄markdown覓僚疸꽩댪깨선쨆僚쮢뱤眄庸껃깏ocs雷坍윈뱣剌쒾즲걀覓&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;跋뻖쒼걇啖GitHub Pages疸귎쨃坍봱린綿雷遼燎酩曇眄雷좭씟卵쐕땶眠얹꿘房付&lt;a href="https://TrickyGo.github.io/Dive-into-DL-TensorFlow2.0" rel="nofollow"&gt;燎설몿眠쉲쪰孚왢곋&lt;/a&gt;濫윈북瞭럯먿찆紡놳疸謗넏몗眠전坍滅眄放뢛찆僚籃굵燎설몿眠쇾lone疸療봺쨃윈냁謗넏몗code雷坍윈뱣疸眠전坍滅&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-雷좭씟啖" class="anchor" aria-hidden="true" href="#雷좭씟啖"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;雷좭씟啖&lt;/h3&gt;
&lt;p&gt;擔먿찆낊坍봰랻燎선럃房付쉱둖遼庸껀건술醱&lt;code&gt;docsify-cli&lt;/code&gt;剌봰:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm i docsify-cli -g&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;윈냁卵燎설몿眠쇾lone걇燎선럃:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; Dive-into-DL-TensorFlow2.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;윈냁謗넏몗疸疸쀥랿喇썭랤궈謄뻞쨃謗뙍먕卵쐔낊坍봰쭏雷좫眄喇&lt;code&gt;http://localhost:3000&lt;/code&gt;丹륁딨房付쉱둖遼母놸못琉쒽릡賴걵뤂&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docsify serve docs&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-眠쉰쪳" class="anchor" aria-hidden="true" href="#眠쉰쪳"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;眠쉰쪳&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;酩坍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="read_guide.md"&gt;俯放깮꼥&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter01_DL-intro/deep-learning-intro.md"&gt;1. 瀏쐔줝駱뷗먾坍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2. 孵邏麵봳싅
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.1_install.md"&gt;2.1 꿢螺꽪쫉&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.2_tensor.md"&gt;2.2 賴썭꼹賂꽧쪺&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.3_autograd.md"&gt;2.3 쀤궏劉鬧擥&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.4_document.md"&gt;2.4 聊봴땐雷遼&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3. 瀏쐔줝駱뷗먼맄蔑
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.1_linear-regression.md"&gt;3.1 暮騰륀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.2_linear-regression-scratch.md"&gt;3.2 暮騰륀뉞뚟坍뀔띴欖烙丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.3_linear-regression-tensorflow2.0.md"&gt;3.3 暮騰륀뉞뚟酩榴丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.4_softmax-regression.md"&gt;3.4 softmax騰륀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.5_fashion-mnist.md"&gt;3.5 騰쮠갡袂賴썭꼹副庸Fashion-MNIST庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.6_softmax-regression-scratch.md"&gt;3.6 softmax騰륀뉞뚟坍뀔띴欖烙丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.7_softmax-regression-tensorflow2.0.md"&gt;3.7 softmax騰륀뉞뚟酩榴丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.8_mlp.md"&gt;3.8 邏뛴쎽麵봱럌&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.9_mlp-scratch.md"&gt;3.9 邏뛴쎽麵봱럌眄坍뀔띴欖烙丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.10_mlp-tensorflow2.0.md"&gt;3.10 邏뛴쎽麵봱럌眄酩榴丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.11_underfit-overfit.md"&gt;3.11 淚뫤룲괝길縷먽낻得껃쯶낻&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.12_weight-decay.md"&gt;3.12 療꽫몷&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.13_dropout.md"&gt;3.13 疸뮌짺柳&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.14_backprop.md"&gt;3.14 蔞냄憺먽눬끬냄憺먽눬得껃숴酩騰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.15_numerical-stability-and-init.md"&gt;3.15 賴썬쩒뺩丹뛵得껁뺘拏갷烙깼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.16_kaggle-house-price.md"&gt;3.16 丹륁갲Kaggle鏤덛옡庸뛵坍孵流&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4. 瀏쐔줝駱뷗먿숴酩
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.1_model-construction.md"&gt;4.1 淚뫤룲瞭&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.2_parameters.md"&gt;4.2 淚뫤룲끡賴썮뚟房付쉮갷烙깼得껀쐓줢&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.3_deferred-init.md"&gt;4.3 淚뫤룲끡賴썮뚟攬윈냁갷烙깼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.4_custom-layer.md"&gt;4.4 쀤숰達괜쎽&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.5_read-write.md"&gt;4.5 放끴得껀솆&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.6_use-gpu.md"&gt;4.6 GPU房뫦숭&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;5. 꽁椧明륂즲母놶즾
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.1_conv-layer.md"&gt;5.1 啖껂짒꽁椧欄&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.2_padding-and-strides.md"&gt;5.2 蘿得껁손嵐&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.3_channels.md"&gt;5.3 邏뛷쭙봴뛸得껀뱈訪쥘뛸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.4_pooling.md"&gt;5.4 劉먼깼欄&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.5_lenet.md"&gt;5.5 꽁椧明륂즲母놶즾庸LeNet庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.6_alexnet.md"&gt;5.6 瀏쐔줝꽁椧明륂즲母놶즾庸ㄱlexNet庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.7_vgg.md"&gt;5.7 擔覓뻘꽨밼募먾뚟母놶즾庸VGG庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.8_nin.md"&gt;5.8 母놶즾疸쇊뚟母놶즾庸NiN庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.9_googlenet.md"&gt;5.9 냚嵐윋몗謗륂즵眄母놶즾庸GoogLeNet庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.10_batch-norm.md"&gt;5.10 곷濫뉛깼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.11_resnet.md"&gt;5.11 褸剌쉲쪰某럺짿ResNet庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.12_densenet.md"&gt;5.12 溟먼싅謗륁꿘母놶즾庸DenseNet庸&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;6. 籃쀦꿢明륂즲母놶즾
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.1_lang-model.md"&gt;6.1 放쇋淚뫤룲&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.2_rnn.md"&gt;6.2 籃쀦꿢明륂즲母놶즾&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.3_lang-model-dataset.md"&gt;6.3 放쇋淚뫤룲賴썭꼹副庸건녿療썫쨙疸訪놵셻放꽲쨀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.4_rnn-scratch.md"&gt;6.4 籃쀦꿢明륂즲母놶즾眄坍뀔띴欖烙丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.5_rnn-pytorch.md"&gt;6.5 籃쀦꿢明륂즲母놶즾眄酩榴丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.6_bptt.md"&gt;6.6 뛷쯶了윌딦끬냄憺먽눬&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.7_gru.md"&gt;6.7 付뻕꿚籃쀦꿢꼣庸GRU庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.8_lstm.md"&gt;6.8 鳳麵쇉房썬쯵庸LSTM庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.9_deep-rnn.md"&gt;6.9 瀏쐔줝籃쀦꿢明륂즲母놶즾&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.10_bi-rnn.md"&gt;6.10 끫냄籃쀦꿢明륂즲母놶즾&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;7. 憺떤깼酩柳
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.1_optimization-intro.md"&gt;7.1 憺떤깼疸뀑임擥뷘솑達&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.2_gd-sgd.md"&gt;7.2 鬧擥뷗잶傅꽨눏剖燎쥕뭞擥뷗잶傅&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.3_minibatch-sgd.md"&gt;7.3 卵곷剖燎쥕뭞擥뷗잶傅&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.4_momentum.md"&gt;7.4 궏柳&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.5_adagrad.md"&gt;7.5 AdaGrad酩柳&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.6_rmsprop.md"&gt;7.6 RMSProp酩柳&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.7_adadelta.md"&gt;7.7 AdaDelta酩柳&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.8_adam.md"&gt;7.8 Adam酩柳&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;8. 房뫦숭
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.1_hybridize.md"&gt;8.1 鈍쫷짃欖得껂샩낑欖瀏낻模뒪뺆&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.2_async-computation.md"&gt;8.2 欖蔞봳숴酩&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.3_auto-parallelism.md"&gt;8.3 쀤궏嵐윋몗房뫦숭&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.4_multiple-gpus.md"&gt;8.4 邏뙪PU房뫦숭&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9. 房뫦숭燎쥗븭倣
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.1_image-augmentation.md"&gt;9.1 騰쮠螺륀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.2_fine-tuning.md"&gt;9.2 籃쉳쌖&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.3_bounding-box.md"&gt;9.3 眠쉱맪龍流得껃쭻免껁몑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.4_anchor.md"&gt;9.4 鋒뛵몑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.5_multiscale-object-detection.md"&gt;9.5 邏뛴썈擥뷚띭蓼龍流&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.6_object-detection-dataset.md"&gt;9.6 眠쉱맪龍流賴썭꼹副庸걶뛅꼬疸떮쨀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;籃料얹둻...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;10. 쀦윋싨傍邏낹
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.1_word2vec.md"&gt;10.1 放꽨옓봺짿word2vec庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.2_approx-training.md"&gt;10.2 謗놳쨮房쇊즦&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.3_word2vec-pytorch.md"&gt;10.3 word2vec眄丹륂꿣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.4_fasttext.md"&gt;10.4 駱넏싌蘭껀봺짿fastText庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.5_glove.md"&gt;10.5 뻔냄眄放꽨옓봺짿GloVe庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.6_similarity-analogy.md"&gt;10.6 劉謗놳졜放꽨눏袂鏤덛싌&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md"&gt;10.7 雷燎섡갡袂庸뛳覓뻔쭭꿢明륂즲母놶즾&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md"&gt;10.8 雷燎섡갡袂庸뛳覓뻔꽁椧明륂즲母놶즾庸값extCNN庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.9_seq2seq.md"&gt;10.9 模뒪매謄뻑덛빆滅謄뻞짿seq2seq庸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.10_beam-search.md"&gt;10.10 療냎募&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.11_attention.md"&gt;10.11 柳뻕궅燎쥔걌&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.12_machine-translation.md"&gt;10.12 燎쥔똗牟放&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;깨某쇉띲雷썫쟔......&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-達뷘럃懶" class="anchor" aria-hidden="true" href="#達뷘럃懶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;達뷘럃懶&lt;/h2&gt;
&lt;p&gt;疸쇉둖곋庸&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;궏곎駱뷙임擥뷘솑達&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;Github坍擥&lt;/a&gt;&lt;br&gt;
English Version: &lt;a href="https://d2l.ai/" rel="nofollow"&gt;Dive into Deep Learning&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-en"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-欖됂댣" class="anchor" aria-hidden="true" href="#欖됂댣"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;欖됂댣&lt;/h2&gt;
&lt;p&gt;洛瞭럱뻔랻滅덚뽔疸쇇覓뻓줁謗뙋쟑孚좮띭放欖됂댣達:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>TrickyGo</author><guid isPermaLink="false">https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0</guid><pubDate>Sun, 05 Jan 2020 00:04:00 GMT</pubDate></item><item><title>tensorflow/examples #5 in Jupyter Notebook, Today</title><link>https://github.com/tensorflow/examples</link><description>&lt;p&gt;&lt;i&gt;TensorFlow examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-examples" class="anchor" aria-hidden="true" href="#tensorflow-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Examples&lt;/h1&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67" data-canonical-src="https://www.tensorflow.org/images/tf_logo_social.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-most-important-links" class="anchor" aria-hidden="true" href="#most-important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Most important links!&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./community"&gt;Community examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="./courses/udacity_deep_learning"&gt;Course materials&lt;/a&gt; for the &lt;a href="https://www.udacity.com/course/deep-learning--ud730" rel="nofollow"&gt;Deep Learning&lt;/a&gt; class on Udacity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are looking to learn TensorFlow, don't miss the
&lt;a href="http://github.com/tensorflow/docs"&gt;core TensorFlow documentation&lt;/a&gt;
which is largely runnable code.
Those notebooks can be opened in Colab from
&lt;a href="https://tensorflow.org" rel="nofollow"&gt;tensorflow.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-this-repo" class="anchor" aria-hidden="true" href="#what-is-this-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this repo?&lt;/h2&gt;
&lt;p&gt;This is the TensorFlow example repo.  It has several classes of material:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Showcase examples and documentation for our fantastic &lt;a href="https://tensorflow.org/community" rel="nofollow"&gt;TensorFlow Community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Provide examples mentioned on TensorFlow.org&lt;/li&gt;
&lt;li&gt;Publish material supporting official TensorFlow courses&lt;/li&gt;
&lt;li&gt;Publish supporting material for the &lt;a href="https://blog.tensorflow.org" rel="nofollow"&gt;TensorFlow Blog&lt;/a&gt; and &lt;a href="https://youtube.com/tensorflow" rel="nofollow"&gt;TensorFlow YouTube Channel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We welcome community contributions, see &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; and, for style help,
&lt;a href="https://www.tensorflow.org/community/documentation" rel="nofollow"&gt;Writing TensorFlow documentation&lt;/a&gt;
guide.&lt;/p&gt;
&lt;p&gt;To file an issue, use the tracker in the
&lt;a href="https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md"&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/examples</guid><pubDate>Sun, 05 Jan 2020 00:05:00 GMT</pubDate></item><item><title>nyukat/breast_cancer_classifier #6 in Jupyter Notebook, Today</title><link>https://github.com/nyukat/breast_cancer_classifier</link><description>&lt;p&gt;&lt;i&gt;Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-neural-networks-improve-radiologists-performance-in-breast-cancer-screening" class="anchor" aria-hidden="true" href="#deep-neural-networks-improve-radiologists-performance-in-breast-cancer-screening"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is an implementation of the model used for breast cancer classification as described in our paper &lt;a href="https://ieeexplore.ieee.org/document/8861376" rel="nofollow"&gt;Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening&lt;/a&gt;. The implementation allows users to get breast cancer predictions by applying one of our pretrained models: a model which takes images as input (&lt;em&gt;image-only&lt;/em&gt;) and a model which takes images and heatmaps as input (&lt;em&gt;image-and-heatmaps&lt;/em&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input images: 2 CC view mammography images of size 2677x1942 and 2 MLO view mammography images of size 2974x1748. Each image is saved as 16-bit png file and gets standardized separately before being fed to the models.&lt;/li&gt;
&lt;li&gt;Input heatmaps: output of the patch classifier constructed to be the same size as its corresponding mammogram. Two heatmaps are generated for each mammogram, one for benign and one for malignant category. The value of each pixel in both of them is between 0 and 1.&lt;/li&gt;
&lt;li&gt;Output: 2 predictions for each breast, probability of benign and malignant findings: &lt;code&gt;left_benign&lt;/code&gt;, &lt;code&gt;right_benign&lt;/code&gt;, &lt;code&gt;left_malignant&lt;/code&gt;, and &lt;code&gt;right_malignant&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both models act on screening mammography exams with four standard views (L-CC, R-CC, L-MLO, R-MLO). As a part of this repository, we provide 4 sample exams (in &lt;code&gt;sample_data/images&lt;/code&gt; directory and exam list stored in &lt;code&gt;sample_data/exam_list_before_cropping.pkl&lt;/code&gt;). Heatmap generation model and cancer classification models are implemented in PyTorch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (2019/10/26)&lt;/strong&gt;: &lt;a href="https://ieeexplore.ieee.org/document/8861376" rel="nofollow"&gt;Our paper&lt;/a&gt; will be published in the IEEE Transactions on Medical Imaging!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (2019/08/26)&lt;/strong&gt;: We have added a &lt;a href="using_tensorflow.md"&gt;TensorFlow implementation&lt;/a&gt; of our &lt;em&gt;image-wise&lt;/em&gt; model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (2019/06/21)&lt;/strong&gt;: We have included the &lt;em&gt;image-wise&lt;/em&gt; model as described in the paper that generates predictions based on a single mammogram image. This model slightly under-performs the &lt;em&gt;view-wise&lt;/em&gt; model used above, but can be used on single mammogram images as opposed to full exams.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (2019/05/15)&lt;/strong&gt;: Fixed a minor bug that caused the output DataFrame columns (&lt;code&gt;left_malignant&lt;/code&gt;, &lt;code&gt;right_benign&lt;/code&gt;) to be swapped. Note that this does not affect the operation of the model.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-prerequisites" class="anchor" aria-hidden="true" href="#prerequisites"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python (3.6)&lt;/li&gt;
&lt;li&gt;PyTorch (0.4.1)&lt;/li&gt;
&lt;li&gt;torchvision (0.2.0)&lt;/li&gt;
&lt;li&gt;NumPy (1.14.3)&lt;/li&gt;
&lt;li&gt;SciPy (1.0.0)&lt;/li&gt;
&lt;li&gt;H5py (2.7.1)&lt;/li&gt;
&lt;li&gt;imageio (2.4.1)&lt;/li&gt;
&lt;li&gt;pandas (0.22.0)&lt;/li&gt;
&lt;li&gt;tqdm (4.19.8)&lt;/li&gt;
&lt;li&gt;opencv-python (3.4.2)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This repository is licensed under the terms of the GNU AGPLv3 license.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-run-the-code" class="anchor" aria-hidden="true" href="#how-to-run-the-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to run the code&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-exam-level" class="anchor" aria-hidden="true" href="#exam-level"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Exam-level&lt;/h3&gt;
&lt;p&gt;Here we describe how to get predictions from &lt;em&gt;view-wise&lt;/em&gt; model, which is our best-performing model. This model takes 4 images from each view as input and outputs predictions for each exam.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash run.sh&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;will automatically run the entire pipeline and save the prediction results in csv.&lt;/p&gt;
&lt;p&gt;We recommend running the code with a gpu (set by default). To run the code with cpu only, please change &lt;code&gt;DEVICE_TYPE&lt;/code&gt; in &lt;code&gt;run.sh&lt;/code&gt; to 'cpu'.&lt;/p&gt;
&lt;p&gt;If running the individual Python scripts, please include the path to this repository in your &lt;code&gt;PYTHONPATH&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;You should obtain the following outputs for the sample exams provided in the repository.&lt;/p&gt;
&lt;p&gt;Predictions using &lt;em&gt;image-only&lt;/em&gt; model (found in &lt;code&gt;sample_output/image_predictions.csv&lt;/code&gt; by default):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;index&lt;/th&gt;
&lt;th&gt;left_benign&lt;/th&gt;
&lt;th&gt;right_benign&lt;/th&gt;
&lt;th&gt;left_malignant&lt;/th&gt;
&lt;th&gt;right_malignant&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.0580&lt;/td&gt;
&lt;td&gt;0.0754&lt;/td&gt;
&lt;td&gt;0.0091&lt;/td&gt;
&lt;td&gt;0.0179&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.0646&lt;/td&gt;
&lt;td&gt;0.9536&lt;/td&gt;
&lt;td&gt;0.0012&lt;/td&gt;
&lt;td&gt;0.7258&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.4388&lt;/td&gt;
&lt;td&gt;0.3526&lt;/td&gt;
&lt;td&gt;0.2325&lt;/td&gt;
&lt;td&gt;0.1061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.3765&lt;/td&gt;
&lt;td&gt;0.6483&lt;/td&gt;
&lt;td&gt;0.0909&lt;/td&gt;
&lt;td&gt;0.2579&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Predictions using &lt;em&gt;image-and-heatmaps&lt;/em&gt; model (found in &lt;code&gt;sample_output/imageheatmap_predictions.csv&lt;/code&gt; by default):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;index&lt;/th&gt;
&lt;th&gt;left_benign&lt;/th&gt;
&lt;th&gt;right_benign&lt;/th&gt;
&lt;th&gt;left_malignant&lt;/th&gt;
&lt;th&gt;right_malignant&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.0612&lt;/td&gt;
&lt;td&gt;0.0555&lt;/td&gt;
&lt;td&gt;0.0099&lt;/td&gt;
&lt;td&gt;0.0063&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.0507&lt;/td&gt;
&lt;td&gt;0.8025&lt;/td&gt;
&lt;td&gt;0.0009&lt;/td&gt;
&lt;td&gt;0.9000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.2877&lt;/td&gt;
&lt;td&gt;0.2286&lt;/td&gt;
&lt;td&gt;0.2524&lt;/td&gt;
&lt;td&gt;0.0461&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.4181&lt;/td&gt;
&lt;td&gt;0.3172&lt;/td&gt;
&lt;td&gt;0.3174&lt;/td&gt;
&lt;td&gt;0.0485&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-single-image" class="anchor" aria-hidden="true" href="#single-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Single Image&lt;/h3&gt;
&lt;p&gt;Here we also upload &lt;em&gt;image-wise&lt;/em&gt; model, which is different from and performs worse than the &lt;em&gt;view-wise&lt;/em&gt; model described above. The csv output from &lt;em&gt;view-wise&lt;/em&gt; model will be different from that of &lt;em&gt;image-wise&lt;/em&gt; model in this section. Because this model has the benefit of creating predictions for each image separately, we make this model public to facilitate transfer learning.&lt;/p&gt;
&lt;p&gt;To use the &lt;em&gt;image-wise&lt;/em&gt; model, run a command such as the following:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;bash run_single.sh &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sample_data/images/0_L_CC.png&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where the first argument is path to a mammogram image, and the second argument is the view corresponding to that image.&lt;/p&gt;
&lt;p&gt;You should obtain the following output based on the above example command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Stage 1: Crop Mammograms
Stage 2: Extract Centers
Stage 3: Generate Heatmaps
Stage 4a: Run Classifier (Image)
{"benign": 0.040191903710365295, "malignant": 0.008045293390750885}
Stage 4b: Run Classifier (Image+Heatmaps)
{"benign": 0.052365876734256744, "malignant": 0.005510155577212572}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-image-level-notebook" class="anchor" aria-hidden="true" href="#image-level-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image-level Notebook&lt;/h4&gt;
&lt;p&gt;We have included a &lt;a href="sample_notebook.ipynb"&gt;sample notebook&lt;/a&gt; that contains code for running the classifiers with and without heatmaps (excludes preprocessing).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-data" class="anchor" aria-hidden="true" href="#data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data&lt;/h2&gt;
&lt;p&gt;To use one of the pretrained models, the input is required to consist of at least four images, at least one for each view (L-CC, L-MLO, R-CC, R-MLO).&lt;/p&gt;
&lt;p&gt;The original 12-bit mammograms are saved as rescaled 16-bit images to preserve the granularity of the pixel intensities, while still being correctly displayed in image viewers.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sample_data/exam_list_before_cropping.pkl&lt;/code&gt; contains a list of exam information before preprocessing. Each exam is represented as a dictionary with the following format:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;{
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;horizontal_flip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_L_CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_R_CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_L_MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_R_MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We expect images from &lt;code&gt;L-CC&lt;/code&gt; and &lt;code&gt;L-MLO&lt;/code&gt; views to be facing right direction, and images from &lt;code&gt;R-CC&lt;/code&gt; and &lt;code&gt;R-MLO&lt;/code&gt; views are facing left direction. &lt;code&gt;horizontal_flip&lt;/code&gt; indicates whether all images in the exam are flipped horizontally from expected. Values for &lt;code&gt;L-CC&lt;/code&gt;, &lt;code&gt;R-CC&lt;/code&gt;, &lt;code&gt;L-MLO&lt;/code&gt;, and &lt;code&gt;R-MLO&lt;/code&gt; are list of image filenames without extension and directory name.&lt;/p&gt;
&lt;p&gt;Additional information for each image gets included as a dictionary. Such dictionary has all 4 views as keys, and the values are the additional information for the corresponding key. For example, &lt;code&gt;window_location&lt;/code&gt;, which indicates the top, bottom, left and right edges of cropping window, is a dictionary that has 4 keys and has 4 lists as values which contain the corresponding information for the images. Additionally, &lt;code&gt;rightmost_pixels&lt;/code&gt;, &lt;code&gt;bottommost_pixels&lt;/code&gt;, &lt;code&gt;distance_from_starting_side&lt;/code&gt; and &lt;code&gt;best_center&lt;/code&gt; are added after preprocessing.
Description for these attributes can be found in the preprocessing section.
The following is an example of exam information after cropping and extracting optimal centers:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;{
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;horizontal_flip&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;NO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_L_CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_R_CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_L_MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0_R_MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;window_location&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;353&lt;/span&gt;, &lt;span class="pl-c1"&gt;4009&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;2440&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;71&lt;/span&gt;, &lt;span class="pl-c1"&gt;3771&lt;/span&gt;, &lt;span class="pl-c1"&gt;952&lt;/span&gt;, &lt;span class="pl-c1"&gt;3328&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;3818&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;2607&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;3724&lt;/span&gt;, &lt;span class="pl-c1"&gt;848&lt;/span&gt;, &lt;span class="pl-c1"&gt;3328&lt;/span&gt;)]
   },
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rightmost_points&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [((&lt;span class="pl-c1"&gt;1879&lt;/span&gt;, &lt;span class="pl-c1"&gt;1958&lt;/span&gt;), &lt;span class="pl-c1"&gt;2389&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [((&lt;span class="pl-c1"&gt;2207&lt;/span&gt;, &lt;span class="pl-c1"&gt;2287&lt;/span&gt;), &lt;span class="pl-c1"&gt;2326&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [((&lt;span class="pl-c1"&gt;2493&lt;/span&gt;, &lt;span class="pl-c1"&gt;2548&lt;/span&gt;), &lt;span class="pl-c1"&gt;2556&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [((&lt;span class="pl-c1"&gt;2492&lt;/span&gt;, &lt;span class="pl-c1"&gt;2523&lt;/span&gt;), &lt;span class="pl-c1"&gt;2430&lt;/span&gt;)]
   },
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bottommost_points&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;3605&lt;/span&gt;, (&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-c1"&gt;100&lt;/span&gt;))],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;3649&lt;/span&gt;, (&lt;span class="pl-c1"&gt;101&lt;/span&gt;, &lt;span class="pl-c1"&gt;106&lt;/span&gt;))],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;3767&lt;/span&gt;, (&lt;span class="pl-c1"&gt;1456&lt;/span&gt;, &lt;span class="pl-c1"&gt;1524&lt;/span&gt;))],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;3673&lt;/span&gt;, (&lt;span class="pl-c1"&gt;1164&lt;/span&gt;, &lt;span class="pl-c1"&gt;1184&lt;/span&gt;))]
   },
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;distance_from_starting_side&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-c1"&gt;0&lt;/span&gt;],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-c1"&gt;0&lt;/span&gt;],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-c1"&gt;0&lt;/span&gt;],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [&lt;span class="pl-c1"&gt;0&lt;/span&gt;]
   },
  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;best_center&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: {
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;1850&lt;/span&gt;, &lt;span class="pl-c1"&gt;1417&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-CC&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;2173&lt;/span&gt;, &lt;span class="pl-c1"&gt;1354&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;L-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;2279&lt;/span&gt;, &lt;span class="pl-c1"&gt;1681&lt;/span&gt;)],
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;R-MLO&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: [(&lt;span class="pl-c1"&gt;2185&lt;/span&gt;, &lt;span class="pl-c1"&gt;1555&lt;/span&gt;)]
   }
}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The labels for the included exams are as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;index&lt;/th&gt;
&lt;th&gt;left_benign&lt;/th&gt;
&lt;th&gt;right_benign&lt;/th&gt;
&lt;th&gt;left_malignant&lt;/th&gt;
&lt;th&gt;right_malignant&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-pipeline" class="anchor" aria-hidden="true" href="#pipeline"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pipeline&lt;/h2&gt;
&lt;p&gt;The pipeline consists of four stages.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Crop mammograms&lt;/li&gt;
&lt;li&gt;Calculate optimal centers&lt;/li&gt;
&lt;li&gt;Generate Heatmaps&lt;/li&gt;
&lt;li&gt;Run classifiers&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following variables defined in &lt;code&gt;run.sh&lt;/code&gt; can be modified as needed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;NUM_PROCESSES&lt;/code&gt;: The number of processes to be used in preprocessing (&lt;code&gt;src/cropping/crop_mammogram.py&lt;/code&gt; and &lt;code&gt;src/optimal_centers/get_optimal_centers.py&lt;/code&gt;). Default: 10.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DEVICE_TYPE&lt;/code&gt;: Device type to use in heatmap generation and classifiers, either 'cpu' or 'gpu'. Default: 'gpu'&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;NUM_EPOCHS&lt;/code&gt;: The number of epochs to be averaged in the output of the classifiers. Default: 10.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HEATMAP_BATCH_SIZE&lt;/code&gt;: The batch size to use in heatmap generation. Default: 100.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;GPU_NUMBER&lt;/code&gt;: Specify which one of the GPUs to use when multiple GPUs are available. Default: 0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DATA_FOLDER&lt;/code&gt;: The directory where the mammogram is stored.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;INITIAL_EXAM_LIST_PATH&lt;/code&gt;: The path where the initial exam list without any metadata is stored.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PATCH_MODEL_PATH&lt;/code&gt;: The path where the saved weights for the patch classifier is saved.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;IMAGE_MODEL_PATH&lt;/code&gt;: The path where the saved weights for the &lt;em&gt;image-only&lt;/em&gt; model is saved.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;IMAGEHEATMAPS_MODEL_PATH&lt;/code&gt;: The path where the saved weights for the &lt;em&gt;image-and-heatmaps&lt;/em&gt; model is saved.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;CROPPED_IMAGE_PATH&lt;/code&gt;: The directory to save cropped mammograms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;CROPPED_EXAM_LIST_PATH&lt;/code&gt;: The path to save the new exam list with cropping metadata.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXAM_LIST_PATH&lt;/code&gt;: The path to save the new exam list with best center metadata.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HEATMAPS_PATH&lt;/code&gt;: The directory to save heatmaps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;IMAGE_PREDICTIONS_PATH&lt;/code&gt;: The path to save predictions of &lt;em&gt;image-only&lt;/em&gt; model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;IMAGEHEATMAPS_PREDICTIONS_PATH&lt;/code&gt;: The path to save predictions of &lt;em&gt;image-and-heatmaps&lt;/em&gt; model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-preprocessing" class="anchor" aria-hidden="true" href="#preprocessing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preprocessing&lt;/h3&gt;
&lt;p&gt;Run the following commands to crop mammograms and calculate information about augmentation windows.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-crop-mammograms" class="anchor" aria-hidden="true" href="#crop-mammograms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Crop mammograms&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 src/cropping/crop_mammogram.py \
    --input-data-folder &lt;span class="pl-smi"&gt;$DATA_FOLDER&lt;/span&gt; \
    --output-data-folder &lt;span class="pl-smi"&gt;$CROPPED_IMAGE_PATH&lt;/span&gt; \
    --exam-list-path &lt;span class="pl-smi"&gt;$INITIAL_EXAM_LIST_PATH&lt;/span&gt;  \
    --cropped-exam-list-path &lt;span class="pl-smi"&gt;$CROPPED_EXAM_LIST_PATH&lt;/span&gt;  \
    --num-processes &lt;span class="pl-smi"&gt;$NUM_PROCESSES&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;src/import_data/crop_mammogram.py&lt;/code&gt; crops the mammogram around the breast and discards the background in order to improve image loading time and time to run segmentation algorithm and saves each cropped image to &lt;code&gt;$PATH_TO_SAVE_CROPPED_IMAGES/short_file_path.png&lt;/code&gt; using h5py. In addition, it adds additional information for each image and creates a new image list to &lt;code&gt;$CROPPED_IMAGE_LIST_PATH&lt;/code&gt; while discarding images which it fails to crop. Optional --verbose argument prints out information about each image. The additional information includes the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;window_location&lt;/code&gt;: location of cropping window w.r.t. original dicom image so that segmentation map can be cropped in the same way for training.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rightmost_points&lt;/code&gt;: rightmost nonzero pixels after correctly being flipped.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bottommost_points&lt;/code&gt;: bottommost nonzero pixels after correctly being flipped.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;distance_from_starting_side&lt;/code&gt;: records if zero-value gap between the edge of the image and the breast is found in the side where the breast starts to appear and thus should have been no gap. Depending on the dataset, this value can be used to determine wrong value of &lt;code&gt;horizontal_flip&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-calculate-optimal-centers" class="anchor" aria-hidden="true" href="#calculate-optimal-centers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Calculate optimal centers&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 src/optimal_centers/get_optimal_centers.py \
    --cropped-exam-list-path &lt;span class="pl-smi"&gt;$CROPPED_EXAM_LIST_PATH&lt;/span&gt; \
    --data-prefix &lt;span class="pl-smi"&gt;$CROPPED_IMAGE_PATH&lt;/span&gt; \
    --output-exam-list-path &lt;span class="pl-smi"&gt;$EXAM_LIST_PATH&lt;/span&gt; \
    --num-processes &lt;span class="pl-smi"&gt;$NUM_PROCESSES&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;src/optimal_centers/get_optimal_centers.py&lt;/code&gt; outputs new exam list with additional metadata to &lt;code&gt;$EXAM_LIST_PATH&lt;/code&gt;. The additional information includes the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;best_center&lt;/code&gt;: optimal center point of the window for each image. The augmentation windows drawn with &lt;code&gt;best_center&lt;/code&gt; as exact center point could go outside the boundary of the image. This usually happens when the cropped image is smaller than the window size. In this case, we pad the image and shift the window to be inside the padded image in augmentation. Refer to &lt;a href="https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf" rel="nofollow"&gt;the data report&lt;/a&gt; for more details.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-heatmap-generation" class="anchor" aria-hidden="true" href="#heatmap-generation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Heatmap Generation&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 src/heatmaps/run_producer.py \
    --model-path &lt;span class="pl-smi"&gt;$PATCH_MODEL_PATH&lt;/span&gt; \
    --data-path &lt;span class="pl-smi"&gt;$EXAM_LIST_PATH&lt;/span&gt; \
    --image-path &lt;span class="pl-smi"&gt;$CROPPED_IMAGE_PATH&lt;/span&gt; \
    --batch-size &lt;span class="pl-smi"&gt;$HEATMAP_BATCH_SIZE&lt;/span&gt; \
    --output-heatmap-path &lt;span class="pl-smi"&gt;$HEATMAPS_PATH&lt;/span&gt; \
    --device-type &lt;span class="pl-smi"&gt;$DEVICE_TYPE&lt;/span&gt; \
    --gpu-number &lt;span class="pl-smi"&gt;$GPU_NUMBER&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;src/heatmaps/run_producer.py&lt;/code&gt; generates heatmaps by combining predictions for patches of images and saves them as hdf5 format in &lt;code&gt;$HEATMAPS_PATH&lt;/code&gt; using &lt;code&gt;$DEVICE_TYPE&lt;/code&gt; device. &lt;code&gt;$DEVICE_TYPE&lt;/code&gt; can either be 'gpu' or 'cpu'. &lt;code&gt;$HEATMAP_BATCH_SIZE&lt;/code&gt; should be adjusted depending on available memory size.  An optional argument &lt;code&gt;--gpu-number&lt;/code&gt;  can be used to specify which GPU to use.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-running-the-models" class="anchor" aria-hidden="true" href="#running-the-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running the models&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;src/modeling/run_model.py&lt;/code&gt; can provide predictions using cropped images either with or without heatmaps. When using heatmaps, please use the&lt;code&gt;--use-heatmaps&lt;/code&gt; flag and provide appropriate the &lt;code&gt;--model-path&lt;/code&gt; and &lt;code&gt;--heatmaps-path&lt;/code&gt; arguments. Depending on the available memory, the optional argument &lt;code&gt;--batch-size&lt;/code&gt; can be provided. Another optional argument &lt;code&gt;--gpu-number&lt;/code&gt; can be used to specify which GPU to use.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-image-only-model" class="anchor" aria-hidden="true" href="#run-image-only-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run image only model&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 src/modeling/run_model.py \
    --model-path &lt;span class="pl-smi"&gt;$IMAGE_MODEL_PATH&lt;/span&gt; \
    --data-path &lt;span class="pl-smi"&gt;$EXAM_LIST_PATH&lt;/span&gt; \
    --image-path &lt;span class="pl-smi"&gt;$CROPPED_IMAGE_PATH&lt;/span&gt; \
    --output-path &lt;span class="pl-smi"&gt;$IMAGE_PREDICTIONS_PATH&lt;/span&gt; \
    --use-augmentation \
    --num-epochs &lt;span class="pl-smi"&gt;$NUM_EPOCHS&lt;/span&gt; \
    --device-type &lt;span class="pl-smi"&gt;$DEVICE_TYPE&lt;/span&gt; \
    --gpu-number &lt;span class="pl-smi"&gt;$GPU_NUMBER&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This command makes predictions only using images for &lt;code&gt;$NUM_EPOCHS&lt;/code&gt; epochs with random augmentation and outputs averaged predictions per exam to &lt;code&gt;$IMAGE_PREDICTIONS_PATH&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-imageheatmaps-model" class="anchor" aria-hidden="true" href="#run-imageheatmaps-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run image+heatmaps model&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 src/modeling/run_model.py \
    --model-path &lt;span class="pl-smi"&gt;$IMAGEHEATMAPS_MODEL_PATH&lt;/span&gt; \
    --data-path &lt;span class="pl-smi"&gt;$EXAM_LIST_PATH&lt;/span&gt; \
    --image-path &lt;span class="pl-smi"&gt;$CROPPED_IMAGE_PATH&lt;/span&gt; \
    --output-path &lt;span class="pl-smi"&gt;$IMAGEHEATMAPS_PREDICTIONS_PATH&lt;/span&gt; \
    --use-heatmaps \
    --heatmaps-path &lt;span class="pl-smi"&gt;$HEATMAPS_PATH&lt;/span&gt; \
    --use-augmentation \
    --num-epochs &lt;span class="pl-smi"&gt;$NUM_EPOCHS&lt;/span&gt; \
    --device-type &lt;span class="pl-smi"&gt;$DEVICE_TYPE&lt;/span&gt; \
    --gpu-number &lt;span class="pl-smi"&gt;$GPU_NUMBER&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This command makes predictions using images and heatmaps for &lt;code&gt;$NUM_EPOCHS&lt;/code&gt; epochs with random augmentation and outputs averaged predictions per exam to &lt;code&gt;$IMAGEHEATMAPS_PREDICTIONS_PATH&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-image-from-dicom-files-and-saving-as-16-bit-png-files" class="anchor" aria-hidden="true" href="#getting-image-from-dicom-files-and-saving-as-16-bit-png-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting image from dicom files and saving as 16-bit png files&lt;/h2&gt;
&lt;p&gt;Dicom files can be converted into png files with the following function, which then can be used by the code in our repository (pypng 0.0.19 and pydicom 1.2.2 libraries are required).&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; png
&lt;span class="pl-k"&gt;import&lt;/span&gt; pydicom

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;save_dicom_image_as_png&lt;/span&gt;(&lt;span class="pl-smi"&gt;dicom_filename&lt;/span&gt;, &lt;span class="pl-smi"&gt;png_filename&lt;/span&gt;, &lt;span class="pl-smi"&gt;bitdepth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;12&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;    Save 12-bit mammogram from dicom as rescaled 16-bit png file.&lt;/span&gt;
&lt;span class="pl-s"&gt;    :param dicom_filename: path to input dicom file.&lt;/span&gt;
&lt;span class="pl-s"&gt;    :param png_filename: path to output png file.&lt;/span&gt;
&lt;span class="pl-s"&gt;    :param bitdepth: bit depth of the input image. Set it to 12 for 12-bit mammograms.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    image &lt;span class="pl-k"&gt;=&lt;/span&gt; pydicom.read_file(dicom_filename).pixel_array
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(png_filename, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;wb&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        writer &lt;span class="pl-k"&gt;=&lt;/span&gt; png.Writer(&lt;span class="pl-v"&gt;height&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;image.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-v"&gt;width&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;image.shape[&lt;span class="pl-c1"&gt;1&lt;/span&gt;], &lt;span class="pl-v"&gt;bitdepth&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;bitdepth, &lt;span class="pl-v"&gt;greyscale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
        writer.write(f, image.tolist())&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;If you found this code useful, please cite our paper:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening&lt;/strong&gt;&lt;br&gt;
Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanis켹aw Jastrz캧bski, Thibault F칠vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras&lt;br&gt;
IEEE Transactions on Medical Imaging&lt;br&gt;
2019&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{wu2019breastcancer, 
    title = {Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening},
    author = {Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanis\l{}aw Jastrz\k{e}bski, Thibault F\'{e}vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras}, 
    journal = {IEEE Transactions on Medical Imaging},
    year = {2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nyukat</author><guid isPermaLink="false">https://github.com/nyukat/breast_cancer_classifier</guid><pubDate>Sun, 05 Jan 2020 00:06:00 GMT</pubDate></item><item><title>pytorch/vision #7 in Jupyter Notebook, Today</title><link>https://github.com/pytorch/vision</link><description>&lt;p&gt;&lt;i&gt;Datasets, Transforms and Models specific to Computer Vision&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a name="user-content-torchvision"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-torchvision" class="anchor" aria-hidden="true" href="#torchvision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;torchvision&lt;/h2&gt;
&lt;a href="https://travis-ci.org/pytorch/vision" rel="nofollow"&gt;&lt;img alt="https://travis-ci.org/pytorch/vision.svg?branch=master" src="https://camo.githubusercontent.com/066c54ca32f191cb2a7dff8eb895dae99fa62364/68747470733a2f2f7472617669732d63692e6f72672f7079746f7263682f766973696f6e2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/pytorch/vision.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codecov.io/gh/pytorch/vision" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/d206df8ce39b22dbcfcbc8044d7058b8c47bc1b3/68747470733a2f2f636f6465636f762e696f2f67682f7079746f7263682f766973696f6e2f6272616e63682f6d61737465722f67726170682f62616467652e737667" data-canonical-src="https://codecov.io/gh/pytorch/vision/branch/master/graph/badge.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://pepy.tech/project/torchvision" rel="nofollow"&gt;&lt;img alt="https://pepy.tech/badge/torchvision" src="https://camo.githubusercontent.com/a080a02ba27e8ab65f937476759d12f72b9a1c28/68747470733a2f2f706570792e746563682f62616467652f746f726368766973696f6e" data-canonical-src="https://pepy.tech/badge/torchvision" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pytorch.org/docs/stable/torchvision/index.html" rel="nofollow"&gt;&lt;img alt="https://img.shields.io/badge/dynamic/json.svg?label=docs&amp;amp;url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&amp;amp;query=%24.info.version&amp;amp;colorB=brightgreen&amp;amp;prefix=v" src="https://camo.githubusercontent.com/891cde72d6784500640b8c547eee3201b233dc8a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f64796e616d69632f6a736f6e2e7376673f6c6162656c3d646f63732675726c3d6874747073253341253246253246707970692e6f726725324670797069253246746f726368766973696f6e2532466a736f6e2671756572793d2532342e696e666f2e76657273696f6e26636f6c6f72423d627269676874677265656e267072656669783d76" data-canonical-src="https://img.shields.io/badge/dynamic/json.svg?label=docs&amp;amp;url=https%3A%2F%2Fpypi.org%2Fpypi%2Ftorchvision%2Fjson&amp;amp;query=%24.info.version&amp;amp;colorB=brightgreen&amp;amp;prefix=v" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.&lt;/p&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;TorchVision requires PyTorch 1.2 or newer.&lt;/p&gt;
&lt;p&gt;Anaconda:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install torchvision -c pytorch&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;pip:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install torchvision&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From source:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python setup.py install
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or, for OSX&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By default, GPU support is built if CUDA is found and &lt;code&gt;torch.cuda.is_available()&lt;/code&gt; is true.
It's possible to force building GPU support by setting &lt;code&gt;FORCE_CUDA=1&lt;/code&gt; environment variable,
which is useful when building a docker image.&lt;/p&gt;
&lt;a name="user-content-image-backend"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-image-backend" class="anchor" aria-hidden="true" href="#image-backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Backend&lt;/h2&gt;
&lt;p&gt;Torchvision currently supports the following image backends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://python-pillow.org/" rel="nofollow"&gt;Pillow&lt;/a&gt; (default)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/uploadcare/pillow-simd"&gt;Pillow-SIMD&lt;/a&gt; - a &lt;strong&gt;much faster&lt;/strong&gt; drop-in replacement for Pillow with SIMD. If installed will be used as the default.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pytorch/accimage"&gt;accimage&lt;/a&gt; - if installed can be activated by calling &lt;code&gt;torchvision.set_image_backend('accimage')&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-c-api"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-c-api" class="anchor" aria-hidden="true" href="#c-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;C++ API&lt;/h2&gt;
&lt;p&gt;TorchVision also offers a C++ API that contains C++ equivalent of python models.&lt;/p&gt;
&lt;p&gt;Installation From source:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;mkdir build
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; build
cmake ..
make
make install&lt;/pre&gt;&lt;/div&gt;
&lt;a name="user-content-documentation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;You can find the API documentation on the pytorch website: &lt;a href="http://pytorch.org/docs/master/torchvision/" rel="nofollow"&gt;http://pytorch.org/docs/master/torchvision/&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us.&lt;/p&gt;
&lt;a name="user-content-disclaimer-on-datasets"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer-on-datasets" class="anchor" aria-hidden="true" href="#disclaimer-on-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer on Datasets&lt;/h2&gt;
&lt;p&gt;This is a utility library that downloads and prepares public datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the dataset's license.&lt;/p&gt;
&lt;p&gt;If you're a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your dataset to be included in this library, please get in touch through a GitHub issue. Thanks for your contribution to the ML community!&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/vision</guid><pubDate>Sun, 05 Jan 2020 00:07:00 GMT</pubDate></item><item><title>datawhalechina/team-learning #8 in Jupyter Notebook, Today</title><link>https://github.com/datawhalechina/team-learning</link><description>&lt;p&gt;&lt;i&gt;Datawhale某俯駱뷗먿숴뉛잹放쮢뺆丹&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-datawhale-某俯駱뷗" class="anchor" aria-hidden="true" href="#datawhale-某俯駱뷗"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datawhale 某俯駱뷗&lt;/h1&gt;
&lt;p&gt;蓂섟졯燎륛atawhale某俯駱뷗먿숴뉠뽋疸귄썀魃欖烙燈뷢째&lt;/p&gt;
&lt;p&gt;謗뙍샤쐖즧某疸괛쟑某俯駱뷗멆쨃溜왢띘啖模뒪뺆燎쥔똗駱뷗먾낹房쥓진끩궏곎丹륃잇眄丹좶쨃邏丹윈낊坍봱깰否魃괝길끡먺&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-賴썭꼹某瞭疸뀒숭柳돿잵" class="anchor" aria-hidden="true" href="#賴썭꼹某瞭疸뀒숭柳돿잵"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;賴썭꼹某瞭疸뀒숭柳됊짿疸귎쨀&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;丹좯쮣숴&lt;/strong&gt;庸뛴괜릝LeoLRH뒪&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹좮즧某&lt;/strong&gt;庸뛷浮섢뚟溟擥啖쥖super父떥썆憺&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹뫦啖&lt;/strong&gt;庸뛴괜릝&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹뛳쪬啖쥖쭧&lt;/strong&gt;庸뛵랠模뒪뺆放쇋癩쥖麵봳싅庸껂괠록냄亂좯쏘溟擥房쮣숴&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了윌딦丹괝&lt;/strong&gt;庸9邏뾆쨃鏤邏뽼좁懶궘榜좭딨付3卵了-5卵了윇잸螟괦쨃蓼좭꼹疸쀣줰駱뷗먽꿘끵쫸궅欖쥔쨤燎괝流쉰궏&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;坍궈酩坍&lt;/strong&gt;庸&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Task01 賴썮즧庸1邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task02庸뛸뫀擥發뻔눏逢쮣몯庸2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task03庸뛵맫疸뀔뉜뉦짿2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task04庸뛸갱庸2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task05庸뛴솅蓂뷗쓆짿2邏뾆쨀&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;放뷚즩丹괝&lt;/strong&gt;庸&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/datawhalechina/team-learning/blob/master/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%8A%EF%BC%89/%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1.md"&gt;https://github.com/datawhalechina/team-learning/blob/master/賴썭꼹某瞭疸뀒숭柳됊짿疸귎쨀/駱뷗먻짙궈.md&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-갷暮酩柳됁뭡낹" class="anchor" aria-hidden="true" href="#갷暮酩柳됁뭡낹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;갷暮酩柳됁뭡낹&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;丹좯쮣숴&lt;/strong&gt;庸뛷긢咐뙊擥왠왞部놵몎&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹좮즧某&lt;/strong&gt;庸뛷뒪윆方뮍둖僚돾父떦쁾疸&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹뫦啖&lt;/strong&gt;庸뛸뗻柳&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹뛳쪬啖쥖쭧&lt;/strong&gt;庸뛵랠屢꾾房쥒麵뾀땻謗넎숭劉亂쩎柳썬뉜쏏欖螟괜맄蔑賴썬솑麵봳싅&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了윌딦丹괝&lt;/strong&gt;庸11邏뾆쨃鏤邏뽼좁懶궘榜좭딨付3卵了-5卵了윇잸螟괦쨃蓼좭꼹疸쀣줰駱뷗먽꿘끵쫸궅欖쥔쨤燎괝流쉰궏&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;坍궈丹괝&lt;/strong&gt;庸&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Task01庸뛵럌謄뻔솑達먽북謗썶짿1邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task02庸뛶騰륀뉦짿2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task03庸뛸訪놴륀뉦짿2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task04庸뛴螟뒩맴庸2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task05庸뛷뛶쏱庸2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task06庸뛵럆募먿앻낐雷庸2邏뾆쨀&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;放뷚즩丹괝&lt;/strong&gt;庸&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/datawhalechina/team-learning/blob/master/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86/%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1.md"&gt;https://github.com/datawhalechina/team-learning/blob/master/갷暮酩柳됁뭡낹/駱뷗먻짙궈.md&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-賴썭꼹茗륃옡椧孵流" class="anchor" aria-hidden="true" href="#賴썭꼹茗륃옡椧孵流"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;賴썭꼹茗륃옡庸걵椧孵流庸&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;某俯放얹땙庸&lt;/strong&gt; 坍봲丹륂륃옡眄賴썭꼹庸껂丹륁싒滂喇쥕똞疸쥓쭣곚庸껀烙疸喇쥕뺘椧孵流鏤덛옡&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹좯쮣숴&lt;/strong&gt;庸뛸즴遼庸껀띝燎걾쨃療뀐빼&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹좮즧某&lt;/strong&gt;庸뚌agicyangrebornZH療뻔썭&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹뫦啖&lt;/strong&gt;庸뛶꿂깩否&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;丹뛳쪬啖쥖쭧&lt;/strong&gt;庸뙳ython癩쥕랿꿃끼庸껃쫸方깬丹륂꿣酩柳됊쨃亂좭돓꼹燎괛丹뛶뚟갡瞭넌밳낹쫸궅&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;了윌딦丹괝&lt;/strong&gt;庸12邏뾆쨃鏤邏뽼좁懶궘榜좭딨付3卵了-5卵了윇잸螟괦쨃蓼좭꼹疸쀣줰駱뷗먽꿘끵쫸궅欖쥔쨤燎괝流쉰궏&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;坍궈丹괝&lt;/strong&gt;庸&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Task01庸뛷옡孵떤갡瞭넖짿2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task02庸뛵돓꼹琉榴庸2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task03庸뛶곷籃剌봲뺆&amp;amp;곷籃괝길庸2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task04庸뛵뺘拏괝길&amp;amp;淚뫤룲方憺떮짿2邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task05庸뛵뺘拏撥꽨낻庸3邏뾆쨀&lt;/li&gt;
&lt;li&gt;Task06庸뛵싒滂賴얺낹庸1邏뾆쨀&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;放뷚즩丹괝&lt;/strong&gt;庸&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/datawhalechina/team-learning/blob/master/%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9B%EF%BC%88%E6%88%BF%E7%A7%9F%E9%A2%84%E6%B5%8B%EF%BC%89/%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1.md"&gt;https://github.com/datawhalechina/team-learning/blob/master/賴썭꼹茗륃옡庸걵椧孵流庸/駱뷗먻짙궈.md&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-擔倣갳" class="anchor" aria-hidden="true" href="#擔倣갳"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;擔倣갳&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;柳뻔 CSDN 갰 Github 榜뷙걍&lt;/li&gt;
&lt;li&gt;깰坍궈丹괝뉟찉發껀솑達멆쨃丹껁갫냁뙌솑達먾샚房Blog&lt;/li&gt;
&lt;li&gt;喇뻕싎縷뫣짙궈걁蔞뮋졞곐喇뻖쭧곕꼬庸건끰Blog逢쮡꿘庸괦쨃걇付쉴뭊喇뻖쭧房뻗쉌&lt;/li&gt;
&lt;li&gt;燎쀥깰了윉곕꼬眄낿駱뷛븭疸쥗쀤궏賚쮠짺庸껃뭚궘쥔솑達먾쭧&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>datawhalechina</author><guid isPermaLink="false">https://github.com/datawhalechina/team-learning</guid><pubDate>Sun, 05 Jan 2020 00:08:00 GMT</pubDate></item><item><title>Azure/MachineLearningNotebooks #9 in Jupyter Notebook, Today</title><link>https://github.com/Azure/MachineLearningNotebooks</link><description>&lt;p&gt;&lt;i&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning | Microsoft&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-azure-machine-learning-service-example-notebooks" class="anchor" aria-hidden="true" href="#azure-machine-learning-service-example-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Azure Machine Learning service example notebooks&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks demonstrating the &lt;a href="https://azure.microsoft.com/en-us/services/machine-learning-service/" rel="nofollow"&gt;Azure Machine Learning&lt;/a&gt; Python SDK which allows you to build, train, deploy and manage machine learning solutions using Azure.  The AML SDK allows you the choice of using local or cloud compute resources, while managing and maintaining the complete data science workflow from the cloud.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/machine-learning/service/media/concept-azure-machine-learning-architecture/workflow.png"&gt;&lt;img src="https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/machine-learning/service/media/concept-azure-machine-learning-architecture/workflow.png" alt="Azure ML Workflow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-installation" class="anchor" aria-hidden="true" href="#quick-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick installation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install azureml-sdk&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Read more detailed instructions on &lt;a href="./NBSETUP.md"&gt;how to set up your environment&lt;/a&gt; using Azure Notebook service, your own Jupyter notebook server, or Docker.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-navigate-and-use-the-example-notebooks" class="anchor" aria-hidden="true" href="#how-to-navigate-and-use-the-example-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to navigate and use the example notebooks?&lt;/h2&gt;
&lt;p&gt;If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, you should always run the &lt;a href="./configuration.ipynb"&gt;Configuration&lt;/a&gt; notebook first when setting up a notebook library on a new machine or in a new environment. It configures your notebook library to connect to an Azure Machine Learning workspace, and sets up your workspace and compute to be used by many of the other examples.
This &lt;a href=".index.md"&gt;index&lt;/a&gt; should assist in navigating the Azure Machine Learning notebook samples and encourage efficient retrieval of topics and content.&lt;/p&gt;
&lt;p&gt;If you want to...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;...try out and explore Azure ML, start with image classification tutorials: &lt;a href="./tutorials/img-classification-part1-training.ipynb"&gt;Part 1 (Training)&lt;/a&gt; and &lt;a href="./tutorials/img-classification-part2-deploy.ipynb"&gt;Part 2 (Deployment)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...learn about experimentation and tracking run history, first &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;train within Notebook&lt;/a&gt;, then try &lt;a href="./how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"&gt;training on remote VM&lt;/a&gt; and &lt;a href="./how-to-use-azureml/training/logging-api/logging-api.ipynb"&gt;using logging APIs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...train deep learning models at scale, first learn about &lt;a href="./how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"&gt;Machine Learning Compute&lt;/a&gt;, and then try &lt;a href="./how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"&gt;distributed hyperparameter tuning&lt;/a&gt; and &lt;a href="./how-to-use-azureml/training-with-deep-learning/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"&gt;distributed training&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...deploy models as a realtime scoring service, first learn the basics by &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;training within Notebook and deploying to Azure Container Instance&lt;/a&gt;, then learn how to &lt;a href="./how-to-use-azureml/deployment/register-model-create-image-deploy-service/register-model-create-image-deploy-service.ipynb"&gt;register and manage models, and create Docker images&lt;/a&gt;, and &lt;a href="./how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"&gt;production deploy models on Azure Kubernetes Cluster&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...deploy models as a batch scoring service, first &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;train a model within Notebook&lt;/a&gt;, learn how to &lt;a href="./how-to-use-azureml/deployment/register-model-create-image-deploy-service/register-model-create-image-deploy-service.ipynb"&gt;register and manage models&lt;/a&gt;, then &lt;a href="./how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"&gt;create Machine Learning Compute for scoring compute&lt;/a&gt;, and &lt;a href="https://aka.ms/pl-batch-scoring" rel="nofollow"&gt;use Machine Learning Pipelines to deploy your model&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...monitor your deployed models, learn about using &lt;a href="./how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"&gt;App Insights&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;The &lt;a href="./tutorials"&gt;Tutorials&lt;/a&gt; folder contains notebooks for the tutorials described in the &lt;a href="https://aka.ms/aml-docs" rel="nofollow"&gt;Azure Machine Learning documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-azure-ml" class="anchor" aria-hidden="true" href="#how-to-use-azure-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use Azure ML&lt;/h2&gt;
&lt;p&gt;The &lt;a href="./how-to-use-azureml"&gt;How to use Azure ML&lt;/a&gt; folder contains specific examples demonstrating the features of the Azure Machine Learning SDK&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/training"&gt;Training&lt;/a&gt; - Examples of how to build models using Azure ML's logging and execution capabilities on local and remote compute targets&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/training-with-deep-learning"&gt;Training with Deep Learning&lt;/a&gt; - Examples demonstrating how to build deep learning models using estimators and parameter sweeps&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/manage-azureml-service"&gt;Manage Azure ML Service&lt;/a&gt; - Examples how to perform tasks, such as authenticate against Azure ML service in different ways.&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/automated-machine-learning"&gt;Automated Machine Learning&lt;/a&gt; - Examples using Automated Machine Learning to automatically generate optimal machine learning pipelines and models&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/machine-learning-pipelines"&gt;Machine Learning Pipelines&lt;/a&gt; - Examples showing how to create and use reusable pipelines for training and batch scoring&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/deployment"&gt;Deployment&lt;/a&gt; - Examples showing how to deploy and manage machine learning models and solutions&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/azure-databricks"&gt;Azure Databricks&lt;/a&gt; - Examples showing how to use Azure ML with Azure Databricks&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/monitor-models"&gt;Monitor Models&lt;/a&gt; - Examples showing how to enable model monitoring services such as DataDrift&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Quickstarts, end-to-end tutorials, and how-tos on the &lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/" rel="nofollow"&gt;official documentation site for Azure Machine Learning service&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py" rel="nofollow"&gt;Python SDK reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure ML Data Prep SDK &lt;a href="https://aka.ms/data-prep-sdk" rel="nofollow"&gt;overview&lt;/a&gt;, &lt;a href="https://aka.ms/aml-data-prep-apiref" rel="nofollow"&gt;Python SDK reference&lt;/a&gt;, and &lt;a href="https://aka.ms/aml-data-prep-notebooks" rel="nofollow"&gt;tutorials and how-tos&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-community-repository" class="anchor" aria-hidden="true" href="#community-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community Repository&lt;/h2&gt;
&lt;p&gt;Visit this &lt;a href="https://github.com/microsoft/MLOps/tree/master/examples"&gt;community repository&lt;/a&gt; to find useful end-to-end sample notebooks. Also, please follow these &lt;a href="https://github.com/microsoft/MLOps/blob/master/contributing.md"&gt;contribution guidelines&lt;/a&gt; when contributing to this repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects-using-azure-machine-learning" class="anchor" aria-hidden="true" href="#projects-using-azure-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects using Azure Machine Learning&lt;/h2&gt;
&lt;p&gt;Visit following repos to see projects contributed by Azure ML users:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Azure/AMLSamples"&gt;AMLSamples&lt;/a&gt; Number of end-to-end examples, including face recognition, predictive maintenance, customer churn and sentiment analysis.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/nlp"&gt;Learn about Natural Language Processing best practices using Azure Machine Learning service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Microsoft/AzureML-BERT"&gt;Pre-Train BERT models using Azure Machine Learning service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/amynic/azureml-sdk-fashion"&gt;Fashion MNIST with Azure ML SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/katiehouse3/microsoft-azure-ml-notebooks"&gt;UMass Amherst Student Samples&lt;/a&gt; - A number of end-to-end machine learning notebooks, including machine translation, image classification, and customer churn, created by students in the 696DS course at UMass Amherst.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-datatelemetry" class="anchor" aria-hidden="true" href="#datatelemetry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data/Telemetry&lt;/h2&gt;
&lt;p&gt;This repository collects usage data and sends it to Mircosoft to help improve our products and services. Read Microsoft's &lt;a href="https://privacy.microsoft.com/en-US/privacystatement" rel="nofollow"&gt;privacy statement to learn more&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To opt out of tracking, please go to the raw markdown or .ipynb files and remove the following line of code:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/README.png)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This URL will be slightly different depending on the file.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c46ad7e1ce8c1c4ffaf085f08619fe9c1b16eb50/68747470733a2f2f506978656c53657276657232303139303432333131343233382e617a75726577656273697465732e6e65742f6170692f696d7072657373696f6e732f4d616368696e654c6561726e696e674e6f7465626f6f6b732f524541444d452e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c46ad7e1ce8c1c4ffaf085f08619fe9c1b16eb50/68747470733a2f2f506978656c53657276657232303139303432333131343233382e617a75726577656273697465732e6e65742f6170692f696d7072657373696f6e732f4d616368696e654c6561726e696e674e6f7465626f6f6b732f524541444d452e706e67" alt="Impressions" data-canonical-src="https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/README.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Azure</author><guid isPermaLink="false">https://github.com/Azure/MachineLearningNotebooks</guid><pubDate>Sun, 05 Jan 2020 00:09:00 GMT</pubDate></item><item><title>codebasics/py #10 in Jupyter Notebook, Today</title><link>https://github.com/codebasics/py</link><description>&lt;p&gt;&lt;i&gt;Repository to store sample python programs for python learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-py" class="anchor" aria-hidden="true" href="#py"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;py&lt;/h1&gt;
&lt;p&gt;Repository to store sample python programs for python learning
Youtube channel &lt;a href="https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ" rel="nofollow"&gt;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>codebasics</author><guid isPermaLink="false">https://github.com/codebasics/py</guid><pubDate>Sun, 05 Jan 2020 00:10:00 GMT</pubDate></item><item><title>jeffheaton/t81_558_deep_learning #11 in Jupyter Notebook, Today</title><link>https://github.com/jeffheaton/t81_558_deep_learning</link><description>&lt;p&gt;&lt;i&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-t81-558applications-of-deep-neural-networks" class="anchor" aria-hidden="true" href="#t81-558applications-of-deep-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.wustl.edu" rel="nofollow"&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instructor: &lt;a href="https://sites.wustl.edu/jeffheaton/" rel="nofollow"&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href="https://github.com/jeffheaton"&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Section 2. Spring 2020, Monday, 2:30 PM - 5:20 PM Online &amp;amp; Cupples I / 215&lt;/li&gt;
&lt;li&gt;Section 1. Spring 2020, Monday, 6:00 PM - 9:00 PM Online &amp;amp; Cupples I / 215&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-course-description" class="anchor" aria-hidden="true" href="#course-description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course Description&lt;/h1&gt;
&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-objectives" class="anchor" aria-hidden="true" href="#objectives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Objectives&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt;
&lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt;
&lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus&lt;/h1&gt;
&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments.  &lt;a href="https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/pdf/t81_558_spring2020_syllabus.pdf" rel="nofollow"&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Module&lt;/th&gt;
&lt;th&gt;Content&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_01_1_overview.ipynb"&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 01/13/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (first meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_02_1_python_pandas.ipynb"&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 01/27/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;	Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class1.ipynb"&gt;Module 1 Assignment&lt;/a&gt; Due: 01/28/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_03_1_neural_net.ipynb"&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 02/03/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class2.ipynb"&gt;Module 2: Assignment&lt;/a&gt; due: 02/04/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_04_1_feature_encode.ipynb"&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 02/10/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class3.ipynb"&gt;Module 3 Assignment&lt;/a&gt; due: 02/11/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_05_1_reg_ridge_lasso.ipynb"&gt;Module 5&lt;/a&gt;&lt;br&gt;Week of 02/17/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class4.ipynb"&gt;Module 4 Assignment&lt;/a&gt; due: 02/18/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_06_1_python_images.ipynb"&gt;Module 6&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 02/24/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&lt;ul&gt;	Part 6.1: Image Processing in Python&lt;li&gt;Part 6.2: Keras Neural Networks for MINST and Fashion MINST&lt;/li&gt;&lt;li&gt;Part 6.3: Implementing a ResNet in Keras&lt;/li&gt;&lt;li&gt;Part 6.4: Computer Vision with OpenCV&lt;/li&gt;&lt;li&gt;Part 6.5: Recognizing Multiple Images with Darknet&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class5.ipynb"&gt;Module 5 Assignment&lt;/a&gt; due: 02/25/2020&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (2nd Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_07_1_gan_intro.ipynb"&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 03/02/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 7: GAN&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&lt;li&gt;Part 7.2: Implementing a GAN in Keras&lt;/li&gt;&lt;li&gt;Part 7.3: Face Generation with StyleGAN and Python&lt;/li&gt;&lt;li&gt;Part 7.4: GANS for Semi-Supervised Learning in Keras&lt;/li&gt;&lt;li&gt;Part 7.5: An Overview of GAN Research&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class6.ipynb"&gt;Module 6 Assignment&lt;/a&gt; due: 03/03/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_08_1_kaggle_intro.ipynb"&gt;Module 8&lt;/a&gt;&lt;br&gt;Week of 03/16/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&lt;li&gt;Part 8.5: Current Semester's Kaggle&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class7.ipynb"&gt;Module 7 Assignment&lt;/a&gt; due: 03/17/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_09_1_keras_transfer.ipynb"&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 03/23/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&lt;li&gt;Part 9.2: Popular Pretrained Neural Networks for Keras. &lt;/li&gt;&lt;li&gt;Part 9.3: Transfer Learning for Computer Vision and Keras&lt;/li&gt;&lt;li&gt;Part 9.4: Transfer Learning for Languages and Keras&lt;/li&gt;&lt;li&gt;Part 9.5: Transfer Learning for Keras Feature Engineering&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class8.ipynb"&gt;Module 8 Assignment&lt;/a&gt; due: 03/24/2020&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (3rd Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_10_1_timeseries.ipynb"&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 03/30/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, TensorFlow and Keras&lt;/li&gt;&lt;li&gt;Part 10.2: Programming LSTM with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.3: Image Captioning with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.4: Temporal CNN in Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.5: Predicting the Stock Market with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class9.ipynb"&gt;Module 9 Assignment&lt;/a&gt; due: 03/31/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_11_01_spacy.ipynb"&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 04/06/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 11.1: Getting Started with Spacy in Python&lt;/li&gt;&lt;li&gt;Part 11.2: Word2Vec and Text Classification&lt;/li&gt;&lt;li&gt;Part 11.3: Natural Language Processing with Spacy and Keras&lt;/li&gt;&lt;li&gt;Part 11.4: What are Embedding Layers in Keras&lt;/li&gt;&lt;li&gt;Part 11.5: Learning English from Scratch with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class10.ipynb"&gt;Module 10 Assignment&lt;/a&gt; due: 04/07/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_12_01_ai_gym.ipynb"&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 04/13/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Kaggle Assignment due: 04/13/2020 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&lt;li&gt;Part 12.5: How Alpha Zero used Reinforcement Learning to Master Chess&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_13_01_flask.ipynb"&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 04/20/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 13.1: Deploying a Model to AWS&lt;/li&gt;&lt;li&gt;Part 13.2: Flask and Deep Learning Web Services&lt;/li&gt;&lt;li&gt;Part 13.3: AI at the Edge: Using Keras on a Mobile Device&lt;/li&gt;&lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&lt;li&gt;Part 13.5: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (4th Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_14_01_automl.ipynb"&gt;Module 14&lt;/a&gt;&lt;br&gt;Week of 04/27/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 14: Other Neural Network Techniques&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 14.1: What is AutoML&lt;/li&gt;&lt;li&gt;Part 14.2: Using Denoising AutoEncoders in Keras&lt;/li&gt;&lt;li&gt;Part 14.3: Training an Intrusion Detection System with KDD99&lt;/li&gt;&lt;li&gt;Part 14.4: Anomaly Detection in Keras&lt;/li&gt;&lt;li&gt;Part 14.5: New Technology in Deep Learning&lt;/li&gt;&lt;li&gt;Final Project due 05/04/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://data.heatonresearch.com/data/t81-558/index.html" rel="nofollow"&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jeffheaton</author><guid isPermaLink="false">https://github.com/jeffheaton/t81_558_deep_learning</guid><pubDate>Sun, 05 Jan 2020 00:11:00 GMT</pubDate></item><item><title>dataquestio/solutions #12 in Jupyter Notebook, Today</title><link>https://github.com/dataquestio/solutions</link><description>&lt;p&gt;&lt;i&gt;Solutions for projects.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dataquest-project-solutions" class="anchor" aria-hidden="true" href="#dataquest-project-solutions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dataquest Project Solutions&lt;/h1&gt;
&lt;p&gt;This repository is a series of notebooks that show solutions for the &lt;a href="https://www.dataquest.io/apply" rel="nofollow"&gt;projects&lt;/a&gt; at &lt;a href="https://www.dataquest.io/" rel="nofollow"&gt;Dataquest.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, there are always going to be multiple ways to solve any one problem, so these notebooks just show one possible solution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission9Solutions.ipynb"&gt;Guided Project: Explore U.S. Births&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission103Solutions.ipynb"&gt;Guided Project: Customizing Data Visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission201Solution.ipynb"&gt;Guided Project: Star Wars survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission202Solution.ipynb"&gt;Guided Project: Police killings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission205Solutions.ipynb"&gt;Guided Project: Visualizing Pixar's Roller Coaster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission207Solutions.ipynb"&gt;Guided Project: Using Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission209Solution.ipynb"&gt;Guided Project: Analyzing movie reviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission210Solution.ipynb"&gt;Guided Project: Winning Jeopardy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission211Solution.ipynb"&gt;Guided Project: Predicting board game reviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission213Solution.ipynb"&gt;Guided Project: Predicting bike rentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission215Solutions.ipynb"&gt;Guided Project: Preparing data for SQLite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission216Solutions.ipynb"&gt;Guided Project: Creating relations in SQLite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission217Solutions.ipynb"&gt;Guided Project: Analyzing NYC High School Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission146Solutions.ipynb"&gt;Guided Project: Visualizing Earnings Based On College Majors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission218Solution.ipynb"&gt;Guided Project: Exploring Gun Deaths in the US&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission219Solution.ipynb"&gt;Guided Project: Analyzing Thanksgiving Dinner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission227Solutions.ipynb"&gt;Guided Project: Analyzing Wikipedia Pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission177Solutions.ipynb"&gt;Guided Project: Analyzing Stock Prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission188Solution.ipynb"&gt;Guided Project: Creating A Kaggle Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission167Solutions.ipynb"&gt;Guided Project: Analyzing Startup Fundraising Deals from Crunchbase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission240Solutions.ipynb"&gt;Guided Project: Predicting House Sale Prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission191Solutions.ipynb"&gt;Guided Project: Answering Business Questions using SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission193Solutions.ipynb"&gt;Guided Project: Designing and Creating a Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission288Solutions.ipynb"&gt;Guided Project: Investigating Fandango's Movie Rating System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission277Solutions.Rmd"&gt;Guided Project: Forest Fires Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission327Solutions.Rmd"&gt;Guided Project: NYC Schools Perceptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission348Solutions.ipynb"&gt;Guided Project: Clean and Analyze Employee Exit Surveys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission449Solutions.Rmd"&gt;Guided Project: Finding the Best Markets to Advertise In&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dataquestio</author><guid isPermaLink="false">https://github.com/dataquestio/solutions</guid><pubDate>Sun, 05 Jan 2020 00:12:00 GMT</pubDate></item><item><title>zergtant/pytorch-handbook #13 in Jupyter Notebook, Today</title><link>https://github.com/zergtant/pytorch-handbook</link><description>&lt;p&gt;&lt;i&gt;pytorch handbook僚疸燎선留넎뚟達뷚쏈庸껂띭蓼僚辣쉰궐啖辣껁랱得꺿覓PyTorch謗發껁임擥뷘솑達먼끰得껂맶皿윊뚟燎끪纜봴딛庸껀윇쟔깬냚眄ㅀytorch賴뙎뺆뻘뻘뛷쯶流放돿찋放낊坍봱갫謗넏몗&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-疸쇉둖곎깛ytorch-handbook" class="anchor" aria-hidden="true" href="#pytorch-疸쇉둖곎깛ytorch-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch 疸쇉둖곎껊짿pytorch handbook庸&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png"&gt;&lt;img src="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png" alt="pytorch" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-達뷚쏈坍某" class="anchor" aria-hidden="true" href="#達뷚쏈坍某"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;達뷚쏈坍某&lt;/h2&gt;
&lt;p&gt;謗뙍땶疸燎선留넎뚟達뷚쏈庸껂띭蓼僚辣쉰궐啖辣껁랱得꺿覓PyTorch謗發껁임擥뷘솑達먼끰得껂맶皿윊뚟燎끪纜봴딛&lt;/p&gt;
&lt;p&gt;覓쐓줉燎섟줰陋언좁燎괠똃庸껀랻뙍솏賴뙎뺆眄了윈뙌끡啖疸啖母놳잵眄滂雷뙖쨃喇뻗찇껀싳坍뒧짋發뻖뱤賴섡庸껁갬憺뛴랻鏤疸쀤쨋覓뻓쟔傅疸귄雷喇썬庸껁뒃曇邏丹윈끡&lt;/p&gt;
&lt;p&gt;瀏쐔줝駱뷗먾뚟燎喇뻘륄眄끰欄됊쨃낿了욇yTorch達喇뻓잸雷쇉띲雷썶쨃疸덙랿啖쥓쨐넍손丹껀둓眠전丹좪&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-곋燎섣싮僚" class="anchor" aria-hidden="true" href="#곋燎섣싮僚"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;곋燎섣싮僚&lt;/h2&gt;
&lt;p&gt;覓쐓줉PyTorch곋燎섡띲謗쇒쨃賴뙎뺆眄곋燎섟쨐疸꾏yTorch곋燎섪쨃淡뢙깨疸얶&lt;/p&gt;
&lt;p&gt;2019.10.10 PyTorch剌쒾즲끰辣1.3眄溟丹뛶곋&lt;/p&gt;
&lt;p&gt;剌쒾즲뻘뻕옒放됀숣鏤 坍滅낊丹껀뻔쩐쉋1.3&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-qq-3毛" class="anchor" aria-hidden="true" href="#qq-3毛"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QQ 3毛&lt;/h2&gt;
&lt;p&gt;毛번낑庸773681699&lt;/p&gt;
&lt;p&gt;곪끮啖껂짒滅&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Pytorch-Handbook-3.png"&gt;&lt;img src="Pytorch-Handbook-3.png" alt="QR" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="//shang.qq.com/wpa/qunwpa?idkey=ee402d5f0e7732b2171e643d729177ce55ac404eafda5edd9b740d73fabe6a96" rel="nofollow"&gt;좬逢쮡꿘먼봲쭧 꾏yTorch Handbook 啖벉예3毛벆&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1毛(985896536)剌쒽지庸2毛(681980831)剌쒽지&lt;/p&gt;
&lt;p&gt;疸꽫부꽨먻줁&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-放얹땙" class="anchor" aria-hidden="true" href="#放얹땙"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;放얹땙&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;淡쉱댳鋒뙌걂駱放眠얹꿘끯issue갰PR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PR了윋싱柳뻕곋燎&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;燎괠딡孵떣放眠얹꿘끯issue&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;方&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-眠쉰쪳" class="anchor" aria-hidden="true" href="#眠쉰쪳"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;眠쉰쪳&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-蓂섟茗맗ytorch-봴딛" class="anchor" aria-hidden="true" href="#蓂섟茗맗ytorch-봴딛"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟茗멆쨐PyTorch 봴딛&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter1/1.1-pytorch-introduction.md"&gt;PyTorch 酩坍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.2-pytorch-installation.md"&gt;PyTorch 꿢螺냜攬&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md"&gt;PyTorch 瀏쐔줝駱뷗멆쨐60갡蜂纜봴딛庸건숮雷좶쨀&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter1/1_tensor_tutorial.ipynb"&gt;欖멀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/2_autograd_tutorial.ipynb"&gt;Autograd庸뛷쀤궏劉亂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/3_neural_networks_tutorial.ipynb"&gt;明륂즲母놶즾&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/4_cifar10_tutorial.ipynb"&gt;房쇊즦疸疸쀤갡袂謄&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/5_data_parallel_tutorial.ipynb"&gt;괟싵庸뛵돓꼹嵐윋몗邏낹庸건뱈GPU庸&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.4-pytorch-resource.md"&gt;眠전滂留넋즮某&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-蓂섟줇茗-癩쥖" class="anchor" aria-hidden="true" href="#蓂섟줇茗-癩쥖"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줇茗 癩쥖&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟국-pytorch-癩쥖" class="anchor" aria-hidden="true" href="#蓂섟국-pytorch-癩쥖"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟국 PyTorch 癩쥖&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.1.pytorch-basics-tensor.ipynb"&gt;欖멀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.2-pytorch-basics-autograd.ipynb"&gt;쀤궏劉亂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.3-pytorch-basics-nerual-network.ipynb"&gt;明륂즲母놶즾깬nn得꺿쨎깼謄빮ptm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.4-pytorch-basics-data-loader.ipynb"&gt;賴썭꼹眄먿쫗得껄묷邏낹&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟줇국-瀏쐔줝駱뷗먼맄蔑끩賴썬솑낹" class="anchor" aria-hidden="true" href="#蓂섟줇국-瀏쐔줝駱뷗먼맄蔑끩賴썬솑낹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줇국 瀏쐔줝駱뷗먼맄蔑끩賴썬솑낹&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.2-deep-learning-basic-mathematics.ipynb"&gt;瀏쐔줝駱뷗먼맄蔑끩賴썬솑낹&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟잴국-明륂즲母놶즾酩坍" class="anchor" aria-hidden="true" href="#蓂섟잴국-明륂즲母놶즾酩坍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟잴국 明륂즲母놶즾酩坍&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.3-deep-learning-neural-network-introduction.ipynb"&gt;明륂즲母놶즾酩坍&lt;/a&gt;  柳뻞쨐燎섢먼랻燎선럃擔覓뻔쭱蚌眄ㄴdge곕欖憺뛴얄留庸껃싱擔쮺hrome Firefox곕欖聊봲랢&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂선띝국-꽁椧明륂즲母놶즾" class="anchor" aria-hidden="true" href="#蓂선띝국-꽁椧明륂즲母놶즾"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂선띝국 꽁椧明륂즲母놶즾&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.4-cnn.ipynb"&gt;꽁椧明륂즲母놶즾&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟줎국-籃쀦꿢明륂즲母놶즾" class="anchor" aria-hidden="true" href="#蓂섟줎국-籃쀦꿢明륂즲母놶즾"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줎국 籃쀦꿢明륂즲母놶즾&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.5-rnn.ipynb"&gt;籃쀦꿢明륂즲母놶즾&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-蓂섟잴茗-丹륃잇" class="anchor" aria-hidden="true" href="#蓂섟잴茗-丹륃잇"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟잴茗 丹륃잇&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟국-logistic騰륀뉛줇갡袂" class="anchor" aria-hidden="true" href="#蓂섟국-logistic騰륀뉛줇갡袂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟국 logistic騰륀뉛줇갡袂&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.1-logistic-regression.ipynb"&gt;logistic騰륀뉛줇갡袂&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟줇국-cnnmnist賴썭꼹副곎뙍돓駱放걂" class="anchor" aria-hidden="true" href="#蓂섟줇국-cnnmnist賴썭꼹副곎뙍돓駱放걂"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줇국 CNN:MNIST賴썭꼹副곎뙍돓駱放걂&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.2-mnist.ipynb"&gt;CNN:MNIST賴썭꼹副곎뙍돓駱放걂&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟잴국-rnn丹뤿쭒뛷쯶sin孵流귺os" class="anchor" aria-hidden="true" href="#蓂섟잴국-rnn丹뤿쭒뛷쯶sin孵流귺os"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟잴국 RNN丹뤿쭒庸뛸뛷쯶Sin孵流귞os&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.3-rnn.ipynb"&gt;RNN丹뤿쭒庸뛸뛷쯶Sin孵流귞os&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-蓂선띝茗-끯父" class="anchor" aria-hidden="true" href="#蓂선띝茗-끯父"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂선띝茗 끯父&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟국-fine-tuning" class="anchor" aria-hidden="true" href="#蓂섟국-fine-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟국 Fine-tuning&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.1-fine-tuning.ipynb"&gt;Fine-tuning&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟줇국-낊倣깼" class="anchor" aria-hidden="true" href="#蓂섟줇국-낊倣깼"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줇국 낊倣깼&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.1-visdom.ipynb"&gt;visdom&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.2-tensorboardx.ipynb"&gt;tensorboardx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.3-cnn-visualizing.ipynb"&gt;낊倣깼낹倣꽁椧明륂즲母놶즾&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟잴국-fastai" class="anchor" aria-hidden="true" href="#蓂섟잴국-fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟잴국 Fast.ai&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.3-fastai.ipynb"&gt;Fast.ai&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂선띝국-房쇊즦眄疸啖剌" class="anchor" aria-hidden="true" href="#蓂선띝국-房쇊즦眄疸啖剌"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂선띝국 房쇊즦眄疸啖剌&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟줎국-邏뚆pu嵐윋몗房쇊즦" class="anchor" aria-hidden="true" href="#蓂섟줎국-邏뚆pu嵐윋몗房쇊즦"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줎국 邏뙪PU嵐윋몗房쇊즦&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.5-multiply-gpu-parallel-training.ipynb"&gt;邏뙪PU嵐윋몗房뫦숭&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-蓂섟줎茗-擥덚댣" class="anchor" aria-hidden="true" href="#蓂섟줎茗-擥덚댣"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줎茗 擥덚댣&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟국-kaggle坍某" class="anchor" aria-hidden="true" href="#蓂섟국-kaggle坍某"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟국 Kaggle坍某&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.1-kaggle.md"&gt;Kaggle坍某&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟줇국-某瞭깼賴썭꼹" class="anchor" aria-hidden="true" href="#蓂섟줇국-某瞭깼賴썭꼹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줇국 某瞭깼賴썭꼹&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟잴국-房뫦숭燎쥗븭倣" class="anchor" aria-hidden="true" href="#蓂섟잴국-房뫦숭燎쥗븭倣"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟잴국 房뫦숭燎쥗븭倣&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.3-Fashion-MNIST.ipynb"&gt;Fashion MNIST 騰쮠갡袂&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-蓂선띝국-쀦윋싨傍邏낹" class="anchor" aria-hidden="true" href="#蓂선띝국-쀦윋싨傍邏낹"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂선띝국 쀦윋싨傍邏낹&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-蓂섟줎국-꼞낿謗瘤" class="anchor" aria-hidden="true" href="#蓂섟줎국-꼞낿謗瘤"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟줎국 꼞낿謗瘤&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-蓂선쇊-滂留" class="anchor" aria-hidden="true" href="#蓂선쇊-滂留"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂선쇊 滂留&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-蓂섟잮茗-傅濫" class="anchor" aria-hidden="true" href="#蓂섟잮茗-傅濫"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;蓂섟잮茗 傅濫&lt;/h3&gt;
&lt;p&gt;transforms眄辣젆댣賂꽧쪺某&lt;/p&gt;
&lt;p&gt;pytorch眄邏쐔쫹돓某&lt;/p&gt;
&lt;p&gt;pytorch眄憺떤깼謄뻕某&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67" alt="" data-canonical-src="https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn" rel="nofollow"&gt;燎섟쪺嶝覓뻖린放쐓줢母쒼냀-咐륀뎭疸뛵擔覓-眠전낿雷좬쨆쐓줢 3.0  疸쇈띻邏傅房전낊꼞房쉳찉發껃쉊낊&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zergtant</author><guid isPermaLink="false">https://github.com/zergtant/pytorch-handbook</guid><pubDate>Sun, 05 Jan 2020 00:13:00 GMT</pubDate></item><item><title>dibgerge/ml-coursera-python-assignments #14 in Jupyter Notebook, Today</title><link>https://github.com/dibgerge/ml-coursera-python-assignments</link><description>&lt;p&gt;&lt;i&gt;Python assignments for the machine learning class by andrew ng on coursera with complete submission for grading capability and re-written instructions.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-coursera-machine-learning-mooc-by-andrew-ng" class="anchor" aria-hidden="true" href="#coursera-machine-learning-mooc-by-andrew-ng"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;Coursera Machine Learning MOOC by Andrew Ng&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-python-programming-assignments" class="anchor" aria-hidden="true" href="#python-programming-assignments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Programming Assignments&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="machinelearning.jpg"&gt;&lt;img src="machinelearning.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repositry contains the python versions of the programming assignments for the &lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;Machine Learning online class&lt;/a&gt; taught by Professor Andrew Ng. This is perhaps the most popular introductory online machine learning class. In addition to being popular, it is also one of the best Machine learning classes any interested student can take to get started with machine learning. An unfortunate aspect of this class is that the programming assignments are in MATLAB or OCTAVE, probably because this class was made before python became the go-to language in machine learning.&lt;/p&gt;
&lt;p&gt;The Python machine learning ecosystem has grown exponentially in the past few years, and is still gaining momentum. I suspect that many students who want to get started with their machine learning journey would like to start it with Python also. It is for those reasons I have decided to re-write all the programming assignments in Python, so students can get acquainted with its ecosystem from the start of their learning journey.&lt;/p&gt;
&lt;p&gt;These assignments work seamlessly with the class and do not require any of the materials published in the MATLAB assignments. Here are some new and useful features for these sets of assignments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The assignments use &lt;a href="http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html" rel="nofollow"&gt;Jupyter Notebook&lt;/a&gt;, which provides an intuitive flow easier than the original MATLAB/OCTAVE assignments.&lt;/li&gt;
&lt;li&gt;The original assignment instructions have been completely re-written and the parts which used to reference MATLAB/OCTAVE functionality have been changed to reference its &lt;code&gt;python&lt;/code&gt; counterpart.&lt;/li&gt;
&lt;li&gt;The re-written instructions are now embedded within the Jupyter Notebook along with the &lt;code&gt;python&lt;/code&gt; starter code. For each assignment, all work is done solely within the notebook.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;python&lt;/code&gt; assignments can be submitted for grading. They were tested to work perfectly well with the original Coursera grader that is currently used to grade the MATLAB/OCTAVE versions of the assignments.&lt;/li&gt;
&lt;li&gt;After each part of a given assignment, the Jupyter Notebook contains a cell which prompts the user for submitting the current part of the assignment for grading.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-downloading-the-assignments" class="anchor" aria-hidden="true" href="#downloading-the-assignments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading the Assignments&lt;/h2&gt;
&lt;p&gt;To get started, you can start by either downloading a zip file of these assignments by clicking on the &lt;code&gt;Clone or download&lt;/code&gt; button. If you have &lt;code&gt;git&lt;/code&gt; installed on your system, you can clone this repository using :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;clone https://github.com/dibgerge/ml-coursera-python-assignments.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each assignment is contained in a separate folder. For example, assignment 1 is contained within the folder &lt;code&gt;Exercise1&lt;/code&gt;. Each folder contains two files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The assignment &lt;code&gt;jupyter&lt;/code&gt; notebook, which has a &lt;code&gt;.ipynb&lt;/code&gt; extension. All the code which you need to write will be written within this notebook.&lt;/li&gt;
&lt;li&gt;A python module &lt;code&gt;utils.py&lt;/code&gt; which contains some helper functions needed for the assignment. Functions within the &lt;code&gt;utils&lt;/code&gt; module are called from the python notebook. You do not need to modify or add any code to this file.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;p&gt;These assignments has been tested and developed using the following libraries:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- python==3.6.4
- numpy==1.13.3
- scipy==1.0.0
- matplotlib==2.1.2
- jupyter==1.0.0
- jupyter-client==5.0.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We recommend using at least these versions of the required libraries or later. Python 2 is not supported.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python-installation" class="anchor" aria-hidden="true" href="#python-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Installation&lt;/h2&gt;
&lt;p&gt;We highly recommend using anaconda for installing python. &lt;a href="https://www.anaconda.com/download/" rel="nofollow"&gt;Click here&lt;/a&gt; to go to Anaconda's download page. Make sure to download Python 3.6 version.
If you are on a windows machine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open the executable after download is complete and follow instructions.&lt;/li&gt;
&lt;li&gt;Once installation is complete, open &lt;code&gt;Anaconda prompt&lt;/code&gt; from the start menu. This will open a terminal with python enabled.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are on a linux machine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Open a terminal and navigate to the directory where Anaconda was downloaded.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the permission to the downloaded file so that it can be executed. So if the downloaded file name is &lt;code&gt;Anaconda3-5.1.0-Linux-x86_64.sh&lt;/code&gt;, then use the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;chmod a+x Anaconda3-5.1.0-Linux-x86_64.sh&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now, run the installation script using &lt;code&gt;./Anaconda3-5.1.0-Linux-x86_64.sh&lt;/code&gt;, and follow installation instructions in the terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once you have installed python, create a new python environment will all the requirements using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create -n machine_learning python=3.6 scipy=1 numpy=1.13 matplotlib=2.1 jupyter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the new environment is setup, activate it using (windows)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;activate machine_learning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or if you are on a linux machine&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source activate machine_learning 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our python environment all set up, we can start working on the assignments. To do so, navigate to the directory where the assignments were installed, and launch the jupyter notebook from the terminal using the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should automatically open a tab in the default browser. To start with assignment 1, open the notebook &lt;code&gt;./Exercise1/exercise1.ipynb&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python-tutorials" class="anchor" aria-hidden="true" href="#python-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Tutorials&lt;/h2&gt;
&lt;p&gt;If you are new to python and to &lt;code&gt;jupyter&lt;/code&gt; notebooks, no worries! There is a plethora of tutorials and documentation to get you started. Here are a few links which might be of help:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pythonprogramming.net/introduction-to-python-programming/" rel="nofollow"&gt;Python Programming&lt;/a&gt;: A turorial with videos about the basics of python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://cs231n.github.io/python-numpy-tutorial/" rel="nofollow"&gt;Numpy and matplotlib tutorial&lt;/a&gt;: We will be using numpy extensively for matrix and vector operations. This is great tutorial to get you started with using numpy and matplotlib for plotting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://medium.com/codingthesmartway-com-blog/getting-started-with-jupyter-notebook-for-python-4e7082bd5d46" rel="nofollow"&gt;Jupyter notebook&lt;/a&gt;: Getting started with the jupyter notebook.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/mstampfer/Coursera-Stanford-ML-Python/blob/master/Coursera%20Stanford%20ML%20Python%20wiki.ipynb"&gt;Python introduction based on the class's MATLAB tutorial&lt;/a&gt;: This is the equivalent of class's MATLAB tutorial, in python.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-caveats-and-tips" class="anchor" aria-hidden="true" href="#caveats-and-tips"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Caveats and tips&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In many of the exercises, the regularization parameter $\lambda$ is denoted as the variable name &lt;code&gt;lambda_&lt;/code&gt;, notice the underscore at the end of the name. This is because &lt;code&gt;lambda&lt;/code&gt; is a reserved python keyword, and should never be used as a variable name.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;code&gt;numpy&lt;/code&gt;, the function &lt;code&gt;dot&lt;/code&gt; is used to perform matrix multiplication. The operation '*' only does element-by-element multiplication (unlike MATLAB). If you are using python version 3.5+, the operator '@' is the new matrix multiplication, and it is equivalent to the &lt;code&gt;dot&lt;/code&gt; function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I would like to thank professor Andrew Ng and the crew of the Stanford Machine Learning class on Coursera for such an awesome class.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some of the material used, especially the code for submitting assignments for grading is based on &lt;a href="https://github.com/mstampfer/Coursera-Stanford-ML-Python"&gt;&lt;code&gt;mstampfer&lt;/code&gt;'s&lt;/a&gt; python implementation of the assignments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dibgerge</author><guid isPermaLink="false">https://github.com/dibgerge/ml-coursera-python-assignments</guid><pubDate>Sun, 05 Jan 2020 00:14:00 GMT</pubDate></item><item><title>slundberg/shap #15 in Jupyter Notebook, Today</title><link>https://github.com/slundberg/shap</link><description>&lt;p&gt;&lt;i&gt;A game theoretic approach to explain the output of any machine learning model.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.png"&gt;&lt;img src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_header.png" width="800" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/slundberg/shap" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/19de279a6f67f8eea3f52ccecc779c3e1aff55e7/68747470733a2f2f7472617669732d63692e6f72672f736c756e64626572672f736861702e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/slundberg/shap.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://mybinder.org/v2/gh/slundberg/shap/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SHAP (SHapley Additive exPlanations)&lt;/strong&gt; is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see &lt;a href="#citations"&gt;papers&lt;/a&gt; for details and citations).&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-install" class="anchor" aria-hidden="true" href="#install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install&lt;/h2&gt;
&lt;p&gt;Shap can be installed from either &lt;a href="https://pypi.org/project/shap" rel="nofollow"&gt;PyPI&lt;/a&gt; or &lt;a href="https://anaconda.org/conda-forge/shap" rel="nofollow"&gt;conda-forge&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;pip install shap
&lt;i&gt;or&lt;/i&gt;
conda install -c conda-forge shap
&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models" class="anchor" aria-hidden="true" href="#tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tree ensemble example with TreeExplainer (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)&lt;/h2&gt;
&lt;p&gt;While SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (&lt;a href="https://arxiv.org/abs/1802.03888" rel="nofollow"&gt;Tree SHAP arXiv paper&lt;/a&gt;). Fast C++ implementations are supported for &lt;em&gt;XGBoost&lt;/em&gt;, &lt;em&gt;LightGBM&lt;/em&gt;, &lt;em&gt;CatBoost&lt;/em&gt;, &lt;em&gt;scikit-learn&lt;/em&gt; and &lt;em&gt;pyspark&lt;/em&gt; tree models:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; xgboost
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load JS visualization code to notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train XGBoost model&lt;/span&gt;
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.boston()
model &lt;span class="pl-k"&gt;=&lt;/span&gt; xgboost.train({&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.01&lt;/span&gt;}, xgboost.DMatrix(X, &lt;span class="pl-v"&gt;label&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;y), &lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain the model's predictions using SHAP&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.TreeExplainer(model)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;The above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue (these force plots are introduced in our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature BME paper&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; visualize the training set predictions&lt;/span&gt;
shap.force_plot(explainer.expected_value, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png"&gt;&lt;img width="811" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as RM (the average number of rooms per house in an area) changes. Vertical dispersion at a single value of RM represents interaction effects with other features. To help reveal these interactions &lt;code&gt;dependence_plot&lt;/code&gt; automatically selects another feature for coloring. In this case coloring by RAD (index of accessibility to radial highways) highlights that the average number of rooms per house has less impact on home price for areas with a high RAD value.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; create a dependence plot to show the effect of a single feature across the whole dataset&lt;/span&gt;
shap.dependence_plot(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;RM&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png"&gt;&lt;img width="544" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_dependence_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that a high LSTAT (% lower status of the population) lowers the predicted home price.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; summarize the effects of all the features&lt;/span&gt;
shap.summary_plot(shap_values, X)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;shap.summary_plot(shap_values, X, &lt;span class="pl-v"&gt;plot_type&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;bar&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png"&gt;&lt;img width="470" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_summary_plot_bar.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-deepexplainer-tensorflowkeras-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-deepexplainer-tensorflowkeras-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with DeepExplainer (TensorFlow/Keras models)&lt;/h2&gt;
&lt;p&gt;Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with &lt;a href="https://arxiv.org/abs/1704.02685" rel="nofollow"&gt;DeepLIFT&lt;/a&gt; described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...include code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&lt;/span&gt;

&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; select a set of background examples to take an expectation over&lt;/span&gt;
background &lt;span class="pl-k"&gt;=&lt;/span&gt; x_train[np.random.choice(x_train.shape[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;replace&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain predictions of the model on four images&lt;/span&gt;
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.DeepExplainer(model, background)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ...or pass tensors directly&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)&lt;/span&gt;
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the feature attributions&lt;/span&gt;
shap.image_plot(shap_values, &lt;span class="pl-k"&gt;-&lt;/span&gt;x_test[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;5&lt;/span&gt;])&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png"&gt;&lt;img width="820" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/mnist_image_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models" class="anchor" aria-hidden="true" href="#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)&lt;/h2&gt;
&lt;p&gt;Expected gradients combines ideas from &lt;a href="https://arxiv.org/abs/1703.01365" rel="nofollow"&gt;Integrated Gradients&lt;/a&gt;, SHAP, and &lt;a href="https://arxiv.org/abs/1706.03825" rel="nofollow"&gt;SmoothGrad&lt;/a&gt; into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;VGG16&lt;/span&gt;
&lt;span class="pl-k"&gt;from&lt;/span&gt; keras.applications.vgg16 &lt;span class="pl-k"&gt;import&lt;/span&gt; preprocess_input
&lt;span class="pl-k"&gt;import&lt;/span&gt; keras.backend &lt;span class="pl-k"&gt;as&lt;/span&gt; K
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy &lt;span class="pl-k"&gt;as&lt;/span&gt; np
&lt;span class="pl-k"&gt;import&lt;/span&gt; json
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load pre-trained model and choose two images to explain&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; VGG16(&lt;span class="pl-v"&gt;weights&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;imagenet&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;include_top&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
X,y &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.imagenet50()
to_explain &lt;span class="pl-k"&gt;=&lt;/span&gt; X[[&lt;span class="pl-c1"&gt;39&lt;/span&gt;,&lt;span class="pl-c1"&gt;41&lt;/span&gt;]]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; load the ImageNet class names&lt;/span&gt;
url &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
fname &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.datasets.cache(url)
&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(fname) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
    class_names &lt;span class="pl-k"&gt;=&lt;/span&gt; json.load(f)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; explain how the input to the 7th layer of the model explains the top two classes&lt;/span&gt;
&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;map2layer&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;layer&lt;/span&gt;):
    feed_dict &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;dict&lt;/span&gt;(&lt;span class="pl-c1"&gt;zip&lt;/span&gt;([model.layers[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].input], [preprocess_input(x.copy())]))
    &lt;span class="pl-k"&gt;return&lt;/span&gt; K.get_session().run(model.layers[layer].input, feed_dict)
e &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.GradientExplainer(
    (model.layers[&lt;span class="pl-c1"&gt;7&lt;/span&gt;].input, model.layers[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;].output),
    map2layer(X, &lt;span class="pl-c1"&gt;7&lt;/span&gt;),
    &lt;span class="pl-v"&gt;local_smoothing&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; std dev of smoothing noise&lt;/span&gt;
)
shap_values,indexes &lt;span class="pl-k"&gt;=&lt;/span&gt; e.shap_values(map2layer(to_explain, &lt;span class="pl-c1"&gt;7&lt;/span&gt;), &lt;span class="pl-v"&gt;ranked_outputs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; get the names for the classes&lt;/span&gt;
index_names &lt;span class="pl-k"&gt;=&lt;/span&gt; np.vectorize(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: class_names[&lt;span class="pl-c1"&gt;str&lt;/span&gt;(x)][&lt;span class="pl-c1"&gt;1&lt;/span&gt;])(indexes)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the explanations&lt;/span&gt;
shap.image_plot(shap_values, to_explain, index_names)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png"&gt;&lt;img width="500" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/gradient_imagenet_plot.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Predictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using &lt;code&gt;ranked_outputs=2&lt;/code&gt; we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-agnostic-example-with-kernelexplainer-explains-any-function" class="anchor" aria-hidden="true" href="#model-agnostic-example-with-kernelexplainer-explains-any-function"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model agnostic example with KernelExplainer (explains any function)&lt;/h2&gt;
&lt;p&gt;Kernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sklearn
&lt;span class="pl-k"&gt;import&lt;/span&gt; shap
&lt;span class="pl-k"&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class="pl-k"&gt;import&lt;/span&gt; train_test_split

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; print the JS visualization code to the notebook&lt;/span&gt;
shap.initjs()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; train a SVM classifier&lt;/span&gt;
X_train,X_test,Y_train,Y_test &lt;span class="pl-k"&gt;=&lt;/span&gt; train_test_split(&lt;span class="pl-k"&gt;*&lt;/span&gt;shap.datasets.iris(), &lt;span class="pl-v"&gt;test_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-v"&gt;random_state&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
svm &lt;span class="pl-k"&gt;=&lt;/span&gt; sklearn.svm.SVC(&lt;span class="pl-v"&gt;kernel&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;rbf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;probability&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
svm.fit(X_train, Y_train)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; use Kernel SHAP to explain test set predictions&lt;/span&gt;
explainer &lt;span class="pl-k"&gt;=&lt;/span&gt; shap.KernelExplainer(svm.predict_proba, X_train, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
shap_values &lt;span class="pl-k"&gt;=&lt;/span&gt; explainer.shap_values(X_test, &lt;span class="pl-v"&gt;nsamples&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of the first instance&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;][&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], X_test.iloc[&lt;span class="pl-c1"&gt;0&lt;/span&gt;,:], &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png"&gt;&lt;img width="810" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_instance.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;The above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.&lt;/p&gt;
&lt;p&gt;If we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; plot the SHAP values for the Setosa output of all instances&lt;/span&gt;
shap.force_plot(explainer.expected_value[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], shap_values[&lt;span class="pl-c1"&gt;0&lt;/span&gt;], X_test, &lt;span class="pl-v"&gt;link&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;logit&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png"&gt;&lt;img width="813" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/iris_dataset.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-shap-interaction-values" class="anchor" aria-hidden="true" href="#shap-interaction-values"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SHAP Interaction Values&lt;/h2&gt;
&lt;p&gt;SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with &lt;code&gt;shap.TreeExplainer(model).shap_interaction_values(X)&lt;/code&gt;. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png"&gt;&lt;img width="483" src="https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/nhanes_age_sex_interaction.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sample-notebooks" class="anchor" aria-hidden="true" href="#sample-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sample notebooks&lt;/h2&gt;
&lt;p&gt;The notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-treeexplainer" class="anchor" aria-hidden="true" href="#treeexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TreeExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html" rel="nofollow"&gt;&lt;strong&gt;NHANES survival model with XGBoost and SHAP interaction values&lt;/strong&gt;&lt;/a&gt; - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and &lt;code&gt;shap&lt;/code&gt; to uncover complex risk factor relationships.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with LightGBM&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html" rel="nofollow"&gt;&lt;strong&gt;League of Legends Win Prediction with XGBoost&lt;/strong&gt;&lt;/a&gt; - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deepexplainer" class="anchor" aria-hidden="true" href="#deepexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html" rel="nofollow"&gt;&lt;strong&gt;MNIST Digit classification with Keras&lt;/strong&gt;&lt;/a&gt; - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html" rel="nofollow"&gt;&lt;strong&gt;Keras LSTM for IMDB Sentiment Classification&lt;/strong&gt;&lt;/a&gt; - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-gradientexplainer" class="anchor" aria-hidden="true" href="#gradientexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GradientExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html" rel="nofollow"&gt;&lt;strong&gt;Explain an Intermediate Layer of VGG16 on ImageNet&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-linearexplainer" class="anchor" aria-hidden="true" href="#linearexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LinearExplainer&lt;/h3&gt;
&lt;p&gt;For a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covaraince matrix. LinearExplainer supports both of these options.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html" rel="nofollow"&gt;&lt;strong&gt;Sentiment Analysis with Logistic Regression&lt;/strong&gt;&lt;/a&gt; - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kernelexplainer" class="anchor" aria-hidden="true" href="#kernelexplainer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;KernelExplainer&lt;/h3&gt;
&lt;p&gt;An implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes not assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Census income classification with scikit-learn&lt;/strong&gt;&lt;/a&gt; - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html" rel="nofollow"&gt;&lt;strong&gt;ImageNet VGG16 Model with Keras&lt;/strong&gt;&lt;/a&gt; - Explain the classic VGG16 convolutional nerual network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html" rel="nofollow"&gt;&lt;strong&gt;Iris classification&lt;/strong&gt;&lt;/a&gt; - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using &lt;code&gt;shap&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-documentation-notebooks" class="anchor" aria-hidden="true" href="#documentation-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation notebooks&lt;/h2&gt;
&lt;p&gt;These notebooks comprehensively demonstrate how to use specific functions and objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/decision_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.decision_plot&lt;/code&gt; and &lt;code&gt;shap.multioutput_decision_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html" rel="nofollow"&gt;&lt;code&gt;shap.dependence_plot&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-methods-unified-by-shap" class="anchor" aria-hidden="true" href="#methods-unified-by-shap"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Methods Unified by SHAP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;LIME:&lt;/em&gt; Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley sampling values:&lt;/em&gt; Strumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;DeepLIFT:&lt;/em&gt; Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." arXiv preprint arXiv:1704.02685 (2017).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;QII:&lt;/em&gt; Datta, Anupam, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems." Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Layer-wise relevance propagation:&lt;/em&gt; Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015): e0130140.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Shapley regression values:&lt;/em&gt; Lipovetsky, Stan, and Michael Conklin. "Analysis of regression in game theory approach." Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Tree interpreter:&lt;/em&gt; Saabas, Ando. Interpreting random forests. &lt;a href="http://blog.datadive.net/interpreting-random-forests/" rel="nofollow"&gt;http://blog.datadive.net/interpreting-random-forests/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-citations" class="anchor" aria-hidden="true" href="#citations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citations&lt;/h2&gt;
&lt;p&gt;The algorithms and visualizations used in this package came primarily out of research in &lt;a href="https://suinlee.cs.washington.edu" rel="nofollow"&gt;Su-In Lee's lab&lt;/a&gt; at the University of Washington. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For general use of SHAP you can read/cite our &lt;a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions" rel="nofollow"&gt;NeurIPS paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/shap_nips.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For TreeExplainer you can (for now) read/cite our &lt;a href="https://arxiv.org/abs/1905.04610" rel="nofollow"&gt;arXiv paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/treeshap_arxiv.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;For &lt;code&gt;force_plot&lt;/code&gt; visualizations and medical applications you can read/cite our &lt;a href="https://www.nature.com/articles/s41551-018-0304-0" rel="nofollow"&gt;Nature Biomedical Engineering paper&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/slundberg/shap/master/docs/references/nature_bme.bib" rel="nofollow"&gt;bibtex&lt;/a&gt;; &lt;a href="https://rdcu.be/baVbR" rel="nofollow"&gt;free access&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31"&gt;&lt;img height="1" width="1" src="https://camo.githubusercontent.com/db8c6ffa9af4cf6d17c411a9f7ad56cc1b508c35/68747470733a2f2f7777772e66616365626f6f6b2e636f6d2f74723f69643d3138393134373039313835353939312665763d5061676556696577266e6f7363726970743d31" data-canonical-src="https://www.facebook.com/tr?id=189147091855991&amp;amp;ev=PageView&amp;amp;noscript=1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>slundberg</author><guid isPermaLink="false">https://github.com/slundberg/shap</guid><pubDate>Sun, 05 Jan 2020 00:15:00 GMT</pubDate></item><item><title>tiepvupsu/ebookMLCB #16 in Jupyter Notebook, Today</title><link>https://github.com/tiepvupsu/ebookMLCB</link><description>&lt;p&gt;&lt;i&gt;ebook Machine Learning c쿼 b故믍&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-m칚-ngu敲늝-cu敲녍-ebook-machine-learning-c쿼-b故믍-v콜-h敲슷-ti敲p" class="anchor" aria-hidden="true" href="#m칚-ngu敲늝-cu敲녍-ebook-machine-learning-c쿼-b故믍-v콜-h敲슷-ti敲p"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;M칚 ngu敲늝 cu敲녍 ebook "Machine Learning c쿼 b故믍", V콜 H敲슷 Ti敲p.&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-ebook-machine-learning-c쿼-b故믍" class="anchor" aria-hidden="true" href="#ebook-machine-learning-c쿼-b故믍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/tiepvupsu/ebookMLCB/blob/master/book_ML.pdf"&gt;ebook Machine Learning c쿼 b故믍&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;M敲껼 h칣nh th敲뼊 sao ch칠p, in 故볉 캠敲u c故븙 캠퀋敲믂 s敲 캠敲늝g 칳 c敲브 t치c gi故. M敲껼 chia s故 캠敲u c故븙 캠퀋敲믂 d故쁥 ngu敲늝 t敲뜰 &lt;a href="https://github.com/tiepvupsu/ebookMLCB"&gt;https://github.com/tiepvupsu/ebookMLCB&lt;/a&gt; ho故윾 &lt;a href="https://machinelearningcoban.com" rel="nofollow"&gt;https://machinelearningcoban.com&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;B故멽 v故쁥 c칩 th敲 敲븙g h敲 t치c gi故 b故쎭g c치ch mua s치ch gi故볓 &lt;a href="https://machinelearningcoban.com/ebook/" rel="nofollow"&gt;t故멸 캠칙y&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;N故쯧 b故멽 g故읋 b故볎 c敲 l敲들 n맖 ho故윾 cho r故쎭g n敲뗠 dung c칩 th敲 캠퀋敲믂 c故믈 thi敲n, b故멽 c칩 th敲 t故멾 m敲뗪 issue &lt;a href="https://github.com/tiepvupsu/ebookMLCB/issues"&gt;t故멸 캠칙y&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Click Star n故쯧 b故멽 th故볓 n敲뗠 dung cu敲녍 s치ch c칩 칤ch. C故믌 쿼n b故멽.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tiepvupsu</author><guid isPermaLink="false">https://github.com/tiepvupsu/ebookMLCB</guid><pubDate>Sun, 05 Jan 2020 00:16:00 GMT</pubDate></item><item><title>fastai/course-v3 #17 in Jupyter Notebook, Today</title><link>https://github.com/fastai/course-v3</link><description>&lt;p&gt;&lt;i&gt;The 3rd edition of course.fast.ai&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-course-v3" class="anchor" aria-hidden="true" href="#course-v3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;course-v3&lt;/h1&gt;
&lt;p&gt;The 3rd edition of &lt;a href="https://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt;. See the &lt;code&gt;nbs&lt;/code&gt; folder for the notebooks.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/course-v3</guid><pubDate>Sun, 05 Jan 2020 00:17:00 GMT</pubDate></item><item><title>awslabs/amazon-sagemaker-examples #18 in Jupyter Notebook, Today</title><link>https://github.com/awslabs/amazon-sagemaker-examples</link><description>&lt;p&gt;&lt;i&gt;Example notebooks that show how to apply machine learning, deep learning and reinforcement learning in Amazon SageMaker&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-amazon-sagemaker-examples" class="anchor" aria-hidden="true" href="#amazon-sagemaker-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Examples&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks that show how to apply machine learning and deep learning in &lt;a href="https://aws.amazon.com/sagemaker" rel="nofollow"&gt;Amazon SageMaker&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-ground-truth-labeling-jobs" class="anchor" aria-hidden="true" href="#introduction-to-ground-truth-labeling-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification"&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/ground_truth_object_detection_tutorial"&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output"&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/object_detection_augmented_manifest_training"&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an "augmented manifest" and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/annotation_consolidation"&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-applying-machine-learning" class="anchor" aria-hidden="true" href="#introduction-to-applying-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Applying Machine Learning&lt;/h3&gt;
&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; predicts potential customers that are most likely to convert based on customer and aggregate level metrics, using Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives.  This uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/linear_time_series_forecast"&gt;Time-series Forecasting&lt;/a&gt; generates a forecast for topline product demand using Amazon SageMaker's Linear Learner algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/breast_cancer_prediction"&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker's Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/ensemble_modeling"&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/video_game_sales"&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/gluon_recommender_system"&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/fair_linear_learner"&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans"&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/object2vec_document_embedding"&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sagemaker-automatic-model-tuning" class="anchor" aria-hidden="true" href="#sagemaker-automatic-model-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SageMaker Automatic Model Tuning&lt;/h3&gt;
&lt;p&gt;These examples introduce SageMaker's hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/xgboost_direct_marketing"&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fits for the &lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; task.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/tensorflow_mnist"&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/mxnet_mnist"&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/keras_bring_your_own"&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/r_bring_your_own"&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/analyze_results"&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-amazon-algorithms" class="anchor" aria-hidden="true" href="#introduction-to-amazon-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Amazon Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker's custom developed algorithms.  Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/1P_kmeans_highlevel"&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker.  It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/factorization_machines_mnist"&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker's implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/linear_learner_mnist"&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ntm_synthetic"&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/pca_mnist"&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/seq2seq_translation_en-de"&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that's built on top of &lt;a href="https://github.com/awslabs/sockeye"&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet.  Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation.  This notebook shows translation from English to German text.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker's Image Classification algorithm.  This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_abalone"&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html" rel="nofollow"&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_mnist"&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/deepar_synthetic"&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/blazingtext_word2vec_text8"&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker's fast and scalable BlazingText implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_pascalvoc_coco"&gt;Object Detection&lt;/a&gt; illustrates how to train an object detector using the Amazon SageMaker Object Detection algorithm with different input formats (RecordIO and image).  It uses the Pascal VOC dataset. A third notebook is provided to demonstrate the use of incremental training.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_birds"&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_movie_recommendation"&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification"&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_sentence_similarity"&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ipinsights_login"&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc"&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentaion masks and probability of segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-rl" class="anchor" aria-hidden="true" href="#amazon-sagemaker-rl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker RL&lt;/h3&gt;
&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_cartpole_coach"&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel's RL Coach.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_deepracer_robomaker_coach_gazebo"&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_hvac_coach_energyplus"&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_knapsack_coach_custom"&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_mountain_car_coach_gymEnv"&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_network_compression_ray_custom"&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_objecttracker_robomaker_coach_gazebo"&gt;Turtlebot Tracker&lt;/a&gt; This notebook demonstrates object tracking using AWS Robomaker and RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_portfolio_management_coach_customEnv"&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_predictive_autoscaling_coach_customEnv"&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_ray"&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_stable_baselines"&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach"&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_tic_tac_toe_coach_customEnv"&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-scientific-details-of-algorithms" class="anchor" aria-hidden="true" href="#scientific-details-of-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scientific Details of Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/streaming_median"&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker's spectral decomposition approach to LDA.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/linear_learner_class_weights_loss_functions"&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-debugger" class="anchor" aria-hidden="true" href="#amazon-sagemaker-debugger"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Debugger&lt;/h3&gt;
&lt;p&gt;These examples provide and introduction to SageMaker Debugger which allows debugging and monitoring capabilities for training of machine learning and deep learning algorithms. Note that although these notebooks focus on a specific framework, the same approach works with all the frameworks that Amazon SageMaker Debugger supports. The notebooks below are listed in the order in which we recommend you review them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_builtin_rule/"&gt;Using a built-in rule with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_keras_custom_rule/"&gt;Using a custom rule with TensorFlow Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mnist_tensor_analysis/"&gt;Interactive tensor analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mnist_tensor_plot/"&gt;Visualizing Debugging Tensors of MXNet training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mxnet_realtime_analysis/"&gt;Real-time analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/xgboost_builtin_rules/"&gt;Using a built in rule with XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/xgboost_realtime_analysis/"&gt;Real-time analysis in notebook with XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mxnet_spot_training/"&gt;Using SageMaker Debugger with Managed Spot Training and MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_action_on_rule/"&gt;Reacting to CloudWatch Events from Rules to take an action based on status with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/pytorch_custom_container/"&gt;Using SageMaker Debugger with a custom PyTorch container&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-advanced-amazon-sagemaker-functionality" class="anchor" aria-hidden="true" href="#advanced-amazon-sagemaker-functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt;
&lt;p&gt;These examples that showcase unique functionality available in Amazon SageMaker.  They cover a broad range of topics and will utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="advanced_functionality/data_distribution_types"&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances.  This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/handling_kms_encrypted_data"&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/parquet_to_recordio_protobuf"&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href="https://parquet.apache.org/" rel="nofollow"&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/working_with_redshift_data"&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/xgboost_bring_your_own_model"&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/kmeans_bring_your_own_model"&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that's been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/install_r_kernel"&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/scikit_bring_your_own"&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/mxnet_mnist_byom"&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/tensorflow_iris_byom"&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_xgboost_abalone"&gt;Inference Pipeline with SparkML and XGBoost&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and XGBoost for training on the Abalone dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_blazingtext_dbpedia"&gt;Inference Pipeline with SparkML and BlazingText&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and BlazingText for training on the DBPedia dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/search"&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_bring_your_own"&gt;Host Multiple Models with Your Own Algorithm&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint with your own custom algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_xgboost_home_value"&gt;Host Multiple Models with XGBoost&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled XGBoost container.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_sklearn_home_value"&gt;Host Multiple Models with SKLearn&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled SKLearn container.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-neo-compilation-jobs" class="anchor" aria-hidden="true" href="#amazon-sagemaker-neo-compilation-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide you an introduction to how to use Neo to optimizes deep learning model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; Adapts form &lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;image classification&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;mxnet mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/pytorch_torchvision"&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;tensorflow mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; Adapts form &lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;xgboost customer churn&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-procesing" class="anchor" aria-hidden="true" href="#amazon-sagemaker-procesing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Procesing&lt;/h3&gt;
&lt;p&gt;These examples show you how to use SageMaker Processing jobs to run data processing workloads.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_processing/scikit_learn_data_processing_and_model_evaluation"&gt;Scikit-Learn Data Processing and Model Evaluation&lt;/a&gt; shows how to use SageMaker Processing and the Scikit-Learn container to run data preprocessing and model evaluation workloads.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_processing/feature_transformation_with_sagemaker_processing"&gt;Feature transformation with Amazon SageMaker Processing and SparkML&lt;/a&gt; shows how to use SageMaker Processing to run data processing workloads using SparkML prior to training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk" class="anchor" aria-hidden="true" href="#amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-deep-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-deep-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you to write idiomatic TensorFlow or MXNet and then train or host in pre-built containers using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_cifar10"&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_mnist"&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_sentiment_analysis"&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_iris"&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_cifar10"&gt;CIFAR-10 with MXNet Gluon&lt;/a&gt; trains a ResNet-34  image classification model using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_mnist"&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet's symbolic syntax&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_sentiment"&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_layers"&gt;TensorFlow Neural Networks with Layers&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow layers&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_keras"&gt;TensorFlow Networks with Keras&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow and Keras&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators"&gt;Introduction to Estimators in TensorFlow&lt;/a&gt; trains a DNN classifier estimator on the Iris dataset using TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_resnet_cifar10_with_tensorboard"&gt;TensorFlow and TensorBoard&lt;/a&gt; trains a ResNet image classification model on CIFAR-10 using TensorFlow and showcases how to track results using TensorBoard&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; trains a simple convolutional neural network on MNIST using TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-machine-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-machine-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone"&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_inference_pipeline"&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-using-amazon-sagemaker-with-apache-spark" class="anchor" aria-hidden="true" href="#using-amazon-sagemaker-with-apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt;
&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href="https://github.com/aws/sagemaker-spark"&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-spark/pyspark_mnist"&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aws-marketplace" class="anchor" aria-hidden="true" href="#aws-marketplace"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS Marketplace&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;This example shows you how to package a model-package/algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/creating_marketplace_products"&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use algorithms and model packages from AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_algorithms"&gt;Using Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/amazon_demo_product"&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/automl"&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_model_packages"&gt;Using Model Packages&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/amazon_demo_product"&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/auto_insurance"&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/improving_industrial_workplace_safety"&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_data"&gt;Using Data&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_data/using_data_from_aws_data_exchange_to_predict_product_popularity"&gt;Using data and algorithm from AWS Marketplace for training a model&lt;/a&gt; provides a detailed walkthrough on how to use data from AWS Marketplace for training a model that predicts popularity of a bath product.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-under-development" class="anchor" aria-hidden="true" href="#under-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Under Development&lt;/h3&gt;
&lt;p&gt;These Amazon SageMaker examples fully illustrate a concept, but may require some additional configuration on the users part to complete.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What do I need in order to get started?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The quickest setup to run example notebooks includes:
&lt;ul&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html" rel="nofollow"&gt;AWS account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Proper &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html" rel="nofollow"&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html" rel="nofollow"&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html" rel="nofollow"&gt;S3 bucket&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Will these examples work outside of Amazon SageMaker Notebook Instances?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;How do I contribute my own example notebook?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources.  Please bear with us in the short-term if pull requests take longer than expected or are closed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>awslabs</author><guid isPermaLink="false">https://github.com/awslabs/amazon-sagemaker-examples</guid><pubDate>Sun, 05 Jan 2020 00:18:00 GMT</pubDate></item><item><title>jakevdp/PythonDataScienceHandbook #19 in Jupyter Notebook, Today</title><link>https://github.com/jakevdp/PythonDataScienceHandbook</link><description>&lt;p&gt;&lt;i&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-data-science-handbook" class="anchor" aria-hidden="true" href="#python-data-science-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Data Science Handbook&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository contains the entire &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="notebooks/figures/PDSH-cover.png"&gt;&lt;img src="notebooks/figures/PDSH-cover.png" alt="cover image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-this-book" class="anchor" aria-hidden="true" href="#how-to-use-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use this Book&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Read the book in its entirety online at &lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/" rel="nofollow"&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the code using the Jupyter notebooks available in this repository's &lt;a href="notebooks"&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch executable versions of these notebooks using &lt;a href="http://colab.research.google.com" rel="nofollow"&gt;Google Colab&lt;/a&gt;: &lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href="https://beta.mybinder.org/" rel="nofollow"&gt;binder&lt;/a&gt;: &lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Buy the printed book through &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;O'Reilly Media&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt;
&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href="http://ipython.org" rel="nofollow"&gt;IPython&lt;/a&gt;, &lt;a href="http://numpy.org" rel="nofollow"&gt;NumPy&lt;/a&gt;, &lt;a href="http://pandas.pydata.org" rel="nofollow"&gt;Pandas&lt;/a&gt;, &lt;a href="http://matplotlib.org" rel="nofollow"&gt;Matplotlib&lt;/a&gt;, &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;Scikit-Learn&lt;/a&gt;, and related packages.
Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project,
&lt;a href="https://github.com/jakevdp/WhirlwindTourOfPython"&gt;A Whirlwind Tour of Python&lt;/a&gt;: it's a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt;
&lt;p&gt;See &lt;a href="http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-software" class="anchor" aria-hidden="true" href="#software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Software&lt;/h2&gt;
&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt;
&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href="requirements.txt"&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use).
To install the requirements using &lt;a href="http://conda.pydata.org" rel="nofollow"&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can read more about using conda environments in the &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h3&gt;
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href="LICENSE-CODE"&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-text" class="anchor" aria-hidden="true" href="#text"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text&lt;/h3&gt;
&lt;p&gt;The text content of the book is released under the &lt;a href="LICENSE-TEXT"&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode" rel="nofollow"&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jakevdp</author><guid isPermaLink="false">https://github.com/jakevdp/PythonDataScienceHandbook</guid><pubDate>Sun, 05 Jan 2020 00:19:00 GMT</pubDate></item><item><title>ljpzzz/machinelearning #20 in Jupyter Notebook, Today</title><link>https://github.com/ljpzzz/machinelearning</link><description>&lt;p&gt;&lt;i&gt;My blogs and code for machine learning. http://cnblogs.com/pinard&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-갲攬쥔좁pinard眄꼨丹뮐꽨볭坍滅" class="anchor" aria-hidden="true" href="#갲攬쥔좁pinard眄꼨丹뮐꽨볭坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;갲攬쥔좁Pinard眄꼨丹뮐꽨볭坍滅&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/pinard" rel="nofollow"&gt;http://www.cnblogs.com/pinard&lt;/a&gt; 갲攬쥔좁Pinard&lt;/p&gt;
&lt;p&gt;達곐疸꽨쌤燎끪끬擥덙갬꼨丹뮋쟔眄坍滅쫹땶謗륂짌眄곊褸왪쨃疸꽨봏駱뷗멆쨃騰먽솏謗뙐껁굵雷茗먼눏坍滅뛳疸쀥돖낹
坍滅燎괠뻔갡療봱줋啖뀒쪰某럺쨃剌쒼먻잵眠전雷좮곋療淡뫥뻔갡疸쥗쀤임갵庸껀쒼먻잵갬眄곋療淡뫥&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-眠쉰쪳" class="anchor" aria-hidden="true" href="#眠쉰쪳"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;眠쉰쪳&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#2"&gt;燎쥔똗駱뷗먼맄蔑疸뀐륀뉞숭柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#3"&gt;燎쥔똗駱뷗먼갡袂酩柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#4"&gt;燎쥔똗駱뷗먿뛶쏱酩柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#5"&gt;燎쥔똗駱뷗멀똀某얺숭柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#6"&gt;燎쥔똗駱뷗멀띉갫駱뷗먾숭柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#7"&gt;賴썬솑某房뫤솑&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#8"&gt;燎쥔똗駱뷗먼덚숭柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#9"&gt;燎쥔똗駱뷗먽꿛꼟酩柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#10"&gt;瀏쐔줝駱뷗먾숭柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#11"&gt;쀦윋싨傍邏낹酩柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#1"&gt;欖쥔깼駱뷗먾숭柳&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="#12"&gt;곷籃剌봲뺆疸뀒숭柳됃냫喇&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-柳뻕" class="anchor" aria-hidden="true" href="#柳뻕"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;柳뻕&lt;/h2&gt;
&lt;p&gt;2016-2017嵐언뙎뚟꼨丹뮋覓뻖뚟python곋燎섡땶2.7庸 2018嵐언먻쟠TensorFlow亂젫ython3眄疸啖魃劉庸껁坍봰뙌꼨丹뮋覓뻖뚟Python곋燎섡땶3.6卵놸뻔갡2016庸2017嵐얺뚟꼨丹뮋짂滅了먽씟쮠걇庸껄꽩둻覓Python3.6紡놷쯶疸귃멆쨃騰먽솏낊쫷쨐쥖꿣得껀꼨丹뮋쟔坍滅溟꽩랠疸꽧얺뚟喇썭뒃庸꺿쟡魃溜괜끩걇print眄放쇉씟得range眄覓뻕씟庸껃긴걇付쉴뭊庸껂뺈籃쉯찙賚좬꼽낊紡놸뛲&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-滂륀궐갬" class="anchor" aria-hidden="true" href="#滂륀궐갬"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="#13"&gt;滂륀궐갬&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="user-content-1"&gt;&lt;a id="user-content-欖쥔깼駱뷗먽둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#欖쥔깼駱뷗먽둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;欖쥔깼駱뷗먽둖茗먻잹坍滅庸:&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9385570.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿疸庸괝뺘拏癩쥖&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/introduction.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9426283.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿啖껊쨀浮선쌦椧놴뱖螟뒫쯶溟(MDP)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9463815.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿疸괦쨀覓뻔궏倣뉦짿DP庸괝쎽倣&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9492980.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿騰庸괞댣盼뙎곷꼬母柳됊짿MC庸괝쎽倣&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9529828.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿啖덢쨀覓뻕딨擥剌쉰갡柳됊짿TD庸괝쎽倣&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9614290.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿쇒쨀了윈줊剌쉰갡喇뻖꿚걌酩柳뎁ARSA&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/sarsa_windy_world.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9669263.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿疸庸괝딨擥剌쉰갡暝暮꿚걌酩柳뎀-Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/q_learning_windy_world.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9714655.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿庸괛징쩐쫹돓眄謗놳쨮發뻖뱤疸꾃eep Q-Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9756075.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿達뢢쨀Deep Q-Learning謗俯윇졞Nature DQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/nature_dqn.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9778063.html" rel="nofollow"&gt;欖쥔깼駱뷗멆짿꼐庸겏ouble DQN (DDQN)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9797695.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐疸) Prioritized Replay DQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddqn_prioritised_replay.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9923859.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐啖) Dueling DQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/duel_dqn.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10137696.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐疸) 螟뒪돈鬧擥(Policy Gradient)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/policy_gradient.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10272023.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐騰) Actor-Critic&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/actor_critic.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10334127.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐啖) A3C&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/a3c.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10345762.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐) 瀏쐔줝蔑쉰숰螟뒪돈鬧擥(DDPG)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/ddpg.py"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10384424.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐疸) 癩쥓줉淚뫤룲眄欖쥔깼駱뷗먻잹Dyna酩柳됁몑瞭&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10470571.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐) 癩쥓줉淚뫥眄냎募뮋잹盼뙎곷꼬母蓼놵냎募(MCTS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10609228.html" rel="nofollow"&gt;欖쥔깼駱뷗(꼐達) AlphaGo Zero欖쥔깼駱뷗먼낹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-2"&gt;&lt;a id="user-content-燎쥔똗駱뷗먼맄蔑疸뀐륀뉞숭柳됁둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#燎쥔똗駱뷗먼맄蔑疸뀐륀뉞숭柳됁둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;燎쥔똗駱뷗먼맄蔑疸뀐륀뉞숭柳됁둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5970503.html" rel="nofollow"&gt;鬧擥뷗잶傅꽲짿Gradient Descent庸괜쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5976811.html" rel="nofollow"&gt;燎卵啖꺿졪柳됀쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5992719.html" rel="nofollow"&gt;啖번끨溥껃싀(Cross Validation)낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/5993450.html" rel="nofollow"&gt;侮쮢몵꾾疸뀐낇騰륂꾾庸RoC料쒾疸꾏R料쒾&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6004041.html" rel="nofollow"&gt;暮騰륀뉜낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6007200.html" rel="nofollow"&gt;燎쥔똗駱뷗먾맶皿윇잹欖끰嵐낋眄괝길&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6013484.html" rel="nofollow"&gt;scikit-learn 得깛andas 癩쥓줉windows꼣燎쥕럌謄뻔솑達먾꿢螺眄냜攬&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6016029.html" rel="nofollow"&gt;覓빱cikit-learn得깛andas駱뷗먾騰륀&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/linear-regression.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6018889.html" rel="nofollow"&gt;Lasso騰륀뉞숭柳됊쨐 懶넍맪蚌얷잶傅꽩씟疸뀑卵倣뉜륀뉝씟卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6023000.html" rel="nofollow"&gt;覓빱cikit-learn得깛andas駱뷗Ridge騰륀&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/ridge_regression_1.ipynb"&gt;坍滅1&lt;/a&gt; &lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/ridge_regression.ipynb"&gt;坍滅2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6026343.html" rel="nofollow"&gt;scikit-learn 暮騰륀뉞숭柳됀줍卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9314198.html" rel="nofollow"&gt;欖辣젆좭流酩柳됀쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-3"&gt;&lt;a id="user-content-燎쥔똗駱뷗먼갡袂酩柳됁둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#燎쥔똗駱뷗먼갡袂酩柳됁둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;燎쥔똗駱뷗먼갡袂酩柳됁둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6029432.html" rel="nofollow"&gt;訪놴륀뉜낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6035872.html" rel="nofollow"&gt;scikit-learn 訪놴륀뉞쏱擥擔覓뻔쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6042320.html" rel="nofollow"&gt;麵봱럌낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6050306.html" rel="nofollow"&gt;螟뒩맴酩柳됀낹(疸)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6053344.html" rel="nofollow"&gt;螟뒩맴酩柳됀낹(疸)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6056319.html" rel="nofollow"&gt;scikit-learn螟뒩맴酩柳됂쏱擥擔覓뻔쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/decision_tree_classifier.ipynb"&gt;坍滅1&lt;/a&gt; &lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/decision_tree_classifier_1.ipynb"&gt;坍滅2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6061661.html" rel="nofollow"&gt;K謗놸柳(KNN)낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6065607.html" rel="nofollow"&gt;scikit-learn K謗놸柳됂쏱擥擔覓뻔쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/knn_classifier.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6069267.html" rel="nofollow"&gt;燎얺먿앻낐雷酩柳됀낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6074222.html" rel="nofollow"&gt;scikit-learn 燎얺먿앻낐雷袂擥擔覓뻔쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/native_bayes.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6093948.html" rel="nofollow"&gt;燎邏왡뺘拏낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6097604.html" rel="nofollow"&gt;賚깨냄燎쥔낹(疸) 暮賚깨냄燎&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6100722.html" rel="nofollow"&gt;賚깨냄燎쥔낹(啖) 暮賚깨냄燎쥖뚟蚌付얼뚮燎邏깼淚뫤룲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6103615.html" rel="nofollow"&gt;賚깨냄燎쥔낹(疸)暮疸꽨낊갡賚깨냄燎쥓잹蓼전쫹돓&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6111471.html" rel="nofollow"&gt;賚깨냄燎쥔낹(騰)SMO酩柳됀낹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6113120.html" rel="nofollow"&gt;賚깨냄燎쥔낹(啖)暮賚깨騰륀&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6117515.html" rel="nofollow"&gt;scikit-learn 賚깨냄燎쥖숭柳됀줍擔覓뻔쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6126077.html" rel="nofollow"&gt;賚깨냄燎쥘삊雷蓼젇쌖끡卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/svm_classifier.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-7"&gt;&lt;a id="user-content-賴썬솑某房뫤솑雷茗먻잹坍滅" class="anchor" aria-hidden="true" href="#賴썬솑某房뫤솑雷茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;賴썬솑某房뫤솑雷茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6047802.html" rel="nofollow"&gt;燎쥔똗駱뷗먾숭柳됂뚟剖燎쥕돓꼹覓갫&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/random_data_generation.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6625739.html" rel="nofollow"&gt;MCMC(疸)盼뙎곷꼬母雷좭씟&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6632399.html" rel="nofollow"&gt;MCMC(啖)浮선쌦椧놴뱖逢&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/mcmc_2.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6638955.html" rel="nofollow"&gt;MCMC(疸)MCMC蓼得M-H蓼&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/mcmc_3_4.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6645766.html" rel="nofollow"&gt;MCMC(騰)Gibbs蓼&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/mathematics/mcmc_3_4.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10750718.html" rel="nofollow"&gt;燎쥔똗駱뷗먻쟔眄麵뾀땻냄劉亂(疸) 劉亂쩐숰達괛잹劉亂쩐잮欄&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10773942.html" rel="nofollow"&gt;燎쥔똗駱뷗먻쟔眄麵뾀땻냄劉亂(啖) 麵뾀땻냄劉亂쩏졞丹뛳졜柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10791506.html" rel="nofollow"&gt;燎쥔똗駱뷗먻쟔眄麵뾀땻냄劉亂(疸) 麵뾀땻냄劉亂쩏졞籃쉰갡柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10825264.html" rel="nofollow"&gt;燎쥔똗駱뷗먻쟔眄麵뾀땻냄劉亂(騰) 麵뾀땻냄劉亂쩔쮠쨆柳됀갳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10930902.html" rel="nofollow"&gt;燎쥔똗駱뷗먻쟔眄麵뾀땻냄劉亂(啖) 麵뾀땻亂좮릴俯왢뚟劉亂&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-6"&gt;&lt;a id="user-content-燎쥔똗駱뷗멀띉갫駱뷗먽둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#燎쥔똗駱뷗멀띉갫駱뷗먽둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;燎쥔똗駱뷗멀띉갫駱뷗먽둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6131423.html" rel="nofollow"&gt;副갫駱뷗먼낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6133937.html" rel="nofollow"&gt;副갫駱뷗먻졞Adaboost酩柳됀낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6136914.html" rel="nofollow"&gt;scikit-learn Adaboost袂擥擔覓뻔쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/adaboost-classifier.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6140514.html" rel="nofollow"&gt;鬧擥뷙끯꼖蓼(GBDT)낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6143927.html" rel="nofollow"&gt;scikit-learn 鬧擥뷙끯꼖蓼(GBDT)方끡卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/gbdt_classifier.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6156009.html" rel="nofollow"&gt;Bagging疸뀔뚪燎쥕밅瞭酩柳됀낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6160412.html" rel="nofollow"&gt;scikit-learn剖燎쥕밅瞭方끡卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/random_forest_classifier.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/10979808.html" rel="nofollow"&gt;XGBoost酩柳됀낹卵某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/11114748.html" rel="nofollow"&gt;XGBoost袂擥擔覓뻔쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/ensemble-learning/xgboost-example.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-4"&gt;&lt;a id="user-content-燎쥔똗駱뷗먿뛶쏱酩柳됁둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#燎쥔똗駱뷗먿뛶쏱酩柳됁둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;燎쥔똗駱뷗먿뛶쏱酩柳됁둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6164214.html" rel="nofollow"&gt;K-Means뛶쏱酩柳됀낹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6169370.html" rel="nofollow"&gt;覓빱cikit-learn駱뷗K-Means뛶쏱&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/kmeans_cluster.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6179132.html" rel="nofollow"&gt;BIRCH뛶쏱酩柳됀낹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6200579.html" rel="nofollow"&gt;覓빱cikit-learn駱뷗BIRCH뛶쏱&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/birch_cluster.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6208966.html" rel="nofollow"&gt;DBSCAN亂擥뷛뛶쏱酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6217852.html" rel="nofollow"&gt;覓빱cikit-learn駱뷗DBSCAN뛶쏱&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/dbscan_cluster.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6221564.html" rel="nofollow"&gt;方쐗뛶쏱庸갑pectral clustering庸괜낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6235920.html" rel="nofollow"&gt;覓빱cikit-learn駱뷗먿썀뛶쏱&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/spectral_cluster.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-5"&gt;&lt;a id="user-content-燎쥔똗駱뷗멀똀某얺숭柳됁둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#燎쥔똗駱뷗멀똀某얺숭柳됁둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;燎쥔똗駱뷗멀똀某얺숭柳됁둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6239403.html" rel="nofollow"&gt;疸갫갡갡瞭넖짿PCA庸괜낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6243025.html" rel="nofollow"&gt;覓빱cikit-learn駱뷗먻쟡갫갡갡瞭(PCA)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/pca.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6244265.html" rel="nofollow"&gt;暮갻걂갡瞭LDA낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6249328.html" rel="nofollow"&gt;覓빱cikit-learn謗發LDA傅꽪짒&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/lda.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6251584.html" rel="nofollow"&gt;樂欖쩐갡倣(SVD)낹疸뀐랻傅꽪짒疸쇊뚟擥덚댣&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6266408.html" rel="nofollow"&gt;欄뻖蘭껀(LLE)낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6273377.html" rel="nofollow"&gt;覓빱cikit-learn滅덚뽔欄뻖蘭껀(LLE)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/lle.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-8"&gt;&lt;a id="user-content-燎쥔똗駱뷗먼덚숭柳됁둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#燎쥔똗駱뷗먼덚숭柳됁둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;燎쥔똗駱뷗먼덚숭柳됁둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6288716.html" rel="nofollow"&gt;전룲던갡瞭(CCA)낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6293298.html" rel="nofollow"&gt;Apriori酩柳됀낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6307064.html" rel="nofollow"&gt;FP Tree酩柳됀낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6323182.html" rel="nofollow"&gt;PrefixSpan酩柳됀낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6340162.html" rel="nofollow"&gt;覓Spark駱뷗FP Tree酩柳됀눏PrefixSpan酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/fp_tree_prefixspan.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6039099.html" rel="nofollow"&gt;了봰찅得껀녥彷뷙돓꼹깼꿎某溥껃쌛&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-9"&gt;&lt;a id="user-content-燎쥔똗駱뷗먽꿛꼟酩柳됁둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#燎쥔똗駱뷗먽꿛꼟酩柳됁둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;燎쥔똗駱뷗먽꿛꼟酩柳됁둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6349233.html" rel="nofollow"&gt;꼞낿謗瘤벉꿛꼟酩柳됁某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6351319.html" rel="nofollow"&gt;麵뾀땻갡倣喇뻔꼞낿謗瘤벉꿛꼟酩柳돿쟔眄擥덚댣&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6362647.html" rel="nofollow"&gt;SimRank꼞낿謗瘤벉꿛꼟酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6364932.html" rel="nofollow"&gt;覓Spark駱뷗먾릴俯왠갡倣꿛꼟酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/matrix_factorization.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6370127.html" rel="nofollow"&gt;갡倣燎(Factorization Machines)꿛꼟酩柳됀낹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9128682.html" rel="nofollow"&gt;榜뢘낐雷疸쀥깼뉜줊(BPR)酩柳됀쌢某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9163481.html" rel="nofollow"&gt;覓빲ensorflow駱뷗먿앻낐雷疸쀥깼뉜줊(BPR)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/bpr.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-10"&gt;&lt;a id="user-content-瀏쐔줝駱뷗먾숭柳됁둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#瀏쐔줝駱뷗먾숭柳됁둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;瀏쐔줝駱뷗먾숭柳됁둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6418668.html" rel="nofollow"&gt;瀏쐔줝明륂즲母놶즾庸DNN庸괝뺘拏疸뀐곐냄憺먽눬酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6422831.html" rel="nofollow"&gt;瀏쐔줝明륂즲母놶즾庸DNN庸괜끬냄憺먽눬酩柳(BP)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6437495.html" rel="nofollow"&gt;瀏쐔줝明륂즲母놶즾庸DNN庸괝邏쐔쫹돓得껁榴쫹돓眄괝길&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6472666.html" rel="nofollow"&gt;瀏쐔줝明륂즲母놶즾庸DNN庸괞뚟蔞갳깼&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6483207.html" rel="nofollow"&gt;꽁椧明륂즲母놶즾(CNN)淚뫤룲某瞭&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6489633.html" rel="nofollow"&gt;꽁椧明륂즲母놶즾(CNN)곐냄憺먽눬酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6494810.html" rel="nofollow"&gt;꽁椧明륂즲母놶즾(CNN)끬냄憺먽눬酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6509630.html" rel="nofollow"&gt;籃쀦꿢明륂즲母놶즾(RNN)淚뫤룲疸뀐곐냄끬냄憺먽눬酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6519110.html" rel="nofollow"&gt;LSTM淚뫤룲疸뀐곐냄끬냄憺먽눬酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6530523.html" rel="nofollow"&gt;끵傅넎꿭卵던좭띺燎쥞짿RBM庸괜낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-11"&gt;&lt;a id="user-content-쀦윋싨傍邏낹雷茗먻잹坍滅" class="anchor" aria-hidden="true" href="#쀦윋싨傍邏낹雷茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;쀦윋싨傍邏낹雷茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6677078.html" rel="nofollow"&gt;雷燎섡깼꿎眄갡放꽨낹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6688348.html" rel="nofollow"&gt;雷燎섡깼꿎孵邏낹達냄깼疸꾇ash Trick&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/hash_trick.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6693230.html" rel="nofollow"&gt;雷燎섡깼꿎孵邏낹達귮F-IDF&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/tf-idf.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6744056.html" rel="nofollow"&gt;疸쇉둖雷燎섡깼꿎孵邏낹流溟某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/chinese_digging.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6756534.html" rel="nofollow"&gt;김雷雷燎섡깼꿎孵邏낹流溟某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/english_digging.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6805861.html" rel="nofollow"&gt;雷燎섟쟡孵떥뺘拏達謬런랻放쇇졜募뮌쨋(LSI)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6812011.html" rel="nofollow"&gt;雷燎섟쟡孵떥뺘拏達咐륃麵뾀땻갡倣(NMF)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/nmf.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6831308.html" rel="nofollow"&gt;雷燎섟쟡孵떥뺘拏達귧DA(疸) LDA癩쥖&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6867828.html" rel="nofollow"&gt;雷燎섟쟡孵떥뺘拏達귧DA(啖) LDA劉倣達귢ibbs蓼酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6873703.html" rel="nofollow"&gt;雷燎섟쟡孵떥뺘拏達귧DA(疸) LDA劉倣達끶갡꿛雷센M酩柳&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6908150.html" rel="nofollow"&gt;覓빱cikit-learn駱뷗LDA疸孵떥뺘拏&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/lda.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6912636.html" rel="nofollow"&gt;EM酩柳됀낹某&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6945257.html" rel="nofollow"&gt;剖널뽋卵덚븸邏淚뫤룲HMM庸걳庸겓MM淚뫤룲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6955871.html" rel="nofollow"&gt;剖널뽋卵덚븸邏淚뫤룲HMM庸걳줇庸괜곐냄냁냄酩柳됃싃憺썯븩亂擥갱屢꾾&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6972299.html" rel="nofollow"&gt;剖널뽋卵덚븸邏淚뫤룲HMM庸걳잴庸괠쑰烙-夫뷘쌦樂酩柳됁쎽倣뮫MM끡賴&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/6991852.html" rel="nofollow"&gt;剖널뽋卵덚븸邏淚뫤룲HMM庸건띝庸괞짒곷鏤덚숭柳됃빆滅剖넏딆궜擥갱&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7001397.html" rel="nofollow"&gt;覓빧mmlearn駱뷗멀뚫浮선쌦椧놴뱖淚뫤룲HMM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/hmm.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7048333.html" rel="nofollow"&gt;療뫣짔剖燎쥔럌CRF(疸)坍뀔뚪燎쥔럌걇暮逢쮡로坍윌뚪燎쥔럌&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7055072.html" rel="nofollow"&gt;療뫣짔剖燎쥔럌CRF(啖) 곐냄냁냄酩柳됃싃憺썭맪房썬줊갱屢꾾&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7068574.html" rel="nofollow"&gt;療뫣짔剖燎쥔럌CRF(疸) 淚뫤룲駱뷗먻잹某얺곷鏤덚숭柳됃빆滅&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7160330.html" rel="nofollow"&gt;word2vec낹(疸) CBOW疸꾑kip-Gram淚뫤룲癩쥖&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7243513.html" rel="nofollow"&gt;word2vec낹(啖) 癩쥓줉Hierarchical Softmax眄淚뫤룲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7249903.html" rel="nofollow"&gt;word2vec낹(疸) 癩쥓줉Negative Sampling眄淚뫤룲&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/7278324.html" rel="nofollow"&gt;覓빦ensim駱뷗망ord2vec&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/word2vec.ipynb"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-12"&gt;&lt;a id="user-content-곷籃剌봲뺆疸뀒숭柳됃냫喇썭둖茗먻잹坍滅" class="anchor" aria-hidden="true" href="#곷籃剌봲뺆疸뀒숭柳됃냫喇썭둖茗먻잹坍滅"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;곷籃剌봲뺆疸뀒숭柳됃냫喇썭둖茗먻잹坍滅庸&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;雷茗&lt;/th&gt;
&lt;th&gt;坍滅&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9032759.html" rel="nofollow"&gt;곷籃剌봲뺆達곷籃괝길&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9061549.html" rel="nofollow"&gt;곷籃剌봲뺆達곷籃發뻗&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9093890.html" rel="nofollow"&gt;곷籃剌봲뺆達곷籃孵邏낹&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9220199.html" rel="nofollow"&gt;覓PMML丹륂꿣燎쥔똗駱뷗먽뺘拏眄紡뻔좁낋疸귆&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/model-in-product/sklearn-jpmml"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://www.cnblogs.com/pinard/p/9251296.html" rel="nofollow"&gt;tensorflow燎쥔똗駱뷗먽뺘拏眄紡뻔좁낋疸귆&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/ljpzzz/machinelearning/blob/master/model-in-product/tensorflow-java"&gt;坍滅&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="user-content-13"&gt;&lt;a id="user-content-滂륀궐갬-1" class="anchor" aria-hidden="true" href="#滂륀궐갬-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;滂륀궐갬&lt;/h3&gt;
&lt;p&gt;擔먾뚟賚깨僚갬뙋쪺眄궏궅(1.籃쉯찌/2.賚坍떤숳)庸&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./assert/invoice.bmp"&gt;&lt;img src="./assert/invoice.bmp" alt="籃쉯찌滂륀궐" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./assert/invoice_ali.bmp"&gt;&lt;img src="./assert/invoice_ali.bmp" alt="賚坍떤숳滂륀궐" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;License MIT.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ljpzzz</author><guid isPermaLink="false">https://github.com/ljpzzz/machinelearning</guid><pubDate>Sun, 05 Jan 2020 00:20:00 GMT</pubDate></item><item><title>bgweber/DS_Production #21 in Jupyter Notebook, Today</title><link>https://github.com/bgweber/DS_Production</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ds_production" class="anchor" aria-hidden="true" href="#ds_production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DS_Production&lt;/h1&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bgweber</author><guid isPermaLink="false">https://github.com/bgweber/DS_Production</guid><pubDate>Sun, 05 Jan 2020 00:21:00 GMT</pubDate></item><item><title>BlackArbsCEO/Adv_Fin_ML_Exercises #22 in Jupyter Notebook, Today</title><link>https://github.com/BlackArbsCEO/Adv_Fin_ML_Exercises</link><description>&lt;p&gt;&lt;i&gt;Experimental solutions to selected exercises from the book [Advances in Financial Machine Learning by Marcos Lopez De Prado]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-advances-in-financial-machine-learning-exercises" class="anchor" aria-hidden="true" href="#advances-in-financial-machine-learning-exercises"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advances in Financial Machine Learning Exercises&lt;/h1&gt;
&lt;p&gt;Experimental solutions to selected exercises from the book &lt;a href="https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482109" rel="nofollow"&gt;Advances in Financial Machine Learning by Marcos Lopez De Prado&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Make sure to use &lt;code&gt;python setup.py install&lt;/code&gt; in your environment so the &lt;code&gt;src&lt;/code&gt; scripts which include &lt;code&gt;bars.py&lt;/code&gt; and &lt;code&gt;snippets.py&lt;/code&gt; can be found by the jupyter notebooks and other scripts you may develop.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-additional-afml-projects-and-resources" class="anchor" aria-hidden="true" href="#additional-afml-projects-and-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional AFML Projects and Resources&lt;/h2&gt;
&lt;p&gt;There are other github projects and links that people share that are inspired by the book. I'd like to collect them here to share with others in the spirit of collaboration and idea sharing. If you have more to add please let me know.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-github-projects" class="anchor" aria-hidden="true" href="#github-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Github Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.quantsportal.com/the-open-source-hedge-fund-project/" rel="nofollow"&gt;The Open Source Hedge Fund Project&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/hudson-and-thames/mlfinlab"&gt;Github MLFinLab Repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hudson-and-thames/research"&gt;Github Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rspadim/Adv_Fin_ML/"&gt;rspadim Github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-article-links" class="anchor" aria-hidden="true" href="#article-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Article Links&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/financial-machine-learning-part-0-bars-745897d4e4ba" rel="nofollow"&gt;Financial Machine Learning Part 0: Bars by Maks Ivanov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gmarti.gitlab.io/qfin/2018/05/30/deflated-sharpe-ratio.html" rel="nofollow"&gt;Deflated Sharpe Ratio&lt;/a&gt; - &lt;a href="https://gmarti.gitlab.io/" rel="nofollow"&gt;Gautier Marti blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-project-organization" class="anchor" aria-hidden="true" href="#project-organization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project Organization&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;較럭較 LICENSE
較럭較 Makefile           &amp;lt;- Makefile with commands like `make data` or `make train`
較럭較 README.md          &amp;lt;- The top-level README for developers using this project.
較럭較 data
較먝 較럭較 external       &amp;lt;- Data from third party sources.
較먝 較럭較 interim        &amp;lt;- Intermediate data that has been transformed.
較먝 較럭較 processed      &amp;lt;- The final, canonical data sets for modeling.
較먝 較덕較 raw            &amp;lt;- The original, immutable data dump.
較
較럭較 docs               &amp;lt;- A default Sphinx project; see sphinx-doc.org for details
較
較럭較 models             &amp;lt;- Trained and serialized models, model predictions, or model summaries
較
較럭較 notebooks          &amp;lt;- Jupyter notebooks. Naming convention is a number (for ordering),
較                         the creator's initials, and a short `-` delimited description, e.g.
較                         `1.0-jqp-initial-data-exploration`.
較
較럭較 references         &amp;lt;- Data dictionaries, manuals, and all other explanatory materials.
較
較럭較 reports            &amp;lt;- Generated analysis as HTML, PDF, LaTeX, etc.
較먝 較덕較 figures        &amp;lt;- Generated graphics and figures to be used in reporting
較
較럭較 requirements.txt   &amp;lt;- The requirements file for reproducing the analysis environment, e.g.
較                         generated with `pip freeze &amp;gt; requirements.txt`
較
較럭較 setup.py           &amp;lt;- makes project pip installable (pip install -e .) so src can be imported
較럭較 src                &amp;lt;- Source code for use in this project.
較먝 較럭較 __init__.py    &amp;lt;- Makes src a Python module
較   較
較먝 較럭較 data           &amp;lt;- Scripts to download or generate data
較먝 較먝 較덕較 make_dataset.py
較   較
較먝 較럭較 features       &amp;lt;- Scripts to turn raw data into features for modeling
較먝 較먝 較덕較 build_features.py
較   較
較먝 較럭較 models         &amp;lt;- Scripts to train models and then use trained models to make
較   較   較                 predictions
較먝 較먝 較럭較 predict_model.py
較먝 較먝 較덕較 train_model.py
較   較
較먝 較덕較 visualization  &amp;lt;- Scripts to create exploratory and results oriented visualizations
較먝     較덕較 visualize.py
較
較덕較 tox.ini            &amp;lt;- tox file with settings for running tox; see tox.testrun.org
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;Project based on the &lt;a href="https://drivendata.github.io/cookiecutter-data-science/" rel="nofollow"&gt;cookiecutter data science project template&lt;/a&gt;. #cookiecutterdatascience&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>BlackArbsCEO</author><guid isPermaLink="false">https://github.com/BlackArbsCEO/Adv_Fin_ML_Exercises</guid><pubDate>Sun, 05 Jan 2020 00:22:00 GMT</pubDate></item><item><title>xiaohu2015/DeepLearning_tutorials #23 in Jupyter Notebook, Today</title><link>https://github.com/xiaohu2015/DeepLearning_tutorials</link><description>&lt;p&gt;&lt;i&gt;The deeplearning algorithms implemented by tensorflow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-tutorials-with-tensorflow" class="anchor" aria-hidden="true" href="#deep-learning-tutorials-with-tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Tutorials with Tensorflow&lt;/h1&gt;
&lt;p&gt;The deeplearning algorithms are carefully implemented by &lt;a href="https://www.tensorflow.org/" rel="nofollow"&gt;tensorflow&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-environment" class="anchor" aria-hidden="true" href="#environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.5&lt;/li&gt;
&lt;li&gt;tensorflow 1.4&lt;/li&gt;
&lt;li&gt;pytorch 0.2.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-the-deeplearning-algorithms-includes-now" class="anchor" aria-hidden="true" href="#the-deeplearning-algorithms-includes-now"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The deeplearning algorithms includes (now):&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Logistic Regression  &lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/logisticRegression.py"&gt;logisticRegression.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Multi-Layer Perceptron (MLP) &lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/mlp.py"&gt;mlp.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convolution Neural Network (CNN) &lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/cnn.py"&gt;cnn.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Denoising Aotoencoder (DA) &lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/da.py"&gt;da.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stacked Denoising Autoencoder (SDA) &lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/sda.py"&gt;sda.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Restricted Boltzmann Machine (RBM) [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/rbm.py"&gt;rbm.py&lt;/a&gt;    &lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/gbrbm.py"&gt;gbrbm.py&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Deep Belief Network (DBN) &lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/dbn.py"&gt;dbn.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: the project aims at imitating the well-implemented algorithms in &lt;a href="http://www.deeplearning.net/tutorial/" rel="nofollow"&gt;Deep Learning Tutorials&lt;/a&gt; (coded by &lt;a href="http://deeplearning.net/software/theano/index.html" rel="nofollow"&gt;Theano&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-cnn-models" class="anchor" aria-hidden="true" href="#cnn-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CNN Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MobileNet [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/MobileNet.py"&gt;self&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1704.04861" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/Zehaos/MobileNet/blob/master/nets/mobilenet.py"&gt;ref&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;MobileNetv2 [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/mobilenet_v2.py"&gt;self&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/1801.04381.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet"&gt;ref&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;SqueezeNet [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/SqueezeNet.py"&gt;self&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1602.07360" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/ResNet50.py"&gt;self&lt;/a&gt; &lt;a href="https://github.com/KaimingHe/deep-residual-networks"&gt;caffe ref&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;paper1&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1603.05027" rel="nofollow"&gt;paper2&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ShuffleNet [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/ShuffleNet.py"&gt;self&lt;/a&gt; by pytorch &lt;a href="http://cn.arxiv.org/pdf/1707.01083v2" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ShuffleNetv2 [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/shufflenet_v2.py"&gt;self&lt;/a&gt; &lt;a href="https://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/shufflenet.py"&gt;ref&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1807.11164" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DenseNet [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/densenet.py"&gt;self&lt;/a&gt; &lt;a href="https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py"&gt;pytorch_ref&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1608.06993" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-object-detection" class="anchor" aria-hidden="true" href="#object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;YOLOv1 [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/ObjectDetections/yolo/yolo_tf.py"&gt;self&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1506.02640" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/gliese581gg/YOLO_tensorflow"&gt;ref&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;SSD [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/ObjectDetections/SSD/SSD_demo.py"&gt;self&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/1611.10012.pdf" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf" rel="nofollow"&gt;slides&lt;/a&gt; &lt;a href="https://github.com/weiliu89/caffe/tree/ssd"&gt;cafe&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1512.02325" rel="nofollow"&gt;TF&lt;/a&gt; &lt;a href="https://github.com/amdegroot/ssd.pytorch"&gt;pytorch&lt;/a&gt; ]&lt;/li&gt;
&lt;li&gt;YOLOv2 [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/ObjectDetections/yolo2"&gt;self&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1612.08242" rel="nofollow"&gt;paper&lt;/a&gt; &lt;a href="https://github.com/yhcc/yolo2"&gt;ref&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-practical-examples" class="anchor" aria-hidden="true" href="#practical-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Practical examples&lt;/h3&gt;
&lt;p&gt;You can find more practical examples with tensorflow here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNN for setence classification [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/cnn_setence_classification"&gt;self&lt;/a&gt;] [&lt;a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/" rel="nofollow"&gt;blog&lt;/a&gt;] [&lt;a href="https://arxiv.org/pdf/1408.5882v2.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN for language model [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/rnn_language_model"&gt;self&lt;/a&gt;] [&lt;a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/" rel="nofollow"&gt;blog&lt;/a&gt;] [&lt;a href="http://blog.csdn.net/xiaohu2022/article/details/54578013" rel="nofollow"&gt;blog_cn&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LSTM for language model (PTB data) [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/lstm_model_ptb"&gt;self&lt;/a&gt;] [&lt;a href="https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html#recurrent-neural-networks" rel="nofollow"&gt;tutorial&lt;/a&gt;] [&lt;a href="https://arxiv.org/pdf/1409.2329.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VGG model for image classification (object recongnition) [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/VGG"&gt;self&lt;/a&gt;] [&lt;a href="https://github.com/machrisaa/tensorflow-vgg"&gt;source&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Residual network for cifar10_dataset [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/Resnet"&gt;self&lt;/a&gt;] [&lt;a href="https://github.com/wenxinxu/resnet-in-tensorflow"&gt;source&lt;/a&gt;] [&lt;a href="https://arxiv.org/pdf/1603.05027v3.pdf" rel="nofollow"&gt;paper&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LSTM for time series prediction [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/examples/lstm_time_series_regression"&gt;self&lt;/a&gt;] [&lt;a href="https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf20_RNN2.2/full_code.py"&gt;source&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative adversarial network (GAN) [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/examples/gan"&gt;self&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Variational autoencoder (VAE) [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/VAE"&gt;self&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/filters_corruption_30.png"&gt;&lt;img src="https://github.com/xiaohu2015/DeepLearning_tutorials/raw/master/results/filters_corruption_30.png" alt="1" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/new_filters_at_epoch_14.png"&gt;&lt;img src="https://github.com/xiaohu2015/DeepLearning_tutorials/raw/master/results/new_filters_at_epoch_14.png" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/new_original_and_10samples.png"&gt;&lt;img src="https://github.com/xiaohu2015/DeepLearning_tutorials/raw/master/results/new_original_and_10samples.png" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/DBN_results.png"&gt;&lt;img src="https://github.com/xiaohu2015/DeepLearning_tutorials/raw/master/results/DBN_results.png" alt="4" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/examples/lstm_time_series_regression/lstm_regression_results.png"&gt;&lt;img src="https://github.com/xiaohu2015/DeepLearning_tutorials/raw/master/examples/lstm_time_series_regression/lstm_regression_results.png" alt="5" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fun-blogs" class="anchor" aria-hidden="true" href="#fun-blogs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fun Blogs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/" rel="nofollow"&gt;Chatbots with Seq2Seq&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-personal-notes" class="anchor" aria-hidden="true" href="#personal-notes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Personal Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tensorflow for RNNs [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/notes/tf_rnn.ipynb"&gt;tf_rnn.ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Tensorflow for Autoencoder [&lt;a href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/notes/tf_autoencoder.ipynb"&gt;tf_autoencoder.ipynb&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-other-tutorials" class="anchor" aria-hidden="true" href="#other-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other Tutorials&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ageron/handson-ml/"&gt;ageron/handson-ml
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials"&gt;Hvass-Labs/TensorFlow-Tutorials
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/BinRoot/TensorFlow-Book"&gt;BinRoot/TensorFlow-Book
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sjchoi86/dl_tutorials_10weeks"&gt;sjchoi86/dl_tutorials_10weeks
&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-dont-hesitate-to-star-this-project-if-it-is-helpful" class="anchor" aria-hidden="true" href="#dont-hesitate-to-star-this-project-if-it-is-helpful"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Don't hesitate to star this project if it is helpful!&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-if-you-benefit-from-the-tutorial-please-make-a-small-donation-by-wechat-sweep" class="anchor" aria-hidden="true" href="#if-you-benefit-from-the-tutorial-please-make-a-small-donation-by-wechat-sweep"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If you benefit from the tutorial, please make a small donation by WeChat sweep.&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/weichat.jpg"&gt;&lt;img src="https://github.com/xiaohu2015/DeepLearning_tutorials/raw/master/results/weichat.jpg" alt="weichat" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-籃쉯찌낑xiaoxiaohu1994" class="anchor" aria-hidden="true" href="#籃쉯찌낑xiaoxiaohu1994"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;籃쉯찌낑庸뚖iaoxiaohu1994&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-縷뮏쯽柳뻔쭱淡뫤섟쨍낑燎쥔똗駱뷗먾숭柳됀뻕맫剌봲뺆辣갉eemy110" class="anchor" aria-hidden="true" href="#縷뮏쯽柳뻔쭱淡뫤섟쨍낑燎쥔똗駱뷗먾숭柳됀뻕맫剌봲뺆辣갉eemy110"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;縷뮏쯽柳뻔쭱淡뫤섟쨍낑庸뛵럌謄뻔솑達먾숭柳됀뻕맫剌봲뺆辣(Jeemy110)&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/654362565405877642.jpg"&gt;&lt;img src="https://github.com/xiaohu2015/DeepLearning_tutorials/raw/master/results/654362565405877642.jpg" alt="섟쨍낑" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xiaohu2015</author><guid isPermaLink="false">https://github.com/xiaohu2015/DeepLearning_tutorials</guid><pubDate>Sun, 05 Jan 2020 00:23:00 GMT</pubDate></item><item><title>fivethirtyeight/data #24 in Jupyter Notebook, Today</title><link>https://github.com/fivethirtyeight/data</link><description>&lt;p&gt;&lt;i&gt;Data and code behind the articles and graphics at FiveThirtyEight&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;See &lt;a href="https://data.fivethirtyeight.com/" rel="nofollow"&gt;https://data.fivethirtyeight.com/&lt;/a&gt; for a list of the data and code we've published.&lt;/p&gt;
&lt;p&gt;Unless otherwise noted, our data sets are available under the &lt;a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;, and the code is available under the &lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;MIT License&lt;/a&gt;. If you find this information useful, please &lt;a href="mailto:data@fivethirtyeight.com"&gt;let us know&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fivethirtyeight</author><guid isPermaLink="false">https://github.com/fivethirtyeight/data</guid><pubDate>Sun, 05 Jan 2020 00:24:00 GMT</pubDate></item><item><title>OpenGenus/cosmos #25 in Jupyter Notebook, Today</title><link>https://github.com/OpenGenus/cosmos</link><description>&lt;p&gt;&lt;i&gt;Algorithms that run our universe | Your personal library of every algorithm and data structure code that you will ever encounter | Ask us anything at our forum | Participate at Hacktoberfest&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cosmos" class="anchor" aria-hidden="true" href="#cosmos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cosmos&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://discourse.opengenus.org/" rel="nofollow"&gt;Join our discussion now&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The universe of algorithm and data structures&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Cosmos&lt;/strong&gt; is your personal offline collection of every algorithm and data structure one will ever encounter and use in a lifetime. This provides solutions in various languages spanning &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;JavaScript&lt;/code&gt;, &lt;code&gt;Swift&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt; and others.&lt;/p&gt;
&lt;p&gt;This work is maintained by a community of hundreds of people and is a &lt;em&gt;massive collaborative effort&lt;/em&gt; to bring the readily available coding knowledge &lt;strong&gt;offline&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Many coders ask me how to improve their own performances. I cannot say anything except "solve and review and prepare your library"&lt;/strong&gt; - &lt;em&gt;Uwi Tenpen&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-cosmic-structure" class="anchor" aria-hidden="true" href="#cosmic-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cosmic Structure&lt;/h1&gt;
&lt;p&gt;Following is the high-level structure of cosmos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/code/artificial_intelligence"&gt;Artificial intelligence&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;游뱄&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/backtracking"&gt;Backtracking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/bit_manipulation"&gt;Bit manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/cellular_automaton"&gt;Cellular automaton&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="shell" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41a.png"&gt;游냌&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/compression"&gt;Compression algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/computational_geometry"&gt;Computational geometry&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;丘뙖잺&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/cryptography"&gt;Cryptography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/data_structures"&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/design_pattern"&gt;Design pattern&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/divide_conquer"&gt;Divide conquering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/dynamic_programming"&gt;Dynamic programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/graph_algorithms"&gt;Graph algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/greedy_algorithms"&gt;Greedy algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/mathematical_algorithms"&gt;Mathematical algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;游댝&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/networking"&gt;Networking&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;游깷&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/numerical_analysis"&gt;Numerical analysis&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;游늳&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/online_challenges"&gt;Online challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/operating_system"&gt;Operating system&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;游눹&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/quantum_algorithms"&gt;Quantum algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="cyclone" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f300.png"&gt;游&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/randomized_algorithms"&gt;Randomized algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="slot_machine" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b0.png"&gt;游꿣&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/search"&gt;Searching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/selection_algorithms"&gt;Selecting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/sorting"&gt;Sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/square_root_decomposition"&gt;Square root decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/string_algorithms"&gt;String algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/unclassified"&gt;Unclassified&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="ghost" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f47b.png"&gt;游놑&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each type has several hundreds of problems with solutions in several languages spanning &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt; and others.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-maintainers" class="anchor" aria-hidden="true" href="#maintainers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Maintainers&lt;/h1&gt;
&lt;p&gt;This is a massive collaboration. Hence, to keep the quality intact and drive the vision in the proper direction, we have maintainers.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Maintainers are your friends forever. They are vastly different from moderators.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Currently, we have &lt;strong&gt;5 active maintainers&lt;/strong&gt; and we are expanding quickly.&lt;/p&gt;
&lt;p&gt;The task of maintainers is to review pull requests, suggest further quality additions and keep the work up to date with the current state of the world. &lt;g-emoji class="g-emoji" alias="earth_africa" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png"&gt;游깴&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/OpenGenus/cosmos/wiki/maintainers"&gt;Check out our current maintainers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let us know if you would like to be a maintainer in the Slack channel &lt;em&gt;#algorithms&lt;/em&gt; and we will review and add you upon subsequent contributions. To join our massive community at &lt;a href="https://opengenus.slack.com" rel="nofollow"&gt;Slack&lt;/a&gt; open an issue &lt;a href="https://github.com/OpenGenus/OpenGenus-Slack"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h1&gt;
&lt;p&gt;The success of our vision to bring knowledge offline depends on you. Even a small contribution helps. All forms of contributions are highly welcomed and valued.&lt;/p&gt;
&lt;p&gt;Currently, we have over &lt;strong&gt;700 contributors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you contribute, your name with a link (if available) is added to our &lt;a href="https://github.com/OpenGenus/cosmos/wiki/contributors"&gt;contributors list&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can contribute by writing &lt;code&gt;code&lt;/code&gt;, documentation in the form of &lt;code&gt;installation guides&lt;/code&gt; and &lt;code&gt;style guides&lt;/code&gt;, making Cosmos search friendly and many others. There are endless possibilities.&lt;/p&gt;
&lt;p&gt;Additionally, you might want to take a look at this &lt;a href="https://github.com/OpenGenus/cosmos/wiki/contribute"&gt;contributing guidelines&lt;/a&gt; before you make Cosmos better.&lt;/p&gt;
&lt;p&gt;You may, also, refer to the available &lt;a href="/guides/coding_style"&gt;style guides&lt;/a&gt; before contributing code.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;We believe in freedom and improvement. &lt;a href="https://github.com/OpenGenus/cosmos/blob/master/LICENSE"&gt;GNU General Public License v3.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>OpenGenus</author><guid isPermaLink="false">https://github.com/OpenGenus/cosmos</guid><pubDate>Sun, 05 Jan 2020 00:25:00 GMT</pubDate></item></channel></rss>