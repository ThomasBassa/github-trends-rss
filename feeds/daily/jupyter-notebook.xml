<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, Today</title><link>https://github.com/trending/jupyter-notebook?since=daily</link><description>The top repositories on GitHub for jupyter-notebook, measured daily</description><pubDate>Thu, 30 Jan 2020 01:14:20 GMT</pubDate><lastBuildDate>Thu, 30 Jan 2020 01:14:20 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>giswqs/earthengine-py-notebooks #1 in Jupyter Notebook, Today</title><link>https://github.com/giswqs/earthengine-py-notebooks</link><description>&lt;p&gt;&lt;i&gt;A collection of 300+ Jupyter Python notebook examples for using Google Earth Engine with interactive mapping&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-earthengine-py-notebooks" class="anchor" aria-hidden="true" href="#earthengine-py-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;earthengine-py-notebooks&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3ccf4c50a1576b0dd30b286717451fa56b783512/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A collection of &lt;strong&gt;300+&lt;/strong&gt; Jupyter Python notebook examples for using Google Earth Engine with interactive mapping&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contact:&lt;/strong&gt; Qiusheng Wu (&lt;a href="https://wetlands.io" rel="nofollow"&gt;https://wetlands.io&lt;/a&gt;)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-1-description" class="anchor" aria-hidden="true" href="#1-description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Description&lt;/h2&gt;
&lt;p&gt;This repository is a collection of &lt;strong&gt;300+&lt;/strong&gt; Jupyter Python notebook examples. I developed these examples by converting my other repo &lt;a href="https://github.com/giswqs/qgis-earthengine-examples"&gt;qgis-earthengine-examples&lt;/a&gt; from Python scripts to Jupyter notebooks. Now you can display Earth Engine data layers interactively in Jupyter notebooks without having to install &lt;a href="https://www.qgis.org/" rel="nofollow"&gt;QGIS&lt;/a&gt;. Three Python packages are being used in these examples, including the &lt;a href="https://developers.google.com/earth-engine/python_install" rel="nofollow"&gt;Earth Engine Python API&lt;/a&gt;, &lt;a href="https://github.com/python-visualization/folium"&gt;folium&lt;/a&gt;, and &lt;a href="https://github.com/giswqs/geehydro"&gt;geehydro&lt;/a&gt;. The &lt;strong&gt;geehydro&lt;/strong&gt; Python package builds on the folium package and implements several methods for displaying Earth Engine data layers, such as &lt;code&gt;Map.addLayer()&lt;/code&gt;, &lt;code&gt;Map.setCenter()&lt;/code&gt;, &lt;code&gt;Map.centerObject()&lt;/code&gt;, and &lt;code&gt;Map.setOptions()&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-2-usage" class="anchor" aria-hidden="true" href="#2-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Usage&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Explore this repository and open any available Jupyter notebook in your browser (e.g., &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Template/template.ipynb"&gt;template.ipynb&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; When the selected Jupyter notebook is open, click the &lt;a href="https://nbviewer.jupyter.org/github/giswqs/earthengine-py-notebooks/blob/master/Template/template.ipynb" rel="nofollow"&gt;Notebook Viewer&lt;/a&gt; link to view the interactive map.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; If you would like to execute cells in the notebook interactively, you will need to &lt;a href="https://earthengine.google.com/signup/" rel="nofollow"&gt;Sign up&lt;/a&gt; for a &lt;a href="https://earthengine.google.com/" rel="nofollow"&gt;Google Earth Engine&lt;/a&gt; account. Then you can click either &lt;a href="https://colab.research.google.com/github/giswqs/earthengine-py-notebooks/blob/master/Template/template.ipynb" rel="nofollow"&gt;Run in Google Colab&lt;/a&gt; or &lt;a href="https://mybinder.org/v2/gh/giswqs/earthengine-py-notebooks/master?filepath=Template/template.ipynb" rel="nofollow"&gt;Run in binder&lt;/a&gt; to execute code interactively. This will allow you to add your own Earth Engine Python script.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-3-demo" class="anchor" aria-hidden="true" href="#3-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Demo&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/b6f17f06134c1375ab827952a40a97b9cbeaa3f2/68747470733a2f2f692e696d6775722e636f6d2f746e4a434d7a4a2e676966"&gt;&lt;img src="https://camo.githubusercontent.com/b6f17f06134c1375ab827952a40a97b9cbeaa3f2/68747470733a2f2f692e696d6775722e636f6d2f746e4a434d7a4a2e676966" alt="ee-py-notebooks" data-canonical-src="https://i.imgur.com/tnJCMzJ.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-4-examples" class="anchor" aria-hidden="true" href="#4-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. Examples&lt;/h2&gt;
&lt;p&gt;The Table of Contents below mimics the structure of the Google Earth Engine &lt;a href="https://developers.google.com/earth-engine" rel="nofollow"&gt;API Documentation&lt;/a&gt;. I strongly encourage you to check out the API Documentation if you need an in-depth explanation of each Python example. Please note that the list below does not include all the Python examples contained in this repository. You are welcome to explore the repository and find more examples to suit your needs.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-get-started" class="anchor" aria-hidden="true" href="#get-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/GetStarted"&gt;Get Started&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/01_hello_world.ipynb"&gt;Hello world!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/02_adding_data_to_qgis.ipynb"&gt;Adding data to Map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/03_finding_images.ipynb"&gt;Finding images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/04_band_math.ipynb"&gt;Band math&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/05_map_function.ipynb"&gt;Mapping (what to do instead of a for-loop)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/06_reducing.ipynb"&gt;Reducing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/07_image_statistics.ipynb"&gt;Image statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/08_masking.ipynb"&gt;Masking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/GetStarted/09_a_complete_example.ipynb"&gt;A complete example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-machine-learning" class="anchor" aria-hidden="true" href="#machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/MachineLearning"&gt;Machine Learning&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Supervised Classification Algorithms
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/MachineLearning/cart_classifier.ipynb"&gt;Classification and Regression Trees (CART)&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/MachineLearning/svm_classifier.ipynb"&gt;Support Vector Machine (SVM)&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/MachineLearning/confusion_matrix.ipynb"&gt;Confusion Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Unsupervised Classification Algorithms
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/MachineLearning/clustering.ipynb"&gt;KMeans Clustering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-image" class="anchor" aria-hidden="true" href="#image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image"&gt;Image&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/image_overview.ipynb"&gt;Image Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/image_vis.ipynb"&gt;Image Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/image_metadata.ipynb"&gt;Image information and metadata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/band_math.ipynb"&gt;Mathematical operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/conditional_operations.ipynb"&gt;Relational, conditional and Boolean operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/convolutions.ipynb"&gt;Convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/morphological_operations.ipynb"&gt;Morphological Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/gradients.ipynb"&gt;Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/edge_detection.ipynb"&gt;Edge detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/spectral_unmixing.ipynb"&gt;Spectral transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/texture.ipynb"&gt;Texture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/object_based.ipynb"&gt;Object-based methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/cumulative_cost_mapping.ipynb"&gt;Cumulative Cost Mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/image_displacement.ipynb"&gt;Registering Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Miscellaneous
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/band_stats.ipynb"&gt;Band statistics (min, max, mean, std)&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/image_stats_by_band.ipynb"&gt;Image statistics by band&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Image/extract_value_to_points.ipynb"&gt;Extract value to points&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/rename_bands.ipynb"&gt;Rename bands&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/clipping.ipynb"&gt;Clipping&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/find_image_by_path_row.ipynb"&gt;Find image by path and row&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/get_image_resolution.ipynb"&gt;Get image resolution&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/get_image_extent.ipynb"&gt;Get image extent&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/set_image_properties.ipynb"&gt;Set image properties&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/select_bands.ipynb"&gt;Select bands&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/convert_bands_to_image_collection.ipynb"&gt;Convert bands to ImageCollection&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/reclassify.ipynb"&gt;Reclassify&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/composite_bands.ipynb"&gt;Composite bands&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/image_smoothing.ipynb"&gt;Image smoothing&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/download.ipynb"&gt;Download image&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/cell_statistics.ipynb"&gt;Cell statistics&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/image_patch_area.ipynb"&gt;Image patch area&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/get_image_id.ipynb"&gt;Get image id&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Image/get_band_name_and_type.ipynb"&gt;Get band name and type&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/filtering_by_calendar_range.ipynb"&gt;Filtering by calendar range&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-imagecollection" class="anchor" aria-hidden="true" href="#imagecollection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection"&gt;ImageCollection&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/ImageCollection/overview.ipynb"&gt;ImageCollection Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/ImageCollection/metadata.ipynb"&gt;ImageCollection Information and Metadata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/ImageCollection/filtering_collection.ipynb"&gt;Filtering an ImageCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/ImageCollection/map_function.ipynb"&gt;Mapping over an ImageCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/ImageCollection/reducing_collection.ipynb"&gt;Reducing an ImageCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/ImageCollection/mosaicking.ipynb"&gt;Compositing and Mosaicking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Miscellaneous
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/get_image_centroid.ipynb"&gt;Get image centroid&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/convert_imagecollection_to_image.ipynb"&gt;Convert ImageCollection to Image&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/sort_by_cloud_and_date.ipynb"&gt;Sort by cloud and date&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/filtering_by_metadata.ipynb"&gt;Filtering by metadata&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/filtering_by_band_names.ipynb"&gt;Filtering by band names&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/select_image_by_index.ipynb"&gt;Select image by index&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/ImageCollection/creating_monthly_imagery.ipynb"&gt;Creating monthly imagery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-geometry-feature-featurecollection" class="anchor" aria-hidden="true" href="#geometry-feature-featurecollection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection"&gt;Geometry, Feature, FeatureCollection&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/creating_feature.ipynb"&gt;Geometry Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/visualizing_geometries.ipynb"&gt;Geodesic vs. Planar Geometries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/visualizing_geometries.ipynb"&gt;Geometry Visualization and Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/geometric_operations.ipynb"&gt;Geometric Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/creating_feature.ipynb"&gt;Feature Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/from_polygons.ipynb"&gt;FeatureCollection Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/visualizing_feature_collection.ipynb"&gt;Feature and FeatureCollection Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/metadata_aggregation.ipynb"&gt;FeatureCollection Information and Metadata&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/filtering_feature_collection.ipynb"&gt;Filtering a FeatureCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/map_function.ipynb"&gt;Mapping over a FeatureCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/reducing_feature_collection.ipynb"&gt;Reducing a FeatureCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/idw_interpolation.ipynb"&gt;Vector to Raster Interpolation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Miscellaneous
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/FeatureCollection/add_new_attribute.ipynb"&gt;Add new attribute&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/add_area_column.ipynb"&gt;Add area column&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/add_random_value_column.ipynb"&gt;Add random value column&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/column_statistics.ipynb"&gt;Single column statistics&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/column_statistics_multiple.ipynb"&gt;Multiple column statistics&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/simplify_polygons.ipynb"&gt;Simplify polygons&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/column_statistics_by_group.ipynb"&gt;Column statistics by group&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/select_by_location.ipynb"&gt;Select by location&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/select_by_attributes.ipynb"&gt;Select by attributes&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/select_by_strings.ipynb"&gt;Select by strings&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/vector_symbology.ipynb"&gt;Vector symbology&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/merge_feature_collections.ipynb"&gt;Merge FeatureCollection&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/search_by_buffer_distance.ipynb"&gt;Search by buffer distance&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/select_columns.ipynb"&gt;Select columns&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/minimum_bounding_geometry.ipynb"&gt;Mimimum bounding geometry&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/FeatureCollection/clipping.ipynb"&gt;Clipping polygons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-reducer" class="anchor" aria-hidden="true" href="#reducer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Reducer"&gt;Reducer&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/min_max_reducer.ipynb"&gt;Reducer Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/median_reducer.ipynb"&gt;ImageCollection Reductions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/image_reductions.ipynb"&gt;Image Reductions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/stats_of_an_image_region.ipynb"&gt;Statistics of an Image Region&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/stats_of_image_regions.ipynb"&gt;Statistics of Image Regions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/stats_of_image_neighborhoods.ipynb"&gt;Statistics of Image Neighborhoods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/stats_of_columns.ipynb"&gt;Statistics of FeatureCollection Columns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/convert_raster_to_vector.ipynb"&gt;Raster to Vector Conversion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/convert_vector_to_raster.ipynb"&gt;Vector to Raster Conversion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Grouped Reductions and Zonal Statistics
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/stats_by_group.ipynb"&gt;Statistics by group&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/zonal_statistics.ipynb"&gt;Zonal Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/weighted_reductions.ipynb"&gt;Weighted Reductions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Reducer/linear_regression.ipynb"&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-join" class="anchor" aria-hidden="true" href="#join"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Join"&gt;Join&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Join/simple_joins.ipynb"&gt;Simple Joins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Join/inverted_joins.ipynb"&gt;Inverted Joins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Join/inner_joins.ipynb"&gt;Inner Joins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Join/save_all_joins.ipynb"&gt;Save-All Joins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Join/save_best_joins.ipynb"&gt;Save-Best Joins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Join/spatial_joins.ipynb"&gt;Spatial Joins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-array" class="anchor" aria-hidden="true" href="#array"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Array"&gt;Array&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Array/array_images.ipynb"&gt;Arrays and Array Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Array/array_transformations.ipynb"&gt;Array Transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Array/eigen_analysis.ipynb"&gt;Eigen Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Array/array_sorting.ipynb"&gt;Array Sorting and Reducing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-specialized-algorithms" class="anchor" aria-hidden="true" href="#specialized-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Algorithms"&gt;Specialized Algorithms&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Landsat Algorithms
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/landsat_radiance.ipynb"&gt;Radiance&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/landsat_surface_reflectance.ipynb"&gt;Surface Reflectance&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/landsat_cloud_score.ipynb"&gt;Simple cloud score&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/landsat_simple_composite.ipynb"&gt;Simple composite&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/sentinel-1_filtering.ipynb"&gt;Sentinel-1 Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Resampling and Reducing Resolution
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/resampling.ipynb"&gt;Resampling&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/reduce_resolution.ipynb"&gt;Reducing Resolution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/ntl_linear_fit.ipynb"&gt;Linear fit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pattern recognition
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Algorithms/center_pivot_irrigation_detector.ipynb"&gt;Center-pivot Irrigation Detector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-asset-management" class="anchor" aria-hidden="true" href="#asset-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/AssetManagement"&gt;Asset Management&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/AssetManagement/export_raster.ipynb"&gt;Exporting Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/AssetManagement/export_ImageCollection.ipynb"&gt;Exporting ImageCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/AssetManagement/export_vector.ipynb"&gt;Exporting Vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/AssetManagement/export_FeatureCollection.ipynb"&gt;Exporting FeatureCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/AssetManagement/export_csv.ipynb"&gt;Exporting CSV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/AssetManagement/export_table.ipynb"&gt;Exporting Table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/AssetManagement/export_TimeSeries.ipynb"&gt;Exporting TimeSeries&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-how-earth-engine-works" class="anchor" aria-hidden="true" href="#how-earth-engine-works"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/HowEarthEngineWorks"&gt;How Earth Engine Works&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/HowEarthEngineWorks/ClientVsServer.ipynb"&gt;Client vs. Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/HowEarthEngineWorks/DeferredExecution.ipynb"&gt;Deferred Execution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/HowEarthEngineWorks/Projections.ipynb"&gt;Projections&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-filter" class="anchor" aria-hidden="true" href="#filter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter"&gt;Filter&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter/filter_eq.ipynb"&gt;Filter to metadata equal to the given value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter/filter_neq.ipynb"&gt;Filter to metadata not equal to the given value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter/filter_in_list.ipynb"&gt;Filter on metadata contained in a list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter/filter_string_contains.ipynb"&gt;Filter on metadata that cotains a certain string&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter/filter_string_starts_with.ipynb"&gt;Filter on metadata that starts with a certain string&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter/filter_string_ends_with.ipynb"&gt;Filter on metadata that ends with a certain string&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Filter/filter_range_contains.ipynb"&gt;Filter on metadata that falls within a specified range&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-visualization" class="anchor" aria-hidden="true" href="#visualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Visualization"&gt;Visualization&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/image_rgb_composite.ipynb"&gt;RGB composite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/image_color_palettes.ipynb"&gt;Color palettes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/image_color_ramp.ipynb"&gt;Color ramp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/hillshade.ipynb"&gt;Hillshade&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/image_stretch.ipynb"&gt;Image stretch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/image_thumbanil.ipynb"&gt;Image thumbnail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/rendering_categorical_maps.ipynb"&gt;Rendering categorical maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/styled_layer_descriptors.ipynb"&gt;Styled layer descriptors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/terrain_visualization.ipynb"&gt;Terrain visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/visualizing_feature_collection.ipynb"&gt;Visualizing FeatureCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/visualizing_geometries.ipynb"&gt;Visualizing Geometry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/nlcd_land_cover.ipynb"&gt;NLCD Land Cover&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Visualization/us_counties.ipynb"&gt;US Counties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Miscellaneous
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Visualization/ndvi_symbology.ipynb"&gt;NDVI symbology&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Visualization/ndvi_symbology.ipynb"&gt;NDWI symbology&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Visualization/landsat_symbology.ipynb"&gt;Landsat symbology&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Visualization/nwi_wetlands_symbology.ipynb"&gt;NWI wetlands symbology&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Visualization/color_by_attribute.ipynb"&gt;Color by attribute&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Visualization/random_color_visualizer.ipynb"&gt;Random color visualizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets"&gt;Datasets&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Terrain"&gt;Terrain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Water"&gt;Water&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors"&gt;Vector datasets catalog&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/international_boundary.ipynb"&gt;Large Scale International Boundary Polygons (LSIB)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/us_census_counties.ipynb"&gt;TIGER: US 2018 Census Counties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/us_census_states.ipynb"&gt;TIGER: US 2018 Census States&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/us_census_roads.ipynb"&gt;TIGER: US 2016 Census Roads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/us_census_blocks.ipynb"&gt;TIGER: US 2010 Census Blocks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/us_census_tracts.ipynb"&gt;TIGER: US Census 2010 Census Tracts + Demographic Profile 1 aggregate statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/us_census_zip_code.ipynb"&gt;TIGER: US Census 2010 5-digit ZIP Code Tabulation Areas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/global_land_ice_measurements.ipynb"&gt;GLIMS: Global Land Ice Measurements from Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/usgs_watershed_boundary.ipynb"&gt;USGS Watershed Boundary Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/us_epa_ecoregions.ipynb"&gt;USA EPA Ecoregions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/resolve_ecoregions.ipynb"&gt;RESOLVE Ecoregions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/world_database_on_protected_areas.ipynb"&gt;World Database on Protected Areas (WDPA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/global_power_plant_database.ipynb"&gt;WRI Global Power Plant Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Datasets/Vectors/landsat_wrs2_grid.ipynb"&gt;Landsat WRS-2 grid&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Tutorials"&gt;Tutorials&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Tutorials/GlobalSurfaceWater"&gt;Global Surface Water&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-genas-examples" class="anchor" aria-hidden="true" href="#genas-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/tree/master/Gena"&gt;Gena's Examples&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Gena/map_add_features.ipynb"&gt;Map.addLayer()&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Gena/map_center_object.ipynb"&gt;Map.centerObject()&lt;/a&gt; | &lt;a href="https://github.com/giswqs/earthengine-py-notebooks/blob/master/Gena/map_set_center.ipynb"&gt;Map.setCenter()&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>giswqs</author><guid isPermaLink="false">https://github.com/giswqs/earthengine-py-notebooks</guid><pubDate>Thu, 30 Jan 2020 00:01:00 GMT</pubDate></item><item><title>aamini/introtodeeplearning #2 in Jupyter Notebook, Today</title><link>https://github.com/aamini/introtodeeplearning</link><description>&lt;p&gt;&lt;i&gt;Lab Materials for MIT 6.S191: Introduction to Deep Learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;&lt;img src="assets/banner.png" alt="banner" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository contains all of the code and software labs for &lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;MIT 6.S191: Introduction to Deep Learning&lt;/a&gt;! All lecture slides and videos are available on the course website.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-opening-the-labs-in-google-colaboratory" class="anchor" aria-hidden="true" href="#opening-the-labs-in-google-colaboratory"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Opening the labs in Google Colaboratory:&lt;/h2&gt;
&lt;p&gt;The 2020 6.S191 labs will be run in Google's Colaboratory, a Jupyter notebook environment that runs entirely in the cloud, you don't need to download anything. To run these labs, you must have a Google account.&lt;/p&gt;
&lt;p&gt;On this Github repo, navigate to the lab folder you want to run (&lt;code&gt;lab1&lt;/code&gt;, &lt;code&gt;lab2&lt;/code&gt;, &lt;code&gt;lab3&lt;/code&gt;) and open the appropriate python notebook (*.ipynb). Click the "Run in Colab" link on the top of the lab. That's it!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-the-labs" class="anchor" aria-hidden="true" href="#running-the-labs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running the labs&lt;/h2&gt;
&lt;p&gt;Now, to run the labs, open the Jupyter notebook on Colab. Navigate to the "Runtime" tab --&amp;gt; "Change runtime type". In the pop-up window, under "Runtime type" select "Python 3", and under "Hardware accelerator" select "GPU". Go through the notebooks and fill in the &lt;code&gt;#TODO&lt;/code&gt; cells to get the code to compile for yourself!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-mit-deep-learning-package" class="anchor" aria-hidden="true" href="#mit-deep-learning-package"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MIT Deep Learning package&lt;/h3&gt;
&lt;p&gt;You might notice that inside the labs we install the &lt;code&gt;mitdeeplearning&lt;/code&gt; python package from the Python Package repository:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install mitdeeplearning&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This package contains convienence functions that we use throughout the course and can be imported like any other Python package.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import mitdeeplearning as mdl&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We do this for you in each of the labs, but the package is also open source under the same license so you can also use it outside the class.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lecture-videos" class="anchor" aria-hidden="true" href="#lecture-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lecture Videos&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=5v1JnYv_yWs&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=1" rel="nofollow"&gt;&lt;img src="assets/video_play.png" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All lecture videos are available publicly online and linked above! Use and/or modification of lecture slides outside of 6.S191 must reference:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;
&lt;p&gt;&lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;All code in this repository is copyright 2020 &lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;MIT 6.S191 Introduction to Deep Learning&lt;/a&gt;. All Rights Reserved.&lt;/p&gt;
&lt;p&gt;Licensed under the MIT License. You may not use this file except in compliance with the License. Use and/or modification of this code outside of 6.S191 must reference:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;
&lt;p&gt;&lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>aamini</author><guid isPermaLink="false">https://github.com/aamini/introtodeeplearning</guid><pubDate>Thu, 30 Jan 2020 00:02:00 GMT</pubDate></item><item><title>dragen1860/TensorFlow-2.x-Tutorials #3 in Jupyter Notebook, Today</title><link>https://github.com/dragen1860/TensorFlow-2.x-Tutorials</link><description>&lt;p&gt;&lt;i&gt;TensorFlow 2.x version's  Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT examples, etc. TF 2.0&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-20-tutorials" class="anchor" aria-hidden="true" href="#tensorflow-20-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 Tutorials&lt;/h1&gt;
&lt;p&gt;Our repo. is the &lt;strong&gt;Winner&lt;/strong&gt; of &lt;a href="https://devpost.com/software/tensorflow-2-0-tutorials" rel="nofollow"&gt;#PoweredByTF 2.0 Challenge!&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="res/tensorflow-2.0.gif"&gt;&lt;img src="res/tensorflow-2.0.gif" width="250" align="middle" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Timeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oct. 1, 2019: TensorFlow 2.0 Stable!&lt;/li&gt;
&lt;li&gt;Aug. 24, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow 2.0 rc0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jun. 8, 2019: &lt;a href="https://twitter.com/fchollet/status/1134583289384120320" rel="nofollow"&gt;TensorFlow 2.0 Beta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mar. 7, 2019: &lt;a href="https://www.tensorflow.org/alpha" rel="nofollow"&gt;Tensorflow 2.0 Alpha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jan. 11, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow r2.0 preview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Aug. 14, 2018: &lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A" rel="nofollow"&gt;TensorFlow 2.0 is coming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;make sure you are using python 3.x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;GPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install &lt;code&gt;CUDA 10.0&lt;/code&gt;(or after) and &lt;code&gt;cudnn&lt;/code&gt; by yourself. and set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; up.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow&lt;span class="pl-k"&gt;-&lt;/span&gt;gpu  &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Test installation:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;In [&lt;span class="pl-c1"&gt;2&lt;/span&gt;]: &lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow  &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

In [&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: tf.&lt;span class="pl-c1"&gt;__version__&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;2.0.0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
In [&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: tf.test.is_gpu_available()
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
totalMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;95GiB&lt;/span&gt; freeMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;00GiB&lt;/span&gt;
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: &lt;span class="pl-c1"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-tf2" class="anchor" aria-hidden="true" href="#tf2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TF2&lt;/h1&gt;
&lt;p align="center"&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/cover.png" width="400" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/TF_QR_163.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt; 
&lt;p&gt;TensorFlow 2.0&lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;TensorFlow 2&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;-  &lt;a target="_blank" rel="noopener noreferrer" href="res/weibo.jpg"&gt;&lt;img src="res/weibo.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-includes" class="anchor" aria-hidden="true" href="#includes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Includes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow 2.0 Overview&lt;/li&gt;
&lt;li&gt;TensorFlow 2.0 Basic Usage&lt;/li&gt;
&lt;li&gt;Linear Regression&lt;/li&gt;
&lt;li&gt;MNIST, FashionMNIST&lt;/li&gt;
&lt;li&gt;CIFAR10&lt;/li&gt;
&lt;li&gt;Fully Connected Layer&lt;/li&gt;
&lt;li&gt;VGG16&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;ResNet18&lt;/li&gt;
&lt;li&gt;Naive RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;ColorBot&lt;/li&gt;
&lt;li&gt;Auto-Encoders&lt;/li&gt;
&lt;li&gt;Variational Auto-Encoders&lt;/li&gt;
&lt;li&gt;DCGAN&lt;/li&gt;
&lt;li&gt;CycleGAN&lt;/li&gt;
&lt;li&gt;WGAN&lt;/li&gt;
&lt;li&gt;Pixel2Pixel&lt;/li&gt;
&lt;li&gt;Faster RCNN&lt;/li&gt;
&lt;li&gt;A2C&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;GCN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to submit a &lt;strong&gt;PR&lt;/strong&gt; request to make this repo. more complete!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-refered-repos" class="anchor" aria-hidden="true" href="#refered-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Refered Repos.&lt;/h1&gt;
&lt;p&gt;Our work is not built from scratch. Great appreciation to these open works&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/madalinabuzau/tensorflow-eager-tutorials"&gt;https://github.com/madalinabuzau/tensorflow-eager-tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/herbiebradley/CycleGAN-Tensorflow"&gt;https://github.com/herbiebradley/CycleGAN-Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/moono/tf-eager-on-GAN"&gt;https://github.com/moono/tf-eager-on-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Viredery/tf-eager-fasterrcnn"&gt;https://github.com/Viredery/tf-eager-fasterrcnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/github/gitignore/blob/master/Python.gitignore"&gt;https://github.com/github/gitignore/blob/master/Python.gitignore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dragen1860</author><guid isPermaLink="false">https://github.com/dragen1860/TensorFlow-2.x-Tutorials</guid><pubDate>Thu, 30 Jan 2020 00:03:00 GMT</pubDate></item><item><title>mml-book/mml-book.github.io #4 in Jupyter Notebook, Today</title><link>https://github.com/mml-book/mml-book.github.io</link><description>&lt;p&gt;&lt;i&gt;Companion webpage to the book "Mathematics For Machine Learning"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mml-bookgithubio" class="anchor" aria-hidden="true" href="#mml-bookgithubio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mml-book.github.io&lt;/h1&gt;
&lt;p&gt;Companion webpage to the book "Mathematics For Machine Learning"&lt;/p&gt;
&lt;p&gt;&lt;a href="https://mml-book.com" rel="nofollow"&gt;https://mml-book.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyright 2020 by Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. To be published by Cambridge University Press.&lt;/p&gt;
&lt;p&gt;We are in the process of writing a book on Mathematics for Machine Learning that motivates people to learn mathematical concepts. The book is not intended to cover advanced machine learning techniques because there are already plenty of books doing this. Instead, we aim to provide the necessary mathematical skills to read those other books.&lt;/p&gt;
&lt;p&gt;We split the book into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mathematical foundations&lt;/li&gt;
&lt;li&gt;Example machine learning algorithms that use the mathematical foundations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We aim to keep this book reasonably short, so we cannot cover everything. We will also provide exercises for part 1 and jupyter notebooks for part 2 of the book.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mml-book</author><guid isPermaLink="false">https://github.com/mml-book/mml-book.github.io</guid><pubDate>Thu, 30 Jan 2020 00:04:00 GMT</pubDate></item><item><title>hunkim/DeepLearningZeroToAll #5 in Jupyter Notebook, Today</title><link>https://github.com/hunkim/DeepLearningZeroToAll</link><description>&lt;p&gt;&lt;i&gt;TensorFlow Basic Tutorial Labs&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-lab-code-wip-but-call-for-comments" class="anchor" aria-hidden="true" href="#lab-code-wip-but-call-for-comments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lab code (WIP), but call for comments&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/hunkim/DeepLearningZeroToAll" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3a08d3822c942ca48219579a6b3824e009d70ca4/68747470733a2f2f7472617669732d63692e6f72672f68756e6b696d2f446565704c6561726e696e675a65726f546f416c6c2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/hunkim/DeepLearningZeroToAll.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is code for labs covered in TensorFlow basic tutorials (in Korean) at &lt;a href="https://youtu.be/BS6O0zOGX4E" rel="nofollow"&gt;https://youtu.be/BS6O0zOGX4E&lt;/a&gt;.
(We also have a plan to record videos in English.)&lt;/p&gt;
&lt;p&gt;This is work in progress, and may have bugs.
However, we call for your comments and pull requests. Check out our style guide line:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More TF (1.0) style: use more recent and decent TF APIs.&lt;/li&gt;
&lt;li&gt;More Pythonic: fully leverage the power of python&lt;/li&gt;
&lt;li&gt;Readability (over efficiency): Since it's for instruction purposes, we prefer &lt;em&gt;readability&lt;/em&gt; over others.&lt;/li&gt;
&lt;li&gt;Understandability (over everything): Understanding TF key concepts is the main goal of this code.&lt;/li&gt;
&lt;li&gt;KISS: Keep It Simple Stupid! &lt;a href="https://www.techopedia.com/definition/20262/keep-it-simple-stupid-principle-kiss-principle" rel="nofollow"&gt;https://www.techopedia.com/definition/20262/keep-it-simple-stupid-principle-kiss-principle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lab-slides" class="anchor" aria-hidden="true" href="#lab-slides"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lab slides:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://goo.gl/jPtWNt" rel="nofollow"&gt;https://goo.gl/jPtWNt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We welcome your comments on slides.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-file-naming-rule" class="anchor" aria-hidden="true" href="#file-naming-rule"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;File naming rule:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;klab-XX-X-[name].py: Keras labs code&lt;/li&gt;
&lt;li&gt;lab-XX-X-[name].py: TensorFlow lab code&lt;/li&gt;
&lt;li&gt;mxlab-XX-X-[name].py: MXNet lab code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-install-requirements" class="anchor" aria-hidden="true" href="#install-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Install requirements&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-run-test-and-autopep8" class="anchor" aria-hidden="true" href="#run-test-and-autopep8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run test and autopep8&lt;/h2&gt;
&lt;p&gt;TODO: Need to add more test cases&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m unittest discover -s tests&lt;span class="pl-k"&gt;;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; http://stackoverflow.com/questions/14328406/&lt;/span&gt;
pip install autopep8 &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; if you haven't install&lt;/span&gt;
autopep8 &lt;span class="pl-c1"&gt;.&lt;/span&gt; --recursive --in-place --pep8-passes 2000 --verbose&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-automatically-create-requirementstxt" class="anchor" aria-hidden="true" href="#automatically-create-requirementstxt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Automatically create requirements.txt&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install pipreqs

pipreqs /path/to/project&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="http://stackoverflow.com/questions/31684375" rel="nofollow"&gt;http://stackoverflow.com/questions/31684375&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributionscomments" class="anchor" aria-hidden="true" href="#contributionscomments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions/Comments&lt;/h2&gt;
&lt;p&gt;We always welcome your comments and pull requests.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reference-implementations" class="anchor" aria-hidden="true" href="#reference-implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference Implementations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nlintz/TensorFlow-Tutorials/"&gt;https://github.com/nlintz/TensorFlow-Tutorials/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/golbin/TensorFlow-ML-Exercises"&gt;https://github.com/golbin/TensorFlow-ML-Exercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/FuZer/Study_TensorFlow"&gt;https://github.com/FuZer/Study_TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fchollet/keras/tree/master/examples"&gt;https://github.com/fchollet/keras/tree/master/examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hunkim</author><guid isPermaLink="false">https://github.com/hunkim/DeepLearningZeroToAll</guid><pubDate>Thu, 30 Jan 2020 00:05:00 GMT</pubDate></item><item><title>priya-dwivedi/Deep-Learning #6 in Jupyter Notebook, Today</title><link>https://github.com/priya-dwivedi/Deep-Learning</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep-Learning&lt;/h3&gt;
&lt;p&gt;This repository contains deep learning related projects I have done over time. I am very passionate about deep learning and explore interesting ideas and tools. Each project is contained in its own folder.&lt;/p&gt;
&lt;p&gt;I have blogged about a lot of these projects on Medium - &lt;a href="https://medium.com/@priya.dwivedi" rel="nofollow"&gt;https://medium.com/@priya.dwivedi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I also run a deep learning consultancy - &lt;a href="https://deeplearninganalytics.org/" rel="nofollow"&gt;https://deeplearninganalytics.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you want to collaborate on a project please reach out through my website.&lt;/p&gt;
&lt;p&gt;Hope you enjoy cloning this repo and trying out things yourself&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>priya-dwivedi</author><guid isPermaLink="false">https://github.com/priya-dwivedi/Deep-Learning</guid><pubDate>Thu, 30 Jan 2020 00:06:00 GMT</pubDate></item><item><title>MicrosoftLearning/DP100 #7 in Jupyter Notebook, Today</title><link>https://github.com/MicrosoftLearning/DP100</link><description>&lt;p&gt;&lt;i&gt;Labs for Course DP-100: Designing and Implementing Data Science Solutions on Microsoft Azure&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dp-100-designing-and-implementing-a-data-science-solution-on-azure" class="anchor" aria-hidden="true" href="#dp-100-designing-and-implementing-a-data-science-solution-on-azure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DP-100: Designing and Implementing a Data Science Solution on Azure&lt;/h1&gt;
&lt;p&gt;This repo contains the lab files for Microsoft course &lt;a href="https://docs.microsoft.com/en-us/learn/certifications/courses/dp-100t01" rel="nofollow"&gt;DP-100T01-A: &lt;em&gt;Designing and Implementing a Data Science Solution on Azure&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The lab instructions are &lt;a href="labdocs/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributions" class="anchor" aria-hidden="true" href="#contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions&lt;/h2&gt;
&lt;p&gt;At this time, we are not accepting external contributions to this repo. If you have suggestions or spot any errors, please report them as &lt;a href="https://github.com/MicrosoftLearning/DP100/issues"&gt;issues&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>MicrosoftLearning</author><guid isPermaLink="false">https://github.com/MicrosoftLearning/DP100</guid><pubDate>Thu, 30 Jan 2020 00:07:00 GMT</pubDate></item><item><title>awslabs/amazon-sagemaker-examples #8 in Jupyter Notebook, Today</title><link>https://github.com/awslabs/amazon-sagemaker-examples</link><description>&lt;p&gt;&lt;i&gt;Example notebooks that show how to apply machine learning, deep learning and reinforcement learning in Amazon SageMaker&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-amazon-sagemaker-examples" class="anchor" aria-hidden="true" href="#amazon-sagemaker-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Examples&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks that show how to apply machine learning and deep learning in &lt;a href="https://aws.amazon.com/sagemaker" rel="nofollow"&gt;Amazon SageMaker&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-ground-truth-labeling-jobs" class="anchor" aria-hidden="true" href="#introduction-to-ground-truth-labeling-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification"&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/ground_truth_object_detection_tutorial"&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output"&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/object_detection_augmented_manifest_training"&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an "augmented manifest" and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/annotation_consolidation"&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-applying-machine-learning" class="anchor" aria-hidden="true" href="#introduction-to-applying-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Applying Machine Learning&lt;/h3&gt;
&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; predicts potential customers that are most likely to convert based on customer and aggregate level metrics, using Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives.  This uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/linear_time_series_forecast"&gt;Time-series Forecasting&lt;/a&gt; generates a forecast for topline product demand using Amazon SageMaker's Linear Learner algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/breast_cancer_prediction"&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker's Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/ensemble_modeling"&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/video_game_sales"&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/gluon_recommender_system"&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/fair_linear_learner"&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans"&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/object2vec_document_embedding"&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sagemaker-automatic-model-tuning" class="anchor" aria-hidden="true" href="#sagemaker-automatic-model-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SageMaker Automatic Model Tuning&lt;/h3&gt;
&lt;p&gt;These examples introduce SageMaker's hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/xgboost_direct_marketing"&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fits for the &lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; task.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/tensorflow_mnist"&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/mxnet_mnist"&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/keras_bring_your_own"&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/r_bring_your_own"&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/analyze_results"&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-amazon-algorithms" class="anchor" aria-hidden="true" href="#introduction-to-amazon-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Amazon Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker's custom developed algorithms.  Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/1P_kmeans_highlevel"&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker.  It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/factorization_machines_mnist"&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker's implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/linear_learner_mnist"&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ntm_synthetic"&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/pca_mnist"&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/seq2seq_translation_en-de"&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that's built on top of &lt;a href="https://github.com/awslabs/sockeye"&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet.  Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation.  This notebook shows translation from English to German text.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker's Image Classification algorithm.  This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_abalone"&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html" rel="nofollow"&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_mnist"&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/deepar_synthetic"&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/blazingtext_word2vec_text8"&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker's fast and scalable BlazingText implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_pascalvoc_coco"&gt;Object Detection&lt;/a&gt; illustrates how to train an object detector using the Amazon SageMaker Object Detection algorithm with different input formats (RecordIO and image).  It uses the Pascal VOC dataset. A third notebook is provided to demonstrate the use of incremental training.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_birds"&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_movie_recommendation"&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification"&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_sentence_similarity"&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ipinsights_login"&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc"&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentaion masks and probability of segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-rl" class="anchor" aria-hidden="true" href="#amazon-sagemaker-rl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker RL&lt;/h3&gt;
&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_cartpole_coach"&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel's RL Coach.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_deepracer_robomaker_coach_gazebo"&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_hvac_coach_energyplus"&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_knapsack_coach_custom"&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_mountain_car_coach_gymEnv"&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_network_compression_ray_custom"&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_objecttracker_robomaker_coach_gazebo"&gt;Turtlebot Tracker&lt;/a&gt; This notebook demonstrates object tracking using AWS Robomaker and RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_portfolio_management_coach_customEnv"&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_predictive_autoscaling_coach_customEnv"&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_ray"&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_stable_baselines"&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach"&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_tic_tac_toe_coach_customEnv"&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-scientific-details-of-algorithms" class="anchor" aria-hidden="true" href="#scientific-details-of-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scientific Details of Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/streaming_median"&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker's spectral decomposition approach to LDA.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/linear_learner_class_weights_loss_functions"&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-debugger" class="anchor" aria-hidden="true" href="#amazon-sagemaker-debugger"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Debugger&lt;/h3&gt;
&lt;p&gt;These examples provide and introduction to SageMaker Debugger which allows debugging and monitoring capabilities for training of machine learning and deep learning algorithms. Note that although these notebooks focus on a specific framework, the same approach works with all the frameworks that Amazon SageMaker Debugger supports. The notebooks below are listed in the order in which we recommend you review them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_builtin_rule/"&gt;Using a built-in rule with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_keras_custom_rule/"&gt;Using a custom rule with TensorFlow Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mnist_tensor_analysis/"&gt;Interactive tensor analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mnist_tensor_plot/"&gt;Visualizing Debugging Tensors of MXNet training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mxnet_realtime_analysis/"&gt;Real-time analysis in notebook with MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/xgboost_builtin_rules/"&gt;Using a built in rule with XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/xgboost_realtime_analysis/"&gt;Real-time analysis in notebook with XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/mxnet_spot_training/"&gt;Using SageMaker Debugger with Managed Spot Training and MXNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/tensorflow_action_on_rule/"&gt;Reacting to CloudWatch Events from Rules to take an action based on status with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-debugger/pytorch_custom_container/"&gt;Using SageMaker Debugger with a custom PyTorch container&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-advanced-amazon-sagemaker-functionality" class="anchor" aria-hidden="true" href="#advanced-amazon-sagemaker-functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt;
&lt;p&gt;These examples that showcase unique functionality available in Amazon SageMaker.  They cover a broad range of topics and will utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="advanced_functionality/data_distribution_types"&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances.  This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/handling_kms_encrypted_data"&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/parquet_to_recordio_protobuf"&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href="https://parquet.apache.org/" rel="nofollow"&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/working_with_redshift_data"&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/xgboost_bring_your_own_model"&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/kmeans_bring_your_own_model"&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that's been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/install_r_kernel"&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/scikit_bring_your_own"&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/mxnet_mnist_byom"&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/tensorflow_iris_byom"&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_xgboost_abalone"&gt;Inference Pipeline with SparkML and XGBoost&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and XGBoost for training on the Abalone dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_blazingtext_dbpedia"&gt;Inference Pipeline with SparkML and BlazingText&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and BlazingText for training on the DBPedia dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/search"&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_bring_your_own"&gt;Host Multiple Models with Your Own Algorithm&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint with your own custom algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_xgboost_home_value"&gt;Host Multiple Models with XGBoost&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled XGBoost container.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/multi_model_sklearn_home_value"&gt;Host Multiple Models with SKLearn&lt;/a&gt; shows how to deploy multiple models to a realtime hosted endpoint using a multi-model enabled SKLearn container.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-neo-compilation-jobs" class="anchor" aria-hidden="true" href="#amazon-sagemaker-neo-compilation-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide you an introduction to how to use Neo to optimizes deep learning model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; Adapts form &lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;image classification&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;mxnet mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/pytorch_torchvision"&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;tensorflow mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; Adapts form &lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;xgboost customer churn&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-procesing" class="anchor" aria-hidden="true" href="#amazon-sagemaker-procesing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Procesing&lt;/h3&gt;
&lt;p&gt;These examples show you how to use SageMaker Processing jobs to run data processing workloads.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_processing/scikit_learn_data_processing_and_model_evaluation"&gt;Scikit-Learn Data Processing and Model Evaluation&lt;/a&gt; shows how to use SageMaker Processing and the Scikit-Learn container to run data preprocessing and model evaluation workloads.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_processing/feature_transformation_with_sagemaker_processing"&gt;Feature transformation with Amazon SageMaker Processing and SparkML&lt;/a&gt; shows how to use SageMaker Processing to run data processing workloads using SparkML prior to training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk" class="anchor" aria-hidden="true" href="#amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-deep-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-deep-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you to write idiomatic TensorFlow or MXNet and then train or host in pre-built containers using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_cifar10"&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_mnist"&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_sentiment_analysis"&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_iris"&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_cifar10"&gt;CIFAR-10 with MXNet Gluon&lt;/a&gt; trains a ResNet-34  image classification model using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_mnist"&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet's symbolic syntax&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_sentiment"&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_layers"&gt;TensorFlow Neural Networks with Layers&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow layers&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_keras"&gt;TensorFlow Networks with Keras&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow and Keras&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators"&gt;Introduction to Estimators in TensorFlow&lt;/a&gt; trains a DNN classifier estimator on the Iris dataset using TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_resnet_cifar10_with_tensorboard"&gt;TensorFlow and TensorBoard&lt;/a&gt; trains a ResNet image classification model on CIFAR-10 using TensorFlow and showcases how to track results using TensorBoard&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; trains a simple convolutional neural network on MNIST using TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-machine-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-machine-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone"&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_inference_pipeline"&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-using-amazon-sagemaker-with-apache-spark" class="anchor" aria-hidden="true" href="#using-amazon-sagemaker-with-apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt;
&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href="https://github.com/aws/sagemaker-spark"&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-spark/pyspark_mnist"&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aws-marketplace" class="anchor" aria-hidden="true" href="#aws-marketplace"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS Marketplace&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;This example shows you how to package a model-package/algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/creating_marketplace_products"&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use algorithms and model packages from AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_algorithms"&gt;Using Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/amazon_demo_product"&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/automl"&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_model_packages"&gt;Using Model Packages&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/amazon_demo_product"&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/auto_insurance"&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/improving_industrial_workplace_safety"&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_data"&gt;Using Data&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_data/using_data_from_aws_data_exchange_to_predict_product_popularity"&gt;Using data and algorithm from AWS Marketplace for training a model&lt;/a&gt; provides a detailed walkthrough on how to use data from AWS Marketplace for training a model that predicts popularity of a bath product.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-under-development" class="anchor" aria-hidden="true" href="#under-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Under Development&lt;/h3&gt;
&lt;p&gt;These Amazon SageMaker examples fully illustrate a concept, but may require some additional configuration on the users part to complete.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What do I need in order to get started?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The quickest setup to run example notebooks includes:
&lt;ul&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html" rel="nofollow"&gt;AWS account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Proper &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html" rel="nofollow"&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html" rel="nofollow"&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html" rel="nofollow"&gt;S3 bucket&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Will these examples work outside of Amazon SageMaker Notebook Instances?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;How do I contribute my own example notebook?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources.  Please bear with us in the short-term if pull requests take longer than expected or are closed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>awslabs</author><guid isPermaLink="false">https://github.com/awslabs/amazon-sagemaker-examples</guid><pubDate>Thu, 30 Jan 2020 00:08:00 GMT</pubDate></item><item><title>jackfrued/Python-100-Days #9 in Jupyter Notebook, Today</title><link>https://github.com/jackfrued/Python-100-Days</link><description>&lt;p&gt;&lt;i&gt;Python - 100&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python---100" class="anchor" aria-hidden="true" href="#python---100"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python - 100&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;PythonPythonPython15101510Python100&lt;a href="./%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.md"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-qq-group.png"&gt;&lt;img src="./res/python-qq-group.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h3&gt;
&lt;p&gt;Python&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;C/C++&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; - Python / Java / Go&lt;/li&gt;
&lt;li&gt;DevOps - Python / Shell / Ruby / Go&lt;/li&gt;
&lt;li&gt; - Python / PHP / C++&lt;/li&gt;
&lt;li&gt; - Python / R / Scala / Matlab&lt;/li&gt;
&lt;li&gt; - Python / R / Java / Lisp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python /  / &lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Python /  / &lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Python /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python20185&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-top-10.png"&gt;&lt;img src="./res/python-top-10.png" alt="PythonTop 10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-bj-salary.png"&gt;&lt;img src="./res/python-bj-salary.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-salary-chengdu.png"&gt;&lt;img src="./res/python-salary-chengdu.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make English as your working language.&lt;/li&gt;
&lt;li&gt;Practice makes perfect.&lt;/li&gt;
&lt;li&gt;All experience comes from mistakes.&lt;/li&gt;
&lt;li&gt;Don't be one of the leeches.&lt;/li&gt;
&lt;li&gt;Either stand out or kicked out.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day0115---python" class="anchor" aria-hidden="true" href="#day0115---python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01~15 - &lt;a href="./Day01-15"&gt;Python&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day01---python" class="anchor" aria-hidden="true" href="#day01---python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01 - &lt;a href="./Day01-15/01.%E5%88%9D%E8%AF%86Python.md"&gt;Python&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python - Python / Python / Python&lt;/li&gt;
&lt;li&gt; - Windows / Linux / MacOS&lt;/li&gt;
&lt;li&gt;Python - Hello, world / print / &lt;/li&gt;
&lt;li&gt;IDLE - (REPL) /  /  / IDLE&lt;/li&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day02---" class="anchor" aria-hidden="true" href="#day02---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day02 - &lt;a href="./Day01-15/02.%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt; -  /  / input /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  /  / &lt;/li&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day03---" class="anchor" aria-hidden="true" href="#day03---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day03 - &lt;a href="./Day01-15/03.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt;if - if / if-else / if-elif-else / if&lt;/li&gt;
&lt;li&gt; -  /  /  /  /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day04---" class="anchor" aria-hidden="true" href="#day04---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day04 - &lt;a href="./Day01-15/04.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt;while -  / break / continue&lt;/li&gt;
&lt;li&gt;for -  / range /  /  / &lt;/li&gt;
&lt;li&gt; - 1~100 /  /  /  /  /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day05---" class="anchor" aria-hidden="true" href="#day05---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day05 - &lt;a href="./Day01-15/05.%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; /  / Craps&lt;/li&gt;
&lt;li&gt; /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day06---" class="anchor" aria-hidden="true" href="#day06---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day06 - &lt;a href="./Day01-15/06.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  / &lt;/li&gt;
&lt;li&gt; - def /  /  / return / &lt;/li&gt;
&lt;li&gt; - Python /  &lt;/li&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt; -   /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  / &lt;/li&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day07---" class="anchor" aria-hidden="true" href="#day07---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day07 - &lt;a href="./Day01-15/07.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  /  /  /  / &lt;/li&gt;
&lt;li&gt; -  / () /  /  /  / &lt;/li&gt;
&lt;li&gt; - range /  / &lt;/li&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt; -  /   /  /  /  &lt;/li&gt;
&lt;li&gt; -  /  /  /  /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  /  / &lt;/li&gt;
&lt;li&gt; - keys() / values() / items() / setdefault()&lt;/li&gt;
&lt;li&gt; -  /  /  / Fibonacci / &lt;/li&gt;
&lt;li&gt; -  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day08---" class="anchor" aria-hidden="true" href="#day08---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day08 - &lt;a href="./Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  / __str__&lt;/li&gt;
&lt;li&gt; -  / &lt;/li&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day09---" class="anchor" aria-hidden="true" href="#day09---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day09 - &lt;a href="./Day01-15/09.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  /  /  /  / __slots__&lt;/li&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;li&gt; - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__&lt;/li&gt;
&lt;li&gt;() -  /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  /  /  / ()C3&lt;/li&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day10---" class="anchor" aria-hidden="true" href="#day10---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day10 - &lt;a href="./Day01-15/10.%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;tkinterGUI&lt;/li&gt;
&lt;li&gt;pygame&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day11---" class="anchor" aria-hidden="true" href="#day11---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day11 - &lt;a href="./Day01-15/11.%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;li&gt; -  /  /  / &lt;/li&gt;
&lt;li&gt; -  / try-except / else / finally /  /  / raise&lt;/li&gt;
&lt;li&gt; - CSV / csv / JSON / json&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day12---" class="anchor" aria-hidden="true" href="#day12---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day12 - &lt;a href="./Day01-15/12.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  /  / in not in / is / joinsplit / strip / pyperclip /  / StringIO&lt;/li&gt;
&lt;li&gt; -  /  /  /  /  /  / / re&lt;/li&gt;
&lt;li&gt; - re / compile / groupgroups / match / search / findallfinditer / subsubn / split&lt;/li&gt;
&lt;li&gt; - &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day13---" class="anchor" aria-hidden="true" href="#day13---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day13 - &lt;a href="./Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  /  / &lt;/li&gt;
&lt;li&gt; - fork / multiprocessing /  / &lt;/li&gt;
&lt;li&gt; - thread / threading / Thread / Lock / Condition / &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day14---" class="anchor" aria-hidden="true" href="#day14---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day14 - &lt;a href="./Day01-15/14.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt; -  / TCP-IP / IP /  /  / &lt;/li&gt;
&lt;li&gt; - - / -&lt;/li&gt;
&lt;li&gt;HTTP - API / URL / requests / JSON&lt;/li&gt;
&lt;li&gt;Python -  / socket /  socket / TCP / TCP / UDP / UDP / SocketServer&lt;/li&gt;
&lt;li&gt; - SMTP / POP3 / IMAP / smtplib / poplib / imaplib&lt;/li&gt;
&lt;li&gt; - &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day15---" class="anchor" aria-hidden="true" href="#day15---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day15 - &lt;a href="./Day01-15/15.%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pillow -  /  /  /  / &lt;/li&gt;
&lt;li&gt;Word -  /  /  / &lt;/li&gt;
&lt;li&gt;Excel - xlrd / xlwt&lt;/li&gt;
&lt;li&gt;PDF - pypdf2 / reportlab&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day16day20---python-" class="anchor" aria-hidden="true" href="#day16day20---python-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day16~Day20 - &lt;a href="./Day16-20/16-20.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md"&gt;Python &lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt; -  /  / Lambda /  / &lt;/li&gt;
&lt;li&gt; -  /  /  /  /  /  /  / GoF&lt;/li&gt;
&lt;li&gt; -  /  /&lt;/li&gt;
&lt;li&gt; -  /  / IO / asyncawait&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day2130---web" class="anchor" aria-hidden="true" href="#day2130---web"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day21~30 - &lt;a href="./Day21-30/21-30.Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0.md"&gt;Web&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HTML&lt;/li&gt;
&lt;li&gt;CSS&lt;/li&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;li&gt;jQuery&lt;/li&gt;
&lt;li&gt;Vue.js&lt;/li&gt;
&lt;li&gt;Element&lt;/li&gt;
&lt;li&gt;Bootstrap&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3135---linux" class="anchor" aria-hidden="true" href="#day3135---linux"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day31~35 - &lt;a href="./Day31-35/31-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md"&gt;Linux&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Vim&lt;/li&gt;
&lt;li&gt;Shell&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3640---" class="anchor" aria-hidden="true" href="#day3640---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day36~40 - &lt;a href="./Day36-40"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./Day36-40/36-38.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL.md"&gt;MySQL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;SQL
&lt;ul&gt;
&lt;li&gt;DDL -  - create / drop / alter&lt;/li&gt;
&lt;li&gt;DML -  - insert / delete / update / select&lt;/li&gt;
&lt;li&gt;DCL -  - grant / revoke&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt; - &lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PythonMySQL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="./Day36-40/39-40.NoSQL%E5%85%A5%E9%97%A8.md"&gt;NoSQL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;NoSQL&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;li&gt;Mongo&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day4155---django" class="anchor" aria-hidden="true" href="#day4155---django"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41~55 - &lt;a href="./Day41-55"&gt;Django&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day41---" class="anchor" aria-hidden="true" href="#day41---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41 - &lt;a href="./Day41-55/41.%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebHTTP&lt;/li&gt;
&lt;li&gt;Django&lt;/li&gt;
&lt;li&gt;5&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day42---" class="anchor" aria-hidden="true" href="#day42---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day42 - &lt;a href="./Day41-55/42.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;ORMCRUD&lt;/li&gt;
&lt;li&gt;Django&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day43---ajax" class="anchor" aria-hidden="true" href="#day43---ajax"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day43 - &lt;a href="./Day41-55/43.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md"&gt;Ajax&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Ajax&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day44---" class="anchor" aria-hidden="true" href="#day44---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day44 - &lt;a href="./Day41-55/44.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;CSRF&lt;/li&gt;
&lt;li&gt;FormModelForm&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day45---cookiesession" class="anchor" aria-hidden="true" href="#day45---cookiesession"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day45 - &lt;a href="./Day41-55/45.Cookie%E5%92%8CSession.md"&gt;CookieSession&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;cookiesession&lt;/li&gt;
&lt;li&gt;Djangosession&lt;/li&gt;
&lt;li&gt;cookie&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day46---" class="anchor" aria-hidden="true" href="#day46---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day46 - &lt;a href="./Day41-55/46.%E6%8A%A5%E8%A1%A8%E5%92%8C%E6%97%A5%E5%BF%97.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;HttpResponse&lt;/li&gt;
&lt;li&gt;StreamingHttpResponse&lt;/li&gt;
&lt;li&gt;xlwtExcel&lt;/li&gt;
&lt;li&gt;reportlabPDF&lt;/li&gt;
&lt;li&gt;ECharts&lt;/li&gt;
&lt;li&gt;Django-Debug-Toolbar&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day47---" class="anchor" aria-hidden="true" href="#day47---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day47 - &lt;a href="./Day41-55/47.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Django&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day48---" class="anchor" aria-hidden="true" href="#day48---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day48 - &lt;a href="./Day41-55/48.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JSON&lt;/li&gt;
&lt;li&gt;Vue.js&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day49---restfuldrf" class="anchor" aria-hidden="true" href="#day49---restfuldrf"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day49 - &lt;a href="./Day41-55/49.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md"&gt;RESTfulDRF&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day50---restfuldrf" class="anchor" aria-hidden="true" href="#day50---restfuldrf"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day50 - &lt;a href="./Day41-55/50.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md"&gt;RESTfulDRF&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day51---" class="anchor" aria-hidden="true" href="#day51---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day51 - &lt;a href="./Day41-55/51.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DjangoRedis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day52---" class="anchor" aria-hidden="true" href="#day52---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day52 - &lt;a href="./Day41-55/52.%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;wangEditor&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day53---" class="anchor" aria-hidden="true" href="#day53---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day53 - &lt;a href="./Day41-55/53.%E7%9F%AD%E4%BF%A1%E5%92%8C%E9%82%AE%E4%BB%B6.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Django&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day54---" class="anchor" aria-hidden="true" href="#day54---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day54 - &lt;a href="./Day41-55/54.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;celery&lt;/li&gt;
&lt;li&gt;celery&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day55---" class="anchor" aria-hidden="true" href="#day55---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day55 - &lt;a href="./Day41-55/55.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Django&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;uWSGI&lt;/li&gt;
&lt;li&gt;Nginx&lt;/li&gt;
&lt;li&gt;HTTPS&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day5660---flask" class="anchor" aria-hidden="true" href="#day5660---flask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56~60 - &lt;a href="./Day56-65"&gt;Flask&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day56---flask" class="anchor" aria-hidden="true" href="#day56---flask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56 - &lt;a href="./Day56-60/56.Flask%E5%85%A5%E9%97%A8.md"&gt;Flask&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day57---" class="anchor" aria-hidden="true" href="#day57---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day57 - &lt;a href="./Day56-60/57.%E6%A8%A1%E6%9D%BF%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day58---" class="anchor" aria-hidden="true" href="#day58---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day58 - &lt;a href="./Day56-60/58.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%A4%84%E7%90%86.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day59---" class="anchor" aria-hidden="true" href="#day59---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day59 - &lt;a href="./Day56-60/59.%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day60---" class="anchor" aria-hidden="true" href="#day60---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day60 - &lt;a href="./Day56-60/60.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day6165---tornado" class="anchor" aria-hidden="true" href="#day6165---tornado"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61~65 - &lt;a href="./Day61-65"&gt;Tornado&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day61---" class="anchor" aria-hidden="true" href="#day61---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61 - &lt;a href="./Day61-65/61.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;I/O&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day62---tornado" class="anchor" aria-hidden="true" href="#day62---tornado"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day62 - &lt;a href="./Day61-65/62.Tornado%E5%85%A5%E9%97%A8.md"&gt;Tornado&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tornado&lt;/li&gt;
&lt;li&gt;5Tornado&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day63---" class="anchor" aria-hidden="true" href="#day63---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day63 - &lt;a href="./Day61-65/63.%E5%BC%82%E6%AD%A5%E5%8C%96.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;aiomysqlaioredis&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day64---websocket" class="anchor" aria-hidden="true" href="#day64---websocket"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day64 - &lt;a href="./Day61-65/64.WebSocket%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;WebSocket&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebSocket&lt;/li&gt;
&lt;li&gt;WebSocket&lt;/li&gt;
&lt;li&gt;WebSocket&lt;/li&gt;
&lt;li&gt;Web&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day65---" class="anchor" aria-hidden="true" href="#day65---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day65 - &lt;a href="./Day61-65/65.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Vue.js&lt;/li&gt;
&lt;li&gt;ECharts&lt;/li&gt;
&lt;li&gt;WebSocket&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day6675---" class="anchor" aria-hidden="true" href="#day6675---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66~75 - &lt;a href="./Day66-75"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day66---" class="anchor" aria-hidden="true" href="#day66---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66 - &lt;a href="./Day66-75/66.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day67---" class="anchor" aria-hidden="true" href="#day67---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day67 - &lt;a href="./Day66-75/67.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt; / XPath / CSS&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day68---" class="anchor" aria-hidden="true" href="#day68---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day68 - &lt;a href="./Day66-75/68.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day69---" class="anchor" aria-hidden="true" href="#day69---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day69 - &lt;a href="./Day66-75/69.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;I/O&lt;/li&gt;
&lt;li&gt;asyncawait&lt;/li&gt;
&lt;li&gt;aiohttp&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day70---" class="anchor" aria-hidden="true" href="#day70---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day70 - &lt;a href="./Day66-75/70.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;li&gt;Selenium&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day71---" class="anchor" aria-hidden="true" href="#day71---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day71 - &lt;a href="./Day66-75/71.%E8%A1%A8%E5%8D%95%E4%BA%A4%E4%BA%92%E5%92%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E5%A4%84%E7%90%86.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Cookie&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day72---scrapy" class="anchor" aria-hidden="true" href="#day72---scrapy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day72 - &lt;a href="./Day66-75/72.Scrapy%E5%85%A5%E9%97%A8.md"&gt;Scrapy&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Scrapy&lt;/li&gt;
&lt;li&gt;Scrapy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day73---scrapy" class="anchor" aria-hidden="true" href="#day73---scrapy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day73 - &lt;a href="./Day66-75/73.Scrapy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md"&gt;Scrapy&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Spider&lt;/li&gt;
&lt;li&gt; / &lt;/li&gt;
&lt;li&gt;ScrapySelenium&lt;/li&gt;
&lt;li&gt;ScrapyDocker&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day74---scrapy" class="anchor" aria-hidden="true" href="#day74---scrapy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day74 - &lt;a href="./Day66-75/74.Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0.md"&gt;Scrapy&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Scrapy&lt;/li&gt;
&lt;li&gt;Scrapyd&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day75---" class="anchor" aria-hidden="true" href="#day75---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day75 - &lt;a href="./Day66-75/75.%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day7690---" class="anchor" aria-hidden="true" href="#day7690---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76~90 - &lt;a href="./Day76-90"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day76---" class="anchor" aria-hidden="true" href="#day76---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76 - &lt;a href="./Day76-90/76.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day77---pandas" class="anchor" aria-hidden="true" href="#day77---pandas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day77 - &lt;a href="./Day76-90/77.Pandas%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;Pandas&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day78---numpyscipy" class="anchor" aria-hidden="true" href="#day78---numpyscipy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day78 - &lt;a href="./Day76-90/78.NumPy%E5%92%8CSciPy%E7%9A%84%E5%BA%94%E7%94%A8"&gt;NumPySciPy&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day79---matplotlib" class="anchor" aria-hidden="true" href="#day79---matplotlib"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day79 - &lt;a href="./Day76-90/79.Matplotlib%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"&gt;Matplotlib&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day80---kknn" class="anchor" aria-hidden="true" href="#day80---kknn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day80 - &lt;a href="./Day76-90/80.k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB.md"&gt;k(KNN)&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day81---" class="anchor" aria-hidden="true" href="#day81---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day81 - &lt;a href="./Day76-90/81.%E5%86%B3%E7%AD%96%E6%A0%91.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day82---" class="anchor" aria-hidden="true" href="#day82---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day82 - &lt;a href="./Day76-90/82.%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day83---svm" class="anchor" aria-hidden="true" href="#day83---svm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day83 - &lt;a href="./Day76-90/83.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md"&gt;(SVM)&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day84---k-" class="anchor" aria-hidden="true" href="#day84---k-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day84 - &lt;a href="./Day76-90/84.K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB.md"&gt;K-&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day85---" class="anchor" aria-hidden="true" href="#day85---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day85 - &lt;a href="./Day76-90/85.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day86---" class="anchor" aria-hidden="true" href="#day86---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day86 - &lt;a href="./Day76-90/86.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day87---" class="anchor" aria-hidden="true" href="#day87---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day87 - &lt;a href="./Day76-90/87.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BF%9B%E9%98%B6.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day88---tensorflow" class="anchor" aria-hidden="true" href="#day88---tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day88 - &lt;a href="./Day76-90/88.Tensorflow%E5%85%A5%E9%97%A8.md"&gt;Tensorflow&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day89---tensorflow" class="anchor" aria-hidden="true" href="#day89---tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day89 - &lt;a href="./Day76-90/89.Tensorflow%E5%AE%9E%E6%88%98.md"&gt;Tensorflow&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day90---" class="anchor" aria-hidden="true" href="#day90---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day90 - &lt;a href="./Day76-90/90.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day91100---" class="anchor" aria-hidden="true" href="#day91100---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day91~100 - &lt;a href="./Day91-100"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-91" class="anchor" aria-hidden="true" href="#91"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;91&lt;a href="./Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;ER&lt;/li&gt;
&lt;li&gt; / &lt;/li&gt;
&lt;li&gt; / &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scrum- Scrum Master - Sprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backlog&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;bug&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Showcase&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;/strong&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/agile-scrum-sprint-cycle.png"&gt;&lt;img src="./res/agile-scrum-sprint-cycle.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;8-10&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;LogoUIto doin progressdone&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/company_architecture.png"&gt;&lt;img src="./res/company_architecture.png" alt="company_architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;flake8pylint&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/pylint.png"&gt;&lt;img src="./res/pylint.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python&lt;a href="Python%E6%83%AF%E4%BE%8B.md"&gt;Python-Pythonic&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;-&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GitMercury&lt;/li&gt;
&lt;li&gt;&lt;a href="https://about.gitlab.com/" rel="nofollow"&gt;Gitlab&lt;/a&gt;&lt;a href="http://www.redmine.org.cn/" rel="nofollow"&gt;Redmine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zentao.net/" rel="nofollow"&gt;&lt;/a&gt;&lt;a href="https://www.atlassian.com/software/jira/features" rel="nofollow"&gt;JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jenkins.io/" rel="nofollow"&gt;Jenkins&lt;/a&gt;&lt;a href="https://travis-ci.org/" rel="nofollow"&gt;Travis-CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CMS//&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MIS+KMSKPIHRSCRM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;App+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;XMind&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/requirements_by_xmind.png"&gt;&lt;img src="./res/requirements_by_xmind.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OOAD&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;UML&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/uml-class-diagram.png"&gt;&lt;img src="./res/uml-class-diagram.png" alt="uml" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py makemigrations app
python manage.py migrate&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PowerDesigner&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/power-designer-pdm.png"&gt;&lt;img src="./res/power-designer-pdm.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py inspectdb &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; app/models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-92docker" class="anchor" aria-hidden="true" href="#92docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;92&lt;a href="./Day91-100/92.Docker%E5%AE%B9%E5%99%A8%E8%AF%A6%E8%A7%A3.md"&gt;Docker&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;DockerNginxMySQLRedisGitlabJenkins&lt;/li&gt;
&lt;li&gt;DockerDockerfile&lt;/li&gt;
&lt;li&gt;Docker-compose&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-93mysql" class="anchor" aria-hidden="true" href="#93mysql"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;93&lt;a href="./Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md"&gt;MySQL&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-94api" class="anchor" aria-hidden="true" href="#94api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;94&lt;a href="./Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md"&gt;API&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-95djangoday91-10095djangomd" class="anchor" aria-hidden="true" href="#95djangoday91-10095djangomd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;95[Django](./Day91-100/95.Django	.md)&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;Django-Debug-ToolBar&lt;/li&gt;
&lt;li&gt;PythonAPI&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-rest-api" class="anchor" aria-hidden="true" href="#rest-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;REST API&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;RESTful
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2011/09/restful.html" rel="nofollow"&gt;RESTful&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html" rel="nofollow"&gt;RESTful API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html" rel="nofollow"&gt;RESTful API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;API
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rap2.taobao.org/" rel="nofollow"&gt;RAP2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yapi.demo.qunar.com/" rel="nofollow"&gt;YAPI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.django-rest-framework.org/" rel="nofollow"&gt;django-REST-framework&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt; - Redis&lt;/li&gt;
&lt;li&gt; - Celery + RabbitMQ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-96" class="anchor" aria-hidden="true" href="#96"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;96&lt;a href="Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;unittestpytestnose2toxddt&lt;/li&gt;
&lt;li&gt;coverage&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;SECRET_KEY / DEBUG / ALLOWED_HOSTS /  / &lt;/li&gt;
&lt;li&gt;HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;uWSGI/GunicornNginx
&lt;ul&gt;
&lt;li&gt;GunicornuWSGI
&lt;ul&gt;
&lt;li&gt;GunicornuWSGIGunicornGunicorn&lt;/li&gt;
&lt;li&gt;uWSGI&lt;/li&gt;
&lt;li&gt;NginxuWSGINginxuWSGIuWSGIWSGI&lt;/li&gt;
&lt;li&gt;GunicornuWSGI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;AB&lt;/li&gt;
&lt;li&gt;SQLslap&lt;/li&gt;
&lt;li&gt;sysbench&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;ShellPython&lt;/li&gt;
&lt;li&gt;Selenium
&lt;ul&gt;
&lt;li&gt;Selenium IDE&lt;/li&gt;
&lt;li&gt;Selenium WebDriver&lt;/li&gt;
&lt;li&gt;Selenium Remote Control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Robot Framework&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-97" class="anchor" aria-hidden="true" href="#97"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;97&lt;a href="./Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-98" class="anchor" aria-hidden="true" href="#98"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;98&lt;a href="./Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;Web
&lt;ul&gt;
&lt;li&gt;Nginx&lt;/li&gt;
&lt;li&gt;Keepalived&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;CDN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-99" class="anchor" aria-hidden="true" href="#99"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;99&lt;a href="./Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-100python" class="anchor" aria-hidden="true" href="#100python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;100&lt;a href="./Day91-100/100.Python%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86.md"&gt;Python&lt;/a&gt;&lt;/h4&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jackfrued</author><guid isPermaLink="false">https://github.com/jackfrued/Python-100-Days</guid><pubDate>Thu, 30 Jan 2020 00:09:00 GMT</pubDate></item><item><title>dennybritz/reinforcement-learning #10 in Jupyter Notebook, Today</title><link>https://github.com/dennybritz/reinforcement-learning</link><description>&lt;p&gt;&lt;i&gt;Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h3&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h3&gt;
&lt;p&gt;This repository provides code, exercises and solutions for popular Reinforcement Learning algorithms. These are meant to serve as a learning tool to complement the theoretical materials from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf" rel="nofollow"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="nofollow"&gt;David Silver's Reinforcement Learning Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each folder in corresponds to one or more chapters of the above textbook and/or course. In addition to exercises and solution, each folder also contains a list of learning goals, a brief concept summary, and links to the relevant readings.&lt;/p&gt;
&lt;p&gt;All code is written in Python 3 and uses RL environments from &lt;a href="https://gym.openai.com/" rel="nofollow"&gt;OpenAI Gym&lt;/a&gt;. Advanced techniques use &lt;a href="https://www.tensorflow.org/" rel="nofollow"&gt;Tensorflow&lt;/a&gt; for neural network implementations.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="Introduction/"&gt;Introduction to RL problems &amp;amp; OpenAI Gym&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MDP/"&gt;MDPs and Bellman Equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DP/"&gt;Dynamic Programming: Model-Based RL, Policy Iteration and Value Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/"&gt;Monte Carlo Model-Free Prediction &amp;amp; Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="TD/"&gt;Temporal Difference Model-Free Prediction &amp;amp; Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="FA/"&gt;Function Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DQN/"&gt;Deep Q Learning&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/"&gt;Policy Gradient Methods&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;Learning and Planning (WIP)&lt;/li&gt;
&lt;li&gt;Exploration and Exploitation (WIP)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-list-of-implemented-algorithms" class="anchor" aria-hidden="true" href="#list-of-implemented-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;List of Implemented Algorithms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="DP/Policy%20Evaluation%20Solution.ipynb"&gt;Dynamic Programming Policy Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DP/Policy%20Iteration%20Solution.ipynb"&gt;Dynamic Programming Policy Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DP/Value%20Iteration%20Solution.ipynb"&gt;Dynamic Programming Value Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/MC%20Prediction%20Solution.ipynb"&gt;Monte Carlo Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/MC%20Control%20with%20Epsilon-Greedy%20Policies%20Solution.ipynb"&gt;Monte Carlo Control with Epsilon-Greedy Policies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb"&gt;Monte Carlo Off-Policy Control with Importance Sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="TD/SARSA%20Solution.ipynb"&gt;SARSA (On Policy TD Learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="TD/Q-Learning%20Solution.ipynb"&gt;Q-Learning (Off Policy TD Learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="FA/Q-Learning%20with%20Value%20Function%20Approximation%20Solution.ipynb"&gt;Q-Learning with Linear Function Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DQN/Deep%20Q%20Learning%20Solution.ipynb"&gt;Deep Q-Learning for Atari Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="DQN/Double%20DQN%20Solution.ipynb"&gt;Double Deep-Q Learning for Atari Games&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deep Q-Learning with Prioritized Experience Replay (WIP)&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb"&gt;Policy Gradient: REINFORCE with Baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/CliffWalk%20Actor%20Critic%20Solution.ipynb"&gt;Policy Gradient: Actor Critic with Baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb"&gt;Policy Gradient: Actor Critic with Baseline for Continuous Action Spaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Deterministic Policy Gradients for Continuous Action Spaces (WIP)&lt;/li&gt;
&lt;li&gt;Deep Deterministic Policy Gradients (DDPG) (WIP)&lt;/li&gt;
&lt;li&gt;&lt;a href="PolicyGradient/a3c"&gt;Asynchronous Advantage Actor Critic (A3C)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h3&gt;
&lt;p&gt;Textbooks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://incompleteideas.net/book/RLbook2018.pdf" rel="nofollow"&gt;Reinforcement Learning: An Introduction (2nd Edition)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Classes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="nofollow"&gt;David Silver's Reinforcement Learning Course (UCL, 2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rll.berkeley.edu/deeprlcourse/" rel="nofollow"&gt;CS294 - Deep Reinforcement Learning (Berkeley, Fall 2015)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.udacity.com/course/reinforcement-learning--ud600" rel="nofollow"&gt;CS 8803 - Reinforcement Learning (Georgia Tech)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/" rel="nofollow"&gt;CS885 - Reinforcement Learning (UWaterloo), Spring 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="nofollow"&gt;CS294-112 - Deep Reinforcement Learning (UC Berkeley)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Talks/Tutorials:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/" rel="nofollow"&gt;Introduction to Reinforcement Learning (Joelle Pineau @ Deep Learning Summer School 2016)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/" rel="nofollow"&gt;Deep Reinforcement Learning (Pieter Abbeel @ Deep Learning Summer School 2016)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://techtalks.tv/talks/deep-reinforcement-learning/62360/" rel="nofollow"&gt;Deep Reinforcement Learning ICML 2016 Tutorial (David Silver)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=ggqnxyjaKe4" rel="nofollow"&gt;Tutorial: Introduction to Reinforcement Learning with Function Approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLjKEIQlKCTZYN3CYBlj8r58SbNorobqcp" rel="nofollow"&gt;John Schulman - Deep Reinforcement Learning (4 Lectures)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf" rel="nofollow"&gt;Deep Reinforcement Learning Slides @ NIPS 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spinningup.openai.com/en/latest/user/introduction.html" rel="nofollow"&gt;OpenAI Spinning Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs" rel="nofollow"&gt;Advanced Deep Learning &amp;amp; Reinforcement Learning (UCL 2018, DeepMind)&lt;/a&gt;
-&lt;a href="https://sites.google.com/view/deep-rl-bootcamp/lectures" rel="nofollow"&gt;Deep RL Bootcamp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other Projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/carpedm20/deep-rl-tensorflow"&gt;carpedm20/deep-rl-tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/matthiasplappert/keras-rl"&gt;matthiasplappert/keras-rl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Selected Papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.readcube.com/articles/10.1038/nature14236" rel="nofollow"&gt;Human-Level Control through Deep Reinforcement Learning (2015-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1509.06461" rel="nofollow"&gt;Deep Reinforcement Learning with Double Q-learning (2015-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1509.02971" rel="nofollow"&gt;Continuous control with deep reinforcement learning (2015-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1511.05952" rel="nofollow"&gt;Prioritized Experience Replay (2015-11)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1511.06581" rel="nofollow"&gt;Dueling Network Architectures for Deep Reinforcement Learning (2015-11)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1602.01783" rel="nofollow"&gt;Asynchronous Methods for Deep Reinforcement Learning (2016-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1603.01121" rel="nofollow"&gt;Deep Reinforcement Learning from Self-Play in Imperfect-Information Games (2016-03)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf" rel="nofollow"&gt;Mastering the game of Go with deep neural networks and tree search&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dennybritz</author><guid isPermaLink="false">https://github.com/dennybritz/reinforcement-learning</guid><pubDate>Thu, 30 Jan 2020 00:10:00 GMT</pubDate></item><item><title>ageron/handson-ml2 #11 in Jupyter Notebook, Today</title><link>https://github.com/ageron/handson-ml2</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in the second edition of my O'Reilly book &lt;a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067" title="book" width="150" data-canonical-src="https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href="https://github.com/ageron/handson-ml"&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-play-with-these-notebooks-online-without-having-to-install-anything" class="anchor" aria-hidden="true" href="#want-to-play-with-these-notebooks-online-without-having-to-install-anything"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt;
&lt;p&gt;Use any of the following services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: open this repository in &lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;Colaboratory&lt;/a&gt;:
&lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e69988217d15707bdd8b6b27f1d7d53a0dd00af7/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f696d672f636f6c61625f66617669636f6e2e69636f" width="90" data-canonical-src="https://colab.research.google.com/img/colab_favicon.ico" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;Binder&lt;/a&gt;:
&lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/69ea8abed4df43bca4c671b965aeffef2c4f897a/68747470733a2f2f6d61747468696173627573736f6e6e6965722e636f6d2f706f7374732f696d672f62696e6465725f6c6f676f5f313238783132382e706e67" width="90" data-canonical-src="https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: Most of the time, Binder starts up quickly and works great, but when handson-ml2 is updated, Binder creates a new environment from scratch, and this can take quite some time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;Deepnote&lt;/a&gt;:
&lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3fae03be31b768100aa2a800d2cc3b6650c6cd48/68747470733a2f2f7777772e646565706e6f74652e636f6d2f7374617469632f696c6c757374726174696f6e2e706e67" width="150" data-canonical-src="https://www.deepnote.com/static/illustration.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-just-want-to-quickly-look-at-some-notebooks-without-executing-any-code" class="anchor" aria-hidden="true" href="#just-want-to-quickly-look-at-some-notebooks-without-executing-any-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt;
&lt;p&gt;Browse this repository using &lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;:
&lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/079030b4c39b76eafa0c6c3a5bd18112aafe42dd/68747470733a2f2f6a7570797465722e6f72672f6173736574732f6e61765f6c6f676f2e737667" width="150" data-canonical-src="https://jupyter.org/assets/nav_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;a href="index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math equations are not always displayed correctly.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-run-this-project-using-a-docker-image" class="anchor" aria-hidden="true" href="#want-to-run-this-project-using-a-docker-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to run this project using a Docker image?&lt;/h3&gt;
&lt;p&gt;Read the &lt;a href="https://github.com/ageron/handson-ml2/tree/master/docker"&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-install-this-project-on-your-own-machine" class="anchor" aria-hidden="true" href="#want-to-install-this-project-on-your-own-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to install this project on your own machine?&lt;/h3&gt;
&lt;p&gt;Start by installing &lt;a href="https://www.anaconda.com/distribution/" rel="nofollow"&gt;Anaconda&lt;/a&gt; (or &lt;a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow"&gt;Miniconda&lt;/a&gt;), &lt;a href="https://git-scm.com/downloads" rel="nofollow"&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href="https://www.nvidia.com/Download/index.aspx" rel="nofollow"&gt;GPU driver&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git
$ cd handson-ml2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to use a GPU, then edit &lt;code&gt;environment.yml&lt;/code&gt; (or &lt;code&gt;environment-windows.yml&lt;/code&gt; on Windows) and replace &lt;code&gt;tensorflow=2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-gpu=2.0.0&lt;/code&gt;. Also replace &lt;code&gt;tensorflow-serving-api==2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-serving-api-gpu==2.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml # or environment-windows.yml on Windows
$ conda activate tf2
$ python -m ipykernel install --user --name=python3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then if you're on Windows, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, start Jupyter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need further instructions, read the &lt;a href="INSTALL.md"&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park who helped on some of the exercise solutions, and to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml2</guid><pubDate>Thu, 30 Jan 2020 00:11:00 GMT</pubDate></item><item><title>ultralytics/yolov3 #12 in Jupyter Notebook, Today</title><link>https://github.com/ultralytics/yolov3</link><description>&lt;p&gt;&lt;i&gt;YOLOv3 in PyTorch &gt; ONNX &gt; CoreML &gt; iOS&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td align="center"&gt;
    &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/c7f01c9051691f7f4c6239349b6b55cb5a0871c9/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f756c7472616c79746963732f6c6f676f2f6c6f676f6e616d65313030302e706e67" width="160" data-canonical-src="https://storage.googleapis.com/ultralytics/logo/logoname1000.png" style="max-width:100%;"&gt;&lt;/a&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
          &lt;a href="https://itunes.apple.com/app/id1452689527" rel="nofollow"&gt;
    &lt;img src="https://user-images.githubusercontent.com/26833433/50044365-9b22ac00-0082-11e9-862f-e77aee7aa7b0.png" width="180" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC, and &lt;strong&gt;is freely available for redistribution under the GPL-3.0 license&lt;/strong&gt;. For more information please visit &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;https://www.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/ultralytics/yolov3"&gt;https://github.com/ultralytics/yolov3&lt;/a&gt; repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux, MacOS and Windows. Training is done on the COCO dataset by default: &lt;a href="https://cocodataset.org/#home" rel="nofollow"&gt;https://cocodataset.org/#home&lt;/a&gt;. &lt;strong&gt;Credit to Joseph Redmon for YOLO:&lt;/strong&gt; &lt;a href="https://pjreddie.com/darknet/yolo/" rel="nofollow"&gt;https://pjreddie.com/darknet/yolo/&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Python 3.7 or later with all of the &lt;code&gt;pip install -U -r requirements.txt&lt;/code&gt; packages including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch &amp;gt;= 1.3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;opencv-python&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Pillow&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All dependencies are included in the associated docker images. Docker requirements are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nvidia-docker&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Nvidia Driver Version &amp;gt;= 440.44&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart"&gt;GCP Quickstart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning"&gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image"&gt;Train Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class"&gt;Train Single Class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data"&gt;Train Custom Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-jupyter-notebook" class="anchor" aria-hidden="true" href="#jupyter-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook&lt;/h1&gt;
&lt;p&gt;Our Jupyter &lt;a href="https://colab.research.google.com/github/ultralytics/yolov3/blob/master/examples.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt; provides quick training, inference and testing examples.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Start Training:&lt;/strong&gt; &lt;code&gt;python3 train.py&lt;/code&gt; to begin training after downloading COCO data with &lt;code&gt;data/get_coco_dataset.sh&lt;/code&gt;. Each epoch trains on 117,263 images from the train and validate COCO sets, and tests on 5000 images from the COCO validate set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resume Training:&lt;/strong&gt; &lt;code&gt;python3 train.py --resume&lt;/code&gt; to resume training from &lt;code&gt;weights/last.pt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Plot Training:&lt;/strong&gt; &lt;code&gt;from utils import utils; utils.plot_results()&lt;/code&gt; plots training results from &lt;code&gt;coco_16img.data&lt;/code&gt;, &lt;code&gt;coco_64img.data&lt;/code&gt;, 2 example datasets available in the &lt;code&gt;data/&lt;/code&gt; folder, which train and test on the first 16 and 64 images of the COCO2014-trainval dataset.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-image-augmentation" class="anchor" aria-hidden="true" href="#image-augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;datasets.py&lt;/code&gt; applies random OpenCV-powered (&lt;a href="https://opencv.org/" rel="nofollow"&gt;https://opencv.org/&lt;/a&gt;) augmentation to the input images in accordance with the following specifications. Augmentation is applied &lt;strong&gt;only&lt;/strong&gt; during training, not during inference. Bounding boxes are automatically tracked and updated with the images. 416 x 416 examples pictured below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Augmentation&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Translation&lt;/td&gt;
&lt;td&gt;+/- 10% (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotation&lt;/td&gt;
&lt;td&gt;+/- 5 degrees&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Shear&lt;/td&gt;
&lt;td&gt;+/- 2 degrees (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale&lt;/td&gt;
&lt;td&gt;+/- 10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reflection&lt;/td&gt;
&lt;td&gt;50% probability (horizontal-only)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;strong&gt;S&lt;/strong&gt;V Saturation&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HS&lt;strong&gt;V&lt;/strong&gt; Intensity&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/deep-learning-vm/" rel="nofollow"&gt;https://cloud.google.com/deep-learning-vm/&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Machine type:&lt;/strong&gt; preemptible &lt;a href="https://cloud.google.com/compute/docs/machine-types" rel="nofollow"&gt;n1-standard-16&lt;/a&gt; (16 vCPUs, 60 GB memory)&lt;br&gt;
&lt;strong&gt;CPU platform:&lt;/strong&gt; Intel Skylake&lt;br&gt;
&lt;strong&gt;GPUs:&lt;/strong&gt; K80 ($0.20/hr), T4 ($0.35/hr), V100 ($0.83/hr) CUDA with &lt;a href="https://github.com/NVIDIA/apex"&gt;Nvidia Apex&lt;/a&gt; FP16/32&lt;br&gt;
&lt;strong&gt;HDD:&lt;/strong&gt; 1 TB SSD&lt;br&gt;
&lt;strong&gt;Dataset:&lt;/strong&gt; COCO train 2014 (117,263 images)&lt;br&gt;
&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;yolov3-spp.cfg&lt;/code&gt;&lt;br&gt;
&lt;strong&gt;Command:&lt;/strong&gt;  &lt;code&gt;python3 train.py --img 416 --batch 32 --accum 2&lt;/code&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GPU&lt;/th&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;&lt;code&gt;--batch --accum&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;img/s&lt;/th&gt;
&lt;th&gt;epoch&lt;br&gt;time&lt;/th&gt;
&lt;th&gt;epoch&lt;br&gt;cost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;K80&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;32 x 2&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;175 min&lt;/td&gt;
&lt;td&gt;$0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;1&lt;br&gt;2&lt;/td&gt;
&lt;td&gt;32 x 2&lt;br&gt;64 x 1&lt;/td&gt;
&lt;td&gt;41&lt;br&gt;61&lt;/td&gt;
&lt;td&gt;48 min&lt;br&gt;32 min&lt;/td&gt;
&lt;td&gt;$0.28&lt;br&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100&lt;/td&gt;
&lt;td&gt;1&lt;br&gt;2&lt;/td&gt;
&lt;td&gt;32 x 2&lt;br&gt;64 x 1&lt;/td&gt;
&lt;td&gt;122&lt;br&gt;&lt;strong&gt;178&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;16 min&lt;br&gt;&lt;strong&gt;11 min&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;$0.23&lt;/strong&gt;&lt;br&gt;$0.31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti&lt;/td&gt;
&lt;td&gt;1&lt;br&gt;2&lt;/td&gt;
&lt;td&gt;32 x 2&lt;br&gt;64 x 1&lt;/td&gt;
&lt;td&gt;81&lt;br&gt;140&lt;/td&gt;
&lt;td&gt;24 min&lt;br&gt;14 min&lt;/td&gt;
&lt;td&gt;-&lt;br&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-inference" class="anchor" aria-hidden="true" href="#inference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt; runs inference on any sources:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 detect.py --source ...&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Image:  &lt;code&gt;--source file.jpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Video:  &lt;code&gt;--source file.mp4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Directory:  &lt;code&gt;--source dir/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Webcam:  &lt;code&gt;--source 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;RTSP stream:  &lt;code&gt;--source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;HTTP stream:  &lt;code&gt;--source http://wmccpinetop.axiscam.net/mjpg/video.mjpg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To run a specific models:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3.cfg --weights yolov3.weights&lt;/code&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-tiny:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-tiny.cfg --weights yolov3-tiny.weights&lt;/code&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-SPP:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-spp.cfg --weights yolov3-spp.weights&lt;/code&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h1&gt;
&lt;p&gt;Download from: &lt;a href="https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0" rel="nofollow"&gt;https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-darknet-conversion" class="anchor" aria-hidden="true" href="#darknet-conversion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Darknet Conversion&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/ultralytics/yolov3 &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolov3

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert darknet cfg/weights to pytorch model&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.weights')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert cfg/pytorch model to darknet weights&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.pt')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-map" class="anchor" aria-hidden="true" href="#map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mAP&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 test.py --weights ... --cfg ...&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;mAP@0.5 run at &lt;code&gt;--iou-thr 0.5&lt;/code&gt;, mAP@0.5...0.95 run at &lt;code&gt;--iou-thr 0.7&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;YOLOv3-SPP ultralytics is &lt;code&gt;ultralytics68.pt&lt;/code&gt; with &lt;code&gt;yolov3-spp.cfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Darknet results: &lt;a href="https://arxiv.org/abs/1804.02767" rel="nofollow"&gt;https://arxiv.org/abs/1804.02767&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;i&gt;&lt;/i&gt;&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;th&gt;COCO mAP&lt;br&gt;@0.5...0.95&lt;/th&gt;
&lt;th&gt;COCO mAP&lt;br&gt;@0.5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14.0&lt;br&gt;28.7&lt;br&gt;30.5&lt;br&gt;&lt;strong&gt;35.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;29.1&lt;br&gt;51.8&lt;br&gt;52.3&lt;br&gt;&lt;strong&gt;55.4&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;416&lt;/td&gt;
&lt;td&gt;16.0&lt;br&gt;31.2&lt;br&gt;33.9&lt;br&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;33.0&lt;br&gt;55.4&lt;br&gt;56.9&lt;br&gt;&lt;strong&gt;59.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;16.6&lt;br&gt;32.7&lt;br&gt;35.6&lt;br&gt;&lt;strong&gt;40.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;34.9&lt;br&gt;57.7&lt;br&gt;59.5&lt;br&gt;&lt;strong&gt;61.4&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;608&lt;/td&gt;
&lt;td&gt;16.6&lt;br&gt;33.1&lt;br&gt;37.0&lt;br&gt;&lt;strong&gt;41.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;35.4&lt;br&gt;58.2&lt;br&gt;60.7&lt;br&gt;&lt;strong&gt;61.5&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 test.py --img-size 608 --iou-thr 0.6 --weights ultralytics68.pt --cfg yolov3-spp.cfg

Namespace(batch_size=32, cfg=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;yolov3-spp.cfg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, conf_thres=0.001, data=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/coco2014.data&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, device=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, img_size=608, iou_thres=0.6, save_json=True, task=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;test&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, weights=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ultralytics68.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
Using CUDA device0 _CudaDeviceProperties(name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Tesla V100-SXM2-16GB&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, total_memory=16130MB)
               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 157/157 [03:&lt;span class="pl-k"&gt;30&amp;lt;&lt;/span&gt;00:00,  1.16it/s]
                 all     5e+03  3.51e+04    0.0353     0.891     0.606    0.0673
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.409
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.615
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.75      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.437
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.242
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.448
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.519
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;  1 ] = 0.337
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt; 10 ] = 0.557
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.612
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.438
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.658
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.746&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-reproduce-our-results" class="anchor" aria-hidden="true" href="#reproduce-our-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reproduce Our Results&lt;/h1&gt;
&lt;p&gt;This command trains &lt;code&gt;yolov3-spp.cfg&lt;/code&gt; from scratch to our mAP above. Training takes about one week on a 2080Ti.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 train.py --weights &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; --cfg yolov3-spp.cfg --epochs 273 --batch 16 --accum 4 --multi --pre&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/70661588-76bbca00-1c19-11ea-86f9-23350d8c3193.png"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/70661588-76bbca00-1c19-11ea-86f9-23350d8c3193.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reproduce-our-environment" class="anchor" aria-hidden="true" href="#reproduce-our-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reproduce Our Environment&lt;/h1&gt;
&lt;p&gt;To access an up-to-date working environment (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled), consider a:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GCP&lt;/strong&gt; Deep Learning VM with $300 free credit offer: See our &lt;a href="https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart"&gt;GCP Quickstart Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google Colab Notebook&lt;/strong&gt; with 12 hours of free GPU time: &lt;a href="https://colab.research.google.com/drive/1G8T-VFxQkjDe4idzN8F-hbIBqkkkQnxw" rel="nofollow"&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docker Image&lt;/strong&gt; from &lt;a href="https://hub.docker.com/r/ultralytics/yolov3" rel="nofollow"&gt;https://hub.docker.com/r/ultralytics/yolov3&lt;/a&gt;. See &lt;a href="https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart"&gt;Docker Quickstart Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/146165888" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cd760a8900fd4be0105229509d566b7c9499ef8d/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134363136353838382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/146165888.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Issues should be raised directly in the repository.&lt;/strong&gt; For additional questions or comments please email Glenn Jocher at &lt;a href="mailto:glenn.jocher@ultralytics.com"&gt;glenn.jocher@ultralytics.com&lt;/a&gt; or visit us at &lt;a href="https://contact.ultralytics.com" rel="nofollow"&gt;https://contact.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ultralytics</author><guid isPermaLink="false">https://github.com/ultralytics/yolov3</guid><pubDate>Thu, 30 Jan 2020 00:12:00 GMT</pubDate></item><item><title>guipsamora/pandas_exercises #13 in Jupyter Notebook, Today</title><link>https://github.com/guipsamora/pandas_exercises</link><description>&lt;p&gt;&lt;i&gt;Practice your pandas skills!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pandas-exercises" class="anchor" aria-hidden="true" href="#pandas-exercises"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pandas Exercises&lt;/h1&gt;
&lt;p&gt;Fed up with a ton of tutorials but no easy way to find exercises I decided to create a repo just with exercises to practice pandas.
Don't get me wrong, tutorials are great resources, but to learn is to do. So unless you practice you won't learn.&lt;/p&gt;
&lt;p&gt;There will be three different types of files:&lt;br&gt;
1. Exercise instructions&lt;br&gt;
2. Solutions without code&lt;br&gt;
3. Solutions with code and comments&lt;/p&gt;
&lt;p&gt;My suggestion is that you learn a topic in a tutorial, video or documentation and then do the first exercises.
Learn one more topic and do more exercises. If you are stuck, don't go directly to the solution with code files. Check the solutions only and try to get the correct answer.&lt;/p&gt;
&lt;p&gt;Suggestions and collaborations are more than welcome.&lt;g-emoji class="g-emoji" alias="slightly_smiling_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f642.png"&gt;&lt;/g-emoji&gt; Please open an issue or make a PR indicating the exercise and your problem/solution.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-lessons" class="anchor" aria-hidden="true" href="#lessons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lessons&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#getting-and-knowing"&gt;Getting and knowing&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#merge"&gt;Merge&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#time-series"&gt;Time Series&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#filtering-and-sorting"&gt;Filtering and Sorting&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#stats"&gt;Stats&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#deleting"&gt;Deleting&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#grouping"&gt;Grouping&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#visualization"&gt;Visualization&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Indexing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#apply"&gt;Apply&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#creating-series-and-dataframes"&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Exporting&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-getting-and-knowing" class="anchor" aria-hidden="true" href="#getting-and-knowing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data"&gt;Getting and knowing&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Occupation"&gt;Occupation&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/World%20Food%20Facts"&gt;World Food Facts&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-filtering-and-sorting" class="anchor" aria-hidden="true" href="#filtering-and-sorting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting"&gt;Filtering and Sorting&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Euro12"&gt;Euro12&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Fictional%20Army"&gt;Fictional Army&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-grouping" class="anchor" aria-hidden="true" href="#grouping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping"&gt;Grouping&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Alcohol_Consumption"&gt;Alcohol Consumption&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Occupation"&gt;Occupation&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Regiment"&gt;Regiment&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-apply" class="anchor" aria-hidden="true" href="#apply"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply"&gt;Apply&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/Students_Alcohol_Consumption"&gt;Students Alcohol Consumption&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/US_Crime_Rates"&gt;US_Crime_Rates&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-merge" class="anchor" aria-hidden="true" href="#merge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge"&gt;Merge&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Auto_MPG"&gt;Auto_MPG&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Fictitous%20Names"&gt;Fictitious Names&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Housing%20Market"&gt;House Market&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-stats" class="anchor" aria-hidden="true" href="#stats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats"&gt;Stats&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/US_Baby_Names"&gt;US_Baby_Names&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/Wind_Stats"&gt;Wind_Stats&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visualization" class="anchor" aria-hidden="true" href="#visualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization"&gt;Visualization&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Titanic_Desaster"&gt;Titanic Disaster&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Scores"&gt;Scores&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Online_Retail"&gt;Online Retail&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Tips"&gt;Tips&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-creating-series-and-dataframes" class="anchor" aria-hidden="true" href="#creating-series-and-dataframes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames"&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames/Pokemon"&gt;Pokemon&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-time-series" class="anchor" aria-hidden="true" href="#time-series"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series"&gt;Time Series&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Apple_Stock"&gt;Apple_Stock&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data"&gt;Getting_Financial_Data&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data"&gt;Investor_Flow_of_Funds_US&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deleting" class="anchor" aria-hidden="true" href="#deleting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting"&gt;Deleting&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Iris"&gt;Iris&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Wine"&gt;Wine&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-video-solutions" class="anchor" aria-hidden="true" href="#video-solutions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Solutions&lt;/h1&gt;
&lt;p&gt;Video tutorials of data scientists working through the above exercises:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=pu3IpU937xs&amp;amp;list=PLgJhDSE2ZLxaY_DigHeiIDC1cD09rXgJv" rel="nofollow"&gt;Data Talks - Pandas Learning By Doing&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>guipsamora</author><guid isPermaLink="false">https://github.com/guipsamora/pandas_exercises</guid><pubDate>Thu, 30 Jan 2020 00:13:00 GMT</pubDate></item><item><title>krasserm/super-resolution #14 in Jupyter Notebook, Today</title><link>https://github.com/krasserm/super-resolution</link><description>&lt;p&gt;&lt;i&gt;Tensorflow 2.0 based implementation of EDSR, WDSR and SRGAN for single image super-resolution&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3e872a206869148d08d8dbd7c2fc1f051bbde8db/68747470733a2f2f7472617669732d63692e636f6d2f6b7261737365726d2f73757065722d7265736f6c7574696f6e2e7376673f6272616e63683d6d6173746572"&gt;&lt;img src="https://camo.githubusercontent.com/3e872a206869148d08d8dbd7c2fc1f051bbde8db/68747470733a2f2f7472617669732d63692e636f6d2f6b7261737365726d2f73757065722d7265736f6c7574696f6e2e7376673f6272616e63683d6d6173746572" alt="Travis CI" data-canonical-src="https://travis-ci.com/krasserm/super-resolution.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-single-image-super-resolution-with-edsr-wdsr-and-srgan" class="anchor" aria-hidden="true" href="#single-image-super-resolution-with-edsr-wdsr-and-srgan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Single Image Super-Resolution with EDSR, WDSR and SRGAN&lt;/h1&gt;
&lt;p&gt;A &lt;a href="https://www.tensorflow.org/beta" rel="nofollow"&gt;Tensorflow 2.0&lt;/a&gt; based implementation of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1707.02921" rel="nofollow"&gt;Enhanced Deep Residual Networks for Single Image Super-Resolution&lt;/a&gt; (EDSR), winner
of the &lt;a href="http://www.vision.ee.ethz.ch/ntire17/" rel="nofollow"&gt;NTIRE 2017&lt;/a&gt; super-resolution challenge.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.08718" rel="nofollow"&gt;Wide Activation for Efficient and Accurate Image Super-Resolution&lt;/a&gt; (WDSR), winner
of the &lt;a href="http://www.vision.ee.ethz.ch/ntire18/" rel="nofollow"&gt;NTIRE 2018&lt;/a&gt; super-resolution challenge (realistic tracks).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&lt;/a&gt; (SRGAN).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a complete re-write if the old Keras/Tensorflow 1.x based implementation available &lt;a href="https://github.com/krasserm/super-resolution/tree/previous"&gt;here&lt;/a&gt;.
Some parts are still work in progress but you can already train models as described in the papers via a high-level training
API. &lt;a href="#training"&gt;Training&lt;/a&gt; and &lt;a href="#getting-started"&gt;usage&lt;/a&gt; examples are given in the notebooks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="example-edsr.ipynb"&gt;example-edsr.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="example-wdsr.ipynb"&gt;example-wdsr.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="example-srgan.ipynb"&gt;example-srgan.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A &lt;code&gt;DIV2K&lt;/code&gt; &lt;a href="#div2k-dataset"&gt;data provider&lt;/a&gt; automatically downloads &lt;a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="nofollow"&gt;DIV2K&lt;/a&gt;
training and validation images of given scale (2, 3, 4 or 8) and downgrade operator ("bicubic", "unknown", "mild" or
"difficult").&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-environment-setup" class="anchor" aria-hidden="true" href="#environment-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment setup&lt;/h2&gt;
&lt;p&gt;Create a new &lt;a href="https://conda.io" rel="nofollow"&gt;conda&lt;/a&gt; environment with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and activate it with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda activate sisr
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;You can find an introduction to single-image super-resolution in &lt;a href="https://krasserm.github.io/2019/09/04/super-resolution/" rel="nofollow"&gt;this article&lt;/a&gt;.
It also demonstrates how EDSR and WDSR models can be fine-tuned with SRGAN (see also &lt;a href="#srgan-for-fine-tuning-edsr-and-wdsr-models"&gt;this section&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
&lt;p&gt;Examples in this section require following pre-trained weights for running (see also example notebooks):&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-trained-weights" class="anchor" aria-hidden="true" href="#pre-trained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/open?id=1xjyW_0dDS4jSTxKVZwtlkyfS4Oso-GUF" rel="nofollow"&gt;weights-edsr-16-x4.tar.gz&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;EDSR x4 baseline as described in the EDSR paper: 16 residual blocks, 64 filters, 1.52M parameters.&lt;/li&gt;
&lt;li&gt;PSNR on DIV2K validation set = 28.89 dB (images 801 - 900, 6 + 4 pixel border included).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/open?id=1JfQNGQZ9cG-lyC5EB0W3MpySjPlDJXpU" rel="nofollow"&gt;weights-wdsr-b-32-x4.tar.gz&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;WDSR B x4 custom model: 32 residual blocks, 32 filters, expansion factor 6, 0.62M parameters.&lt;/li&gt;
&lt;li&gt;PSNR on DIV2K validation set = 28.91 dB (images 801 - 900, 6 + 4 pixel border included).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/open?id=1u9ituA3ScttN9Vi-UkALmpO0dWQLm8Rv" rel="nofollow"&gt;weights-srgan.tar.gz&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;SRGAN as described in the SRGAN paper: 1.55M parameters, trained with VGG54 content loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After download, extract them in the root folder of the project with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tar xvfz weights-&amp;lt;...&amp;gt;.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-edsr" class="anchor" aria-hidden="true" href="#edsr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;EDSR&lt;/h3&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; model &lt;span class="pl-k"&gt;import&lt;/span&gt; resolve_single
&lt;span class="pl-k"&gt;from&lt;/span&gt; model.edsr &lt;span class="pl-k"&gt;import&lt;/span&gt; edsr

&lt;span class="pl-k"&gt;from&lt;/span&gt; utils &lt;span class="pl-k"&gt;import&lt;/span&gt; load_image, plot_sample

model &lt;span class="pl-k"&gt;=&lt;/span&gt; edsr(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-v"&gt;num_res_blocks&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;16&lt;/span&gt;)
model.load_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/edsr-16-x4/weights.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

lr &lt;span class="pl-k"&gt;=&lt;/span&gt; load_image(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;demo/0851x4-crop.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
sr &lt;span class="pl-k"&gt;=&lt;/span&gt; resolve_single(model, lr)

plot_sample(lr, sr)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/images/result-edsr.png"&gt;&lt;img src="docs/images/result-edsr.png" alt="result-edsr" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-wdsr" class="anchor" aria-hidden="true" href="#wdsr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WDSR&lt;/h3&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; model.wdsr &lt;span class="pl-k"&gt;import&lt;/span&gt; wdsr_b

model &lt;span class="pl-k"&gt;=&lt;/span&gt; wdsr_b(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-v"&gt;num_res_blocks&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;)
model.load_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/wdsr-b-32-x4/weights.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

lr &lt;span class="pl-k"&gt;=&lt;/span&gt; load_image(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;demo/0829x4-crop.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
sr &lt;span class="pl-k"&gt;=&lt;/span&gt; resolve_single(model, lr)

plot_sample(lr, sr)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/images/result-wdsr.png"&gt;&lt;img src="docs/images/result-wdsr.png" alt="result-wdsr" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Weight normalization in WDSR models is implemented with the new &lt;code&gt;WeightNormalization&lt;/code&gt; layer wrapper of
&lt;a href="https://github.com/tensorflow/addons"&gt;Tensorflow Addons&lt;/a&gt;. In its latest version, this wrapper seems to
corrupt weights when running &lt;code&gt;model.predict(...)&lt;/code&gt;. A workaround is to set &lt;code&gt;model.run_eagerly = True&lt;/code&gt; or
compile the model with &lt;code&gt;model.compile(loss='mae')&lt;/code&gt; in advance. This issue doesn't arise when calling the
model directly with &lt;code&gt;model(...)&lt;/code&gt; though. To be further investigated ...&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-srgan" class="anchor" aria-hidden="true" href="#srgan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SRGAN&lt;/h3&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; model.srgan &lt;span class="pl-k"&gt;import&lt;/span&gt; generator

model &lt;span class="pl-k"&gt;=&lt;/span&gt; generator()
model.load_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/srgan/gan_generator.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

lr &lt;span class="pl-k"&gt;=&lt;/span&gt; load_image(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;demo/0869x4-crop.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
sr &lt;span class="pl-k"&gt;=&lt;/span&gt; resolve_single(model, lr)

plot_sample(lr, sr)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="docs/images/result-srgan.png"&gt;&lt;img src="docs/images/result-srgan.png" alt="result-srgan" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-div2k-dataset" class="anchor" aria-hidden="true" href="#div2k-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DIV2K dataset&lt;/h2&gt;
&lt;p&gt;For training and validation on &lt;a href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="nofollow"&gt;DIV2K&lt;/a&gt; images, applications should use the
provided &lt;code&gt;DIV2K&lt;/code&gt; data loader. It automatically downloads DIV2K images to &lt;code&gt;.div2k&lt;/code&gt; directory and converts them to a
different format for faster loading.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-training-dataset" class="anchor" aria-hidden="true" href="#training-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training dataset&lt;/h3&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; data &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;DIV2K&lt;/span&gt;

train_loader &lt;span class="pl-k"&gt;=&lt;/span&gt; DIV2K(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;,             &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 2, 3, 4 or 8&lt;/span&gt;
                     &lt;span class="pl-v"&gt;downgrade&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bicubic&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 'bicubic', 'unknown', 'mild' or 'difficult' &lt;/span&gt;
                     &lt;span class="pl-v"&gt;subset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)      &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Training dataset are images 001 - 800&lt;/span&gt;
                     
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a tf.data.Dataset          &lt;/span&gt;
train_ds &lt;span class="pl-k"&gt;=&lt;/span&gt; train_loader.dataset(&lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;16&lt;/span&gt;,         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; batch size as described in the EDSR and WDSR papers&lt;/span&gt;
                                &lt;span class="pl-v"&gt;random_transform&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; random crop, flip, rotate as described in the EDSR paper&lt;/span&gt;
                                &lt;span class="pl-v"&gt;repeat_count&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;)     &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; repeat iterating over training images indefinitely&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Iterate over LR/HR image pairs                                &lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; lr, hr &lt;span class="pl-k"&gt;in&lt;/span&gt; train_ds:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; .... &lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Crop size in HR images is 96x96.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-validation-dataset" class="anchor" aria-hidden="true" href="#validation-dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Validation dataset&lt;/h3&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; data &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-c1"&gt;DIV2K&lt;/span&gt;

valid_loader &lt;span class="pl-k"&gt;=&lt;/span&gt; DIV2K(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;,             &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 2, 3, 4 or 8&lt;/span&gt;
                     &lt;span class="pl-v"&gt;downgrade&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bicubic&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 'bicubic', 'unknown', 'mild' or 'difficult' &lt;/span&gt;
                     &lt;span class="pl-v"&gt;subset&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)      &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Validation dataset are images 801 - 900&lt;/span&gt;
                     
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a tf.data.Dataset          &lt;/span&gt;
valid_ds &lt;span class="pl-k"&gt;=&lt;/span&gt; valid_loader.dataset(&lt;span class="pl-v"&gt;batch_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;,           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; use batch size of 1 as DIV2K images have different size&lt;/span&gt;
                                &lt;span class="pl-v"&gt;random_transform&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;, &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; use DIV2K images in original size &lt;/span&gt;
                                &lt;span class="pl-v"&gt;repeat_count&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;)         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 1 epoch&lt;/span&gt;
                                
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Iterate over LR/HR image pairs                                &lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; lr, hr &lt;span class="pl-k"&gt;in&lt;/span&gt; valid_ds:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ....                                 &lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h2&gt;
&lt;p&gt;The following training examples use the &lt;a href="#div2k-dataset"&gt;training and validation datasets&lt;/a&gt; described earlier. The high-level
training API is designed around &lt;em&gt;steps&lt;/em&gt; (= minibatch updates) rather than &lt;em&gt;epochs&lt;/em&gt; to better match the descriptions in the
papers.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-edsr-1" class="anchor" aria-hidden="true" href="#edsr-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;EDSR&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; model.edsr &lt;span class="pl-k"&gt;import&lt;/span&gt; edsr
&lt;span class="pl-k"&gt;from&lt;/span&gt; train &lt;span class="pl-k"&gt;import&lt;/span&gt; EdsrTrainer

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a training context for an EDSR x4 model with 16 &lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; residual blocks.&lt;/span&gt;
trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; EdsrTrainer(&lt;span class="pl-v"&gt;model&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;edsr(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-v"&gt;num_res_blocks&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;16&lt;/span&gt;), 
                      &lt;span class="pl-v"&gt;checkpoint_dir&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;.ckpt/edsr-16-x4&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)
                      
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train EDSR model for 300,000 steps and evaluate model&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; every 1000 steps on the first 10 images of the DIV2K&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; validation set. Save a checkpoint only if evaluation&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; PSNR has improved.&lt;/span&gt;
trainer.train(train_ds,
              valid_ds.take(&lt;span class="pl-c1"&gt;10&lt;/span&gt;),
              &lt;span class="pl-v"&gt;steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;300000&lt;/span&gt;, 
              &lt;span class="pl-v"&gt;evaluate_every&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1000&lt;/span&gt;, 
              &lt;span class="pl-v"&gt;save_best_only&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
              
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Restore from checkpoint with highest PSNR.&lt;/span&gt;
trainer.restore()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Evaluate model on full validation set.&lt;/span&gt;
psnr &lt;span class="pl-k"&gt;=&lt;/span&gt; trainer.evaluate(valid_ds)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;PSNR = &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;psnr.numpy()&lt;span class="pl-k"&gt;:3f&lt;/span&gt;&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Save weights to separate location.&lt;/span&gt;
trainer.model.save_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/edsr-16-x4/weights.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)                                    &lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Interrupting training and restarting it again resumes from the latest saved checkpoint. The trained Keras model can be
accessed with &lt;code&gt;trainer.model&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wdsr-1" class="anchor" aria-hidden="true" href="#wdsr-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WDSR&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; model.wdsr &lt;span class="pl-k"&gt;import&lt;/span&gt; wdsr_b
&lt;span class="pl-k"&gt;from&lt;/span&gt; train &lt;span class="pl-k"&gt;import&lt;/span&gt; WdsrTrainer

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a training context for a WDSR B x4 model with 32 &lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; residual blocks.&lt;/span&gt;
trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; WdsrTrainer(&lt;span class="pl-v"&gt;model&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;wdsr_b(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-v"&gt;num_res_blocks&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;), 
                      &lt;span class="pl-v"&gt;checkpoint_dir&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;.ckpt/wdsr-b-8-x4&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train WDSR B model for 300,000 steps and evaluate model&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; every 1000 steps on the first 10 images of the DIV2K&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; validation set. Save a checkpoint only if evaluation&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; PSNR has improved.&lt;/span&gt;
trainer.train(train_ds,
              valid_ds.take(&lt;span class="pl-c1"&gt;10&lt;/span&gt;),
              &lt;span class="pl-v"&gt;steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;300000&lt;/span&gt;, 
              &lt;span class="pl-v"&gt;evaluate_every&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1000&lt;/span&gt;, 
              &lt;span class="pl-v"&gt;save_best_only&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Restore from checkpoint with highest PSNR.&lt;/span&gt;
trainer.restore()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Evaluate model on full validation set.&lt;/span&gt;
psnr &lt;span class="pl-k"&gt;=&lt;/span&gt; trainer.evaluate(valid_ds)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;PSNR = &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;psnr.numpy()&lt;span class="pl-k"&gt;:3f&lt;/span&gt;&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Save weights to separate location.&lt;/span&gt;
trainer.model.save_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/wdsr-b-32-x4/weights.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-srgan-1" class="anchor" aria-hidden="true" href="#srgan-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SRGAN&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-generator-pre-training" class="anchor" aria-hidden="true" href="#generator-pre-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generator pre-training&lt;/h3&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; model.srgan &lt;span class="pl-k"&gt;import&lt;/span&gt; generator
&lt;span class="pl-k"&gt;from&lt;/span&gt; train &lt;span class="pl-k"&gt;import&lt;/span&gt; SrganGeneratorTrainer

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a training context for the generator (SRResNet) alone.&lt;/span&gt;
pre_trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; SrganGeneratorTrainer(&lt;span class="pl-v"&gt;model&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;generator(), &lt;span class="pl-v"&gt;checkpoint_dir&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;.ckpt/pre_generator&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Pre-train the generator with 1,000,000 steps (100,000 works fine too). &lt;/span&gt;
pre_trainer.train(train_ds, valid_ds.take(&lt;span class="pl-c1"&gt;10&lt;/span&gt;), &lt;span class="pl-v"&gt;steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1000000&lt;/span&gt;, &lt;span class="pl-v"&gt;evaluate_every&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1000&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Save weights of pre-trained generator (needed for fine-tuning with GAN).&lt;/span&gt;
pre_trainer.model.save_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/srgan/pre_generator.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-generator-fine-tuning-gan" class="anchor" aria-hidden="true" href="#generator-fine-tuning-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generator fine-tuning (GAN)&lt;/h3&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; model.srgan &lt;span class="pl-k"&gt;import&lt;/span&gt; generator, discriminator
&lt;span class="pl-k"&gt;from&lt;/span&gt; train &lt;span class="pl-k"&gt;import&lt;/span&gt; SrganTrainer

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a new generator and init it with pre-trained weights.&lt;/span&gt;
gan_generator &lt;span class="pl-k"&gt;=&lt;/span&gt; generator()
gan_generator.load_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/srgan/pre_generator.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a training context for the GAN (generator + discriminator).&lt;/span&gt;
gan_trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; SrganTrainer(&lt;span class="pl-v"&gt;generator&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;gan_generator, &lt;span class="pl-v"&gt;discriminator&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;discriminator())

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train the GAN with 200,000 steps.&lt;/span&gt;
gan_trainer.train(train_ds, &lt;span class="pl-v"&gt;steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;200000&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Save weights of generator and discriminator.&lt;/span&gt;
gan_trainer.generator.save_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/srgan/gan_generator.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
gan_trainer.discriminator.save_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/srgan/gan_discriminator.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-srgan-for-fine-tuning-edsr-and-wdsr-models" class="anchor" aria-hidden="true" href="#srgan-for-fine-tuning-edsr-and-wdsr-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SRGAN for fine-tuning EDSR and WDSR models&lt;/h2&gt;
&lt;p&gt;It is also possible to fine-tune EDSR and WDSR x4 models with SRGAN. They can be used as drop-in replacement for the
original SRGAN generator. More details in &lt;a href="https://krasserm.github.io/2019/09/04/super-resolution/" rel="nofollow"&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create EDSR generator and init with pre-trained weights&lt;/span&gt;
generator &lt;span class="pl-k"&gt;=&lt;/span&gt; edsr(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-v"&gt;num_res_blocks&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;16&lt;/span&gt;)
generator.load_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/edsr-16-x4/weights.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Fine-tune EDSR model via SRGAN training.&lt;/span&gt;
gan_trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; SrganTrainer(&lt;span class="pl-v"&gt;generator&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;generator, &lt;span class="pl-v"&gt;discriminator&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;discriminator())
gan_trainer.train(train_ds, &lt;span class="pl-v"&gt;steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;200000&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create WDSR B generator and init with pre-trained weights&lt;/span&gt;
generator &lt;span class="pl-k"&gt;=&lt;/span&gt; wdsr_b(&lt;span class="pl-v"&gt;scale&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-v"&gt;num_res_blocks&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;32&lt;/span&gt;)
generator.load_weights(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/wdsr-b-16-32/weights.h5&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Fine-tune WDSR B  model via SRGAN training.&lt;/span&gt;
gan_trainer &lt;span class="pl-k"&gt;=&lt;/span&gt; SrganTrainer(&lt;span class="pl-v"&gt;generator&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;generator, &lt;span class="pl-v"&gt;discriminator&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;discriminator())
gan_trainer.train(train_ds, &lt;span class="pl-v"&gt;steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;200000&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>krasserm</author><guid isPermaLink="false">https://github.com/krasserm/super-resolution</guid><pubDate>Thu, 30 Jan 2020 00:14:00 GMT</pubDate></item><item><title>bentrevett/pytorch-seq2seq #15 in Jupyter Notebook, Today</title><link>https://github.com/bentrevett/pytorch-seq2seq</link><description>&lt;p&gt;&lt;i&gt;Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-seq2seq" class="anchor" aria-hidden="true" href="#pytorch-seq2seq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Seq2Seq&lt;/h1&gt;
&lt;p&gt;This repo contains tutorials covering understanding and implementing sequence-to-sequence (seq2seq) models using &lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt; 1.4 and &lt;a href="https://github.com/pytorch/text"&gt;TorchText&lt;/a&gt; 0.5 using Python 3.6.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you find any mistakes or disagree with any of the explanations, please do not hesitate to &lt;a href="https://github.com/bentrevett/pytorch-seq2seq/issues/new"&gt;submit an issue&lt;/a&gt;. I welcome any feedback, positive or negative!&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;To install PyTorch, see installation instructions on the &lt;a href="pytorch.org"&gt;PyTorch website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To install TorchText:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install torchtext&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We'll also make use of spaCy to tokenize our data. To install spaCy, follow the instructions &lt;a href="https://spacy.io/usage/" rel="nofollow"&gt;here&lt;/a&gt; making sure to install both the English and German models with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m spacy download en
python -m spacy download de&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1 - &lt;a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This first tutorial covers the workflow of a PyTorch with TorchText seq2seq project. We'll cover the basics of seq2seq networks using encoder-decoder models, how to implement these models in PyTorch, and how to use TorchText to do all of the heavy lifting with regards to text processing. The model itself will be based off an implementation of &lt;a href="https://arxiv.org/abs/1409.3215" rel="nofollow"&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;, which uses multi-layer LSTMs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2 - &lt;a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb"&gt;Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now we have the basic workflow covered, this tutorial will focus on improving our results. Building on our knowledge of PyTorch and TorchText gained from the previous tutorial, we'll cover a second second model, which helps with the information compression problem faced by encoder-decoder models. This model will be based off an implementation of &lt;a href="https://arxiv.org/abs/1406.1078" rel="nofollow"&gt;Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation&lt;/a&gt;, which uses GRUs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3 - &lt;a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb"&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next, we learn about attention by implementing &lt;a href="https://arxiv.org/abs/1409.0473" rel="nofollow"&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;. This further allievates the information compression problem by allowing the decoder to "look back" at the input sentence by creating context vectors that are weighted sums of the encoder hidden states. The weights for this weighted sum are calculated via an attention mechanism, where the decoder learns to pay attention to the most relevant words in the input sentence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4 - &lt;a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb"&gt;Packed Padded Sequences, Masking, Inference and BLEU&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this notebook, we will improve the previous model architecture by adding &lt;em&gt;packed padded sequences&lt;/em&gt; and &lt;em&gt;masking&lt;/em&gt;. These are two methods commonly used in NLP. Packed padded sequences allow us to only process the non-padded elements of our input sentence with our RNN. Masking is used to force the model to ignore certain elements we do not want it to look at, such as attention over padded elements. Together, these give us a small performance boost. We also cover a very basic way of using the model for inference, allowing us to get translations for any sentence we want to give to the model and how we can view the attention values over the source sequence for those translations. Finally, we show how to calculate the BLEU metric from our translations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;5 - &lt;a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb"&gt;Convolutional Sequence to Sequence Learning&lt;/a&gt; &lt;a href="https://colab.research.google.com/github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We finally move away from RNN based models and implement a fully convolutional model. One of the downsides of RNNs is that they are sequential. That is, before a word is processed by the RNN, all previous words must also be processed. Convolutional models can be fully parallelized, which allow them to be trained much quicker. We will be implementing the &lt;a href="https://arxiv.org/abs/1705.03122" rel="nofollow"&gt;Convolutional Sequence to Sequence&lt;/a&gt; model, which uses multiple convolutional layers in both the encoder and decoder, with an attention mechanism between them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;6 - &lt;a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"&gt;Attention Is All You Need&lt;/a&gt; &lt;a href="https://colab.research.google.com/github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Continuing with the non-RNN based models, we implement the Transformer model from &lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Attention Is All You Need&lt;/a&gt;. This model is based soley on attention mechanisms and introduces Multi-Head Attention. The encoder and decoder are made of multiple layers, with each layer consisting of Multi-Head Attention and Positionwise Feedforward sublayers. This model is currently used in many state-of-the-art sequence-to-sequence and transfer learning tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;Here are some things I looked at while making these tutorials. Some of it may be out of date.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/spro/practical-pytorch"&gt;https://github.com/spro/practical-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/keon/seq2seq"&gt;https://github.com/keon/seq2seq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pengshuang/CNN-Seq2Seq"&gt;https://github.com/pengshuang/CNN-Seq2Seq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pytorch/fairseq"&gt;https://github.com/pytorch/fairseq&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch"&gt;https://github.com/jadore801120/attention-is-all-you-need-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="nofollow"&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/" rel="nofollow"&gt;https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bentrevett</author><guid isPermaLink="false">https://github.com/bentrevett/pytorch-seq2seq</guid><pubDate>Thu, 30 Jan 2020 00:15:00 GMT</pubDate></item><item><title>NVIDIA-AI-IOT/jetracer #16 in Jupyter Notebook, Today</title><link>https://github.com/NVIDIA-AI-IOT/jetracer</link><description>&lt;p&gt;&lt;i&gt;An autonomous AI racecar using NVIDIA Jetson Nano&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-jetracer" class="anchor" aria-hidden="true" href="#jetracer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;JetRacer&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/4212806/67442981-ce459e00-f5b7-11e9-9c8a-14ab360decb8.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/4212806/67442981-ce459e00-f5b7-11e9-9c8a-14ab360decb8.gif" height="256" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;JetRacer is an autonomous AI racecar using NVIDIA Jetson Nano.  With JetRacer you will&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Go fast - Optimize for high framerates to move at high speeds&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have fun - Follow examples and program interactively from your web browser&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By building and experimenting with JetRacer you will create fast AI pipelines and push the boundaries of speed.&lt;/p&gt;
&lt;p&gt;To get started, follow the &lt;a href="#setup"&gt;setup&lt;/a&gt; below.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cars" class="anchor" aria-hidden="true" href="#cars"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cars&lt;/h2&gt;
&lt;p&gt;There are two different JetRacer cars that you can build.  They differ primarily in size and speed.  Which one to pick depends on your use case&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Latrax Rally&lt;/th&gt;
&lt;th&gt;Tamiya TT02&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/25759564/67250038-b1c22e00-f41e-11e9-82d2-bbb17526310b.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/25759564/67250038-b1c22e00-f41e-11e9-82d2-bbb17526310b.jpg" width="256" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/25759564/67250039-b1c22e00-f41e-11e9-931f-98c1729550d0.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/25759564/67250039-b1c22e00-f41e-11e9-931f-98c1729550d0.jpg" width="320" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1/18th scale&lt;/td&gt;
&lt;td&gt;1/10th scale&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Moderate Speed&lt;/td&gt;
&lt;td&gt;High Speed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;~$400 total build cost&lt;/td&gt;
&lt;td&gt;~$600 total build cost&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Compact and portable&lt;/td&gt;
&lt;td&gt;Large and easy to modify&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Soldering required&lt;/td&gt;
&lt;td&gt;No soldering required&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Base car pre-assembled&lt;/td&gt;
&lt;td&gt;Base car assembly required&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If you have any questions, please reach out by &lt;a href="../..//issues"&gt;creating an issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;p&gt;JetRacer comes with a couple examples to get you up and running.  The examples are in the format of Jupyter Notebooks, which are interactive documents which combine text, code, and visualization.  Once you've completed the notebooks, start tweaking them to create your own racing software!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-example-1---basic-motion" class="anchor" aria-hidden="true" href="#example-1---basic-motion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example 1 - Basic motion&lt;/h3&gt;
&lt;p&gt;In this example you'll learn to progam JetRacer programatically from your web browser.  Learn more in the &lt;a href="docs/examples.md"&gt;examples&lt;/a&gt; documentation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/4212806/60383497-68d90a80-9a26-11e9-9a18-778b7d3a3221.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/4212806/60383497-68d90a80-9a26-11e9-9a18-778b7d3a3221.gif" height="300/" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-example-2---road-following" class="anchor" aria-hidden="true" href="#example-2---road-following"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example 2 - Road following&lt;/h3&gt;
&lt;p&gt;In this example, you'll teach JetRacer how to follow a road using AI.  After training the neural network using the &lt;a href="notebooks/interactive_regression.ipynb"&gt;interactive training notebook&lt;/a&gt;, you'll optimize the model using NVIDIA TensorRT and deploy for a live demo. Learn more in the &lt;a href="docs/examples.md"&gt;examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/4212806/60383389-bd7b8600-9a24-11e9-9f64-926e5edb52cc.gif"&gt;&lt;img src="https://user-images.githubusercontent.com/4212806/60383389-bd7b8600-9a24-11e9-9f64-926e5edb52cc.gif" height="300/" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;p&gt;To get started with JetRacer, follow these steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Order parts from the bill of materials&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/latrax/bill_of_materials.md"&gt;Latrax version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/tamiya/bill_of_materials.md"&gt;Tamiya version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow the hardware setup&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/latrax/hardware_setup.md"&gt;Latrax version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/tamiya/hardware_setup.md"&gt;Tamiya version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Follow the &lt;a href="docs/software_setup.md"&gt;software setup&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run through the &lt;a href="docs/examples.md"&gt;examples&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-see-also" class="anchor" aria-hidden="true" href="#see-also"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;See also&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://github.com/NVIDIA-AI-IOT/jetbot"&gt;JetBot&lt;/a&gt; - An educational AI robot based on NVIDIA Jetson Nano&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://github.com/NVIDIA-AI-IOT/jetcam"&gt;JetCam&lt;/a&gt; - An easy to use Python camera interface for NVIDIA Jetson&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://github.com/NVIDIA-AI-IOT/jetcard"&gt;JetCard&lt;/a&gt; - An SD card image for web programming AI projects with NVIDIA Jetson Nano&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://github.com/NVIDIA-AI-IOT/torch2trt"&gt;torch2trt&lt;/a&gt; - An easy to use PyTorch to TensorRT converter&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA-AI-IOT</author><guid isPermaLink="false">https://github.com/NVIDIA-AI-IOT/jetracer</guid><pubDate>Thu, 30 Jan 2020 00:16:00 GMT</pubDate></item><item><title>google-research/google-research #17 in Jupyter Notebook, Today</title><link>https://github.com/google-research/google-research</link><description>&lt;p&gt;&lt;i&gt;Google AI Research&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-google-ai-research" class="anchor" aria-hidden="true" href="#google-ai-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google AI Research&lt;/h1&gt;
&lt;p&gt;This repository contains code released by
&lt;a href="https://ai.google/research" rel="nofollow"&gt;Google AI Research&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Because the repo is large, we recommend you clone the repo without its history.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/google-research</guid><pubDate>Thu, 30 Jan 2020 00:17:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #18 in Jupyter Notebook, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Thu, 30 Jan 2020 00:18:00 GMT</pubDate></item><item><title>fastai/fastai #19 in Jupyter Notebook, Today</title><link>https://github.com/fastai/fastai</link><description>&lt;p&gt;&lt;i&gt;The fastai deep learning library, plus lessons and tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://dev.azure.com/fastdotai/fastai/_build/latest?definitionId=1" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a1b1234cce0c844f75224d1df07b4f236f78aee7/68747470733a2f2f6465762e617a7572652e636f6d2f66617374646f7461692f6661737461692f5f617069732f6275696c642f7374617475732f6661737461692e666173746169" alt="Build Status" data-canonical-src="https://dev.azure.com/fastdotai/fastai/_apis/build/status/fastai.fastai" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/26f7b20369ea7a096cfb30bdf0d14bc6ceda0275/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6661737461692e737667" alt="pypi fastai version" data-canonical-src="https://img.shields.io/pypi/v/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c20a6e61cb1c612b644253db1e3c1f7972d7f0e/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f762f6661737461692f6661737461692e737667" alt="Conda fastai version" data-canonical-src="https://img.shields.io/conda/v/fastai/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://anaconda.org/fastai/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/39f258e6563b60eef6a88589cd194d3a85033747/68747470733a2f2f616e61636f6e64612e6f72672f6661737461692f6661737461692f6261646765732f706c6174666f726d732e737667" alt="Anaconda-Server Badge" data-canonical-src="https://anaconda.org/fastai/fastai/badges/platforms.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6dc643192dbbbd8edda826d1be289ba06ef2b57f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6661737461692e737667" alt="fastai python compatibility" data-canonical-src="https://img.shields.io/pypi/pyversions/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/fastai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/f600abfa75b593d49643c1710ed42865373f9d75/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6661737461692e737667" alt="fastai license" data-canonical-src="https://img.shields.io/pypi/l/fastai.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-fastai" class="anchor" aria-hidden="true" href="#fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;fastai&lt;/h1&gt;
&lt;p&gt;The fastai library simplifies training fast and accurate neural nets using modern best practices. See the &lt;a href="https://docs.fast.ai" rel="nofollow"&gt;fastai website&lt;/a&gt; to get started. The library is based on research into deep learning best practices undertaken at &lt;a href="http://www.fast.ai" rel="nofollow"&gt;fast.ai&lt;/a&gt;, and includes "out of the box" support for &lt;a href="https://docs.fast.ai/vision.html#vision" rel="nofollow"&gt;&lt;code&gt;vision&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/text.html#text" rel="nofollow"&gt;&lt;code&gt;text&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://docs.fast.ai/tabular.html#tabular" rel="nofollow"&gt;&lt;code&gt;tabular&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://docs.fast.ai/collab.html#collab" rel="nofollow"&gt;&lt;code&gt;collab&lt;/code&gt;&lt;/a&gt; (collaborative filtering) models. For brief examples, see the &lt;a href="https://github.com/fastai/fastai/tree/master/examples"&gt;examples&lt;/a&gt; folder; detailed examples are provided in the full &lt;a href="https://docs.fast.ai/" rel="nofollow"&gt;documentation&lt;/a&gt;. For instance, here's how to train an MNIST model using &lt;a href="https://arxiv.org/abs/1512.03385" rel="nofollow"&gt;resnet18&lt;/a&gt; (from the &lt;a href="https://github.com/fastai/fastai/blob/master/examples/vision.ipynb"&gt;vision example&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; fastai.vision &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;
path &lt;span class="pl-k"&gt;=&lt;/span&gt; untar_data(&lt;span class="pl-c1"&gt;MNIST_PATH&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; image_data_from_folder(path)
learn &lt;span class="pl-k"&gt;=&lt;/span&gt; cnn_learner(data, models.resnet18, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;accuracy)
learn.fit(&lt;span class="pl-c1"&gt;1&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-note-for-coursefastai-students" class="anchor" aria-hidden="true" href="#note-for-coursefastai-students"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note for &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; students&lt;/h2&gt;
&lt;p&gt;This document is written for &lt;code&gt;fastai v1&lt;/code&gt;, which we use for the current version the &lt;a href="http://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt; deep learning courses. If you're following along with a course at &lt;a href="http://course18.fast.ai" rel="nofollow"&gt;course18.fast.ai&lt;/a&gt; (i.e. the machine learning course, which isn't updated for v1) you need to use &lt;code&gt;fastai 0.7&lt;/code&gt;;  please follow the installation instructions &lt;a href="https://forums.fast.ai/t/fastai-v0-install-issues-thread/24652" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; &lt;em&gt;fastai v1 currently supports Linux only, and requires &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; or later. Windows support is at an experimental stage: it should work fine but it's much slower and less well tested. Since Macs don't currently have good Nvidia GPU support, we do not currently prioritize Mac development.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fastai-1.x&lt;/code&gt; can be installed with either &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt; package managers and also from source. At the moment you can't just run &lt;em&gt;install&lt;/em&gt;, since you first need to get the correct &lt;code&gt;pytorch&lt;/code&gt; version installed - thus to get &lt;code&gt;fastai-1.x&lt;/code&gt; installed choose one of the installation recipes below using your favorite python package manager. Note that &lt;strong&gt;PyTorch v1&lt;/strong&gt; and &lt;strong&gt;Python 3.6&lt;/strong&gt; are the minimal version requirements.&lt;/p&gt;
&lt;p&gt;It's highly recommended you install &lt;code&gt;fastai&lt;/code&gt; and its dependencies in a virtual environment (&lt;a href="https://conda.io/docs/user-guide/tasks/manage-environments.html" rel="nofollow"&gt;&lt;code&gt;conda&lt;/code&gt;&lt;/a&gt; or others), so that you don't interfere with system-wide python packages. It's not that you must, but if you experience problems with any dependency packages, please consider using a fresh virtual environment just for &lt;code&gt;fastai&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Starting with pytorch-1.x you no longer need to install a special pytorch-cpu version. Instead use the normal pytorch and it works with and without GPU. But &lt;a href="https://docs.fast.ai/install.html#cpu-build" rel="nofollow"&gt;you can install the cpu build too&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you experience installation problems, please read about &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#installation-issues"&gt;installation issues&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are planning on using &lt;code&gt;fastai&lt;/code&gt; in the jupyter notebook environment, make sure to also install the corresponding &lt;a href="https://docs.fast.ai/install.html#jupyter-notebook-dependencies" rel="nofollow"&gt;packages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More advanced installation issues, such as installing only partial dependencies are covered in a dedicated &lt;a href="https://docs.fast.ai/install.html" rel="nofollow"&gt;installation doc&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-conda-install" class="anchor" aria-hidden="true" href="#conda-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch -c fastai fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will install the &lt;code&gt;pytorch&lt;/code&gt; build with the latest &lt;code&gt;cudatoolkit&lt;/code&gt; version. If you need a higher or lower &lt;code&gt;CUDA XX&lt;/code&gt; build (e.g. CUDA 9.0), following the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install the desired &lt;code&gt;pytorch&lt;/code&gt; build.&lt;/p&gt;
&lt;p&gt;Note that JPEG decoding can be a bottleneck, particularly if you have a fast GPU. You can optionally install an optimized JPEG decoder as follows (Linux):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda uninstall --force jpeg libtiff -y
conda install -c conda-forge libjpeg-turbo pillow==6.0.0
CC=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;cc -mavx2&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; pip install --no-cache-dir -U --force-reinstall --no-binary :all: --compile pillow-simd&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you only care about faster JPEG decompression, it can be &lt;code&gt;pillow&lt;/code&gt; or &lt;code&gt;pillow-simd&lt;/code&gt; in the last command above, the latter speeds up other image processing operations. For the full story see &lt;a href="https://docs.fast.ai/performance.html#faster-image-processing" rel="nofollow"&gt;Pillow-SIMD&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pypi-install" class="anchor" aria-hidden="true" href="#pypi-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyPI Install&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install fastai&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By default pip will install the latest &lt;code&gt;pytorch&lt;/code&gt; with the latest &lt;code&gt;cudatoolkit&lt;/code&gt;. If your hardware doesn't support the latest &lt;code&gt;cudatoolkit&lt;/code&gt;, follow the instructions &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;, to install a &lt;code&gt;pytorch&lt;/code&gt; build that fits your hardware.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-bug-fix-install" class="anchor" aria-hidden="true" href="#bug-fix-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bug Fix Install&lt;/h3&gt;
&lt;p&gt;If a bug fix was made in git and you can't wait till a new release is made, you can install the bleeding edge version of &lt;code&gt;fastai&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/fastai/fastai.git
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-developer-install" class="anchor" aria-hidden="true" href="#developer-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developer Install&lt;/h3&gt;
&lt;p&gt;The following instructions will result in a &lt;a href="https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs" rel="nofollow"&gt;pip editable install&lt;/a&gt;, so that you can &lt;code&gt;git pull&lt;/code&gt; at any time and your environment will automatically get the updates:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/fastai/fastai
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; fastai
tools/run-after-git-clone
pip install -e &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;.[dev]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, you can test that the build works by starting the jupyter notebook:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and executing an example notebook. For example load &lt;code&gt;examples/tabular.ipynb&lt;/code&gt; and run it.&lt;/p&gt;
&lt;p&gt;Please refer to &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; and &lt;a href="https://docs.fast.ai/dev/develop.html" rel="nofollow"&gt;Notes For Developers&lt;/a&gt; for more details on how to contribute to the &lt;code&gt;fastai&lt;/code&gt; project.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-building-from-source" class="anchor" aria-hidden="true" href="#building-from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building From Source&lt;/h3&gt;
&lt;p&gt;If for any reason you can't use the prepackaged packages and have to build from source, this section is for you.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To build &lt;code&gt;pytorch&lt;/code&gt; from source follow the &lt;a href="https://github.com/pytorch/pytorch#from-source"&gt;complete instructions&lt;/a&gt;. Remember to first install CUDA, CuDNN, and other required libraries as suggested - everything will be very slow without those libraries built into &lt;code&gt;pytorch&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you will also need to build &lt;code&gt;torchvision&lt;/code&gt; from source:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/pytorch/vision
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; vision
python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When both &lt;code&gt;pytorch&lt;/code&gt; and &lt;code&gt;torchvision&lt;/code&gt; are installed, first test that you can load each of these libraries:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;import torch
import torchvision&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;to validate that they were installed correctly&lt;/p&gt;
&lt;p&gt;Finally, proceed with &lt;code&gt;fastai&lt;/code&gt; installation as normal, either through prepackaged pip or conda builds or installing from source ("the developer install") as explained in the sections above.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-installation-issues" class="anchor" aria-hidden="true" href="#installation-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Issues&lt;/h2&gt;
&lt;p&gt;If the installation process fails, first make sure &lt;a href="https://github.com/fastai/fastai/blob/master/README.md#is-my-system-supported"&gt;your system is supported&lt;/a&gt;. And if the problem is still not addressed, please refer to the &lt;a href="https://docs.fast.ai/troubleshoot.html" rel="nofollow"&gt;troubleshooting document&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you encounter installation problems with conda, make sure you have the latest &lt;code&gt;conda&lt;/code&gt; client (&lt;code&gt;conda install&lt;/code&gt; will do an update too):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install conda&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-is-my-system-supported" class="anchor" aria-hidden="true" href="#is-my-system-supported"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is My System Supported?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Python: You need to have python 3.6 or higher&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CPU or GPU&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;pytorch&lt;/code&gt; binary package comes with its own CUDA, CuDNN, NCCL, MKL, and other libraries so you don't have to install system-wide NVIDIA's CUDA and related libraries if you don't need them for something else. If you have them installed already it doesn't matter which NVIDIA's CUDA version library you have installed system-wide. Your system could have CUDA 9.0 libraries, and you can still use &lt;code&gt;pytorch&lt;/code&gt; build with CUDA 10.0 libraries without any problem, since the &lt;code&gt;pytorch&lt;/code&gt; binary package is self-contained.&lt;/p&gt;
&lt;p&gt;The only requirement is that you have installed and configured the NVIDIA driver correctly. Usually you can test that by running &lt;code&gt;nvidia-smi&lt;/code&gt;. While it's possible that this application is not available on your system, it's very likely that if it doesn't work, then you don't have your NVIDIA drivers configured properly. And remember that a reboot is always required after installing NVIDIA drivers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Operating System:&lt;/p&gt;
&lt;p&gt;Since fastai-1.0 relies on pytorch-1.0, you need to be able to install pytorch-1.0 first.&lt;/p&gt;
&lt;p&gt;As of this moment pytorch.org's 1.0 version supports:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Platform&lt;/th&gt;
&lt;th&gt;GPU&lt;/th&gt;
&lt;th&gt;CPU&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;linux&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mac&lt;/td&gt;
&lt;td&gt;source&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;windows&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Legend: &lt;code&gt;binary&lt;/code&gt; = can be installed directly, &lt;code&gt;source&lt;/code&gt; = needs to be built from source.&lt;/p&gt;
&lt;p&gt;If there is no &lt;code&gt;pytorch&lt;/code&gt; preview conda or pip package available for your system, you may still be able to &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;build it from source&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How do you know which pytorch cuda version build to choose?&lt;/p&gt;
&lt;p&gt;It depends on the version of the installed NVIDIA driver. Here are the requirements for CUDA versions supported by pre-built &lt;code&gt;pytorch&lt;/code&gt; releases:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CUDA Toolkit&lt;/th&gt;
&lt;th&gt;NVIDIA (Linux x86_64)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 10.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 410.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 9.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 384.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA 8.0&lt;/td&gt;
&lt;td&gt;&amp;gt;= 367.48&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So if your NVIDIA driver is less than 384, then you can only use CUDA 8.0. Of course, you can upgrade your drivers to more recent ones if your card supports it.&lt;/p&gt;
&lt;p&gt;You can find a complete table with all variations &lt;a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you use NVIDIA driver 410+, you most likely want to install the &lt;code&gt;cudatoolkit=10.0&lt;/code&gt; pytorch variant, via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=10.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or if you need a lower version, use one of:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install -c pytorch pytorch cudatoolkit=8.0
conda install -c pytorch pytorch cudatoolkit=9.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For other options refer to the complete list of &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;the available pytorch variants&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;In order to update your environment, simply install &lt;code&gt;fastai&lt;/code&gt; in exactly the same way you did the initial installation.&lt;/p&gt;
&lt;p&gt;Top level files &lt;code&gt;environment.yml&lt;/code&gt; and &lt;code&gt;environment-cpu.yml&lt;/code&gt; belong to the old fastai (0.7). &lt;code&gt;conda env update&lt;/code&gt; is no longer the way to update your &lt;code&gt;fastai-1.x&lt;/code&gt; environment. These files remain because the fastai course-v2 video instructions rely on this setup. Eventually, once fastai course-v3 p1 and p2 will be completed, they will probably be moved to where they belong - under &lt;code&gt;old/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contribution-guidelines" class="anchor" aria-hidden="true" href="#contribution-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution guidelines&lt;/h2&gt;
&lt;p&gt;If you want to contribute to &lt;code&gt;fastai&lt;/code&gt;, be sure to review the &lt;a href="https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;. This project adheres to fastai's &lt;a href="https://github.com/fastai/fastai/blob/master/CODE-OF-CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code.&lt;/p&gt;
&lt;p&gt;We use GitHub issues for tracking requests and bugs, so please see &lt;a href="https://forums.fast.ai/" rel="nofollow"&gt;fastai forum&lt;/a&gt; for general questions and discussion.&lt;/p&gt;
&lt;p&gt;The fastai project strives to abide by generally accepted best practices in open-source software development:&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;A detailed history of changes can be found &lt;a href="https://github.com/fastai/fastai/blob/master/CHANGES.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h2&gt;
&lt;p&gt;Copyright 2017 onwards, fast.ai, Inc. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project's files except in compliance with the License. A copy of the License is provided in the LICENSE file in this repository.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/fastai</guid><pubDate>Thu, 30 Jan 2020 00:19:00 GMT</pubDate></item><item><title>learn-co-students/dsc-importing-data-using-pandas-lab-onl01-dtsc-ft-012120 #20 in Jupyter Notebook, Today</title><link>https://github.com/learn-co-students/dsc-importing-data-using-pandas-lab-onl01-dtsc-ft-012120</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-importing-data-using-pandas---lab" class="anchor" aria-hidden="true" href="#importing-data-using-pandas---lab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Importing Data Using Pandas - Lab&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this lab, you'll get some practice with loading files with summary or metadata, and if you find that easy, the optional "level up" content covers loading data from a corrupted csv file!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-objectives" class="anchor" aria-hidden="true" href="#objectives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Objectives&lt;/h2&gt;
&lt;p&gt;You will be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use pandas to import data from a CSV and and an Excel spreadsheet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-loading-files-with-summary-or-meta-data" class="anchor" aria-hidden="true" href="#loading-files-with-summary-or-meta-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Loading Files with Summary or Meta Data&lt;/h2&gt;
&lt;p&gt;Load either of the files &lt;code&gt;'Zipcode_Demos.csv'&lt;/code&gt; or &lt;code&gt;'Zipcode_Demos.xlsx'&lt;/code&gt;. What's going on with this dataset? Clean it up into a useable format and describe the nuances of how the data is currently formatted.&lt;/p&gt;
&lt;p&gt;All data files are stored in a folder titled &lt;code&gt;'Data'&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Import pandas using the standard alias&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Import the file and print the first 5 rows&lt;/span&gt;
df &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print the last 5 rows of df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; What is going on with this data set? Anything unusual?&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Clean up the dataset&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-level-up-optional---loading-corrupt-csv-files" class="anchor" aria-hidden="true" href="#level-up-optional---loading-corrupt-csv-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Level Up (Optional) - Loading Corrupt CSV files&lt;/h2&gt;
&lt;p&gt;Occasionally, you encounter some really ill-formatted data. One example of this can be data that has strings containing commas in a csv file. Under the standard protocol, when this occurs, one is supposed to use quotes to differentiate between the commas denoting fields and the commas within those fields themselves. For example, we could have a table like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ReviewerID,Rating,N_reviews,Review,VenueID 123456,4,137,This restaurant was pretty good, we had a great time.,98765&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which should be saved like this if it were a csv (to avoid confusion with the commas in the Review text):
&lt;code&gt;"ReviewerID","Rating","N_reviews","Review","VenueID" "123456","4","137","This restaurant was pretty good, we had a great time.","98765"&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Attempt to import the corrupt file, or at least a small preview of it. It is appropriately titled &lt;code&gt;'Yelp_Reviews_corrupt.csv'&lt;/code&gt;. Investigate some of the intricacies of skipping rows to then pass over this error and comment on what you think is going on.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Hint: Here's a useful programming pattern to use&lt;/span&gt;
&lt;span class="pl-k"&gt;try&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Do something&lt;/span&gt;
&lt;span class="pl-k"&gt;except&lt;/span&gt; &lt;span class="pl-c1"&gt;Exception&lt;/span&gt; &lt;span class="pl-k"&gt;as&lt;/span&gt; e:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Handle your exception e&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;Congratulations, you now practiced your Pandas-importing skills!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>learn-co-students</author><guid isPermaLink="false">https://github.com/learn-co-students/dsc-importing-data-using-pandas-lab-onl01-dtsc-ft-012120</guid><pubDate>Thu, 30 Jan 2020 00:20:00 GMT</pubDate></item><item><title>learn-co-students/dsc-accessing-data-with-pandas-lab-onl01-dtsc-ft-012120 #21 in Jupyter Notebook, Today</title><link>https://github.com/learn-co-students/dsc-accessing-data-with-pandas-lab-onl01-dtsc-ft-012120</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-accessing-data-within-pandas---lab" class="anchor" aria-hidden="true" href="#accessing-data-within-pandas---lab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Accessing Data within Pandas - Lab&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this lab, we'll look at a dataset which contains information on World Cup matches. Let's use the Pandas commands learned in the previous lesson to learn more about our data!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-objectives" class="anchor" aria-hidden="true" href="#objectives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Objectives&lt;/h2&gt;
&lt;p&gt;You will be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use pandas methods and attributes to access information about a dataset&lt;/li&gt;
&lt;li&gt;Index pandas dataframes with .loc, .iloc, and column names&lt;/li&gt;
&lt;li&gt;Use a boolean mask to index pandas series and dataframes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-load-the-data" class="anchor" aria-hidden="true" href="#load-the-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Load the data&lt;/h2&gt;
&lt;p&gt;Load the file &lt;code&gt;'WorldCupMatches.csv'&lt;/code&gt; as a DataFrame in Pandas.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Import pandas using the standard alias&lt;/span&gt;


&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Import 'WorldCupMatches.csv' as a DataFrame&lt;/span&gt;
df &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-common-methods-and-attributes" class="anchor" aria-hidden="true" href="#common-methods-and-attributes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Common methods and attributes&lt;/h2&gt;
&lt;p&gt;Use the correct method to look at the first 7 rows of the dataset.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print the first 7 rows of df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Look at the last 3 rows of the data set.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print the last 3 rows of df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Get a concise summary of the data using &lt;code&gt;.info()&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print a concise summary of df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Obtain a tuple representing the number of rows and number of columns&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print the number of rows and columns in df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Use the appropriate attribute to get the column names&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print the column names of df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-selecting-dataframe-information" class="anchor" aria-hidden="true" href="#selecting-dataframe-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Selecting DataFrame information&lt;/h2&gt;
&lt;p&gt;When looking at the DataFrame's &lt;code&gt;.head()&lt;/code&gt;, you might have noticed that the games are structured chronologically in the DataFrame.&lt;/p&gt;
&lt;p&gt;Use the right selection method to print all the information from the 3rd to the 5th game.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print rows 3 through 5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, print all the info from game 5-9, but we're only interested in printing out the, "Home Team Name" and the, "Away Team Name."&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print rows 5 through 9 and columns 'Home Team Name' and 'Away Team Name'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we'd like the information on all the games played in Group 3 for the 1950 World Cup.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print all info for games played in 1950 for Group 3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's repeat the command above, but now we only want to print out the attendance column for the Group 3 games.&lt;/p&gt;
&lt;p&gt;You can combine conditions like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df[(condition1) | (condition2)]&lt;/code&gt;  -&amp;gt; Returns rows where either condition is true&lt;/p&gt;
&lt;p&gt;&lt;code&gt;df[(condition1) &amp;amp; (condition2)]&lt;/code&gt;  -&amp;gt; Returns rows where both conditions are true&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print the 'Attendance' column for games played in 1950 for Group 3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Throughout the entire history of the World Cup, how many home games were played by the Netherlands?&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Number of home games played by the Netherlands&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;How many games were played by the Netherlands in total?&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Number of games played by the Netherlands in total&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, let's try and figure out how many games the USA played in the 2014 World Cup.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Number of games the USA played in the 2014 world cup&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, let's try to find out how many countries participated in the 1986 World Cup.&lt;/p&gt;
&lt;p&gt;Hint 1: as a first step, create a new dataset that only contains games in that year.&lt;/p&gt;
&lt;p&gt;Hint 2: You can use &lt;code&gt;.unique()&lt;/code&gt; to make sure you don't end up with duplicate country names.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Number of countries participated in the 1986 world cup&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In World Cup history, how matches had more than 5 goals in total?&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Number of matches that had more than 5 goals in total&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-changing-values-and-creating-new-columns" class="anchor" aria-hidden="true" href="#changing-values-and-creating-new-columns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changing values and creating new columns&lt;/h2&gt;
&lt;p&gt;With the information you currently have in your &lt;code&gt;df&lt;/code&gt;, create a new column, "Half-time Goals."&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Create a new column 'Half-time Goals' in df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Run the code below. You'll notice that for Korea, there are records for both North-Korea (Korea DPR) and South-Korea (Korea Republic).&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Print all records containing the string 'Korea'&lt;/span&gt;
df.loc[df[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Home Team Name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;].str.contains(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Korea&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;), &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Home Team Name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Imagine that, for some reason, we simply want Korea listed as one entry, so we want to replace every "Home Team Name" and "Away Team Name" entry that contains "Korea" to simply "Korea". In the same way, we want to change the columns "Home Team Initials" and "Away Team Initials" to NSK (North &amp;amp; South Korea) instead of "KOR" and "PRK".&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Update the 'Home Team Name' and 'Home Team Initials' columns &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make sure to verify your answer!&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Check the updated columns&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this lab, you learned how to access data within Pandas!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>learn-co-students</author><guid isPermaLink="false">https://github.com/learn-co-students/dsc-accessing-data-with-pandas-lab-onl01-dtsc-ft-012120</guid><pubDate>Thu, 30 Jan 2020 00:21:00 GMT</pubDate></item><item><title>codebasics/py #22 in Jupyter Notebook, Today</title><link>https://github.com/codebasics/py</link><description>&lt;p&gt;&lt;i&gt;Repository to store sample python programs for python learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-py" class="anchor" aria-hidden="true" href="#py"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;py&lt;/h1&gt;
&lt;p&gt;Repository to store sample python programs for python learning
Youtube channel &lt;a href="https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ" rel="nofollow"&gt;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>codebasics</author><guid isPermaLink="false">https://github.com/codebasics/py</guid><pubDate>Thu, 30 Jan 2020 00:22:00 GMT</pubDate></item><item><title>NVIDIA/DeepLearningExamples #23 in Jupyter Notebook, Today</title><link>https://github.com/NVIDIA/DeepLearningExamples</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nvidia-deep-learning-examples-for-tensor-cores" class="anchor" aria-hidden="true" href="#nvidia-deep-learning-examples-for-tensor-cores"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA Deep Learning Examples for Tensor Cores&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This repository provides the latest deep learning example networks for training.  These examples focus on achieving the best performance and convergence from NVIDIA Volta Tensor Cores.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-gpu-cloud-ngc-container-registry" class="anchor" aria-hidden="true" href="#nvidia-gpu-cloud-ngc-container-registry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA GPU Cloud (NGC) Container Registry&lt;/h2&gt;
&lt;p&gt;These examples, along with our NVIDIA deep learning software stack, are provided in a monthly updated Docker container on the NGC container registry (&lt;a href="https://ngc.nvidia.com" rel="nofollow"&gt;https://ngc.nvidia.com&lt;/a&gt;). These containers include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest NVIDIA examples from this repository&lt;/li&gt;
&lt;li&gt;The latest NVIDIA contributions shared upstream to the respective framework&lt;/li&gt;
&lt;li&gt;The latest NVIDIA Deep Learning software libraries, such as cuDNN, NCCL, cuBLAS, etc. which have all been through a rigorous monthly quality assurance process to ensure that they provide the best possible performance&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/dgx/index.html#nvidia-optimized-frameworks-release-notes" rel="nofollow"&gt;Monthly release notes&lt;/a&gt; for each of the NVIDIA optimized containers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-directory-structure" class="anchor" aria-hidden="true" href="#directory-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directory structure&lt;/h2&gt;
&lt;p&gt;The examples are organized first by framework, such as TensorFlow, PyTorch, etc. and second by use case, such as computer vision, natural language processing, etc. We hope this structure enables you to quickly locate the example networks that best suit your needs. Here are the currently supported models:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-computer-vision" class="anchor" aria-hidden="true" href="#computer-vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResNet-50&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/MxNet/Classification/RN50v1.5"&gt;MXNet&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Classification/RN50v1.5"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ResNext&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SE-ResNext&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Detection/SSD"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(industrial)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Industrial"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(medical)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Medical"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VNet&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/VNet"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GNMT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/GNMT"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Translation/GNMT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/Transformer"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT"&gt;PyTorch&lt;/a&gt;][&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer-XL&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/Transformer-XL"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recommender-systems" class="anchor" aria-hidden="true" href="#recommender-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommender Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NCF&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/NCF"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VAE-CF&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/VAE-CF"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-text-to-speech" class="anchor" aria-hidden="true" href="#text-to-speech"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text to Speech&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tacotron &amp;amp; WaveGlow&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-speech-recognition" class="anchor" aria-hidden="true" href="#speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech Recognition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jasper&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-support" class="anchor" aria-hidden="true" href="#nvidia-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA support&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate the level of support that will be provided. The range is from ongoing updates and improvements to a point-in-time release for thought leadership.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-feedback--contributions" class="anchor" aria-hidden="true" href="#feedback--contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback / Contributions&lt;/h2&gt;
&lt;p&gt;We're posting these examples on GitHub to better support the community, facilitate feedback, as well as collect and implement contributions using GitHub Issues and pull requests. We welcome all contributions!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known issues&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate any known issues and encourage the community to provide feedback.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/DeepLearningExamples</guid><pubDate>Thu, 30 Jan 2020 00:23:00 GMT</pubDate></item><item><title>xiaolai/regular-investing-in-box #24 in Jupyter Notebook, Today</title><link>https://github.com/xiaolai/regular-investing-in-box</link><description>&lt;p&gt;&lt;i&gt;  &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-on-regular-investing" class="anchor" aria-hidden="true" href="#on-regular-investing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;On Regular Investing&lt;/h1&gt;
&lt;p&gt;Regular investing is quite simple:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Regularly invest a set amount in a particular investment over a long period of time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, &lt;strong&gt;every week&lt;/strong&gt; (regularly) for the next &lt;strong&gt;five to ten years&lt;/strong&gt;  invest &lt;strong&gt;200 USD&lt;/strong&gt; (a set amount) in &lt;strong&gt;&lt;a href="https://b.watch" rel="nofollow"&gt;BOX&lt;/a&gt;&lt;/strong&gt; (a particular investment). Of course, you could change BOX to any investment that is worth investing in an holding over a long period of time, like shares in &lt;a href="https://finance.yahoo.com/quote/AAPL/" rel="nofollow"&gt;Apple&lt;/a&gt;, &lt;a href="https://hk.finance.yahoo.com/quote/600519.SS/" rel="nofollow"&gt;Maotai&lt;/a&gt;, &lt;a href="https://finance.yahoo.com/quote/KO" rel="nofollow"&gt;Coca-Cola&lt;/a&gt;, or an &lt;a href="https://en.wikipedia.org/wiki/S%26P_500_Index" rel="nofollow"&gt;S&amp;amp;P 500&lt;/a&gt; Index Fund.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;On Regular Investing&lt;/em&gt; Chinese:is about my thoughts of WHY and HOW on regular investing.&lt;/p&gt;
&lt;p&gt;This README file is of the first edition of this book.&lt;/p&gt;
&lt;p&gt;&lt;a href="new-edition-for-publishing.md"&gt;The second edition&lt;/a&gt; is for publishing in China.&lt;/p&gt;
&lt;p&gt;Translations in various languages available in the folder &lt;em&gt;&lt;a href="Translations/"&gt;Translations&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Chinese audiobook available in the folder &lt;a href="audiobooks/"&gt;audiobooks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The third edition of the book was published online: &lt;a href="https://onregularinvesting.com" rel="nofollow"&gt;https://onregularinvesting.com&lt;/a&gt;, Chinese and English available. Files are in the folder &lt;code&gt;./docs&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content---" class="anchor" aria-hidden="true" href="#--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;  &lt;/h1&gt;
&lt;p&gt;&lt;em&gt; &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://b.watch" rel="nofollow"&gt;https://b.watch&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt; &lt;a href="audiobook/"&gt;audiobook&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2019.12.25&lt;/strong&gt;: &lt;a href="http://product.dangdang.com/28493272.html?_utm_ad_id=83757" rel="nofollow"&gt;&lt;/a&gt;  &lt;a href="https://item.jd.com/12605781.html" rel="nofollow"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;   &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;  &lt;strong&gt;&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt; &lt;strong&gt;5  10 &lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://b.watch" rel="nofollow"&gt;BOX&lt;/a&gt;&lt;/strong&gt;  ETF &lt;strong&gt;150  1000 &lt;/strong&gt;  &lt;a href="https://b.watch" rel="nofollow"&gt;BOX&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;  &lt;strong&gt;&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;    &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;  &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt; &lt;strong&gt;&lt;/strong&gt;  &lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--1" class="anchor" aria-hidden="true" href="#-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="README.md"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="CHAPTER.01.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.02.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.03.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.04.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.05.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.06.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.07.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.08.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.09.1.md"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;9.1. &lt;a href="CHAPTER.09.1.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;9.2. &lt;a href="CHAPTER.09.2.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;9.3 &lt;a href="CHAPTER.09.3.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;9.4 &lt;a href="CHAPTER.09.4.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;9.5 &lt;a href="CHAPTER.09.5.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;9.6 &lt;a href="CHAPTER.09.6.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;9.7 &lt;a href="CHAPTER.09.7.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.10.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.11.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.12.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="CHAPTER.13.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Finale.md"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-" class="anchor" aria-hidden="true" href="#"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="Z-Appendix.01.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Z-Appendix.02.md"&gt; BOX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Z-Appendix.03.md"&gt; b.watch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Z-Appendix.04.md"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" rel="nofollow"&gt;CC-BY-NC-ND license&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/CC-BY-NC-ND.png?raw=true"&gt;&lt;img src="images/CC-BY-NC-ND.png?raw=true" alt="CC-BY-NC-ND" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xiaolai</author><guid isPermaLink="false">https://github.com/xiaolai/regular-investing-in-box</guid><pubDate>Thu, 30 Jan 2020 00:24:00 GMT</pubDate></item><item><title>fastai/fastai2 #25 in Jupyter Notebook, Today</title><link>https://github.com/fastai/fastai2</link><description>&lt;p&gt;&lt;i&gt;Temporary home for fastai v2 while it's being developed&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;
&lt;h1&gt;&lt;a id="user-content-welcome-to-fastai-v2" class="anchor" aria-hidden="true" href="#welcome-to-fastai-v2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to fastai v2&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;NB: This is still in early development. Use v1 unless you want to contribute to the next version of fastai&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;You can get all the necessary dependencies by simply installing fastai v1: &lt;code&gt;conda install -c fastai -c pytorch fastai&lt;/code&gt;. Or alternatively you can automatically install the dependencies into a new environment:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/fastai/fastai2
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; fastai2
conda env create -f environment.yml
&lt;span class="pl-c1"&gt;source&lt;/span&gt; activate fastai2&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, you can install fastai v2 with pip: &lt;code&gt;pip install fastai2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Or you can use an editable install (which is probably the best approach at the moment, since fastai v2 is under heavy development):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/fastai/fastai2
cd fastai2
pip install -e .[dev]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should also use an editable install of &lt;a href="https://github.com/fastai/fastcore"&gt;&lt;code&gt;fastcore&lt;/code&gt;&lt;/a&gt; to go with it.&lt;/p&gt;
&lt;p&gt;To use &lt;code&gt;fastai2.medical.imaging&lt;/code&gt; you'll also need to:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install pyarrow
pip install pydicom kornia opencv-python scikit-image&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h2&gt;
&lt;p&gt;To run the tests in parallel, launch:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;nbdev_test_nbs&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;make &lt;span class="pl-c1"&gt;test&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;After you clone this repository, please run &lt;code&gt;nbdev_install_git_hooks&lt;/code&gt; in your terminal. This sets up git hooks, which clean up the notebooks to remove the extraneous stuff stored in the notebooks (e.g. which cells you ran) which causes unnecessary merge conflicts.&lt;/p&gt;
&lt;p&gt;Before submitting a PR, check that the local library and notebooks match. The script &lt;code&gt;nbdev_diff_nbs&lt;/code&gt; can let you know if there is a difference between the local library and the notebooks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you made a change to the notebooks in one of the exported cells, you can export it to the library with &lt;code&gt;nbdev_build_lib&lt;/code&gt; or &lt;code&gt;make fastai2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you made a change to the library, you can export it back to the notebooks with &lt;code&gt;nbdev_update_lib&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/fastai2</guid><pubDate>Thu, 30 Jan 2020 00:25:00 GMT</pubDate></item></channel></rss>