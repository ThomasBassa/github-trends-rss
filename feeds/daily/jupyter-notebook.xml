<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, Today</title><link>https://github.com/trending/jupyter-notebook?since=daily</link><description>The top repositories on GitHub for jupyter-notebook, measured daily</description><pubDate>Mon, 20 Jan 2020 01:06:24 GMT</pubDate><lastBuildDate>Mon, 20 Jan 2020 01:06:24 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>google/trax #1 in Jupyter Notebook, Today</title><link>https://github.com/google/trax</link><description>&lt;p&gt;&lt;i&gt;Trax — your path to advanced deep learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-trax--your-path-to-advanced-deep-learning" class="anchor" aria-hidden="true" href="#trax--your-path-to-advanced-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trax — your path to advanced deep learning&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/bddfc5e8ae531f274dd98ab357d4a5d5f0683cd4/68747470733a2f2f696d616765732e706578656c732e636f6d2f70686f746f732f3436313737322f706578656c732d70686f746f2d3436313737322e6a7065673f646c266669743d63726f702663726f703d656e74726f707926773d333226683d3231"&gt;&lt;img src="https://camo.githubusercontent.com/bddfc5e8ae531f274dd98ab357d4a5d5f0683cd4/68747470733a2f2f696d616765732e706578656c732e636f6d2f70686f746f732f3436313737322f706578656c732d70686f746f2d3436313737322e6a7065673f646c266669743d63726f702663726f703d656e74726f707926773d333226683d3231" alt="train tracks" data-canonical-src="https://images.pexels.com/photos/461772/pexels-photo-461772.jpeg?dl&amp;amp;fit=crop&amp;amp;crop=entropy&amp;amp;w=32&amp;amp;h=21" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://badge.fury.io/py/trax" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3ddd29ad9780045cf67e6db455d947a7335c483/68747470733a2f2f62616467652e667572792e696f2f70792f747261782e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/trax.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/google/trax/issues"&gt;&lt;img src="https://camo.githubusercontent.com/80b7490db8d31f8f8ed019832a5c80e2e81fb73d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f676f6f676c652f747261782e737667" alt="GitHub Issues" data-canonical-src="https://img.shields.io/github/issues/google/trax.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="CONTRIBUTING.md"&gt;&lt;img src="https://camo.githubusercontent.com/8f697c48adc5026cc6d83dd45e42b9b93ee1803c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e737667" alt="Contributions welcome" data-canonical-src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/dc5c93f4ddfa92aaed4ace74e89dbc075f7810c8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d627269676874677265656e2e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/trax-ml/community" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/fed5b5512498193ce4bba599fd94cd12b9f56491/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6e776a732f6e772e6a732e737667" alt="Gitter" data-canonical-src="https://img.shields.io/gitter/room/nwjs/nw.js.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/google/trax"&gt;Trax&lt;/a&gt; helps you understand and explore advanced deep learning.
We &lt;a href="#structure"&gt;focus&lt;/a&gt; on making Trax code clear while pushing advanced models like
&lt;a href="https://github.com/google/trax/tree/master/trax/models/reformer"&gt;Reformer&lt;/a&gt; to their limits.
Trax is actively used and maintained in the &lt;a href="https://research.google.com/teams/brain/" rel="nofollow"&gt;Google Brain team&lt;/a&gt;.
Give it a try, &lt;a href="https://gitter.im/trax-ml/community" rel="nofollow"&gt;talk to us&lt;/a&gt;
or &lt;a href="https://github.com/google/trax/issues"&gt;open an issue&lt;/a&gt; if needed.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-use-trax" class="anchor" aria-hidden="true" href="#use-trax"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use Trax&lt;/h3&gt;
&lt;p&gt;You can use Trax either as a library from your own python scripts and notebooks
or as a binary from the shell, which can be more convenient for training large models.
Trax includes a number of deep learning models (ResNet, Transformer, RNNs, ...)
and has bindings to a large number of deep learning datasets, including
&lt;a href="https://github.com/tensorflow/tensor2tensor"&gt;Tensor2Tensor&lt;/a&gt; and &lt;a href="https://www.tensorflow.org/datasets/catalog/overview" rel="nofollow"&gt;TensorFlow datasets&lt;/a&gt;.
It runs without any changes on CPUs, GPUs and TPUs.&lt;/p&gt;
&lt;p&gt;To see how to use Trax as a library, take a look at this &lt;a href="https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb" rel="nofollow"&gt;quick start colab&lt;/a&gt;
which explains how to create data in python, connect it to a Transformer model in Trax, train it and run inference.
You can select a CPU or GPU runtime, or even get a free 8-core TPU as
runtime. With TPUs in colab you need to set extra flags as demonstrated in these
&lt;a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb" rel="nofollow"&gt;training&lt;/a&gt;
and &lt;a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb" rel="nofollow"&gt;inference&lt;/a&gt; colabs.&lt;/p&gt;
&lt;p&gt;To use Trax as a binary and not forget all the parameters (model type, learning
rate, other hyper-paramters and training settings), we recommend &lt;a href="https://github.com/google/gin-config"&gt;gin-config&lt;/a&gt;.
Take a look at &lt;a href="https://github.com/google/trax/blob/master/trax/configs/mlp_mnist.gin"&gt;an example gin config&lt;/a&gt;
for training a simple MLP on MNIST and run it as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m trax.trainer --config_file=$PWD/trax/configs/mlp_mnist.gin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a more advanced example, you can train a &lt;a href="https://github.com/google/trax/tree/master/trax/models/reformer"&gt;Reformer&lt;/a&gt;
on &lt;a href="https://arxiv.org/abs/1707.08819" rel="nofollow"&gt;Imagenet64&lt;/a&gt; to generate images &lt;a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb" rel="nofollow"&gt;like this&lt;/a&gt;
with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python -m trax.trainer --config_file=$PWD/trax/configs/reformer_imagenet64.gin
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-structure" class="anchor" aria-hidden="true" href="#structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Structure&lt;/h3&gt;
&lt;p&gt;Trax code is structured in a way that allows you to understand deep learning
from scratch. We start with basic maths and go through layers, models,
supervised and reinforcement learning. We get to advanced deep learning
results, including recent papers such as &lt;a href="https://arxiv.org/abs/2001.04451" rel="nofollow"&gt;Reformer - The Efficient Transformer&lt;/a&gt;,
selected for oral presentation at &lt;a href="https://iclr.cc/Conferences/2020/" rel="nofollow"&gt;ICLR 2020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main steps needed to understand deep learning correspond to sub-directories
in Trax code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/google/trax/tree/master/trax/math"&gt;math/&lt;/a&gt; — basic math operations and ways to accelerate them on GPUs and TPUs (through &lt;a href="https://github.com/google/jax"&gt;JAX&lt;/a&gt; and &lt;a href="https://www.tensorflow.org/" rel="nofollow"&gt;TensorFlow&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/trax/tree/master/trax/layers"&gt;layers/&lt;/a&gt; are the basic building blocks of neural networks and here you'll find how they are build and all the needed ones&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/trax/tree/master/trax/models"&gt;models/&lt;/a&gt; contains all basic models (MLP, ResNet, Transformer, ...) and a number of new research models&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/trax/tree/master/trax/optimizers"&gt;optimizers/&lt;/a&gt; is a directory with optimizers needed for deep learning&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/trax/tree/master/trax/supervised"&gt;supervised/&lt;/a&gt; contains the utilities needed to run supervised learning and the Trainer class&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/google/trax/tree/master/trax/rl"&gt;rl/&lt;/a&gt; contains our work on reinforcement learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h3&gt;
&lt;p&gt;To get the most recent update on Trax development, &lt;a href="https://gitter.im/trax-ml/community" rel="nofollow"&gt;chat with us&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most common supervised learning models in Trax are running and should have clear
code — if this is not the case, please &lt;a href="https://github.com/google/trax/issues"&gt;open an issue&lt;/a&gt;
or, even better, send along a pull request (see &lt;a href="CONTRIBUTING.md"&gt;our contribution doc&lt;/a&gt;).
In Trax we value documentation, examples and colabs so if you find any
problems with those, please report it and contribute a solution.&lt;/p&gt;
&lt;p&gt;We are still improving a few smaller parts of &lt;a href="https://github.com/google/trax/tree/master/trax/layers"&gt;layers&lt;/a&gt;,
planning to update the &lt;a href="https://github.com/google/trax/tree/master/trax/supervised"&gt;supervised&lt;/a&gt; API and
heavily working on the &lt;a href="https://github.com/google/trax/tree/master/trax/rl"&gt;rl&lt;/a&gt; part,
so expect these parts to change over the next few months. We are also working hard
to improve our documentation and examples and we welcome help with that.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google</author><guid isPermaLink="false">https://github.com/google/trax</guid><pubDate>Mon, 20 Jan 2020 00:01:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #2 in Jupyter Notebook, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Mon, 20 Jan 2020 00:02:00 GMT</pubDate></item><item><title>jakevdp/PythonDataScienceHandbook #3 in Jupyter Notebook, Today</title><link>https://github.com/jakevdp/PythonDataScienceHandbook</link><description>&lt;p&gt;&lt;i&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-data-science-handbook" class="anchor" aria-hidden="true" href="#python-data-science-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Data Science Handbook&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository contains the entire &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="notebooks/figures/PDSH-cover.png"&gt;&lt;img src="notebooks/figures/PDSH-cover.png" alt="cover image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-this-book" class="anchor" aria-hidden="true" href="#how-to-use-this-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Use this Book&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Read the book in its entirety online at &lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/" rel="nofollow"&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the code using the Jupyter notebooks available in this repository's &lt;a href="notebooks"&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch executable versions of these notebooks using &lt;a href="http://colab.research.google.com" rel="nofollow"&gt;Google Colab&lt;/a&gt;: &lt;a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href="https://beta.mybinder.org/" rel="nofollow"&gt;binder&lt;/a&gt;: &lt;a href="https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Buy the printed book through &lt;a href="http://shop.oreilly.com/product/0636920034919.do" rel="nofollow"&gt;O'Reilly Media&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt;
&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href="http://ipython.org" rel="nofollow"&gt;IPython&lt;/a&gt;, &lt;a href="http://numpy.org" rel="nofollow"&gt;NumPy&lt;/a&gt;, &lt;a href="http://pandas.pydata.org" rel="nofollow"&gt;Pandas&lt;/a&gt;, &lt;a href="http://matplotlib.org" rel="nofollow"&gt;Matplotlib&lt;/a&gt;, &lt;a href="http://scikit-learn.org" rel="nofollow"&gt;Scikit-Learn&lt;/a&gt;, and related packages.
Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project,
&lt;a href="https://github.com/jakevdp/WhirlwindTourOfPython"&gt;A Whirlwind Tour of Python&lt;/a&gt;: it's a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt;
&lt;p&gt;See &lt;a href="http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb" rel="nofollow"&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-software" class="anchor" aria-hidden="true" href="#software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Software&lt;/h2&gt;
&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt;
&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href="requirements.txt"&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use).
To install the requirements using &lt;a href="http://conda.pydata.org" rel="nofollow"&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can read more about using conda environments in the &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h3&gt;
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href="LICENSE-CODE"&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-text" class="anchor" aria-hidden="true" href="#text"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text&lt;/h3&gt;
&lt;p&gt;The text content of the book is released under the &lt;a href="LICENSE-TEXT"&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode" rel="nofollow"&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jakevdp</author><guid isPermaLink="false">https://github.com/jakevdp/PythonDataScienceHandbook</guid><pubDate>Mon, 20 Jan 2020 00:03:00 GMT</pubDate></item><item><title>advboxes/AdvBox #4 in Jupyter Notebook, Today</title><link>https://github.com/advboxes/AdvBox</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;p&gt;No README was found for this project.&lt;/p&gt;</description><author>advboxes</author><guid isPermaLink="false">https://github.com/advboxes/AdvBox</guid><pubDate>Mon, 20 Jan 2020 00:04:00 GMT</pubDate></item><item><title>mml-book/mml-book.github.io #5 in Jupyter Notebook, Today</title><link>https://github.com/mml-book/mml-book.github.io</link><description>&lt;p&gt;&lt;i&gt;Companion webpage to the book "Mathematics For Machine Learning"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-mml-bookgithubio" class="anchor" aria-hidden="true" href="#mml-bookgithubio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mml-book.github.io&lt;/h1&gt;
&lt;p&gt;Companion webpage to the book "Mathematics For Machine Learning"&lt;/p&gt;
&lt;p&gt;&lt;a href="https://mml-book.com" rel="nofollow"&gt;https://mml-book.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copyright 2020 by Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. To be published by Cambridge University Press.&lt;/p&gt;
&lt;p&gt;We are in the process of writing a book on Mathematics for Machine Learning that motivates people to learn mathematical concepts. The book is not intended to cover advanced machine learning techniques because there are already plenty of books doing this. Instead, we aim to provide the necessary mathematical skills to read those other books.&lt;/p&gt;
&lt;p&gt;We split the book into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mathematical foundations&lt;/li&gt;
&lt;li&gt;Example machine learning algorithms that use the mathematical foundations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We aim to keep this book reasonably short, so we cannot cover everything. We will also provide exercises for part 1 and jupyter notebooks for part 2 of the book.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>mml-book</author><guid isPermaLink="false">https://github.com/mml-book/mml-book.github.io</guid><pubDate>Mon, 20 Jan 2020 00:05:00 GMT</pubDate></item><item><title>Cyb3rWard0g/HELK #6 in Jupyter Notebook, Today</title><link>https://github.com/Cyb3rWard0g/HELK</link><description>&lt;p&gt;&lt;i&gt;The Hunting ELK&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-helk-alpha" class="anchor" aria-hidden="true" href="#helk-alpha"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HELK [Alpha]&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.gnu.org/licenses/gpl-3.0" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ec385922fa349d9c349f34b7f3bf311843e35ba8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667" alt="License: GPL v3" data-canonical-src="https://img.shields.io/badge/License-GPLv3-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://GitHub.com/Cyb3rWard0g/HELK/issues?q=is%3Aissue+is%3Aclosed"&gt;&lt;img src="https://camo.githubusercontent.com/b40fa4904efe5f6b0f5f33d2c28fb0924cde495a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d636c6f7365642f43796233727761726430672f48454c4b2e737667" alt="GitHub issues-closed" data-canonical-src="https://img.shields.io/github/issues-closed/Cyb3rward0g/HELK.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/THE_HELK" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/547d793fa0533037131310966e83e621e8bb77a3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f5448455f48454c4b2e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/THE_HELK.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/ellerbrock/open-source-badges/"&gt;&lt;img src="https://camo.githubusercontent.com/d41b9884bd102b525c8fb9a8c3c8d3bbed2b67f0/68747470733a2f2f6261646765732e66726170736f66742e636f6d2f6f732f76312f6f70656e2d736f757263652e7376673f763d313033" alt="Open Source Love svg1" data-canonical-src="https://badges.frapsoft.com/os/v1/open-source.svg?v=103" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Hunting ELK or simply the HELK is one of the first open source hunt platforms with  advanced analytics capabilities such as SQL declarative language, graphing, structured streaming, and even machine learning via Jupyter notebooks and Apache Spark over an ELK stack. This project was developed primarily for research, but due to its flexible design and core components, it can be deployed in larger environments with the right configurations and scalable infrastructure.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="resources/images/HELK_Design.png"&gt;&lt;img src="resources/images/HELK_Design.png" alt="alt text" title="HELK Infrastructure" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-goals" class="anchor" aria-hidden="true" href="#goals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Goals&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Provide an open source hunting platform to the community and share the basics of Threat Hunting.&lt;/li&gt;
&lt;li&gt;Expedite the time it takes to deploy a hunt platform.&lt;/li&gt;
&lt;li&gt;Improve the testing and development of hunting use cases in an easier and more affordable way.&lt;/li&gt;
&lt;li&gt;Enable Data Science capabilities while analyzing data via Apache Spark, GraphFrames &amp;amp; Jupyter Notebooks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-current-status-alpha" class="anchor" aria-hidden="true" href="#current-status-alpha"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Current Status: Alpha&lt;/h1&gt;
&lt;p&gt;The project is currently in an alpha stage, which means that the code and the functionality are still changing. We haven't yet tested the system with large data sources and in many scenarios. We invite you to try it and welcome any feedback.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-helk-features" class="anchor" aria-hidden="true" href="#helk-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HELK Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kafka:&lt;/strong&gt; A distributed publish-subscribe messaging system that is designed to be fast, scalable, fault-tolerant, and durable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elasticsearch:&lt;/strong&gt; A highly scalable open-source full-text search and analytics engine.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logstash:&lt;/strong&gt; A data collection engine with real-time pipelining capabilities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kibana:&lt;/strong&gt; An open source analytics and visualization platform designed to work with Elasticsearch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ES-Hadoop:&lt;/strong&gt; An open-source, stand-alone, self-contained, small library that allows Hadoop jobs (whether using Map/Reduce or libraries built upon it such as Hive, Pig or Cascading or new upcoming libraries like Apache Spark ) to interact with Elasticsearch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spark:&lt;/strong&gt; A fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GraphFrames:&lt;/strong&gt; A package for Apache Spark which provides DataFrame-based Graphs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Notebook:&lt;/strong&gt; An open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KSQL:&lt;/strong&gt; Confluent KSQL is the open source, streaming SQL engine that enables real-time data processing against Apache Kafka®. It provides an easy-to-use, yet powerful interactive SQL interface for stream processing on Kafka, without the need to write code in a programming language such as Java or Python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elastalert:&lt;/strong&gt; ElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sigma:&lt;/strong&gt; Sigma is a generic and open signature format that allows you to describe relevant log events in a straightforward manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WIKI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki/Architecture-Overview"&gt;Architecture Overview&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki/Kafka"&gt;Kafka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki/Logstash"&gt;Logstash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki/Elasticsearch"&gt;Elasticsearch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki/Kibana"&gt;Kibana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki/Spark"&gt;Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/wiki/Installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-docker-accessing-the-helks-images" class="anchor" aria-hidden="true" href="#docker-accessing-the-helks-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;(Docker) Accessing the HELK's Images&lt;/h2&gt;
&lt;p&gt;By default, the HELK's containers are run in the background (Detached). You can see all your docker containers by running the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker ps

CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                                            NAMES
a97bd895a2b3        cyb3rward0g/helk-spark-worker:2.3.0    "./spark-worker-entr…"   About an hour ago   Up About an hour    0.0.0.0:8082-&amp;gt;8082/tcp                           helk-spark-worker2
cbb31f688e0a        cyb3rward0g/helk-spark-worker:2.3.0    "./spark-worker-entr…"   About an hour ago   Up About an hour    0.0.0.0:8081-&amp;gt;8081/tcp                           helk-spark-worker
5d58068aa7e3        cyb3rward0g/helk-kafka-broker:1.1.0    "./kafka-entrypoint.…"   About an hour ago   Up About an hour    0.0.0.0:9092-&amp;gt;9092/tcp                           helk-kafka-broker
bdb303b09878        cyb3rward0g/helk-kafka-broker:1.1.0    "./kafka-entrypoint.…"   About an hour ago   Up About an hour    0.0.0.0:9093-&amp;gt;9093/tcp                           helk-kafka-broker2
7761d1e43d37        cyb3rward0g/helk-nginx:0.0.2           "./nginx-entrypoint.…"   About an hour ago   Up About an hour    0.0.0.0:80-&amp;gt;80/tcp                               helk-nginx
ede2a2503030        cyb3rward0g/helk-jupyter:0.32.1        "./jupyter-entrypoin…"   About an hour ago   Up About an hour    0.0.0.0:4040-&amp;gt;4040/tcp, 0.0.0.0:8880-&amp;gt;8880/tcp   helk-jupyter
ede19510e959        cyb3rward0g/helk-logstash:6.2.4        "/usr/local/bin/dock…"   About an hour ago   Up About an hour    5044/tcp, 9600/tcp                               helk-logstash
e92823b24b2d        cyb3rward0g/helk-spark-master:2.3.0    "./spark-master-entr…"   About an hour ago   Up About an hour    0.0.0.0:7077-&amp;gt;7077/tcp, 0.0.0.0:8080-&amp;gt;8080/tcp   helk-spark-master
6125921b310d        cyb3rward0g/helk-kibana:6.2.4          "./kibana-entrypoint…"   About an hour ago   Up About an hour    5601/tcp                                         helk-kibana
4321d609ae07        cyb3rward0g/helk-zookeeper:3.4.10      "./zookeeper-entrypo…"   About an hour ago   Up About an hour    2888/tcp, 0.0.0.0:2181-&amp;gt;2181/tcp, 3888/tcp       helk-zookeeper
9cbca145fb3e        cyb3rward0g/helk-elasticsearch:6.2.4   "/usr/local/bin/dock…"   About an hour ago   Up About an hour    9200/tcp, 9300/tcp                               helk-elasticsearch
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you will just have to pick which container you want to access and run the following following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker exec -ti &amp;lt;image-name&amp;gt; bash
root@ede2a2503030:/opt/helk/scripts#
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cyberwardog.blogspot.com/2018/04/welcome-to-helk-enabling-advanced_9.html" rel="nofollow"&gt;Welcome to HELK! : Enabling Advanced Analytics Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spark.apache.org/docs/latest/index.html" rel="nofollow"&gt;Spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spark.apache.org/docs/latest/spark-standalone.html" rel="nofollow"&gt;Spark Standalone Mode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cyberwardog.blogspot.com/2017/02/setting-up-pentesting-i-mean-threat_98.html" rel="nofollow"&gt;Setting up a Pentesting.. I mean, a Threat Hunting Lab - Part 5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cs.stanford.edu/~matei/papers/2016/grades_graphframes.pdf" rel="nofollow"&gt;An Integrated API for Mixing Graph and Relational Queries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/SparkSummit/graphframes-graph-queries-in-spark-sql" rel="nofollow"&gt;Graph queries in Spark SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://graphframes.github.io/index.html" rel="nofollow"&gt;Graphframes Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/products" rel="nofollow"&gt;Elastic Producs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/subscriptions" rel="nofollow"&gt;Elastic Subscriptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html" rel="nofollow"&gt;Elasticsearch Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spujadas/elk-docker"&gt;spujadas elk-docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/deviantony/docker-elk"&gt;deviantony docker-elk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-author" class="anchor" aria-hidden="true" href="#author"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Author&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Roberto Rodriguez &lt;a href="https://twitter.com/Cyb3rWard0g" rel="nofollow"&gt;@Cyb3rWard0g&lt;/a&gt; &lt;a href="https://twitter.com/THE_HELK" rel="nofollow"&gt;@THE_HELK&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-current-committers" class="anchor" aria-hidden="true" href="#current-committers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Current Committers&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Nate Guagenti &lt;a href="https://twitter.com/neu5ron" rel="nofollow"&gt;@neu5ron&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h1&gt;
&lt;p&gt;There are a few things that I would like to accomplish with the HELK as shown in the To-Do list below. I would love to make the HELK a stable build for everyone in the community. If you are interested on making this build a more robust one and adding some cool features to it, PLEASE feel free to submit a pull request. #SharingIsCaring&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license-gpl-30" class="anchor" aria-hidden="true" href="#license-gpl-30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License: GPL-3.0&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/Cyb3rWard0g/HELK/blob/master/LICENSE"&gt; HELK's GNU General Public License&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-to-do" class="anchor" aria-hidden="true" href="#to-do"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TO-Do&lt;/h1&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Kubernetes Cluster Migration&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; OSQuery Data Ingestion&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; MITRE ATT&amp;amp;CK mapping to logs or dashboards&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Cypher for Apache Spark Integration (Adding option for Zeppelin Notebook)&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Test and integrate neo4j spark connectors with build&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add more network data sources (i.e Bro)&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Research &amp;amp; integrate spark structured direct streaming&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Packer Images&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Terraform integration (AWS, Azure, GC)&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Add more Jupyter Notebooks to teach the basics&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Auditd beat intergation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More coming soon...&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Cyb3rWard0g</author><guid isPermaLink="false">https://github.com/Cyb3rWard0g/HELK</guid><pubDate>Mon, 20 Jan 2020 00:06:00 GMT</pubDate></item><item><title>AtsushiSakai/PythonRobotics #7 in Jupyter Notebook, Today</title><link>https://github.com/AtsushiSakai/PythonRobotics</link><description>&lt;p&gt;&lt;i&gt;Python sample codes for robotics algorithms.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true" align="right" width="300" alt="header pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pythonrobotics" class="anchor" aria-hidden="true" href="#pythonrobotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PythonRobotics&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/AtsushiSakai/PythonRobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/58f87d5d3604646322c28abd8c5a9b2faa05fa51/68747470733a2f2f7472617669732d63692e6f72672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/AtsushiSakai/PythonRobotics.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pythonrobotics.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a60f894ef011c8a7e648348c16aabfdfb603613a/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f707974686f6e726f626f746963732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/pythonrobotics/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2e66a00c9dcf7ecc1f24189c6055aa7e6da233dc/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f73623237396b787576316265333931673f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/AtsushiSakai/PythonRobotics?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c26144817eba34b4ee9f9a6aee913e6b466218b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4174737573686953616b61692f507974686f6e526f626f746963732f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/AtsushiSakai/PythonRobotics/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4c3af4cd47bb2ea2c71cac274f1f7dd392eea893/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: Python" data-canonical-src="https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;amp;logoWidth=18" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/overview/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3cd55e61ef2e22ff00427b50b9e7f1c3547de91/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6174737573686973616b61692f707974686f6e726f626f746963732f62616467652f6d6173746572" alt="CodeFactor" data-canonical-src="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/badge/master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/AtsushiSakai/PythonRobotics"&gt;&lt;img src="https://camo.githubusercontent.com/230f0a1eaa529fa727cad2c9d3c1ace4738bd25d/68747470733a2f2f746f6b65692e72732f62312f6769746875622f4174737573686953616b61692f507974686f6e526f626f74696373" alt="tokei" data-canonical-src="https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Python codes for robotics algorithm.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-this"&gt;What is this?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#localization"&gt;Localization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#extended-kalman-filter-localization"&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#particle-filter-localization"&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#histogram-filter-localization"&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#mapping"&gt;Mapping&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#gaussian-grid-map"&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ray-casting-grid-map"&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lidar-to-grid-map"&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#k-means-object-clustering"&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rectangle-fitting"&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#slam"&gt;SLAM&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#iterative-closest-point-icp-matching"&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fastslam-10"&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-planning"&gt;Path Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dynamic-window-approach"&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-search"&gt;Grid based search&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dijkstra-algorithm"&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-algorithm"&gt;A* algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#potential-field-algorithm"&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-coverage-path-planning"&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#state-lattice-planning"&gt;State Lattice Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#biased-polar-sampling"&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lane-sampling"&gt;Lane sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilistic-road-map-prm-planning"&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rapidly-exploring-random-trees-rrt"&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#rrt"&gt;RRT*&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rrt-with-reeds-shepp-path"&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-rrt"&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#quintic-polynomials-planning"&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reeds-shepp-planning"&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-based-path-planning"&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimal-trajectory-in-a-frenet-frame"&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-tracking"&gt;Path Tracking&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#move-to-a-pose-control"&gt;move to a pose control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stanley-control"&gt;Stanley control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rear-wheel-feedback-control"&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model-predictive-speed-and-steering-control"&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nonlinear-model-predictive-control-with-c-gmres"&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation"&gt;Arm Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#n-joint-arm-to-point-control"&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation-with-obstacle-avoidance"&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#aerial-navigation"&gt;Aerial Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#drone-3d-trajectory-following"&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rocket-powered-landing"&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipedal"&gt;Bipedal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#bipedal-planner-with-inverted-pendulum"&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-case"&gt;Use-case&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support"&gt;Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-this" class="anchor" aria-hidden="true" href="#what-is-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this?&lt;/h1&gt;
&lt;p&gt;This is a Python code collection of robotics algorithms, especially for autonomous navigation.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Easy to read for understanding each algorithm's basic idea.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimum dependency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See this paper for more details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href="https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Python 3.7.x (2.7 is not supported)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;numpy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scipy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;matplotlib&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pandas&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.cvxpy.org/index.html" rel="nofollow"&gt;cvxpy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt;
&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt;
&lt;p&gt;You can check the full documentation online: &lt;a href="https://pythonrobotics.readthedocs.io/" rel="nofollow"&gt;https://pythonrobotics.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All animation gifs are stored here: &lt;a href="https://github.com/AtsushiSakai/PythonRoboticsGifs"&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Clone this repo.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone &lt;a href="https://github.com/AtsushiSakai/PythonRobotics.git"&gt;https://github.com/AtsushiSakai/PythonRobotics.git&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;cd PythonRobotics/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Install the required libraries. You can use environment.yml with conda command.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;conda env create -f environment.yml&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;p&gt;Execute python script in each directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add star to this repo if you like it &lt;g-emoji class="g-emoji" alias="smiley" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png"&gt;😃&lt;/g-emoji&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-localization" class="anchor" aria-hidden="true" href="#localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Localization&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-extended-kalman-filter-localization" class="anchor" aria-hidden="true" href="#extended-kalman-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extended Kalman Filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif" width="640" alt="EKF pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb"&gt;Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-particle-filter-localization" class="anchor" aria-hidden="true" href="#particle-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Particle filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt;
&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt;
&lt;p&gt;and the red line is estimated trajectory with PF.&lt;/p&gt;
&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt;
&lt;p&gt;This measurements are used for PF localization.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-histogram-filter-localization" class="anchor" aria-hidden="true" href="#histogram-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Histogram filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt;
&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt;
&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt;
&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt;
&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt;
&lt;p&gt;Initial position is not needed.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-mapping" class="anchor" aria-hidden="true" href="#mapping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mapping&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-grid-map" class="anchor" aria-hidden="true" href="#gaussian-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ray-casting-grid-map" class="anchor" aria-hidden="true" href="#ray-casting-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ray casting grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lidar-to-grid-map" class="anchor" aria-hidden="true" href="#lidar-to-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lidar to grid map&lt;/h2&gt;
&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Mapping/lidar_to_grid_map/animation.gif"&gt;&lt;img src="Mapping/lidar_to_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k-means-object-clustering" class="anchor" aria-hidden="true" href="#k-means-object-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;k-means object clustering&lt;/h2&gt;
&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rectangle-fitting" class="anchor" aria-hidden="true" href="#rectangle-fitting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rectangle fitting&lt;/h2&gt;
&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-slam" class="anchor" aria-hidden="true" href="#slam"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SLAM&lt;/h1&gt;
&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-iterative-closest-point-icp-matching" class="anchor" aria-hidden="true" href="#iterative-closest-point-icp-matching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt;
&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt;
&lt;p&gt;It can calculate a rotation matrix and a translation vector between points to points.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf" rel="nofollow"&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fastslam-10" class="anchor" aria-hidden="true" href="#fastslam-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FastSLAM 1.0&lt;/h2&gt;
&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt;
&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt;
&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt;
&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm" rel="nofollow"&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-planning" class="anchor" aria-hidden="true" href="#path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Planning&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-dynamic-window-approach" class="anchor" aria-hidden="true" href="#dynamic-window-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamic Window Approach&lt;/h2&gt;
&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf" rel="nofollow"&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-grid-based-search" class="anchor" aria-hidden="true" href="#grid-based-search"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based search&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-dijkstra-algorithm" class="anchor" aria-hidden="true" href="#dijkstra-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dijkstra algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with Dijkstra's algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-a-algorithm" class="anchor" aria-hidden="true" href="#a-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A* algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with A star algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-potential-field-algorithm" class="anchor" aria-hidden="true" href="#potential-field-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Potential Field algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf" rel="nofollow"&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-grid-based-coverage-path-planning" class="anchor" aria-hidden="true" href="#grid-based-coverage-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based coverage path planning&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-state-lattice-planning" class="anchor" aria-hidden="true" href="#state-lattice-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;State Lattice Planning&lt;/h2&gt;
&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt;
&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328" rel="nofollow"&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf" rel="nofollow"&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-biased-polar-sampling" class="anchor" aria-hidden="true" href="#biased-polar-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Biased polar sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lane-sampling" class="anchor" aria-hidden="true" href="#lane-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lane sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-probabilistic-road-map-prm-planning" class="anchor" aria-hidden="true" href="#probabilistic-road-map-prm-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif" alt="PRM" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt;
&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt;
&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt;
&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Probabilistic_roadmap" rel="nofollow"&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rapidly-exploring-random-trees-rrt" class="anchor" aria-hidden="true" href="#rapidly-exploring-random-trees-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-rrt" class="anchor" aria-hidden="true" href="#rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT*&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt;
&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1005.0416" rel="nofollow"&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-rrt-with-reeds-shepp-path" class="anchor" aria-hidden="true" href="#rrt-with-reeds-shepp-path"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT* with reeds-shepp path&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif" alt="Robotics/animation.gif at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lqr-rrt" class="anchor" aria-hidden="true" href="#lqr-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR-RRT*&lt;/h3&gt;
&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt;
&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif" alt="LQRRRT" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://lis.csail.mit.edu/pubs/perez-icra12.pdf" rel="nofollow"&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/MahanFathi/LQR-RRTstar"&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quintic-polynomials-planning" class="anchor" aria-hidden="true" href="#quintic-polynomials-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quintic polynomials planning&lt;/h2&gt;
&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It can calculate 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/637936/" rel="nofollow"&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-reeds-shepp-planning" class="anchor" aria-hidden="true" href="#reeds-shepp-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reeds Shepp planning&lt;/h2&gt;
&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://planning.cs.uiuc.edu/node822.html" rel="nofollow"&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf" rel="nofollow"&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ghliu/pyReedsShepp"&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lqr-based-path-planning" class="anchor" aria-hidden="true" href="#lqr-based-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR based path planning&lt;/h2&gt;
&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-optimal-trajectory-in-a-frenet-frame" class="anchor" aria-hidden="true" href="#optimal-trajectory-in-a-frenet-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt;
&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt;
&lt;p&gt;The red line is predicted path.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf" rel="nofollow"&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Cj6tAQe7UCY" rel="nofollow"&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-tracking" class="anchor" aria-hidden="true" href="#path-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Tracking&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-move-to-a-pose-control" class="anchor" aria-hidden="true" href="#move-to-a-pose-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;move to a pose control&lt;/h2&gt;
&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://link.springer.com/book/10.1007/978-3-642-20144-8" rel="nofollow"&gt;P. I. Corke, "Robotics, Vision and Control" | SpringerLink p102&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stanley-control" class="anchor" aria-hidden="true" href="#stanley-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stanley control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://robots.stanford.edu/papers/thrun.stanley05.pdf" rel="nofollow"&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf" rel="nofollow"&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-rear-wheel-feedback-control" class="anchor" aria-hidden="true" href="#rear-wheel-feedback-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rear wheel feedback control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1604.07446" rel="nofollow"&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-linearquadratic-regulator-lqr-speed-and-steering-control" class="anchor" aria-hidden="true" href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/5940562/" rel="nofollow"&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-model-predictive-speed-and-steering-control" class="anchor" aria-hidden="true" href="#model-predictive-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model predictive speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif" width="640" alt="MPC pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb"&gt;notebook&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://grauonline.de/wordpress/?page_id=3244" rel="nofollow"&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nonlinear-model-predictive-control-with-c-gmres" class="anchor" aria-hidden="true" href="#nonlinear-model-predictive-control-with-c-gmres"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt;
&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-arm-navigation" class="anchor" aria-hidden="true" href="#arm-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-n-joint-arm-to-point-control" class="anchor" aria-hidden="true" href="#n-joint-arm-to-point-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;N joint arm to point control&lt;/h2&gt;
&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt;
&lt;p&gt;This is a interactive simulation.&lt;/p&gt;
&lt;p&gt;You can set the goal position of the end effector with left-click on the ploting area.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-arm-navigation-with-obstacle-avoidance" class="anchor" aria-hidden="true" href="#arm-navigation-with-obstacle-avoidance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm navigation with obstacle avoidance&lt;/h2&gt;
&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-aerial-navigation" class="anchor" aria-hidden="true" href="#aerial-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-drone-3d-trajectory-following" class="anchor" aria-hidden="true" href="#drone-3d-trajectory-following"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;drone 3d trajectory following&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rocket-powered-landing" class="anchor" aria-hidden="true" href="#rocket-powered-landing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;rocket powered landing&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-bipedal" class="anchor" aria-hidden="true" href="#bipedal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bipedal&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-bipedal-planner-with-inverted-pendulum" class="anchor" aria-hidden="true" href="#bipedal-planner-with-inverted-pendulum"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bipedal planner with inverted pendulum&lt;/h2&gt;
&lt;p&gt;This is a bipedal planner for modifying footsteps with inverted pendulum.&lt;/p&gt;
&lt;p&gt;You can set the footsteps and the planner will modify those automatically.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-use-case" class="anchor" aria-hidden="true" href="#use-case"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use-case&lt;/h1&gt;
&lt;p&gt;If this project helps your robotics project, please let me know with &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Your robot's video, which is using PythonRobotics, is very welcome!!&lt;/p&gt;
&lt;p&gt;This is a list of other user's comment and references:&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;users_comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h1&gt;
&lt;p&gt;A small PR like bug fix is welcome.&lt;/p&gt;
&lt;p&gt;If your PR is merged multiple times, I will add your account to the author list.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;If you use this project's code for your academic work, we encourage you to cite &lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;our papers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.patreon.com/myenigma" rel="nofollow"&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/myenigmapay/" rel="nofollow"&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can add your name or your company logo in README if you are a patron.&lt;/p&gt;
&lt;p&gt;E-mail consultant is also available.&lt;/p&gt;
&lt;p&gt;　&lt;/p&gt;
&lt;p&gt;Your comment using &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt; is also welcome.&lt;/p&gt;
&lt;p&gt;This is a list: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;Users comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/"&gt;Atsushi Sakai&lt;/a&gt; (&lt;a href="https://twitter.com/Atsushi_twi" rel="nofollow"&gt;@Atsushi_twi&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/daniel-s-ingram"&gt;Daniel Ingram&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/jwdinius"&gt;Joe Dinius&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/karanchawla"&gt;Karan Chawla&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/araffin"&gt;Antonin RAFFIN&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AlexisTM"&gt;Alexis Paques&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsasaki0109"&gt;Ryohei Sasaki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/goktug97"&gt;Göktuğ Karakaşlı&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Gjacquenot"&gt;Guillaume Jacquenot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AtsushiSakai</author><guid isPermaLink="false">https://github.com/AtsushiSakai/PythonRobotics</guid><pubDate>Mon, 20 Jan 2020 00:07:00 GMT</pubDate></item><item><title>aymericdamien/TensorFlow-Examples #8 in Jupyter Notebook, Today</title><link>https://github.com/aymericdamien/TensorFlow-Examples</link><description>&lt;p&gt;&lt;i&gt;TensorFlow Tutorial and Examples for Beginners (support TF v1 &amp; v2)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-examples" class="anchor" aria-hidden="true" href="#tensorflow-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Examples&lt;/h1&gt;
&lt;p&gt;This tutorial was designed for easily diving into TensorFlow, through examples. For readability, it includes both notebooks and source codes with explanation, for both TF v1 &amp;amp; v2.&lt;/p&gt;
&lt;p&gt;It is suitable for beginners who want to find clear and concise examples about TensorFlow. Besides the traditional 'raw' TensorFlow implementations, you can also find the latest TensorFlow API practices (such as &lt;code&gt;layers&lt;/code&gt;, &lt;code&gt;estimator&lt;/code&gt;, &lt;code&gt;dataset&lt;/code&gt;, ...).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update (08/17/2019):&lt;/strong&gt; Added new &lt;a href="tensorflow_v2"&gt;TensorFlow 2.0 examples&lt;/a&gt;! (more coming soon).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you are using older TensorFlow version (0.11 and under), please take a &lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/tree/0.11"&gt;look here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tutorial-index" class="anchor" aria-hidden="true" href="#tutorial-index"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorial index&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-0---prerequisite" class="anchor" aria-hidden="true" href="#0---prerequisite"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;0 - Prerequisite&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/ml_introduction.ipynb"&gt;Introduction to Machine Learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb"&gt;Introduction to MNIST Dataset&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-1---introduction" class="anchor" aria-hidden="true" href="#1---introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1 - Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hello World&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/1_Introduction/helloworld.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/helloworld.py"&gt;code&lt;/a&gt;). Very simple example to learn how to print "hello world" using TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Basic Operations&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/1_Introduction/basic_operations.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/basic_operations.py"&gt;code&lt;/a&gt;). A simple example that cover TensorFlow basic operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TensorFlow Eager API basics&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/1_Introduction/basic_eager_api.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/basic_eager_api.py"&gt;code&lt;/a&gt;). Get started with TensorFlow's Eager API.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-2---basic-models" class="anchor" aria-hidden="true" href="#2---basic-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2 - Basic Models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py"&gt;code&lt;/a&gt;). Implement a Linear Regression with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear Regression (eager api)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression_eager_api.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression_eager_api.py"&gt;code&lt;/a&gt;). Implement a Linear Regression using TensorFlow's Eager API.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py"&gt;code&lt;/a&gt;). Implement a Logistic Regression with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logistic Regression (eager api)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression_eager_api.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression_eager_api.py"&gt;code&lt;/a&gt;). Implement a Logistic Regression using TensorFlow's Eager API.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nearest Neighbor&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/nearest_neighbor.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/nearest_neighbor.py"&gt;code&lt;/a&gt;). Implement Nearest Neighbor algorithm with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K-Means&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/kmeans.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/kmeans.py"&gt;code&lt;/a&gt;). Build a K-Means classifier with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random Forest&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/random_forest.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/random_forest.py"&gt;code&lt;/a&gt;). Build a Random Forest classifier with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradient Boosted Decision Tree (GBDT)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/gradient_boosted_decision_tree.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/gradient_boosted_decision_tree.py"&gt;code&lt;/a&gt;). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word2Vec (Word Embedding)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/word2vec.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/word2vec.py"&gt;code&lt;/a&gt;). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-3---neural-networks" class="anchor" aria-hidden="true" href="#3---neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3 - Neural Networks&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-supervised" class="anchor" aria-hidden="true" href="#supervised"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supervised&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simple Neural Network&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network_raw.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network_raw.py"&gt;code&lt;/a&gt;). Build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. Raw TensorFlow implementation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simple Neural Network (tf.layers/estimator api)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network.py"&gt;code&lt;/a&gt;). Use TensorFlow 'layers' and 'estimator' API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simple Neural Network (eager api)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network_eager_api.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network_eager_api.py"&gt;code&lt;/a&gt;). Use TensorFlow Eager API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional Neural Network&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network_raw.py"&gt;code&lt;/a&gt;). Build a convolutional neural network to classify MNIST digits dataset. Raw TensorFlow implementation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional Neural Network (tf.layers/estimator api)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py"&gt;code&lt;/a&gt;). Use TensorFlow 'layers' and 'estimator' API to build a convolutional neural network to classify MNIST digits dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py"&gt;code&lt;/a&gt;). Build a recurrent neural network (LSTM) to classify MNIST digits dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bi-directional Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/bidirectional_rnn.py"&gt;code&lt;/a&gt;). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Recurrent Neural Network (LSTM)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py"&gt;code&lt;/a&gt;). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of different length.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-unsupervised" class="anchor" aria-hidden="true" href="#unsupervised"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Unsupervised&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Auto-Encoder&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/autoencoder.py"&gt;code&lt;/a&gt;). Build an auto-encoder to encode an image to a lower dimension and re-construct it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variational Auto-Encoder&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/variational_autoencoder.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/variational_autoencoder.py"&gt;code&lt;/a&gt;). Build a variational auto-encoder (VAE), to encode and generate images from noise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GAN (Generative Adversarial Networks)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/gan.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/gan.py"&gt;code&lt;/a&gt;). Build a Generative Adversarial Network (GAN) to generate images from noise.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DCGAN (Deep Convolutional Generative Adversarial Networks)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/dcgan.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dcgan.py"&gt;code&lt;/a&gt;). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-4---utilities" class="anchor" aria-hidden="true" href="#4---utilities"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4 - Utilities&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Save and Restore a model&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/save_restore_model.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/save_restore_model.py"&gt;code&lt;/a&gt;). Save and Restore a model with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensorboard - Graph and loss visualization&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/tensorboard_basic.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/tensorboard_basic.py"&gt;code&lt;/a&gt;). Use Tensorboard to visualize the computation Graph and plot the loss.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensorboard - Advanced visualization&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/4_Utils/tensorboard_advanced.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/tensorboard_advanced.py"&gt;code&lt;/a&gt;). Going deeper into Tensorboard; visualize the variables, gradients, and more...&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-5---data-management" class="anchor" aria-hidden="true" href="#5---data-management"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5 - Data Management&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Build an image dataset&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/5_DataManagement/build_an_image_dataset.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/build_an_image_dataset.py"&gt;code&lt;/a&gt;). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TensorFlow Dataset API&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/tensorflow_dataset_api.py"&gt;code&lt;/a&gt;). Introducing TensorFlow Dataset API for optimizing the input data pipeline.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load and Parse data&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/5_DataManagement/load_data.ipynb"&gt;notebook&lt;/a&gt;). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build and Load TFRecords&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/5_DataManagement/tfrecords.ipynb"&gt;notebook&lt;/a&gt;). Convert data into TFRecords format, and load them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Image Transformation (i.e. Image Augmentation)&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/5_DataManagement/image_transformation.ipynb"&gt;notebook&lt;/a&gt;). Apply various image augmentation techniques, to generate distorted images for training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-6---multi-gpu" class="anchor" aria-hidden="true" href="#6---multi-gpu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6 - Multi GPU&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Basic Operations on multi-GPU&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/6_MultiGPU/multigpu_basics.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/6_MultiGPU/multigpu_basics.py"&gt;code&lt;/a&gt;). A simple example to introduce multi-GPU in TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Train a Neural Network on multi-GPU&lt;/strong&gt; (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/6_MultiGPU/multigpu_cnn.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/6_MultiGPU/multigpu_cnn.py"&gt;code&lt;/a&gt;). A clear and simple TensorFlow implementation to train a convolutional neural network on multiple GPUs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tensorflow-20" class="anchor" aria-hidden="true" href="#tensorflow-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0&lt;/h2&gt;
&lt;p&gt;The tutorial index for TF v2 is available here: &lt;a href="tensorflow_v2"&gt;TensorFlow 2.0 Examples&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-dataset" class="anchor" aria-hidden="true" href="#dataset"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dataset&lt;/h2&gt;
&lt;p&gt;Some examples require MNIST dataset for training and testing. Don't worry, this dataset will automatically be downloaded when running examples.
MNIST is a database of handwritten digits, for a quick description of that dataset, you can check &lt;a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb"&gt;this notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Official Website: &lt;a href="http://yann.lecun.com/exdb/mnist/" rel="nofollow"&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;To download all the examples, simply clone this repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/aymericdamien/TensorFlow-Examples
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run them, you also need the latest version of TensorFlow. To install it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or (with GPU support):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow_gpu
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more details about TensorFlow installation, you can check &lt;a href="https://www.tensorflow.org/install/" rel="nofollow"&gt;TensorFlow Installation Guide&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-more-examples" class="anchor" aria-hidden="true" href="#more-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Examples&lt;/h2&gt;
&lt;p&gt;The following examples are coming from &lt;a href="https://github.com/tflearn/tflearn"&gt;TFLearn&lt;/a&gt;, a library that provides a simplified interface for TensorFlow. You can have a look, there are many &lt;a href="https://github.com/tflearn/tflearn/tree/master/examples"&gt;examples&lt;/a&gt; and &lt;a href="http://tflearn.org/doc_index/#api" rel="nofollow"&gt;pre-built operations and layers&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tflearn/tflearn/blob/master/tutorials/intro/quickstart.md"&gt;TFLearn Quickstart&lt;/a&gt;. Learn the basics of TFLearn through a concrete machine learning task. Build and train a deep neural network classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tflearn/tflearn/blob/master/examples"&gt;TFLearn Examples&lt;/a&gt;. A large collection of examples using TFLearn.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>aymericdamien</author><guid isPermaLink="false">https://github.com/aymericdamien/TensorFlow-Examples</guid><pubDate>Mon, 20 Jan 2020 00:08:00 GMT</pubDate></item><item><title>rasbt/deeplearning-models #9 in Jupyter Notebook, Today</title><link>https://github.com/rasbt/deeplearning-models</link><description>&lt;p&gt;&lt;i&gt;A collection of various deep learning architectures, models, and tips&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.7" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Models&lt;/h1&gt;
&lt;p&gt;A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-traditional-machine-learning" class="anchor" aria-hidden="true" href="#traditional-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Traditional Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression (Multinomial Logistic Regression)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression with MLxtend's plot_decision_regions on Iris&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multilayer-perceptrons" class="anchor" aria-hidden="true" href="#multilayer-perceptrons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multilayer Perceptrons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multilayer Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Dropout&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Batch Normalization&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Backpropagation from Scratch&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-basic" class="anchor" aria-hidden="true" href="#basic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network with He Initialization&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-he-init.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-he-init.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-concepts" class="anchor" aria-hidden="true" href="#concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concepts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Replacing Fully-Connnected by Equivalent Convolutional Layers&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/fc-to-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/fc-to-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-fully-convolutional" class="anchor" aria-hidden="true" href="#fully-convolutional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully Convolutional&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fully Convolutional Neural Network&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-allconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-allconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-lenet" class="anchor" aria-hidden="true" href="#lenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LeNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5 on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-alexnet" class="anchor" aria-hidden="true" href="#alexnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlexNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AlexNet on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-vgg" class="anchor" aria-hidden="true" href="#vgg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VGG&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network VGG-16&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VGG-16 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network VGG-19&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg19.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg19.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-densenet" class="anchor" aria-hidden="true" href="#densenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DenseNet-121 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DenseNet-121 Image Classifier Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-resnet" class="anchor" aria-hidden="true" href="#resnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ResNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ResNet and Residual Blocks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/resnet-ex-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/resnet-ex-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Object Classifier Trained on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-152 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-network-in-network" class="anchor" aria-hidden="true" href="#network-in-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network in Network&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Network in Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-normalization-layers" class="anchor" aria-hidden="true" href="#normalization-layers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Normalization Layers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BatchNorm before and after Activation for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Filter Response Normalization for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-metric-learning" class="anchor" aria-hidden="true" href="#metric-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metric Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Siamese Network with Multilayer Perceptrons&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/metric/siamese-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/metric/siamese-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-autoencoders" class="anchor" aria-hidden="true" href="#autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autoencoders&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-fully-connected-autoencoders" class="anchor" aria-hidden="true" href="#fully-connected-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully-connected Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoder (MNIST)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autoencoder (MNIST) + Scikit-Learn Random Forest Classifier&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-convolutional-autoencoders" class="anchor" aria-hidden="true" href="#convolutional-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions / Transposed Convolutions&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions and Continuous Jaccard Distance&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions (without pooling operations)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-variational-autoencoders" class="anchor" aria-hidden="true" href="#variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-conditional-variational-autoencoders" class="anchor" aria-hidden="true" href="#conditional-variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fully Connected GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Fully Connected Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST with Label Smoothing&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/dc-wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/dc-wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph-neural-networks-gnns" class="anchor" aria-hidden="true" href="#graph-neural-networks-gnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph Neural Networks (GNNs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most Basic Graph Neural Network with Gaussian Filter on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Edge Prediction on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Spectral Graph Convolution on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-recurrent-neural-networks-rnns" class="anchor" aria-hidden="true" href="#recurrent-neural-networks-rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-one-sentiment-analysis--classification" class="anchor" aria-hidden="true" href="#many-to-one-sentiment-analysis--classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-one: Sentiment Analysis / Classification&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple single-layer RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple single-layer RNN with packed sequences to ignore padding characters (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB) and pre-trained GloVe word vectors&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells and Own Dataset in CSV Format (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with GRU cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer bi-directional RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (AG News)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Yelp Review Polarity)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Amazon Review Polarity)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-many--sequence-to-sequence" class="anchor" aria-hidden="true" href="#many-to-many--sequence-to-sequence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-Many / Sequence-to-Sequence&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple character RNN to generate new text (Charles Dickens)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ordinal-regression" class="anchor" aria-hidden="true" href="#ordinal-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ordinal Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ordinal Regression CNN -- CORAL w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Niu et al. 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Beckham and Pal 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cyclical Learning Rate&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cyclical-learning-rate.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cyclical-learning-rate.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Annealing with Increasing the Batch Size (w. CIFAR-10 &amp;amp; AlexNet)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Gradient Clipping (w. MLP on MNIST)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/gradclipping_mlp.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/gradclipping_mlp.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-transfer-learning" class="anchor" aria-hidden="true" href="#transfer-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transfer Learning Example (VGG16 pre-trained on ImageNet for Cifar-10)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;   [PyTorch: &lt;a href="pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#pytorch-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Custom Data Loader Example for PNG Files&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing" class="anchor" aria-hidden="true" href="#training-and-preprocessing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generating Validation Set Splits&lt;br&gt;
[PyTorch]: &lt;a href="pytorch_ipynb/mechanics/validation-splits.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/validation-splits.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Dataloading with Pinned Memory&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Standardizing Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-standardized.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-standardized.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Image Transformation Examples&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Char-RNN with Own Text File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sentiment Classification RNN with Own CSV File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-parallel-computing" class="anchor" aria-hidden="true" href="#parallel-computing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parallel Computing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using Multiple GPUs with DataParallel -- VGG-16 Gender Classifier on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sequential API and hooks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/mlp-sequential.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/mlp-sequential.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Weight Sharing Within a Layer&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Plotting Live Training Performance in Jupyter Notebooks with just Matplotlib&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-autograd" class="anchor" aria-hidden="true" href="#autograd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autograd&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Getting Gradients of an Intermediate Variable in PyTorch&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/manual-gradients.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/manual-gradients.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tensorflow-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#tensorflow-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets-1" class="anchor" aria-hidden="true" href="#custom-datasets-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Storing an Image Dataset for Minibatch Training using HDF5&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Input Pipelines to Read Data from TFRecords Files&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/tfrecords.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/tfrecords.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Queue Runners to Feed Images Directly from Disk&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/file-queues.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/file-queues.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using TensorFlow's Dataset API&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/dataset-api.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/dataset-api.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing-1" class="anchor" aria-hidden="true" href="#training-and-preprocessing-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Saving and Loading Trained Models -- from TensorFlow Checkpoint Files and NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/deeplearning-models</guid><pubDate>Mon, 20 Jan 2020 00:09:00 GMT</pubDate></item><item><title>realpython/materials #10 in Jupyter Notebook, Today</title><link>https://github.com/realpython/materials</link><description>&lt;p&gt;&lt;i&gt;Bonus materials, exercises, and example projects for our Python tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-python-materials" class="anchor" aria-hidden="true" href="#real-python-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real Python Materials&lt;/h1&gt;
&lt;p&gt;Bonus materials, exercises, and example projects for our &lt;a href="https://realpython.com" rel="nofollow"&gt;Python tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Build Status: &lt;a href="https://circleci.com/gh/realpython/materials" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/991534e54579264db6c49f9225646cb77dd8fd6a/68747470733a2f2f636972636c6563692e636f6d2f67682f7265616c707974686f6e2f6d6174657269616c732e7376673f7374796c653d737667" alt="CircleCI" data-canonical-src="https://circleci.com/gh/realpython/materials.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-code-style-checks" class="anchor" aria-hidden="true" href="#running-code-style-checks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Code Style Checks&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://flake8.pycqa.org/en/latest/" rel="nofollow"&gt;flake8&lt;/a&gt; and &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; to ensure a consistent code style for all of our sample code in this repository.&lt;/p&gt;
&lt;p&gt;Run the following commands to validate your code against the linters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ flake8
$ black --check &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-running-python-code-formatter" class="anchor" aria-hidden="true" href="#running-python-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Python Code Formatter&lt;/h2&gt;
&lt;p&gt;We're using a tool called &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; on this repo to ensure consistent formatting. On CI it runs in "check" mode to ensure any new files added to the repo are following PEP 8. If you see linter warnings that say something like "would reformat some_file.py" it means black disagrees with your formatting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The easiest way to resolve these errors is to just run Black locally on the code and then committing those changes, as explained below.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To automatically re-format your code to be consistent with our code style guidelines, run &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; in the repository root folder:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ black &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>realpython</author><guid isPermaLink="false">https://github.com/realpython/materials</guid><pubDate>Mon, 20 Jan 2020 00:10:00 GMT</pubDate></item><item><title>CoreyMSchafer/code_snippets #11 in Jupyter Notebook, Today</title><link>https://github.com/CoreyMSchafer/code_snippets</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-code_snippets" class="anchor" aria-hidden="true" href="#code_snippets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;code_snippets&lt;/h1&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CoreyMSchafer</author><guid isPermaLink="false">https://github.com/CoreyMSchafer/code_snippets</guid><pubDate>Mon, 20 Jan 2020 00:11:00 GMT</pubDate></item><item><title>nndl/exercise #12 in Jupyter Notebook, Today</title><link>https://github.com/nndl/exercise</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;p&gt;No README was found for this project.&lt;/p&gt;</description><author>nndl</author><guid isPermaLink="false">https://github.com/nndl/exercise</guid><pubDate>Mon, 20 Jan 2020 00:12:00 GMT</pubDate></item><item><title>joanby/masters-desarrollo-udemy #13 in Jupyter Notebook, Today</title><link>https://github.com/joanby/masters-desarrollo-udemy</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introducción-a-tecnologías-del-desarrollo-con-los-mejores" class="anchor" aria-hidden="true" href="#introducción-a-tecnologías-del-desarrollo-con-los-mejores"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.udemy.com/course/masters-desarrollo/" rel="nofollow"&gt;Introducción a Tecnologías del Desarrollo con los Mejores&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-conoce-las-tecnologías-usadas-por-los-12-masters-python-php-c-mysql-unity-angular-golang-cobol-reactjs-spring-firebase" class="anchor" aria-hidden="true" href="#conoce-las-tecnologías-usadas-por-los-12-masters-python-php-c-mysql-unity-angular-golang-cobol-reactjs-spring-firebase"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conoce las tecnologías usadas por los 12 Masters. Python php C# mysql Unity Angular Golang COBOL ReactJS Spring Firebase&lt;/h2&gt;
&lt;p&gt;Solo disponible de forma totalmente gratuita en &lt;a href="https://www.udemy.com/course/masters-desarrollo/" rel="nofollow"&gt;Udemy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Creado por Juan Gabriel Gomila Salas, Alvaro Chirou, Pablo Tilotta, Juan Fernando Urrego, Mariano Rivas, Agustin Navarro Galdon, Anartz Mugika Ledo, Alvaro Ospina, Global Mentoring, Andrés José Guzmán, Fernando Herrera, Juan Villalvazo&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-qué-vas-a-aprender-en-el-curso" class="anchor" aria-hidden="true" href="#qué-vas-a-aprender-en-el-curso"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;¿Qué vas a aprender en el curso?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Análisis y tratamiento de datos para Machine Learning e Inteligencia Artificial usando R y Python repasando en el camino los fundamentos matemáticos de algunos algoritmos&lt;/li&gt;
&lt;li&gt;Aprender sobre Firebase, Cloud Functions, Serveless, Angular y desplegar aplicaciones de Angular en Firebase hosting&lt;/li&gt;
&lt;li&gt;Aprenderán a programar COBOL desde 0 y podrán realizar sus primeros desarrollos con manejo de datos&lt;/li&gt;
&lt;li&gt;Crear un Generador CLI con NodeJS del template que especifiques para nuestros proyectos&lt;/li&gt;
&lt;li&gt;Desarrollo y programación de Videojuegos 2D con Unity 2019.X , C# y Photoshop CC 2020&lt;/li&gt;
&lt;li&gt;Aprenderás Seguridad informatica en Desarrollo y en entornos Cloud&lt;/li&gt;
&lt;li&gt;Aprenderán lenguaje GO desde 0, toda su sintaxis y secretos&lt;/li&gt;
&lt;li&gt;Crear una API GraphQL con NodeJS y Typescript de manera sencilla&lt;/li&gt;
&lt;li&gt;Publicar aplicaciones Angular en la nube con Zeit Now&lt;/li&gt;
&lt;li&gt;Comprende la arquitectura del modelo MVC y el patrón Inyección de Dependencia con Spring Framework&lt;/li&gt;
&lt;li&gt;Aprenderás los principios de la programación con PHP desde cero y paso a paso, no necesitas ningún conocimiento previo.&lt;/li&gt;
&lt;li&gt;Aprenderás programación Orientada a Objetos (POO) y ejecutar el patrón Modelo-Vista-Controlador (MVC) en todos los proyectos de programación que emprendas.&lt;/li&gt;
&lt;li&gt;Bases de datos Relacionales con MySQL y conexión a las bases de datos con protección PDO (PHP DATA OBJECT)&lt;/li&gt;
&lt;li&gt;Introducción a la programación con Kotlin el lenguaje para desarrollar apps Android&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-requisitos" class="anchor" aria-hidden="true" href="#requisitos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requisitos&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Es recomendable tener conocimientos previos de matemáticas de colegio e instituto sobretodo de estadística y de álgebra&lt;/li&gt;
&lt;li&gt;Un ordenador con conexión a internet para poder instalar todo el software y descargar todo el material del curso&lt;/li&gt;
&lt;li&gt;Curso de COBOL : Es necesario conocer algún lenguaje de programación que maneje datos, lógica y conceptos básicos de variables, rutinas y datos&lt;/li&gt;
&lt;li&gt;Curso de Lenguaje GO : Es recomendable contar con conocimientos básicos de programación.&lt;/li&gt;
&lt;li&gt;Generador de proyecto CLI con NodeJS: Es necesario tener conocimientos de Javascript&lt;/li&gt;
&lt;li&gt;API GraphQL: Necesario conocimiento de Typescript y Node Express&lt;/li&gt;
&lt;li&gt;Pubicar App Angular en Zeit Now: Muy recomendable tener conocimientos de Angular&lt;/li&gt;
&lt;li&gt;Para el Vidoejuego te recomiendo tener unity (ES GRATIS) y un editor de imágenes 2D (KRITA es gratis) o Photoshop&lt;/li&gt;
&lt;li&gt;Para la sección de Spring deben dominar el lenguaje Java SE y programación orientada a objetos (POO)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-descripción" class="anchor" aria-hidden="true" href="#descripción"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Descripción&lt;/h3&gt;
&lt;p&gt;Hola y bienvenidos al curso: Introducción a Tecnologías del Desarrollo con los Mejores
En este curso no hemos juntado 12 de los instructores más populares de Udemy para compartir de forma GRATUITA nuestra experiencia y conocimiento en tecnologías y desarrollo.
En Cada Sección del curso un instructor, Master del Desarrollo, te enseñará una tecnología diferente relacionada con su experiencia y especialización.
Es un curso muy completo con mas de mas de 65 horas en el que podrás introducirte y aprender las herramientas y tecnologías más utilizadas por estos 12 Masters del Desarrollo.
Te invitamos a que veas los videos de presentación de cada sección, así podrás elegir la temática y el instructor que mejor prefieras.
Te esperamos en el curso, un saludo y Muchas gracias.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-para-quién-es-este-curso" class="anchor" aria-hidden="true" href="#para-quién-es-este-curso"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;¿Para quién es este curso?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Desarrolladores que quieran iniciarse en el mundo del análisis de datos y el big data con R y Python&lt;/li&gt;
&lt;li&gt;Curso de COBOL : Todos aquellos desarrolladores que deseen aprender el lenguaje mejor pago del planeta.&lt;/li&gt;
&lt;li&gt;Curso de Lenguaje GO : Todos aquellos desarrolladores Web, que deseen conocer el Lenguaje BackEnd mas performante del planeta (creado por Google)&lt;/li&gt;
&lt;li&gt;Generador proyectos CLI: Desarrolladores que quieran crear sus propios generadores de proyectos mediante template para ejecutarlos con el CLI&lt;/li&gt;
&lt;li&gt;API GraphQL: Desarrolladores que quieren iniciarse en el desarrollo de APIs de GraphQL&lt;/li&gt;
&lt;li&gt;Publicar App en Zeit Now: Desarrolladores que quieren una alternativa gratuita para publicar sus proyectos&lt;/li&gt;
&lt;li&gt;Todos los que les guste la programación y quieran enfocarlo al sector de videojuegos&lt;/li&gt;
&lt;li&gt;Para sección Spring, personas que dominan Java SE y/o Java EE y quieran dar el salto con Spring Framework&lt;/li&gt;
&lt;li&gt;Para sección de Kotlin: Esta sección se imparte desde cero así que no es necesario saber programación ya que en ella se enseña&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>joanby</author><guid isPermaLink="false">https://github.com/joanby/masters-desarrollo-udemy</guid><pubDate>Mon, 20 Jan 2020 00:13:00 GMT</pubDate></item><item><title>nfmcclure/tensorflow_cookbook #14 in Jupyter Notebook, Today</title><link>https://github.com/nfmcclure/tensorflow_cookbook</link><description>&lt;p&gt;&lt;i&gt;Code for Tensorflow Machine Learning Cookbook&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/nfmcclure/tensorflow_cookbook/raw/master/images/book_covers.jpg"&gt;&lt;img src="https://github.com/nfmcclure/tensorflow_cookbook/raw/master/images/book_covers.jpg" width="400" height="250" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-tensorflow-machine-learning-cookbook" class="anchor" aria-hidden="true" href="#tensorflow-machine-learning-cookbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook" rel="nofollow"&gt;TensorFlow Machine Learning Cookbook&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-a-packt-publishing-book" class="anchor" aria-hidden="true" href="#a-packt-publishing-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook" rel="nofollow"&gt;A Packt Publishing Book&lt;/a&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-by-nick-mcclure" class="anchor" aria-hidden="true" href="#by-nick-mcclure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;By Nick McClure&lt;/h3&gt;
&lt;p&gt;=================&lt;/p&gt;
&lt;p&gt;Build: &lt;a href="https://travis-ci.org/nfmcclure/tensorflow_cookbook" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/737d325288c2818c84a603bc9ac6d63c24637330/68747470733a2f2f7472617669732d63692e6f72672f6e666d63636c7572652f74656e736f72666c6f775f636f6f6b626f6f6b2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/nfmcclure/tensorflow_cookbook.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;=================&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#ch-1-getting-started-with-tensorflow"&gt;Ch 1: Getting Started with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-2-the-tensorflow-way"&gt;Ch 2: The TensorFlow Way&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-3-linear-regression"&gt;Ch 3: Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-4-support-vector-machines"&gt;Ch 4: Support Vector Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-5-nearest-neighbor-methods"&gt;Ch 5: Nearest Neighbor Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-6-neural-networks"&gt;Ch 6: Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-7-natural-language-processing"&gt;Ch 7: Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-8-convolutional-neural-networks"&gt;Ch 8: Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-9-recurrent-neural-networks"&gt;Ch 9: Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-10-taking-tensorflow-to-production"&gt;Ch 10: Taking TensorFlow to Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-11-more-with-tensorflow"&gt;Ch 11: More with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-ch-1-getting-started-with-tensorflow" class="anchor" aria-hidden="true" href="#ch-1-getting-started-with-tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="01_Introduction#ch-1-getting-started-with-tensorflow"&gt;Ch 1: Getting Started with TensorFlow&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/01_How_TensorFlow_Works#introduction-to-how-tensorflow-graphs-work"&gt;
    &lt;img src="01_Introduction/images/01_outline.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/02_Creating_and_Using_Tensors#creating-and-using-tensors"&gt;
    &lt;img src="01_Introduction/images/02_variable.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/03_Using_Variables_and_Placeholders#variables-and-placeholders"&gt;
    &lt;img src="01_Introduction/images/03_placeholder.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/06_Implementing_Activation_Functions#activation-functions"&gt;
    &lt;img src="01_Introduction/images/06_activation_funs1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/06_Implementing_Activation_Functions#activation-functions"&gt;
    &lt;img src="01_Introduction/images/06_activation_funs2.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;This chapter intends to introduce the main objects and concepts in TensorFlow.  We also introduce how to access the data for the rest of the book and provide additional resources for learning about TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="01_Introduction/01_How_TensorFlow_Works#introduction-to-how-tensorflow-graphs-work"&gt;General Outline of TF Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we introduce TensorFlow and the general outline of how most TensorFlow algorithms work.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="01_Introduction/02_Creating_and_Using_Tensors#creating-and-using-tensors"&gt;Creating and Using Tensors&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to create and initialize tensors in TensorFlow.  We also depict how these operations appear in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="01_Introduction/03_Using_Variables_and_Placeholders#variables-and-placeholders"&gt;Using Variables and Placeholders&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to create and use variables and placeholders in TensorFlow.  We also depict how these operations appear in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="01_Introduction/04_Working_with_Matrices#working-with-matrices"&gt;Working with Matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Understanding how TensorFlow can work with matrices is crucial to understanding how the algorithms work.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="01_Introduction/05_Declaring_Operations#declaring-operations"&gt;Declaring Operations&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to use various mathematical operations in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="01_Introduction/06_Implementing_Activation_Functions#activation-functions"&gt;Implementing Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Activation functions are unique functions that TensorFlow has built in for your use in algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="01_Introduction/07_Working_with_Data_Sources#data-source-information"&gt;Working with Data Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to access all the various required data sources in the book.  There are also links describing the data sources and where they come from.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="01_Introduction/08_Additional_Resources#additional-resources"&gt;Additional Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Mostly official resources and papers.  The papers are TensorFlow papers or Deep Learning resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-2-the-tensorflow-way" class="anchor" aria-hidden="true" href="#ch-2-the-tensorflow-way"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="02_TensorFlow_Way#ch-2-the-tensorflow-way"&gt;Ch 2: The TensorFlow Way&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/01_Operations_as_a_Computational_Graph#operations-as-a-computational-graph"&gt;
    &lt;img src="02_TensorFlow_Way/images/01_Operations_on_a_Graph.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/02_Layering_Nested_Operations#multiple-operations-on-a-computational-graph"&gt;
    &lt;img src="02_TensorFlow_Way/images/02_Multiple_Operations.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/03_Working_with_Multiple_Layers#working-with-multiple-layers"&gt;
    &lt;img src="02_TensorFlow_Way/images/03_Multiple_Layers.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/04_Implementing_Loss_Functions#implementing-loss-functions"&gt;
    &lt;img src="02_TensorFlow_Way/images/04_loss_fun1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/05_Implementing_Back_Propagation#implementing-back-propagation"&gt;
    &lt;img src="02_TensorFlow_Way/images/04_loss_fun2.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/06_Working_with_Batch_and_Stochastic_Training#working-with-batch-and-stochastic-training"&gt;
    &lt;img src="02_TensorFlow_Way/images/06_Back_Propagation.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/07_Combining_Everything_Together#combining-everything-together"&gt;
    &lt;img src="02_TensorFlow_Way/images/07_Combing_Everything_Together.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/08_Evaluating_Models#evaluating-models"&gt;
    &lt;img src="02_TensorFlow_Way/images/08_Evaluating_Models.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;After we have established the basic objects and methods in TensorFlow, we now want to establish the components that make up TensorFlow algorithms.  We start by introducing computational graphs, and then move to loss functions and back propagation.  We end with creating a simple classifier and then show an example of evaluating regression and classification algorithms.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/01_Operations_as_a_Computational_Graph#operations-as-a-computational-graph"&gt;One Operation as a Computational Graph&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to create an operation on a computational graph and how to visualize it using Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/02_Layering_Nested_Operations#multiple-operations-on-a-computational-graph"&gt;Layering Nested Operations&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to create multiple operations on a computational graph and how to visualize them using Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/03_Working_with_Multiple_Layers#working-with-multiple-layers"&gt;Working with Multiple Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we extend the usage of the computational graph to create multiple layers and show how they appear in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/04_Implementing_Loss_Functions#implementing-loss-functions"&gt;Implementing Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In order to train a model, we must be able to evaluate how well it is doing. This is given by loss functions. We plot various loss functions and talk about the benefits and limitations of some.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/05_Implementing_Back_Propagation#implementing-back-propagation"&gt;Implementing Back Propagation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to use loss functions to iterate through data and back propagate errors for regression and classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/06_Working_with_Batch_and_Stochastic_Training#working-with-batch-and-stochastic-training"&gt;Working with Stochastic and Batch Training&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow makes it easy to use both batch and stochastic training. We show how to implement both and talk about the benefits and limitations of each.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/07_Combining_Everything_Together#combining-everything-together"&gt;Combining Everything Together&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We now combine everything together that we have learned and create a simple classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/08_Evaluating_Models#evaluating-models"&gt;Evaluating Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Any model is only as good as it's evaluation.  Here we show two examples of (1) evaluating a regression algorithm and (2) a classification algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-3-linear-regression" class="anchor" aria-hidden="true" href="#ch-3-linear-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="03_Linear_Regression#ch-3-linear-regression"&gt;Ch 3: Linear Regression&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/01_Using_the_Matrix_Inverse_Method#using-the-matrix-inverse-method"&gt;
    &lt;img src="03_Linear_Regression/images/01_Inverse_Matrix_Method.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/02_Implementing_a_Decomposition_Method#using-the-cholesky-decomposition-method"&gt;
    &lt;img src="03_Linear_Regression/images/02_Cholesky_Decomposition.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/03_TensorFlow_Way_of_Linear_Regression#learning-the-tensorflow-way-of-regression"&gt;
    &lt;img src="03_Linear_Regression/images/03_lin_reg_fit.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/04_Loss_Functions_in_Linear_Regressions#loss-functions-in-linear-regression"&gt;
    &lt;img src="03_Linear_Regression/images/04_L1_L2_learningrates.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/05_Implementing_Deming_Regression#implementing-deming-regression"&gt;
    &lt;img src="03_Linear_Regression/images/05_demming_vs_linear_reg.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/06_Implementing_Lasso_and_Ridge_Regression#implementing-lasso-and-ridge-regression"&gt;
    &lt;img src="03_Linear_Regression/images/07_elasticnet_reg_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/07_Implementing_Elasticnet_Regression#implementing-elasticnet-regression"&gt;
    &lt;img src="03_Linear_Regression/images/07_elasticnet_reg_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/08_Implementing_Logistic_Regression#implementing-logistic-regression"&gt;
    &lt;img src="03_Linear_Regression/images/08_logistic_reg_acc.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Here we show how to implement various linear regression techniques in TensorFlow.  The first two sections show how to do standard matrix linear regression solving in TensorFlow.  The remaining six sections depict how to implement various types of regression using computational graphs in TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/01_Using_the_Matrix_Inverse_Method#using-the-matrix-inverse-method"&gt;Using the Matrix Inverse Method&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to solve a 2D regression with a matrix inverse in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/02_Implementing_a_Decomposition_Method#using-the-cholesky-decomposition-method"&gt;Implementing a Decomposition Method&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Solving a 2D linear regression with Cholesky decomposition.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/03_TensorFlow_Way_of_Linear_Regression#learning-the-tensorflow-way-of-regression"&gt;Learning the TensorFlow Way of Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression iterating through a computational graph with L2 Loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/04_Loss_Functions_in_Linear_Regressions#loss-functions-in-linear-regression"&gt;Understanding Loss Functions in Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;L2 vs L1 loss in linear regression.  We talk about the benefits and limitations of both.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/05_Implementing_Deming_Regression#implementing-deming-regression"&gt;Implementing Deming Regression (Total Regression)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Deming (total) regression implemented in TensorFlow by changing the loss function.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/06_Implementing_Lasso_and_Ridge_Regression#implementing-lasso-and-ridge-regression"&gt;Implementing Lasso and Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Lasso and Ridge regression are ways of regularizing the coefficients. We implement both of these in TensorFlow via changing the loss functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/07_Implementing_Elasticnet_Regression#implementing-elasticnet-regression"&gt;Implementing Elastic Net Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Elastic net is a regularization technique that combines the L2 and L1 loss for coefficients.  We show how to implement this in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/08_Implementing_Logistic_Regression#implementing-logistic-regression"&gt;Implementing Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We implement logistic regression by the use of an activation function in our computational graph.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-4-support-vector-machines" class="anchor" aria-hidden="true" href="#ch-4-support-vector-machines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="04_Support_Vector_Machines#ch-4-support-vector-machines"&gt;Ch 4: Support Vector Machines&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/01_Introduction#support-vector-machine-introduction"&gt;
    &lt;img src="04_Support_Vector_Machines/images/01_introduction.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/02_Working_with_Linear_SVMs#working-with-linear-svms"&gt;
    &lt;img src="04_Support_Vector_Machines/images/02_linear_svm_output.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/03_Reduction_to_Linear_Regression#svm-reduction-to-linear-regression"&gt;
    &lt;img src="04_Support_Vector_Machines/images/03_svm_regression_output.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/04_Working_with_Kernels#working-with-kernels"&gt;
    &lt;img src="04_Support_Vector_Machines/images/04_linear_svm_gaussian.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/05_Implementing_Nonlinear_SVMs#implementing-nonlinear-svms"&gt;
    &lt;img src="04_Support_Vector_Machines/images/05_non_linear_svms.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/06_Implementing_Multiclass_SVMs#implementing-multiclass-svms"&gt;
    &lt;img src="04_Support_Vector_Machines/images/06_multiclass_svm.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;This chapter shows how to implement various SVM methods with TensorFlow.  We first create a linear SVM and also show how it can be used for regression.  We then introduce kernels (RBF Gaussian kernel) and show how to use it to split up non-linear data. We finish with a multi-dimensional implementation of non-linear SVMs to work with multiple classes.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/01_Introduction#support-vector-machine-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce the concept of SVMs and how we will go about implementing them in the TensorFlow framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/02_Working_with_Linear_SVMs#working-with-linear-svms"&gt;Working with Linear SVMs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We create a linear SVM to separate I. setosa based on sepal length and pedal width in the Iris data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/03_Reduction_to_Linear_Regression#svm-reduction-to-linear-regression"&gt;Reduction to Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;The heart of SVMs is separating classes with a line.  We change tweek the algorithm slightly to perform SVM regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/04_Working_with_Kernels#working-with-kernels"&gt;Working with Kernels in TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In order to extend SVMs into non-linear data, we explain and show how to implement different kernels in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/05_Implementing_Nonlinear_SVMs#implementing-nonlinear-svms"&gt;Implementing Non-Linear SVMs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We use the Gaussian kernel (RBF) to separate non-linear classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/06_Implementing_Multiclass_SVMs#implementing-multiclass-svms"&gt;Implementing Multi-class SVMs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;SVMs are inherently binary predictors.  We show how to extend them in a one-vs-all strategy in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-5-nearest-neighbor-methods" class="anchor" aria-hidden="true" href="#ch-5-nearest-neighbor-methods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="05_Nearest_Neighbor_Methods#ch-5-nearest-neighbor-methods"&gt;Ch 5: Nearest Neighbor Methods&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/01_Introduction#nearest-neighbor-methods-introduction"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/nearest_neighbor_intro.jpg" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/02_Working_with_Nearest_Neighbors#working-with-nearest-neighbors"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/02_nn_histogram.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/03_Working_with_Text_Distances#working-with-text-distances"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/04_Computing_with_Mixed_Distance_Functions#computing-with-mixed-distance-functions"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/04_pred_vs_actual.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/05_An_Address_Matching_Example#an-address-matching-example"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/06_Nearest_Neighbors_for_Image_Recognition#nearest-neighbors-for-image-recognition"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/06_nn_image_recognition.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Nearest Neighbor methods are a very popular ML algorithm.  We show how to implement k-Nearest Neighbors, weighted k-Nearest Neighbors, and k-Nearest Neighbors with mixed distance functions.  In this chapter we also show how to use the Levenshtein distance (edit distance) in TensorFlow, and use it to calculate the distance between strings. We end this chapter with showing how to use k-Nearest Neighbors for categorical prediction with the MNIST handwritten digit recognition.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/01_Introduction#nearest-neighbor-methods-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce the concepts and methods needed for performing k-Nearest Neighbors in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/02_Working_with_Nearest_Neighbors#working-with-nearest-neighbors"&gt;Working with Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We create a nearest neighbor algorithm that tries to predict housing worth (regression).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/03_Working_with_Text_Distances#working-with-text-distances"&gt;Working with Text Based Distances&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In order to use a distance function on text, we show how to use edit distances in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/04_Computing_with_Mixed_Distance_Functions#computing-with-mixed-distance-functions"&gt;Computing Mixing Distance Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we implement scaling of the distance function by the standard deviation of the input feature for k-Nearest Neighbors.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/05_An_Address_Matching_Example#an-address-matching-example"&gt;Using Address Matching&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We use a mixed distance function to match addresses. We use numerical distance for zip codes, and string edit distance for street names. The street names are allowed to have typos.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/06_Nearest_Neighbors_for_Image_Recognition#nearest-neighbors-for-image-recognition"&gt;Using Nearest Neighbors for Image Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;The MNIST digit image collection is a great data set for illustration of how to perform k-Nearest Neighbors for an image classification task.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-6-neural-networks" class="anchor" aria-hidden="true" href="#ch-6-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="06_Neural_Networks#ch-6-neural-networks"&gt;Ch 6: Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/01_Introduction#neural-networks-introduction"&gt;
    &lt;img src="06_Neural_Networks/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/02_Implementing_an_Operational_Gate#implementing-an-operational-gate"&gt;
    &lt;img src="06_Neural_Networks/images/02_operational_gates.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/03_Working_with_Activation_Functions#working-with-activation-functions"&gt;
    &lt;img src="06_Neural_Networks/images/03_activation1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/04_Single_Hidden_Layer_Network#implementing-a-one-layer-neural-network"&gt;
    &lt;img src="06_Neural_Networks/images/04_nn_layout.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/05_Implementing_Different_Layers#implementing-different-layers"&gt;
    &lt;img src="06_Neural_Networks/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/06_Using_Multiple_Layers#using-multiple-layers"&gt;
    &lt;img src="06_Neural_Networks/images/06_nn_multiple_layers_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/07_Improving_Linear_Regression#improving-linear-regression"&gt;
    &lt;img src="06_Neural_Networks/images/07_lin_reg_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/08_Learning_Tic_Tac_Toe#learning-to-play-tic-tac-toe"&gt;
    &lt;img src="06_Neural_Networks/images/08_tictactoe_layout.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Neural Networks are very important in machine learning and growing in popularity due to the major breakthroughs in prior unsolved problems.  We must start with introducing 'shallow' neural networks, which are very powerful and can help us improve our prior ML algorithm results.  We start by introducing the very basic NN unit, the operational gate.  We gradually add more and more to the neural network and end with training a model to play tic-tac-toe.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/01_Introduction#neural-networks-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce the concept of neural networks and how TensorFlow is built to easily handle these algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/02_Implementing_an_Operational_Gate#implementing-an-operational-gate"&gt;Implementing Operational Gates&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We implement an operational gate with one operation. Then we show how to extend this to multiple nested operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/03_Working_with_Activation_Functions#working-with-activation-functions"&gt;Working with Gates and Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Now we have to introduce activation functions on the gates.  We show how different activation functions operate.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/04_Single_Hidden_Layer_Network#implementing-a-one-layer-neural-network"&gt;Implementing a One Layer Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We have all the pieces to start implementing our first neural network.  We do so here with regression on the Iris data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/05_Implementing_Different_Layers#implementing-different-layers"&gt;Implementing Different Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This section introduces the convolution layer and the max-pool layer.  We show how to chain these together in a 1D and 2D example with fully connected layers as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/06_Using_Multiple_Layers#using-multiple-layers"&gt;Using Multi-layer Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to functionalize different layers and variables for a cleaner multi-layer neural network.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/07_Improving_Linear_Regression#improving-linear-regression"&gt;Improving Predictions of Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how we can improve the convergence of our prior logistic regression with a set of hidden layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/08_Learning_Tic_Tac_Toe#learning-to-play-tic-tac-toe"&gt;Learning to Play Tic-Tac-Toe&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Given a set of tic-tac-toe boards and corresponding optimal moves, we train a neural network classification model to play.  At the end of the script, you can attempt to play against the trained model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-7-natural-language-processing" class="anchor" aria-hidden="true" href="#ch-7-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="07_Natural_Language_Processing#ch-7-natural-language-processing"&gt;Ch 7: Natural Language Processing&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/01_Introduction#natural-language-processing-introduction"&gt;
    &lt;img src="07_Natural_Language_Processing/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/02_Working_with_Bag_of_Words#working-with-bag-of-words"&gt;
    &lt;img src="07_Natural_Language_Processing/images/02_bag_of_words.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/03_Implementing_tf_idf#implementing-tf-idf"&gt;
    &lt;img src="07_Natural_Language_Processing/images/03_tfidf_acc.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/04_Working_With_Skip_Gram_Embeddings#working-with-skip-gram-embeddings"&gt;
    &lt;img src="07_Natural_Language_Processing/images/04_skipgram_model.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/05_Working_With_CBOW_Embeddings#working-with-cbow-embeddings"&gt;
    &lt;img src="07_Natural_Language_Processing/images/05_cbow_model.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/06_Using_Word2Vec_Embeddings#using-word2vec-embeddings"&gt;
    &lt;img src="07_Natural_Language_Processing/images/06_word2vec_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/07_Sentiment_Analysis_With_Doc2Vec#sentiment-analysis-with-doc2vec"&gt;
    &lt;img src="07_Natural_Language_Processing/images/07_sentiment_doc2vec_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Natural Language Processing (NLP) is a way of processing textual information into numerical summaries, features, or models. In this chapter we will motivate and explain how to best deal with text in TensorFlow.  We show how to implement the classic 'Bag-of-Words' and show that there may be better ways to embed text based on the problem at hand. There are neural network embeddings called Word2Vec (CBOW and Skip-Gram) and Doc2Vec.  We show how to implement all of these in TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/01_Introduction#natural-language-processing-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce methods for turning text into numerical vectors. We introduce the TensorFlow 'embedding' feature as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/02_Working_with_Bag_of_Words#working-with-bag-of-words"&gt;Working with Bag-of-Words&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we use TensorFlow to do a one-hot-encoding of words called bag-of-words.  We use this method and logistic regression to predict if a text message is spam or ham.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/03_Implementing_tf_idf#implementing-tf-idf"&gt;Implementing TF-IDF&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We implement Text Frequency - Inverse Document Frequency (TFIDF) with a combination of Sci-kit Learn and TensorFlow. We perform logistic regression on TFIDF vectors to improve on our spam/ham text-message predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/04_Working_With_Skip_Gram_Embeddings#working-with-skip-gram-embeddings"&gt;Working with Skip-Gram&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Our first implementation of Word2Vec called, "skip-gram" on a movie review database.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/05_Working_With_CBOW_Embeddings#working-with-cbow-embeddings"&gt;Working with CBOW&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Next, we implement a form of Word2Vec called, "CBOW" (Continuous Bag of Words) on a movie review database.  We also introduce method to saving and loading word embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/06_Using_Word2Vec_Embeddings#using-word2vec-embeddings"&gt;Implementing Word2Vec Example&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we use the prior saved CBOW word embeddings to improve on our TF-IDF logistic regression of movie review sentiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/07_Sentiment_Analysis_With_Doc2Vec#sentiment-analysis-with-doc2vec"&gt;Performing Sentiment Analysis with Doc2Vec&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we introduce a Doc2Vec method (concatenation of doc and word embeddings) to improve out logistic model of movie review sentiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-8-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#ch-8-convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="08_Convolutional_Neural_Networks#ch-8-convolutional-neural-networks"&gt;Ch 8: Convolutional Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/01_Intro_to_CNN#introduction-to-convolutional-neural-networks"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/01_intro_cnn.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/02_Intro_to_CNN_MNIST#introduction-to-cnn-with-mnist"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/02_cnn1_mnist_output.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/03_CNN_CIFAR10#cifar-10-cnn"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/03_cnn2_loss_acc.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/04_Retraining_Current_Architectures#retraining-fine-tuning-current-cnn-architectures"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/05_Stylenet_NeuralStyle#stylenet--neural-style"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/05_stylenet_ex.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/06_Deepdream#deepdream-in-tensorflow"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/06_deepdream_ex.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) are ways of getting neural networks to deal with image data. CNN derive their name from the use of a convolutional layer that applies a fixed size filter across a larger image, recognizing a pattern in any part of the image. There are many other tools that they use (max-pooling, dropout, etc...) that we show how to implement with TensorFlow.  We also show how to retrain an existing architecture and take CNNs further with Stylenet and Deep Dream.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/01_Intro_to_CNN#introduction-to-convolutional-neural-networks"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce convolutional neural networks (CNN), and how we can use them in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/02_Intro_to_CNN_MNIST#introduction-to-cnn-with-mnist"&gt;Implementing a Simple CNN.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we show how to create a CNN architecture that performs well on the MNIST digit recognition task.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/03_CNN_CIFAR10#cifar-10-cnn"&gt;Implementing an Advanced CNN.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we show how to replicate an architecture for the CIFAR-10 image recognition task.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/04_Retraining_Current_Architectures#retraining-fine-tuning-current-cnn-architectures"&gt;Retraining an Existing Architecture.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to download and setup the CIFAR-10 data for the TensorFlow retraining/fine-tuning tutorial.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/05_Stylenet_NeuralStyle#stylenet--neural-style"&gt;Using Stylenet/NeuralStyle.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this recipe, we show a basic implementation of using Stylenet or Neuralstyle.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/06_Deepdream#deepdream-in-tensorflow"&gt;Implementing Deep Dream.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This script shows a line-by-line explanation of TensorFlow's deepdream tutorial. Taken from &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream"&gt;Deepdream on TensorFlow&lt;/a&gt;. Note that the code here is converted to Python 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-9-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#ch-9-recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="09_Recurrent_Neural_Networks#ch-9-recurrent-neural-networks"&gt;Ch 9: Recurrent Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/01_Introduction#introduction-to-rnns-in-tensorflow"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/01_RNN_Seq2Seq.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction#implementing-an-rnn-for-spam-prediction"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/02_RNN_Spam_Acc_Loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/03_Implementing_LSTM#implementing-an-lstm-model"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/03_LSTM_Loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/04_Stacking_Multiple_LSTM_Layers#stacking-multiple-lstm-layers"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/04_MultipleRNN_Architecture.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/05_Creating_A_Sequence_To_Sequence_Model#creating-a-sequence-to-sequence-model-with-tensorflow-seq2seq"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/05_Seq2Seq_Loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/06_Training_A_Siamese_Similarity_Measure#training-a-siamese-similarity-measure-rnns"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/06_Similarity_RNN.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Recurrent Neural Networks (RNNs) are very similar to regular neural networks except that they allow 'recurrent' connections, or loops that depend on the prior states of the network. This allows RNNs to efficiently deal with sequential data, whereas other types of networks cannot. We then motivate the usage of LSTM (Long Short Term Memory) networks as a way of addressing regular RNN problems. Then we show how easy it is to implement these RNN types in TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/01_Introduction#introduction-to-rnns-in-tensorflow"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce Recurrent Neural Networks and how they are able to feed in a sequence and predict either a fixed target (categorical/numerical) or another sequence (sequence to sequence).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction#implementing-an-rnn-for-spam-prediction"&gt;Implementing an RNN Model for Spam Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we create an RNN model to improve on our spam/ham SMS text predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/03_Implementing_LSTM#implementing-an-lstm-model"&gt;Implementing an LSTM Model for Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to implement a LSTM (Long Short Term Memory) RNN for Shakespeare language generation. (Word level vocabulary)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/04_Stacking_Multiple_LSTM_Layers#stacking-multiple-lstm-layers"&gt;Stacking Multiple LSTM Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We stack multiple LSTM layers to improve on our Shakespeare language generation. (Character level vocabulary)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/05_Creating_A_Sequence_To_Sequence_Model#creating-a-sequence-to-sequence-model-with-tensorflow-seq2seq"&gt;Creating a Sequence to Sequence Translation Model (Seq2Seq)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we use TensorFlow's sequence-to-sequence models to train an English-German translation model.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/06_Training_A_Siamese_Similarity_Measure#training-a-siamese-similarity-measure-rnns"&gt;Training a Siamese Similarity Measure&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we implement a Siamese RNN to predict the similarity of addresses and use it for record matching.  Using RNNs for record matching is very versatile, as we do not have a fixed set of target categories and can use the trained model to predict similarities across new addresses.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-10-taking-tensorflow-to-production" class="anchor" aria-hidden="true" href="#ch-10-taking-tensorflow-to-production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="10_Taking_TensorFlow_to_Production#ch-10-taking-tensorflow-to-production"&gt;Ch 10: Taking TensorFlow to Production&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/01_Implementing_Unit_Tests#implementing-unit-tests"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/02_Using_Multiple_Devices#using-multiple-devices"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/03_Parallelizing_TensorFlow#parallelizing-tensorflow"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/04_Production_Tips#production-tips-with-tensorflow"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/05_Production_Example#a-production-example"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Of course there is more to TensorFlow than just creating and fitting machine learning models.  Once we have a model that we want to use, we have to move it towards production usage.  This chapter will provide tips and examples of implementing unit tests, using multiple processors, using multiple machines (TensorFlow distributed), and finish with a full production example.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/01_Implementing_Unit_Tests#implementing-unit-tests"&gt;Implementing Unit Tests&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to implement different types of unit tests on tensors (placeholders and variables).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/02_Using_Multiple_Devices#using-multiple-devices"&gt;Using Multiple Executors (Devices)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to use a machine with multiple devices.  E.g., a machine with a CPU, and one or more GPUs.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/03_Parallelizing_TensorFlow#parallelizing-tensorflow"&gt;Parallelizing TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to setup and use TensorFlow distributed on multiple machines.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/04_Production_Tips#production-tips-with-tensorflow"&gt;Tips for TensorFlow in Production&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Various tips for developing with TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/05_Production_Example#a-production-example"&gt;An Example of Productionalizing TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to do take the RNN model for predicting ham/spam (from Chapter 9, recipe #2) and put it in two production level files: training and evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-11-more-with-tensorflow" class="anchor" aria-hidden="true" href="#ch-11-more-with-tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="11_More_with_TensorFlow#ch-11-more-with-tensorflow"&gt;Ch 11: More with TensorFlow&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/01_Visualizing_Computational_Graphs#visualizing-computational-graphs-wtensorboard"&gt;
    &lt;img src="11_More_with_TensorFlow/images/01_tensorboard1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/02_Working_with_a_Genetic_Algorithm"&gt;
    &lt;img src="11_More_with_TensorFlow/images/02_genetic_algorithm.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/03_Clustering_Using_KMeans#clustering-using-k-means"&gt;
    &lt;img src="11_More_with_TensorFlow/images/03_kmeans.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/04_Solving_A_System_of_ODEs#solving-a-system-of-odes"&gt;
    &lt;img src="11_More_with_TensorFlow/images/04_ode_system.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;To illustrate how versatile TensorFlow is, we will show additional examples in this chapter. We start with showing how to use the logging/visualizing tool Tensorboard.  Then we illustrate how to do k-means clustering, use a genetic algorithm, and solve a system of ODEs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/01_Visualizing_Computational_Graphs#visualizing-computational-graphs-wtensorboard"&gt;Visualizing Computational Graphs (with Tensorboard)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;An example of using histograms, scalar summaries, and creating images in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/02_Working_with_a_Genetic_Algorithm#working-with-a-genetic-algorithm"&gt;Working with a Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We create a genetic algorithm to optimize an individual (array of 50 numbers) toward the ground truth function.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/03_Clustering_Using_KMeans#clustering-using-k-means"&gt;Clustering Using K-means&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to use TensorFlow to do k-means clustering.  We use the Iris data set, set k=3, and use k-means to make predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/04_Solving_A_System_of_ODEs#solving-a-system-of-odes"&gt;Solving a System of ODEs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we show how to use TensorFlow to solve a system of ODEs.  The system of concern is the Lotka-Volterra predator-prey system.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/05_Using_a_Random_Forest#using-a-random-forest"&gt;Using a Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We illustrate how to use TensorFlow's gradient boosted regression and classification trees.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/06_Using_TensorFlow_with_Keras#using-tensorflow-with-keras"&gt;Using TensorFlow with Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to use the Keras sequential model building for a fully connected neural network and a CNN model with callbacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nfmcclure</author><guid isPermaLink="false">https://github.com/nfmcclure/tensorflow_cookbook</guid><pubDate>Mon, 20 Jan 2020 00:14:00 GMT</pubDate></item><item><title>randerson112358/Python #15 in Jupyter Notebook, Today</title><link>https://github.com/randerson112358/Python</link><description>&lt;p&gt;&lt;i&gt;:snake: Python Programs&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python" class="anchor" aria-hidden="true" href="#python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python&lt;/h1&gt;
&lt;p&gt;This is a repository that holds my Python programs&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2cffe57e3c8276001c970391a67d67cf3a02f165/68747470733a2f2f7777772e707974686f6e2e6f72672f7374617469632f636f6d6d756e6974795f6c6f676f732f707974686f6e2d6c6f676f2d696e6b73636170652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/2cffe57e3c8276001c970391a67d67cf3a02f165/68747470733a2f2f7777772e707974686f6e2e6f72672f7374617469632f636f6d6d756e6974795f6c6f676f732f707974686f6e2d6c6f676f2d696e6b73636170652e737667" width="400" data-canonical-src="https://www.python.org/static/community_logos/python-logo-inkscape.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
To see me programming in Python checkout the YouTube channel: &lt;a href="https://www.youtube.com/playlist?list=PLBhJnyA0V0uIP6tScPs01FW5WtSpJdmcv" rel="nofollow"&gt;Go To YouTube Channel&lt;/a&gt;
&lt;h1&gt;&lt;a id="user-content-relavent-books-on-amazon" class="anchor" aria-hidden="true" href="#relavent-books-on-amazon"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relavent Books On Amazon&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1449355730/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1449355730&amp;amp;linkId=95e6eaf8c12b9fcd483dd06c1dd53e48" rel="nofollow"&gt;Learning Python, 5th Edition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1491962291/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1491962291&amp;amp;linkId=9dec6584d63a7cfcbc32af1ff9737bbf" rel="nofollow"&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1491912057/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1491912057&amp;amp;linkId=af650651a6d71fdea49cd5aa95653e1c" rel="nofollow"&gt;Python Data Science Handbook: Essential Tools for Working with Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&amp;amp;tag=github01d-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=1449369413&amp;amp;linkId=7b6ad9375121575c83af505f2a3ed6f3" rel="nofollow"&gt;Introduction to Machine Learning with Python: A Guide for Data Scientists&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-python-data-cleaning-programs" class="anchor" aria-hidden="true" href="#python-data-cleaning-programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Data Cleaning Programs&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;concatenate_file.py&lt;/td&gt;
&lt;td&gt;Concatenate Multiple CSV files&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/concatenate_file.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA" rel="nofollow"&gt;YouTubeX&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;remove_empty_row.py&lt;/td&gt;
&lt;td&gt;Removes Empty Rows&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/remove_empty_row.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA" rel="nofollow"&gt;YouTubeX&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;replace_strings_with_numbers.py&lt;/td&gt;
&lt;td&gt;Changes Strings in CSV to Numbers&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Replace_Strings_With_Numbers/replace_strings_with_numbers.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/zv_fzW2iA_U" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-web-scraping" class="anchor" aria-hidden="true" href="#web-scraping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Scraping&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;scrape.py&lt;/td&gt;
&lt;td&gt;Scrape Website Links&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/scrape.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/scrape-website-using-python-90619cac7c97" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/LGZEn1OYUTk" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;News_Article.py&lt;/td&gt;
&lt;td&gt;Scrape &amp;amp; Summarize Article&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/News_Article.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://everythingcomputerscience.com/" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/YzMA2O_v5co" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-machine-learning-projects--programs" class="anchor" aria-hidden="true" href="#machine-learning-projects--programs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Projects &amp;amp; Programs&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Project Name&lt;/th&gt;
&lt;th&gt;Program Name&lt;/th&gt;
&lt;th&gt;Algorithm Name&lt;/th&gt;
&lt;th&gt;Link to Program&lt;/th&gt;
&lt;th&gt;Blog&lt;/th&gt;
&lt;th&gt;YouTube&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;sentiment.py&lt;/td&gt;
&lt;td&gt;Sentiment Analysis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/sentiment.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/sentiment-analysis-e2e4442bac13" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/1VHhDSOwJPw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Simple Linear Regression Ex&lt;/td&gt;
&lt;td&gt;LinearRegression.py&lt;/td&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/LinearRegression.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/a-simple-machine-learning-python-program-bf5d156d2cda" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/z7jEJY8FbA8" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Car Classification&lt;/td&gt;
&lt;td&gt;decisionTree.py&lt;/td&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/DecisionTree/decisionTree.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/car-classification-89ad60204acf" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/U-Jm8ugN0Ps" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Golf Predictions&lt;/td&gt;
&lt;td&gt;Golf_Predictions.ipynb&lt;/td&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Golf_Predictions.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/python-decision-tree-classifier-example-d73bc3aeca6" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/bT-43kgYI3o" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Boston House Price&lt;/td&gt;
&lt;td&gt;Predict_Boston_Housing_Price.ipynb&lt;/td&gt;
&lt;td&gt;Linear Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Predict_Boston_Housing_Price.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-boston-house-prices-using-python-linear-regression-90469e0a341" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/gOXoFDrseis" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Stock Price&lt;/td&gt;
&lt;td&gt;stock.ipynb&lt;/td&gt;
&lt;td&gt;Linear Regression &amp;amp; SVR&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/stock.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-stock-prices-using-python-machine-learning-53aa024da20a" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/EYnC4ACIt2g" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Iris Species&lt;/td&gt;
&lt;td&gt;Logistic_Regression.ipynb&lt;/td&gt;
&lt;td&gt;Logistic Regression&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Logistic_Regression.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/python-logistic-regression-program-5e1b32f964db" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/ACdBKML9l4s" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict Median House Price&lt;/td&gt;
&lt;td&gt;Neural_Networks.ipynb&lt;/td&gt;
&lt;td&gt;Deep Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Neural_Networks/Neural_Networks.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/predict-house-median-prices-5f1a768dd256?postPublishedType=repub" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/vSzou5zRwNQ" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Handwritten Digits&lt;/td&gt;
&lt;td&gt;MNIST_ANN.ipynb&lt;/td&gt;
&lt;td&gt;Artificial Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/MNIST_ANN.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-hand-written-digits-5fdbe5d99ee7" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/kOFUQB7u5Ck" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cluster NBA Basketball Players&lt;/td&gt;
&lt;td&gt;Basketball_Data_Exploration.ipynb&lt;/td&gt;
&lt;td&gt;KMeans&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/NBA_Basketball_Exploration/Basketball_Data_Exploration.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/nba-data-analysis-exploration-9293f311e0e8" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/2Pmf6Kqak3w" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict FB Stock Price&lt;/td&gt;
&lt;td&gt;SVM.ipynb&lt;/td&gt;
&lt;td&gt;Support Vector Regression (SVR)&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/SVM_Stock/SVM.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/facebook-stock-prediction-bcfc676bc611" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/tMPfZV_ipOg" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Breast Cancer Detection&lt;/td&gt;
&lt;td&gt;Breast_Cancer_Detection.ipynb&lt;/td&gt;
&lt;td&gt;Random Forest Classifier &amp;amp; Gaussian Naive Bayes &amp;amp; Logistic Regression &amp;amp; Decision Tree Classifier &amp;amp; SVC&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/breast_cancer_detection/Breast_Cancer_Detection.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/breast-cancer-detection-using-machine-learning-38820fe98982" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/NSSOyhJBmWY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Face Detection&lt;/td&gt;
&lt;td&gt;face_detection.py&lt;/td&gt;
&lt;td&gt;Open CV &amp;amp; Adaboost&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/face_detection/face_detection.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/face-detection-using-python-open-cv-d51e27266f7f" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/6klXqQMctPk" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image Classification&lt;/td&gt;
&lt;td&gt;cnn.ipynb&lt;/td&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Classify_Images/cnn.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-images-using-convolutional-neural-networks-python-a89cecc8c679" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/mB7fdy67eFw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classify Handwritten Digits CNN&lt;/td&gt;
&lt;td&gt;mnist_cnn.ipynb&lt;/td&gt;
&lt;td&gt;Convolutional Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/mnist_cnn.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/classify-hand-written-digits-using-python-and-convolutional-neural-networks-26ccfc06b95c" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/V4dd2Bt9OHY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spam Detection&lt;/td&gt;
&lt;td&gt;Email_Spam_Detection.ipynb&lt;/td&gt;
&lt;td&gt;Naive Bayes&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Email_Spam_Detection/Email_Spam_Detection.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/email-spam-detection-using-python-machine-learning-abe38c889855" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/cNLPt02RwF0" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pima-Indians Diabetes&lt;/td&gt;
&lt;td&gt;Diabetes.ipynb&lt;/td&gt;
&lt;td&gt;Artificial Neural Networks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Diabetes/Diabetes.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-your-own-artificial-neural-network-using-python-f37d16be06bf" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=S2sZNlr-4_4&amp;amp;list=PLBhJnyA0V0uIP6tScPs01FW5WtSpJdmcv&amp;amp;index=28&amp;amp;t=0s" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Movie Recommendation Engine&lt;/td&gt;
&lt;td&gt;Movie_Recommendation.ipynb&lt;/td&gt;
&lt;td&gt;Cosine Similarity&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Movie_Recommender/Movie_Recommendation.ipynb"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-a-movie-recommendation-engine-using-python-scikit-learn-machine-learning-e68ba297e163" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/umSM8rFtVMs" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Article Text To Speech&lt;/td&gt;
&lt;td&gt;Article_Text_To_Speech.py&lt;/td&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/Article_Text_To_Speech.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-a-text-to-speech-program-using-python-b70de7105383" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/uPSIUjo_Fhw" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI Smart Dr.Chat Bot&lt;/td&gt;
&lt;td&gt;smartbot.py&lt;/td&gt;
&lt;td&gt;NLP&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/smartbot.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/build-your-own-ai-chat-bot-using-python-machine-learning-682ddd8acc29" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/QpMsT0WuIuI" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neural Network Stock Prediction&lt;/td&gt;
&lt;td&gt;lstm2.py&lt;/td&gt;
&lt;td&gt;LSTM&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/randerson112358/Python/blob/master/LSTM_Stock/lstm2.py"&gt;Program&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://medium.com/@randerson112358/stock-price-prediction-using-python-machine-learning-e82a039ac2bb" rel="nofollow"&gt;Blog&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://youtu.be/QIUxPv5PJOY" rel="nofollow"&gt;YouTube&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>randerson112358</author><guid isPermaLink="false">https://github.com/randerson112358/Python</guid><pubDate>Mon, 20 Jan 2020 00:15:00 GMT</pubDate></item><item><title>fastai/numerical-linear-algebra #16 in Jupyter Notebook, Today</title><link>https://github.com/fastai/numerical-linear-algebra</link><description>&lt;p&gt;&lt;i&gt;Free online textbook of Jupyter notebooks for fast.ai Computational Linear Algebra course&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-computational-linear-algebra-for-coders" class="anchor" aria-hidden="true" href="#computational-linear-algebra-for-coders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computational Linear Algebra for Coders&lt;/h2&gt;
&lt;p&gt;This course is focused on the question: &lt;strong&gt;How do we do matrix computations with acceptable speed and acceptable accuracy?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This course was taught in the &lt;a href="https://www.usfca.edu/arts-sciences/graduate-programs/analytics" rel="nofollow"&gt;University of San Francisco's Masters of Science in Analytics&lt;/a&gt; program, summer 2017 (for graduate students studying to become data scientists).  The course is taught in Python with Jupyter Notebooks, using libraries such as Scikit-Learn and Numpy for most lessons, as well as Numba (a library that compiles Python to C for faster performance) and PyTorch (an alternative to Numpy for the GPU) in a few lessons.&lt;/p&gt;
&lt;p&gt;Accompanying the notebooks is a &lt;a href="https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;playlist of lecture videos, available on YouTube&lt;/a&gt;.  If you are ever confused by a lecture or it goes too quickly, check out the beginning of the next video, where I review concepts from the previous lecture, often explaining things from a new perspective or with different illustrations, and answer questions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-help" class="anchor" aria-hidden="true" href="#getting-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Help&lt;/h2&gt;
&lt;p&gt;You can ask questions or share your thoughts and resources using the &lt;a href="http://forums.fast.ai/c/lin-alg" rel="nofollow"&gt;&lt;strong&gt;Computational Linear Algebra&lt;/strong&gt; category on our fast.ai discussion forums&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;p&gt;The following listing links to the notebooks in this repository, rendered through the &lt;a href="http://nbviewer.jupyter.org" rel="nofollow"&gt;nbviewer&lt;/a&gt; service.  Topics Covered:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-0-course-logistics-video-1" class="anchor" aria-hidden="true" href="#0-course-logistics-video-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb" rel="nofollow"&gt;0. Course Logistics&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=8iGzBMboA0I&amp;amp;index=1&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 1&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Intro" rel="nofollow"&gt;My background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Teaching" rel="nofollow"&gt;Teaching Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Writing-Assignment" rel="nofollow"&gt;Importance of Technical Writing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Excellent-Technical-Blogs" rel="nofollow"&gt;List of Excellent Technical Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Linear-Algebra" rel="nofollow"&gt;Linear Algebra Review Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-1-why-are-we-here-video-1" class="anchor" aria-hidden="true" href="#1-why-are-we-here-video-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb" rel="nofollow"&gt;1. Why are we here?&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=8iGzBMboA0I&amp;amp;index=1&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 1&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;We start with a high level overview of some foundational concepts in numerical linear algebra.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Matrix-and-Tensor-Products" rel="nofollow"&gt;Matrix and Tensor Products&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Matrix-Decompositions" rel="nofollow"&gt;Matrix Decompositions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Accuracy" rel="nofollow"&gt;Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Memory-Use" rel="nofollow"&gt;Memory use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Speed" rel="nofollow"&gt;Speed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Scalability-/-parallelization" rel="nofollow"&gt;Parallelization &amp;amp; Vectorization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-2-topic-modeling-with-nmf-and-svd-video-2-and-video-3" class="anchor" aria-hidden="true" href="#2-topic-modeling-with-nmf-and-svd-video-2-and-video-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb" rel="nofollow"&gt;2. Topic Modeling with NMF and SVD&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=kgd40iDT8yY&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=2" rel="nofollow"&gt;Video 2&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=C8KEtrWjjyo&amp;amp;index=3&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 3&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;We will use the newsgroups dataset to try to identify the topics of different posts.  We use a term-document matrix that represents the frequency of the vocabulary in the documents.  We factor it using NMF, and then with SVD.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#TF-IDF" rel="nofollow"&gt;Topic Frequency-Inverse Document Frequency (TF-IDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Singular-Value-Decomposition-(SVD)" rel="nofollow"&gt;Singular Value Decomposition (SVD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Non-negative-Matrix-Factorization-(NMF)" rel="nofollow"&gt;Non-negative Matrix Factorization (NMF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Gradient-Descent" rel="nofollow"&gt;Stochastic Gradient Descent (SGD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#PyTorch" rel="nofollow"&gt;Intro to PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Truncated-SVD" rel="nofollow"&gt;Truncated SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-3-background-removal-with-robust-pca-video-3-video-4-and-video-5" class="anchor" aria-hidden="true" href="#3-background-removal-with-robust-pca-video-3-video-4-and-video-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb" rel="nofollow"&gt;3. Background Removal with Robust PCA&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=C8KEtrWjjyo&amp;amp;index=3&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 3&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=Ys8R2nUTOAk&amp;amp;index=4&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 4&lt;/a&gt;, and &lt;a href="https://www.youtube.com/watch?v=O2x5KPJr5ag&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=5" rel="nofollow"&gt;Video 5&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;Another application of SVD is to identify the people and remove the background of a surveillance video.  We will cover robust PCA, which uses randomized SVD.  And Randomized SVD uses the LU factorization.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Load-and-view-the-data" rel="nofollow"&gt;Load and View Video Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#SVD" rel="nofollow"&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb"&gt;Principal Component Analysis (PCA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#L1-norm-induces-sparsity" rel="nofollow"&gt;L1 Norm Induces Sparsity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Robust-PCA-(via-Primary-Component-Pursuit)" rel="nofollow"&gt;Robust PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#LU-Factorization" rel="nofollow"&gt;LU factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Stability" rel="nofollow"&gt;Stability of LU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#LU-factorization-with-Partial-Pivoting" rel="nofollow"&gt;LU factorization with Pivoting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#History-of-Gaussian-Elimination" rel="nofollow"&gt;History of Gaussian Elimination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Block-Matrices" rel="nofollow"&gt;Block Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-4-compressed-sensing-with-robust-regression-video-6-and-video-7" class="anchor" aria-hidden="true" href="#4-compressed-sensing-with-robust-regression-video-6-and-video-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression" rel="nofollow"&gt;4. Compressed Sensing with Robust Regression&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=YY9_EYNj5TY&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=6" rel="nofollow"&gt;Video 6&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=ZUGkvIM6ehM&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=7" rel="nofollow"&gt;Video 7&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;Compressed sensing is critical to allowing CT scans with lower radiation-- the image can be reconstructed with less data.  Here we will learn the technique and apply it to CT images.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Broadcasting" rel="nofollow"&gt;Broadcasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Sparse-Matrices-(in-Scipy)" rel="nofollow"&gt;Sparse matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Sparse-Matrices-(in-Scipy)" rel="nofollow"&gt;CT Scans and Compressed Sensing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Regresssion" rel="nofollow"&gt;L1 and L2 regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-5-predicting-health-outcomes-with-linear-regressions-video-8" class="anchor" aria-hidden="true" href="#5-predicting-health-outcomes-with-linear-regressions-video-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb" rel="nofollow"&gt;5. Predicting Health Outcomes with Linear Regressions&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=SjX55V8zDXI&amp;amp;index=8&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 8&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Linear-regression-in-Scikit-Learn" rel="nofollow"&gt;Linear regression in sklearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Polynomial-Features" rel="nofollow"&gt;Polynomial Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Speeding-up-feature-generation" rel="nofollow"&gt;Speeding up with Numba&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Regularization-and-noise" rel="nofollow"&gt;Regularization and Noise&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-6-how-to-implement-linear-regressionvideo-8" class="anchor" aria-hidden="true" href="#6-how-to-implement-linear-regressionvideo-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb" rel="nofollow"&gt;6. How to Implement Linear Regression&lt;/a&gt;(&lt;a href="https://www.youtube.com/watch?v=SjX55V8zDXI&amp;amp;index=8&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 8&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#How-did-sklearn-do-it?" rel="nofollow"&gt;How did Scikit Learn do it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Naive-Solution" rel="nofollow"&gt;Naive solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Normal-Equations-(Cholesky)" rel="nofollow"&gt;Normal equations and Cholesky factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#QR-Factorization" rel="nofollow"&gt;QR factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#SVD" rel="nofollow"&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Timing-Comparison" rel="nofollow"&gt;Timing Comparison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Conditioning-&amp;amp;-stability" rel="nofollow"&gt;Conditioning &amp;amp; Stability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Full-vs-Reduced-Factorizations" rel="nofollow"&gt;Full vs Reduced Factorizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Matrix-Inversion-is-Unstable" rel="nofollow"&gt;Matrix Inversion is Unstable&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-7-pagerank-with-eigen-decompositions-video-9-and-video-10" class="anchor" aria-hidden="true" href="#7-pagerank-with-eigen-decompositions-video-9-and-video-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb" rel="nofollow"&gt;7. PageRank with Eigen Decompositions&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=AbB-w77yxD0&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=9" rel="nofollow"&gt;Video 9&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=1kw8bpA9QmQ&amp;amp;index=10&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 10&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;We have applied SVD to topic modeling, background removal, and linear regression. SVD is intimately connected to the eigen decomposition, so we will now learn how to calculate eigenvalues for a large matrix.  We will use DBpedia data, a large dataset of Wikipedia links, because here the principal eigenvector gives the relative importance of different Wikipedia pages (this is the basic idea of Google's PageRank algorithm).  We will look at 3 different methods for calculating eigenvectors, of increasing complexity (and increasing usefulness!).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#Motivation" rel="nofollow"&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#DBpedia" rel="nofollow"&gt;DBpedia Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#Power-method" rel="nofollow"&gt;Power Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#QR-Algorithm" rel="nofollow"&gt;QR Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#A-Two-Phase-Approach" rel="nofollow"&gt;Two-phase approach to finding eigenvalues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#Arnoldi-Iteration" rel="nofollow"&gt;Arnoldi Iteration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-8-implementing-qr-factorization-video-10" class="anchor" aria-hidden="true" href="#8-implementing-qr-factorization-video-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb" rel="nofollow"&gt;8. Implementing QR Factorization&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=1kw8bpA9QmQ&amp;amp;index=10&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 10&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb#Gram-Schmidt" rel="nofollow"&gt;Gram-Schmidt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb#Householder" rel="nofollow"&gt;Householder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb#Ex-9.2:-Classical-vs-Modified-Gram-Schmidt" rel="nofollow"&gt;Stability Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-why-is-this-course-taught-in-such-a-weird-order" class="anchor" aria-hidden="true" href="#why-is-this-course-taught-in-such-a-weird-order"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why is this course taught in such a weird order?&lt;/h2&gt;
&lt;p&gt;This course is structured with a &lt;em&gt;top-down&lt;/em&gt; teaching method, which is different from how most math courses operate.  Typically, in a &lt;em&gt;bottom-up&lt;/em&gt; approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures.  The problems with this are that students often lose motivation, don't have a sense of the "big picture", and don't know what they'll need.&lt;/p&gt;
&lt;p&gt;Harvard Professor David Perkins has a book, &lt;a href="https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719" rel="nofollow"&gt;Making Learning Whole&lt;/a&gt; in which he uses baseball as an analogy.  We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game.  Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.&lt;/p&gt;
&lt;p&gt;If you took the fast.ai deep learning course, that is what we used.  You can hear more about my teaching philosophy &lt;a href="http://www.fast.ai/2016/10/08/teaching-philosophy/" rel="nofollow"&gt;in this blog post&lt;/a&gt; or &lt;a href="https://vimeo.com/214233053" rel="nofollow"&gt;this talk I gave at the San Francisco Machine Learning meetup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All that to say, don't worry if you don't understand everything at first!  You're not supposed to.  We will start using some "black boxes" or matrix decompositions that haven't yet been explained, and then we'll dig into the lower level details later.&lt;/p&gt;
&lt;p&gt;To start, focus on what things DO, not what they ARE.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/numerical-linear-algebra</guid><pubDate>Mon, 20 Jan 2020 00:16:00 GMT</pubDate></item><item><title>ultralytics/yolov3 #17 in Jupyter Notebook, Today</title><link>https://github.com/ultralytics/yolov3</link><description>&lt;p&gt;&lt;i&gt;YOLOv3 in PyTorch &gt; ONNX &gt; CoreML &gt; iOS&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591130-f7beea00-abc2-11e9-9dc0-d6abcf41d713.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td align="center"&gt;
    &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/c7f01c9051691f7f4c6239349b6b55cb5a0871c9/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f756c7472616c79746963732f6c6f676f2f6c6f676f6e616d65313030302e706e67" width="160" data-canonical-src="https://storage.googleapis.com/ultralytics/logo/logoname1000.png" style="max-width:100%;"&gt;&lt;/a&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591093-2b4d4480-abc2-11e9-8b46-d88eb1dabba1.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
          &lt;a href="https://itunes.apple.com/app/id1452689527" rel="nofollow"&gt;
    &lt;img src="https://user-images.githubusercontent.com/26833433/50044365-9b22ac00-0082-11e9-862f-e77aee7aa7b0.png" width="180" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/61591100-55066b80-abc2-11e9-9647-52c0e045b288.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This directory contains PyTorch YOLOv3 software developed by Ultralytics LLC, and &lt;strong&gt;is freely available for redistribution under the GPL-3.0 license&lt;/strong&gt;. For more information please visit &lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;https://www.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-description" class="anchor" aria-hidden="true" href="#description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Description&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/ultralytics/yolov3"&gt;https://github.com/ultralytics/yolov3&lt;/a&gt; repo contains inference and training code for YOLOv3 in PyTorch. The code works on Linux, MacOS and Windows. Training is done on the COCO dataset by default: &lt;a href="https://cocodataset.org/#home" rel="nofollow"&gt;https://cocodataset.org/#home&lt;/a&gt;. &lt;strong&gt;Credit to Joseph Redmon for YOLO:&lt;/strong&gt; &lt;a href="https://pjreddie.com/darknet/yolo/" rel="nofollow"&gt;https://pjreddie.com/darknet/yolo/&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Python 3.7 or later with the following &lt;code&gt;pip3 install -U -r requirements.txt&lt;/code&gt; packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;torch &amp;gt;= 1.1.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;opencv-python&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart"&gt;GCP Quickstart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning"&gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Image"&gt;Train Single Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Example:-Train-Single-Class"&gt;Train Single Class&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data"&gt;Train Custom Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-jupyter-notebook" class="anchor" aria-hidden="true" href="#jupyter-notebook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook&lt;/h1&gt;
&lt;p&gt;Our Jupyter &lt;a href="https://colab.research.google.com/github/ultralytics/yolov3/blob/master/examples.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt; provides quick training, inference and testing examples.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Start Training:&lt;/strong&gt; &lt;code&gt;python3 train.py&lt;/code&gt; to begin training after downloading COCO data with &lt;code&gt;data/get_coco_dataset.sh&lt;/code&gt;. Each epoch trains on 117,263 images from the train and validate COCO sets, and tests on 5000 images from the COCO validate set.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resume Training:&lt;/strong&gt; &lt;code&gt;python3 train.py --resume&lt;/code&gt; to resume training from &lt;code&gt;weights/last.pt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Plot Training:&lt;/strong&gt; &lt;code&gt;from utils import utils; utils.plot_results()&lt;/code&gt; plots training results from &lt;code&gt;coco_16img.data&lt;/code&gt;, &lt;code&gt;coco_64img.data&lt;/code&gt;, 2 example datasets available in the &lt;code&gt;data/&lt;/code&gt; folder, which train and test on the first 16 and 64 images of the COCO2014-trainval dataset.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/63258271-fe9d5300-c27b-11e9-9a15-95038daf4438.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-image-augmentation" class="anchor" aria-hidden="true" href="#image-augmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image Augmentation&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;datasets.py&lt;/code&gt; applies random OpenCV-powered (&lt;a href="https://opencv.org/" rel="nofollow"&gt;https://opencv.org/&lt;/a&gt;) augmentation to the input images in accordance with the following specifications. Augmentation is applied &lt;strong&gt;only&lt;/strong&gt; during training, not during inference. Bounding boxes are automatically tracked and updated with the images. 416 x 416 examples pictured below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Augmentation&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Translation&lt;/td&gt;
&lt;td&gt;+/- 10% (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotation&lt;/td&gt;
&lt;td&gt;+/- 5 degrees&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Shear&lt;/td&gt;
&lt;td&gt;+/- 2 degrees (vertical and horizontal)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale&lt;/td&gt;
&lt;td&gt;+/- 10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reflection&lt;/td&gt;
&lt;td&gt;50% probability (horizontal-only)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;strong&gt;S&lt;/strong&gt;V Saturation&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HS&lt;strong&gt;V&lt;/strong&gt; Intensity&lt;/td&gt;
&lt;td&gt;+/- 50%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/66699231-27beea80-ece5-11e9-9cad-bdf9d82c500a.jpg" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-speed" class="anchor" aria-hidden="true" href="#speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speed&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/deep-learning-vm/" rel="nofollow"&gt;https://cloud.google.com/deep-learning-vm/&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Machine type:&lt;/strong&gt; preemptible &lt;a href="https://cloud.google.com/compute/docs/machine-types" rel="nofollow"&gt;n1-standard-16&lt;/a&gt; (16 vCPUs, 60 GB memory)&lt;br&gt;
&lt;strong&gt;CPU platform:&lt;/strong&gt; Intel Skylake&lt;br&gt;
&lt;strong&gt;GPUs:&lt;/strong&gt; K80 ($0.20/hr), T4 ($0.35/hr), V100 ($0.83/hr) CUDA with &lt;a href="https://github.com/NVIDIA/apex"&gt;Nvidia Apex&lt;/a&gt; FP16/32&lt;br&gt;
&lt;strong&gt;HDD:&lt;/strong&gt; 1 TB SSD&lt;br&gt;
&lt;strong&gt;Dataset:&lt;/strong&gt; COCO train 2014 (117,263 images)&lt;br&gt;
&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;yolov3-spp.cfg&lt;/code&gt;&lt;br&gt;
&lt;strong&gt;Command:&lt;/strong&gt;  &lt;code&gt;python3 train.py --img 416 --batch 32 --accum 2&lt;/code&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GPU&lt;/th&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;&lt;code&gt;--batch --accum&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;img/s&lt;/th&gt;
&lt;th&gt;epoch&lt;br&gt;time&lt;/th&gt;
&lt;th&gt;epoch&lt;br&gt;cost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;K80&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;32 x 2&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;175 min&lt;/td&gt;
&lt;td&gt;$0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T4&lt;/td&gt;
&lt;td&gt;1&lt;br&gt;2&lt;/td&gt;
&lt;td&gt;32 x 2&lt;br&gt;64 x 1&lt;/td&gt;
&lt;td&gt;41&lt;br&gt;61&lt;/td&gt;
&lt;td&gt;48 min&lt;br&gt;32 min&lt;/td&gt;
&lt;td&gt;$0.28&lt;br&gt;$0.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;V100&lt;/td&gt;
&lt;td&gt;1&lt;br&gt;2&lt;/td&gt;
&lt;td&gt;32 x 2&lt;br&gt;64 x 1&lt;/td&gt;
&lt;td&gt;122&lt;br&gt;&lt;strong&gt;178&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;16 min&lt;br&gt;&lt;strong&gt;11 min&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;$0.23&lt;/strong&gt;&lt;br&gt;$0.31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2080Ti&lt;/td&gt;
&lt;td&gt;1&lt;br&gt;2&lt;/td&gt;
&lt;td&gt;32 x 2&lt;br&gt;64 x 1&lt;/td&gt;
&lt;td&gt;81&lt;br&gt;140&lt;/td&gt;
&lt;td&gt;24 min&lt;br&gt;14 min&lt;/td&gt;
&lt;td&gt;-&lt;br&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-inference" class="anchor" aria-hidden="true" href="#inference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Inference&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;detect.py&lt;/code&gt; runs inference on any sources:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 detect.py --source ...&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Image:  &lt;code&gt;--source file.jpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Video:  &lt;code&gt;--source file.mp4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Directory:  &lt;code&gt;--source dir/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Webcam:  &lt;code&gt;--source 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;RTSP stream:  &lt;code&gt;--source rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;HTTP stream:  &lt;code&gt;--source http://wmccpinetop.axiscam.net/mjpg/video.mjpg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To run a specific models:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3.cfg --weights yolov3.weights&lt;/code&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067835-51d5b500-cc2f-11e9-982e-843f7f9a6ea2.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-tiny:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-tiny.cfg --weights yolov3-tiny.weights&lt;/code&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067834-51d5b500-cc2f-11e9-9357-c485b159a20b.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;YOLOv3-SPP:&lt;/strong&gt; &lt;code&gt;python3 detect.py --cfg cfg/yolov3-spp.cfg --weights yolov3-spp.weights&lt;/code&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/64067833-51d5b500-cc2f-11e9-8208-6fe197809131.jpg" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h1&gt;
&lt;p&gt;Download from: &lt;a href="https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0" rel="nofollow"&gt;https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-darknet-conversion" class="anchor" aria-hidden="true" href="#darknet-conversion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Darknet Conversion&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ git clone https://github.com/ultralytics/yolov3 &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolov3

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert darknet cfg/weights to pytorch model&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.weights')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; convert cfg/pytorch model to darknet weights&lt;/span&gt;
$ python3  -c &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;from models import *; convert('cfg/yolov3-spp.cfg', 'weights/yolov3-spp.pt')&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
Success: converted &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;weights/yolov3-spp.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; to &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;converted.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-map" class="anchor" aria-hidden="true" href="#map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;mAP&lt;/h1&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 test.py --weights ... --cfg ...&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;mAP@0.5 run at &lt;code&gt;--iou-thr 0.5&lt;/code&gt;, mAP@0.5...0.95 run at &lt;code&gt;--iou-thr 0.7&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;YOLOv3-SPP ultralytics is &lt;code&gt;ultralytics68.pt&lt;/code&gt; with &lt;code&gt;yolov3-spp.cfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Darknet results: &lt;a href="https://arxiv.org/abs/1804.02767" rel="nofollow"&gt;https://arxiv.org/abs/1804.02767&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;i&gt;&lt;/i&gt;&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;th&gt;COCO mAP&lt;br&gt;@0.5...0.95&lt;/th&gt;
&lt;th&gt;COCO mAP&lt;br&gt;@0.5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14.0&lt;br&gt;28.7&lt;br&gt;30.5&lt;br&gt;&lt;strong&gt;35.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;29.1&lt;br&gt;51.8&lt;br&gt;52.3&lt;br&gt;&lt;strong&gt;55.4&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;416&lt;/td&gt;
&lt;td&gt;16.0&lt;br&gt;31.2&lt;br&gt;33.9&lt;br&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;33.0&lt;br&gt;55.4&lt;br&gt;56.9&lt;br&gt;&lt;strong&gt;59.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;16.6&lt;br&gt;32.7&lt;br&gt;35.6&lt;br&gt;&lt;strong&gt;40.5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;34.9&lt;br&gt;57.7&lt;br&gt;59.5&lt;br&gt;&lt;strong&gt;61.4&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv3-tiny&lt;br&gt;YOLOv3&lt;br&gt;YOLOv3-SPP&lt;br&gt;&lt;strong&gt;YOLOv3-SPP ultralytics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;608&lt;/td&gt;
&lt;td&gt;16.6&lt;br&gt;33.1&lt;br&gt;37.0&lt;br&gt;&lt;strong&gt;41.1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;35.4&lt;br&gt;58.2&lt;br&gt;60.7&lt;br&gt;&lt;strong&gt;61.5&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 test.py --img-size 608 --iou-thr 0.6 --weights ultralytics68.pt --cfg yolov3-spp.cfg

Namespace(batch_size=32, cfg=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;yolov3-spp.cfg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, conf_thres=0.001, data=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/coco2014.data&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, device=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, img_size=608, iou_thres=0.6, save_json=True, task=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;test&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, weights=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ultralytics68.pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
Using CUDA device0 _CudaDeviceProperties(name=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Tesla V100-SXM2-16GB&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, total_memory=16130MB)
               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 157/157 [03:&lt;span class="pl-k"&gt;30&amp;lt;&lt;/span&gt;00:00,  1.16it/s]
                 all     5e+03  3.51e+04    0.0353     0.891     0.606    0.0673
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.409
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.615
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.75      &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.437
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.242
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.448
 Average Precision  (AP) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.519
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;  1 ] = 0.337
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt; 10 ] = 0.557
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;   all &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.612
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; small &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.438
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt;medium &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.658
 Average Recall     (AR) @[ IoU&lt;span class="pl-k"&gt;=&lt;/span&gt;0.50:0.95 &lt;span class="pl-k"&gt;|&lt;/span&gt; area&lt;span class="pl-k"&gt;=&lt;/span&gt; large &lt;span class="pl-k"&gt;|&lt;/span&gt; maxDets&lt;span class="pl-k"&gt;=&lt;/span&gt;100 ] = 0.746&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-reproduce-our-results" class="anchor" aria-hidden="true" href="#reproduce-our-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reproduce Our Results&lt;/h1&gt;
&lt;p&gt;This command trains &lt;code&gt;yolov3-spp.cfg&lt;/code&gt; from scratch to our mAP above. Training takes about one week on a 2080Ti.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ python3 train.py --weights &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; --cfg yolov3-spp.cfg --epochs 273 --batch 16 --accum 4 --multi --pre&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/26833433/70661588-76bbca00-1c19-11ea-86f9-23350d8c3193.png"&gt;&lt;img src="https://user-images.githubusercontent.com/26833433/70661588-76bbca00-1c19-11ea-86f9-23350d8c3193.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-reproduce-our-environment" class="anchor" aria-hidden="true" href="#reproduce-our-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reproduce Our Environment&lt;/h1&gt;
&lt;p&gt;To access an up-to-date working environment (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled), consider a:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GCP&lt;/strong&gt; Deep Learning VM with $300 free credit offer: See our &lt;a href="https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart"&gt;GCP Quickstart Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google Colab Notebook&lt;/strong&gt; with 12 hours of free GPU time: &lt;a href="https://colab.research.google.com/drive/1G8T-VFxQkjDe4idzN8F-hbIBqkkkQnxw" rel="nofollow"&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Docker Image&lt;/strong&gt; from &lt;a href="https://hub.docker.com/r/ultralytics/yolov3" rel="nofollow"&gt;https://hub.docker.com/r/ultralytics/yolov3&lt;/a&gt;. See &lt;a href="https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart"&gt;Docker Quickstart Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/146165888" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/cd760a8900fd4be0105229509d566b7c9499ef8d/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3134363136353838382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/146165888.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Issues should be raised directly in the repository.&lt;/strong&gt; For additional questions or comments please email Glenn Jocher at &lt;a href="mailto:glenn.jocher@ultralytics.com"&gt;glenn.jocher@ultralytics.com&lt;/a&gt; or visit us at &lt;a href="https://contact.ultralytics.com" rel="nofollow"&gt;https://contact.ultralytics.com&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ultralytics</author><guid isPermaLink="false">https://github.com/ultralytics/yolov3</guid><pubDate>Mon, 20 Jan 2020 00:17:00 GMT</pubDate></item><item><title>Hvass-Labs/TensorFlow-Tutorials #18 in Jupyter Notebook, Today</title><link>https://github.com/Hvass-Labs/TensorFlow-Tutorials</link><description>&lt;p&gt;&lt;i&gt;TensorFlow Tutorials with YouTube Videos&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-tutorials" class="anchor" aria-hidden="true" href="#tensorflow-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Tutorials&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials"&gt;Original repository on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Original author is &lt;a href="http://www.hvass-labs.org" rel="nofollow"&gt;Magnus Erik Hvass Pedersen&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;These tutorials are intended for beginners in Deep Learning and TensorFlow.&lt;/li&gt;
&lt;li&gt;Each tutorial covers a single topic.&lt;/li&gt;
&lt;li&gt;The source-code is well-documented.&lt;/li&gt;
&lt;li&gt;There is a &lt;a href="https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ" rel="nofollow"&gt;YouTube video&lt;/a&gt; for each tutorial.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Simple Linear Model
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/01_Simple_Linear_Model.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/01_Simple_Linear_Model.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Convolutional Neural Network
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;del&gt;Pretty Tensor&lt;/del&gt;
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/03_PrettyTensor.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/03_PrettyTensor.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;3-B. Layers API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/03B_Layers_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/03B_Layers_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;3-C. Keras API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/03C_Keras_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/03C_Keras_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;p&gt;Save &amp;amp; Restore
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/04_Save_Restore.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/04_Save_Restore.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensemble Learning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/05_Ensemble_Learning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/05_Ensemble_Learning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CIFAR-10
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/06_CIFAR-10.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/06_CIFAR-10.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inception Model
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/07_Inception_Model.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/07_Inception_Model.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transfer Learning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Video Data
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/09_Video_Data.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/09_Video_Data.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fine-Tuning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adversarial Examples
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/11_Adversarial_Examples.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/11_Adversarial_Examples.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adversarial Noise for MNIST
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/12_Adversarial_Noise_MNIST.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/12_Adversarial_Noise_MNIST.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Visual Analysis
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/13_Visual_Analysis.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/13_Visual_Analysis.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;13-B. Visual Analysis for MNIST
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/13B_Visual_Analysis_MNIST.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/13B_Visual_Analysis_MNIST.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;ol start="14"&gt;
&lt;li&gt;
&lt;p&gt;DeepDream
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/14_DeepDream.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/14_DeepDream.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Style Transfer
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement Learning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/16_Reinforcement_Learning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/16_Reinforcement_Learning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimator API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/17_Estimator_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/17_Estimator_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TFRecords &amp;amp; Dataset API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/18_TFRecords_Dataset_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/18_TFRecords_Dataset_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hyper-Parameter Optimization
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Natural Language Processing
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Machine Translation
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image Captioning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Time-Series Prediction
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/23_Time-Series-Prediction.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/23_Time-Series-Prediction.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-videos" class="anchor" aria-hidden="true" href="#videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Videos&lt;/h2&gt;
&lt;p&gt;These tutorials are also available as &lt;a href="https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ" rel="nofollow"&gt;YouTube videos&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-obsolete-tutorials" class="anchor" aria-hidden="true" href="#obsolete-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Obsolete Tutorials&lt;/h2&gt;
&lt;p&gt;Some of these tutorials use an API called PrettyTensor for creating
Neural Networks in TensorFlow, but the PrettyTensor API is now obsolete.
Some of the Notebooks are therefore also obsolete and they are clearly
marked at the top of each Notebook. It is recommended that you
instead use the Keras API for creating Neural Networks in TensorFlow.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h2&gt;
&lt;p&gt;These tutorials have been translated to the following languages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials-Chinese"&gt;Chinese&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-new-translations" class="anchor" aria-hidden="true" href="#new-translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New Translations&lt;/h3&gt;
&lt;p&gt;You can help by translating the remaining tutorials or reviewing the ones that have already been translated. You can also help by translating to other languages.&lt;/p&gt;
&lt;p&gt;It is a very big job to translate all the tutorials, so you should just start with Tutorials #01, #02 and #03-C which are the most important for beginners.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-new-videos" class="anchor" aria-hidden="true" href="#new-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New Videos&lt;/h3&gt;
&lt;p&gt;You are also very welcome to record your own YouTube videos in other languages. It is strongly recommended that you get a decent microphone because good sound quality is very important. I used &lt;code&gt;vokoscreen&lt;/code&gt; for recording the videos and the free &lt;a href="https://www.blackmagicdesign.com/products/davinciresolve/" rel="nofollow"&gt;DaVinci Resolve&lt;/a&gt; for editing the videos.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-forks" class="anchor" aria-hidden="true" href="#forks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Forks&lt;/h2&gt;
&lt;p&gt;See the &lt;a href="forks.md"&gt;selected list of forks&lt;/a&gt; for community modifications to these tutorials.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;There are different ways of installing and running TensorFlow. This section describes how I did it
for these tutorials. You may want to do it differently and you can search the internet for instructions.&lt;/p&gt;
&lt;p&gt;If you are new to using Python and Linux then this may be challenging
to get working and you may need to do internet searches for error-messages, etc.
It will get easier with practice. You can also run the tutorials without installing
anything by using Google Colab, see further below.&lt;/p&gt;
&lt;p&gt;Some of the Python Notebooks use source-code located in different files to allow for easy re-use
across multiple tutorials. It is therefore recommended that you download the whole repository
from GitHub, instead of just downloading the individual Python Notebooks.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-git" class="anchor" aria-hidden="true" href="#git"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git&lt;/h3&gt;
&lt;p&gt;The easiest way to download and install these tutorials is by using git from the command-line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create the directory &lt;code&gt;TensorFlow-Tutorials&lt;/code&gt; and download all the files to it.&lt;/p&gt;
&lt;p&gt;This also makes it easy to update the tutorials, simply by executing this command inside that directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git pull
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-download-zip-file" class="anchor" aria-hidden="true" href="#download-zip-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download Zip-File&lt;/h3&gt;
&lt;p&gt;You can also &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/archive/master.zip"&gt;download&lt;/a&gt;
the contents of the GitHub repository as a Zip-file and extract it manually.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-environment" class="anchor" aria-hidden="true" href="#environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment&lt;/h3&gt;
&lt;p&gt;I use &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt; because it comes with many Python
packages already installed and it is easy to work with. After installing Anaconda,
you should create a &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;conda environment&lt;/a&gt;
so you do not destroy your main installation in case you make a mistake somewhere:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name tf python=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When Python gets updated to a new version, it takes a while before TensorFlow also
uses the new Python version. So if the TensorFlow installation fails, then you may
have to specify an older Python version for your new environment, such as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name tf python=3.6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can switch to the new environment by running the following (on Linux):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source activate tf
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-required-packages" class="anchor" aria-hidden="true" href="#required-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Required Packages&lt;/h3&gt;
&lt;p&gt;The tutorials require several Python packages to be installed. The packages are listed in
&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;
First you need to edit this file and select whether you want to install the CPU or GPU
version of TensorFlow.&lt;/p&gt;
&lt;p&gt;To install the required Python packages and dependencies you first have to activate the
conda-environment as described above, and then you run the following command
in a terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the GPU-version of TensorFlow also requires the installation of various
NVIDIA drivers, which is not described here.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-version-35-or-later" class="anchor" aria-hidden="true" href="#python-version-35-or-later"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Version 3.5 or Later&lt;/h3&gt;
&lt;p&gt;These tutorials were developed on Linux using &lt;strong&gt;Python 3.5 / 3.6&lt;/strong&gt; (the &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt; distribution) and &lt;a href="https://www.jetbrains.com/pycharm/" rel="nofollow"&gt;PyCharm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are reports that Python 2.7 gives error messages with these tutorials. Please make sure you are using &lt;strong&gt;Python 3.5&lt;/strong&gt; or later!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-run" class="anchor" aria-hidden="true" href="#how-to-run"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How To Run&lt;/h2&gt;
&lt;p&gt;If you have followed the above installation instructions, you should
now be able to run the tutorials in the Python Notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd ~/development/TensorFlow-Tutorials/  # Your installation directory.
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should start a web-browser that shows the list of tutorials. Click on a tutorial to load it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run-in-google-colab" class="anchor" aria-hidden="true" href="#run-in-google-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run in Google Colab&lt;/h3&gt;
&lt;p&gt;If you do not want to install anything on your own computer, then the Notebooks
can be viewed, edited and run entirely on the internet by using
&lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Google Colab&lt;/a&gt;. There is a
&lt;a href="https://www.youtube.com/watch?v=Hs6HI2YWchM" rel="nofollow"&gt;YouTube video&lt;/a&gt; explaining how to do this.
You click the "Google Colab"-link next to each tutorial listed above.
You can view the Notebook on Colab but in order to run it you need to login using
your Google account.
Then you need to execute the following commands at the top of the Notebook,
which clones the contents of this repository to your work-directory on Colab.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Clone the repository from GitHub to Google Colab's temporary drive.
import os
work_dir = "/content/TensorFlow-Tutorials/"
if not os.path.exists(work_dir):
    !git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git
os.chdir(work_dir)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All required packages should already be installed on Colab, otherwise you
can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-older-versions" class="anchor" aria-hidden="true" href="#older-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Older Versions&lt;/h2&gt;
&lt;p&gt;Sometimes the source-code has changed from that shown in the YouTube videos. This may be due to
bug-fixes, improvements, or because code-sections are moved to separate files for easy re-use.&lt;/p&gt;
&lt;p&gt;If you want to see the exact versions of the source-code that were used in the YouTube videos,
then you can &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/commits/master"&gt;browse the history&lt;/a&gt;
of commits to the GitHub repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license-mit" class="anchor" aria-hidden="true" href="#license-mit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License (MIT)&lt;/h2&gt;
&lt;p&gt;These tutorials and source-code are published under the &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/LICENSE"&gt;MIT License&lt;/a&gt;
which allows very broad use for both academic and commercial purposes.&lt;/p&gt;
&lt;p&gt;A few of the images used for demonstration purposes may be under copyright. These images are included under the "fair usage" laws.&lt;/p&gt;
&lt;p&gt;You are very welcome to modify these tutorials and use them in your own projects.
Please keep a link to the &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials"&gt;original repository&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Hvass-Labs</author><guid isPermaLink="false">https://github.com/Hvass-Labs/TensorFlow-Tutorials</guid><pubDate>Mon, 20 Jan 2020 00:18:00 GMT</pubDate></item><item><title>fastai/course-v3 #19 in Jupyter Notebook, Today</title><link>https://github.com/fastai/course-v3</link><description>&lt;p&gt;&lt;i&gt;The 3rd edition of course.fast.ai&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-course-v3" class="anchor" aria-hidden="true" href="#course-v3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;course-v3&lt;/h1&gt;
&lt;p&gt;The 3rd edition of &lt;a href="https://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt;. See the &lt;code&gt;nbs&lt;/code&gt; folder for the notebooks.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/course-v3</guid><pubDate>Mon, 20 Jan 2020 00:19:00 GMT</pubDate></item><item><title>areed1192/sigma_coding_youtube #20 in Jupyter Notebook, Today</title><link>https://github.com/areed1192/sigma_coding_youtube</link><description>&lt;p&gt;&lt;i&gt;This is a collection of all the code that can be found on my YouTube channel Sigma Coding.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-sigma-coding" class="anchor" aria-hidden="true" href="#sigma-coding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
	Sigma Coding
&lt;/h1&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-tutorials--resources" class="anchor" aria-hidden="true" href="#tutorials--resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
	Tutorials &amp;amp; Resources
&lt;/h3&gt;
&lt;p align="center"&gt;
	&lt;strong&gt;
		&lt;a href="https://www.youtube.com/c/SigmaCoding" rel="nofollow"&gt;YouTube&lt;/a&gt;
		•
		&lt;a href="https://www.facebook.com/codingsigma" rel="nofollow"&gt;Facebook&lt;/a&gt;
		•
		&lt;a href="https://www.patreon.com/sigmacoding" rel="nofollow"&gt;Patreon&lt;/a&gt;
		•
		&lt;a href="https://github.com/sponsors/areed1192"&gt;GitHub Sponsor&lt;/a&gt;
	&lt;/strong&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#overview"&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#topics"&gt;Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#link-to-other-repositories"&gt;Link to Other Repositories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support-the-channel"&gt;Support the Channel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Howdy! My name is &lt;a href="https://www.linkedin.com/in/alex-reed/" rel="nofollow"&gt;Alex&lt;/a&gt;, and if you're like me, you enjoy the world of programming. Or maybe you were like me a few years ago and are beginning to take your first step into this exciting world. The GitHub repository you're currently contains almost all of the code you find on my YouTube channel Sigma Coding. Feel free to clone, download or branch this repository so you can leverage the code I share in my channel.&lt;/p&gt;
&lt;p&gt;Because I cover so many different langages on my YouTube channel, I dedicate a folder to each specific lanaguge. Right now, I cover the following lanagues on my channel:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;VBA&lt;/li&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;li&gt;M Langauge (Used for Power Query &amp;amp; Power BI)&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This list is continuously changing, and I do my best to make tutorials engaging, exciting, and most importantly, easy to follow!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-topics" class="anchor" aria-hidden="true" href="#topics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Topics&lt;/h2&gt;
&lt;p&gt;Now, I cover a lot of topics on my channel and as much I would like to list them all I don't want to overload with you a bunch of information. Here is a list of some of my more popular topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Python:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Win32COM&lt;/strong&gt; The Win32COM library allows us to control the VBA object model from Python.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TD Ameritrade API&lt;/strong&gt; The TD Ameritrade API allows us to stream real-time quote data and execute trades from Python.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt; I cover different machine learning models ranging from regression to classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VBA:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PowerPoint VBA&lt;/strong&gt; This series covers interacting with PowerPoint objects using VBA, topics like linking OLE objects and formatting slides.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word VBA&lt;/strong&gt; With Word VBA, we see how to manipulate different documents and change the underlying format in them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outlook VBA&lt;/strong&gt; In Outlook, we work with email objects and account information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Excel VBA&lt;/strong&gt; In Excel, we do an awful lot even working with non-standard libraries like ADODB.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;JavaScript:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Office API&lt;/strong&gt; Learn how to use the new JavaScript API for Microsoft Office.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TSQL:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;APIs&lt;/strong&gt; Learn how to make API request from Microsoft SQL Server.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;p&gt;If you ever have a question, would like to suggest a topic, found a mistake or just want some input for a project you can always email me at &lt;strong&gt;&lt;a href="mailto:coding.sigma@gmail.com"&gt;coding.sigma@gmail.com&lt;/a&gt;&lt;/strong&gt;. Additionally, you can find dedicated folders in the repository for resources like documentation.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links-to-other-respositories" class="anchor" aria-hidden="true" href="#links-to-other-respositories"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links To Other Respositories&lt;/h2&gt;
&lt;p&gt;Some of my projects are so large that they have dedicated repositories for them. Here is a list of repositiories of my other repositiories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/areed1192/td-ameritrade-python-api"&gt;TD Ameritrade API Client For Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Excel Object Model For Python - COMING SOON&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-support-these-projects" class="anchor" aria-hidden="true" href="#support-these-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support these Projects&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Patreon:&lt;/strong&gt;
If you like what you see! Then Help support the channel and future projects by donating to my &lt;a href="https://www.patreon.com/sigmacoding" rel="nofollow"&gt;Patreon Page&lt;/a&gt;. I'm always looking to add more content for individuals like yourself, unfortuantely some of the APIs I would require me to pay monthly fees.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hire Me:&lt;/strong&gt;
If you have a project, and you think I can help you with feel free to reach out at &lt;a href="mailto:coding.sigma@gmail.com"&gt;coding.sigma@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>areed1192</author><guid isPermaLink="false">https://github.com/areed1192/sigma_coding_youtube</guid><pubDate>Mon, 20 Jan 2020 00:20:00 GMT</pubDate></item><item><title>renelikestacos/Google-Earth-Engine-Python-Examples #21 in Jupyter Notebook, Today</title><link>https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples</link><description>&lt;p&gt;&lt;i&gt;Various examples for Google Earth Engine in Python using Jupyter Notebook&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Now using Folium for image visualization.
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-google-earth-engine-python-api-examples" class="anchor" aria-hidden="true" href="#google-earth-engine-python-api-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Earth Engine Python API Examples&lt;/h1&gt;
&lt;p&gt;A collection of &lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter Notebooks&lt;/a&gt; for &lt;a href="https://earthengine.google.com/" rel="nofollow"&gt;Google Earth Engine&lt;/a&gt; &lt;a href="http://www.python.org" rel="nofollow"&gt;Python&lt;/a&gt; API.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-jupyter-notebook-tutorials-for-google-earth-engine" class="anchor" aria-hidden="true" href="#jupyter-notebook-tutorials-for-google-earth-engine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook Tutorials for Google Earth Engine&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-001-landcover-classfication-for-landsat-8-toa-imagery" class="anchor" aria-hidden="true" href="#001-landcover-classfication-for-landsat-8-toa-imagery"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;001 Landcover Classfication for Landsat 8 TOA imagery&lt;/h4&gt;
&lt;p&gt;Classification Example for Landsat 8 including several vegetation indices and object feature extraction.
This example is based on the scientfic work "&lt;a href="https://www.mdpi.com/2072-4292/6/5/3923" rel="nofollow"&gt;MAD-MEX: Automatic Wall-to-Wall Land Cover Monitoring for the Mexican REDD-MRV Program Using All Landsat Data&lt;/a&gt;" by S.Gebhardt et. al 2014. Eventually you can't access the training data. In case you are interested in the training data, feel free to contact me.&lt;br&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/blob/master/src/img/001_training.png"&gt;&lt;img src="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/raw/master/src/img/001_training.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-002-tasseled-cap-transformation-for-landsat-8-toa-imagery" class="anchor" aria-hidden="true" href="#002-tasseled-cap-transformation-for-landsat-8-toa-imagery"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;002 Tasseled Cap Transformation for Landsat 8 TOA imagery&lt;/h4&gt;
&lt;p&gt;Tasseled Cap Transformation for Landsat 8 TOA imagery based on the scientfic work "Derivation of a tasselled cap transformation based on Landsat 8 at-satellite reflectance" by M.Baigab, L.Zhang, T.Shuai &amp;amp; Q.Tong (2014).&lt;br&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/blob/master/src/img/002_brightness.png"&gt;&lt;img src="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/raw/master/src/img/002_brightness.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-003-proba-v-ndvi-comparison" class="anchor" aria-hidden="true" href="#003-proba-v-ndvi-comparison"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;003 Proba-V NDVI Comparison&lt;/h4&gt;
&lt;p&gt;Comparison on Proba-V NDVI (Normalized Difference Vegetation Index) Imagery. One NDVI is derived on the fly, the other one is the actual NDVI band provided by Proba-V.&lt;br&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/blob/master/src/img/003_ndvi.png"&gt;&lt;img src="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/raw/master/src/img/003_ndvi.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-004-retrieve-proba-v-time-series" class="anchor" aria-hidden="true" href="#004-retrieve-proba-v-time-series"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;004 Retrieve Proba-V Time-Series&lt;/h4&gt;
&lt;p&gt;Display Proba-V NDVI (Normalized Difference Vegetation Index) Time-Series using Pandas and Matplotlib. Extracting Proba-V NDVI data from a randomly chosen point in Luxembourg.&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-005-proba-v-time-series-analysis" class="anchor" aria-hidden="true" href="#005-proba-v-time-series-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;005 Proba-V Time-Series Analysis&lt;/h4&gt;
&lt;p&gt;Basic Time-Series Analysis using Proba-V NDVI (Normalized Difference Vegetation Index) imagery.&lt;br&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/blob/master/src/img/005_timeseries.png"&gt;&lt;img src="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/raw/master/src/img/005_timeseries.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-006-linear-regression" class="anchor" aria-hidden="true" href="#006-linear-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;006 Linear Regression&lt;/h4&gt;
&lt;p&gt;Linear regression on Proba-V, Landsat and Climate Hazards Group InfraRed Precipitation (CHRIPS) data. This tutorial demonstrates the comparison of one of the most common supervised machine learning methods, the linear regression. We are going to compare &lt;a href="http://scikit-learn.org/stable/" rel="nofollow"&gt;scikit-learn&lt;/a&gt; and &lt;a href="http://www.statsmodels.org/stable/index.html" rel="nofollow"&gt;Statsmodels&lt;/a&gt;. For more information about types of Machine Learning, check this &lt;a href="https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861" rel="nofollow"&gt;link&lt;/a&gt;.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-007-time-series-prediction-and-forecast" class="anchor" aria-hidden="true" href="#007-time-series-prediction-and-forecast"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;007 Time-Series Prediction and Forecast&lt;/h4&gt;
&lt;p&gt;Proba-V NDVI Time-Series Prediction, using Fourier extrapolation and ARIMA model. Multiple step Time-Series Forecast on Proba-V NDVI data using &lt;a href="https://github.com/facebook/prophet"&gt;Facebook Prophet&lt;/a&gt;. Landsat and Climate Hazards Group InfraRed Precipitation (CHRIPS) data were used as additional regressors. &lt;br&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/blob/master/src/img/007_timeseries_forecast.png"&gt;&lt;img src="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/raw/master/src/img/007_timeseries_forecast.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-008-google-earth-engine-meets-geopandas" class="anchor" aria-hidden="true" href="#008-google-earth-engine-meets-geopandas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;008 Google Earth Engine meets GeoPandas&lt;/h4&gt;
&lt;p&gt;Extracting Landsat 8 TOA and CHIRPS precipitation data from Google Earth Engine and use &lt;a href="http://geopandas.org/" rel="nofollow"&gt;Geopandas&lt;/a&gt; capabilities to create time series analysis. Furthermore, data will be visualized through a time series viewer&lt;br&gt;&lt;br&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/blob/master/src/img/008_geopandas.png"&gt;&lt;img src="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/raw/master/src/img/008_geopandas.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;and also a precipitation heat map using &lt;a href="https://python-visualization.github.io/folium/" rel="nofollow"&gt;Folium&lt;/a&gt;.
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/blob/master/src/img/008_heatmap.png"&gt;&lt;img src="https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples/raw/master/src/img/008_heatmap.png" alt="alt text" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>renelikestacos</author><guid isPermaLink="false">https://github.com/renelikestacos/Google-Earth-Engine-Python-Examples</guid><pubDate>Mon, 20 Jan 2020 00:21:00 GMT</pubDate></item><item><title>deepmind/deepmind-research #22 in Jupyter Notebook, Today</title><link>https://github.com/deepmind/deepmind-research</link><description>&lt;p&gt;&lt;i&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deepmind-research" class="anchor" aria-hidden="true" href="#deepmind-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepMind Research&lt;/h1&gt;
&lt;p&gt;This repository contains implementations and illustrative code to accompany
DeepMind publications. Along with publishing papers to accompany research
conducted at DeepMind, we release open-source
&lt;a href="https://deepmind.com/research/open-source/open-source-environments/" rel="nofollow"&gt;environments&lt;/a&gt;,
&lt;a href="https://deepmind.com/research/open-source/open-source-datasets/" rel="nofollow"&gt;data sets&lt;/a&gt;,
and &lt;a href="https://deepmind.com/research/open-source/open-source-code/" rel="nofollow"&gt;code&lt;/a&gt; to
enable the broader research community to engage with our work and build upon it,
with the ultimate goal of accelerating scientific progress to benefit society.
For example, you can build on our implementations of the
&lt;a href="https://github.com/deepmind/dqn"&gt;Deep Q-Network&lt;/a&gt; or
&lt;a href="https://github.com/deepmind/dnc"&gt;Differential Neural Computer&lt;/a&gt;, or experiment
in the same environments we use for our research, such as
&lt;a href="https://github.com/deepmind/lab"&gt;DeepMind Lab&lt;/a&gt; or
&lt;a href="https://github.com/deepmind/pysc2"&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you enjoy building tools, environments, software libraries, and other
infrastructure of the kind listed below, you can view open positions to work in
related areas on our &lt;a href="https://deepmind.com/careers/" rel="nofollow"&gt;careers page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a full list of our publications, please see
&lt;a href="https://deepmind.com/research/publications/" rel="nofollow"&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="alphafold_casp13"&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt;
&lt;li&gt;&lt;a href="unrestricted_advx"&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hierarchical_probabilistic_unet"&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="scratchgan"&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="tvt"&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="curl"&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="transporter"&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="bigbigan"&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="cs_gan"&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="side_effects_penalties"&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="PrediNet"&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="unsupervised_adversarial_training"&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="graph_matching_networks"&gt;Graph Matching Networks for Learning the Similarity of Graph Structured
Objects&lt;/a&gt;, ICML 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="regal"&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>deepmind</author><guid isPermaLink="false">https://github.com/deepmind/deepmind-research</guid><pubDate>Mon, 20 Jan 2020 00:22:00 GMT</pubDate></item><item><title>OpenGenus/cosmos #23 in Jupyter Notebook, Today</title><link>https://github.com/OpenGenus/cosmos</link><description>&lt;p&gt;&lt;i&gt;Algorithms that run our universe | Your personal library of every algorithm and data structure code that you will ever encounter | Ask us anything at our forum |&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cosmos" class="anchor" aria-hidden="true" href="#cosmos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cosmos&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://discourse.opengenus.org/" rel="nofollow"&gt;Join our discussion now&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The universe of algorithm and data structures&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Cosmos&lt;/strong&gt; is your personal offline collection of every algorithm and data structure one will ever encounter and use in a lifetime. This provides solutions in various languages spanning &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;JavaScript&lt;/code&gt;, &lt;code&gt;Swift&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt; and others.&lt;/p&gt;
&lt;p&gt;This work is maintained by a community of hundreds of people and is a &lt;em&gt;massive collaborative effort&lt;/em&gt; to bring the readily available coding knowledge &lt;strong&gt;offline&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Many coders ask me how to improve their own performances. I cannot say anything except "solve and review and prepare your library"&lt;/strong&gt; - &lt;em&gt;Uwi Tenpen&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-cosmic-structure" class="anchor" aria-hidden="true" href="#cosmic-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cosmic Structure&lt;/h1&gt;
&lt;p&gt;Following is the high-level structure of cosmos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/code/artificial_intelligence"&gt;Artificial intelligence&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png"&gt;🤖&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/backtracking"&gt;Backtracking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/bit_manipulation"&gt;Bit manipulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/cellular_automaton"&gt;Cellular automaton&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="shell" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41a.png"&gt;🐚&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/compression"&gt;Compression algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/computational_geometry"&gt;Computational geometry&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/cryptography"&gt;Cryptography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/data_structures"&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/design_pattern"&gt;Design pattern&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/divide_conquer"&gt;Divide conquering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/dynamic_programming"&gt;Dynamic programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/graph_algorithms"&gt;Graph algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/greedy_algorithms"&gt;Greedy algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/mathematical_algorithms"&gt;Mathematical algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;🔢&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/networking"&gt;Networking&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;🌐&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/numerical_analysis"&gt;Numerical analysis&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;📈&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/online_challenges"&gt;Online challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/operating_system"&gt;Operating system&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/quantum_algorithms"&gt;Quantum algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="cyclone" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f300.png"&gt;🌀&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/randomized_algorithms"&gt;Randomized algorithms&lt;/a&gt;  &lt;g-emoji class="g-emoji" alias="slot_machine" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b0.png"&gt;🎰&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/search"&gt;Searching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/selection_algorithms"&gt;Selecting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/sorting"&gt;Sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/square_root_decomposition"&gt;Square root decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/string_algorithms"&gt;String algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/code/unclassified"&gt;Unclassified&lt;/a&gt; &lt;g-emoji class="g-emoji" alias="ghost" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f47b.png"&gt;👻&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each type has several hundreds of problems with solutions in several languages spanning &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt; and others.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-maintainers" class="anchor" aria-hidden="true" href="#maintainers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Maintainers&lt;/h1&gt;
&lt;p&gt;This is a massive collaboration. Hence, to keep the quality intact and drive the vision in the proper direction, we have maintainers.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Maintainers are your friends forever. They are vastly different from moderators.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Currently, we have &lt;strong&gt;5 active maintainers&lt;/strong&gt; and we are expanding quickly.&lt;/p&gt;
&lt;p&gt;The task of maintainers is to review pull requests, suggest further quality additions and keep the work up to date with the current state of the world. &lt;g-emoji class="g-emoji" alias="earth_africa" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png"&gt;🌍&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/OpenGenus/cosmos/wiki/maintainers"&gt;Check out our current maintainers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let us know if you would like to be a maintainer in the Slack channel &lt;em&gt;#algorithms&lt;/em&gt; and we will review and add you upon subsequent contributions. To join our massive community at &lt;a href="https://opengenus.slack.com" rel="nofollow"&gt;Slack&lt;/a&gt; open an issue &lt;a href="https://github.com/OpenGenus/OpenGenus-Slack"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h1&gt;
&lt;p&gt;The success of our vision to bring knowledge offline depends on you. Even a small contribution helps. All forms of contributions are highly welcomed and valued.&lt;/p&gt;
&lt;p&gt;Currently, we have over &lt;strong&gt;700 contributors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you contribute, your name with a link (if available) is added to our &lt;a href="https://github.com/OpenGenus/cosmos/wiki/contributors"&gt;contributors list&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can contribute by writing &lt;code&gt;code&lt;/code&gt;, documentation in the form of &lt;code&gt;installation guides&lt;/code&gt; and &lt;code&gt;style guides&lt;/code&gt;, making Cosmos search friendly and many others. There are endless possibilities.&lt;/p&gt;
&lt;p&gt;Additionally, you might want to take a look at this &lt;a href="https://github.com/OpenGenus/cosmos/wiki/contribute"&gt;contributing guidelines&lt;/a&gt; before you make Cosmos better.&lt;/p&gt;
&lt;p&gt;You may, also, refer to the available &lt;a href="/guides/coding_style"&gt;style guides&lt;/a&gt; before contributing code.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;We believe in freedom and improvement. &lt;a href="https://github.com/OpenGenus/cosmos/blob/master/LICENSE"&gt;GNU General Public License v3.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>OpenGenus</author><guid isPermaLink="false">https://github.com/OpenGenus/cosmos</guid><pubDate>Mon, 20 Jan 2020 00:23:00 GMT</pubDate></item><item><title>google/dopamine #24 in Jupyter Notebook, Today</title><link>https://github.com/google/dopamine</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;p&gt;No README was found for this project.&lt;/p&gt;</description><author>google</author><guid isPermaLink="false">https://github.com/google/dopamine</guid><pubDate>Mon, 20 Jan 2020 00:24:00 GMT</pubDate></item><item><title>hse-aml/natural-language-processing #25 in Jupyter Notebook, Today</title><link>https://github.com/hse-aml/natural-language-processing</link><description>&lt;p&gt;&lt;i&gt;Resources for "Natural Language Processing" Coursera course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-natural-language-processing-course-resources" class="anchor" aria-hidden="true" href="#natural-language-processing-course-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing course resources&lt;/h1&gt;
&lt;p&gt;This github contains practical assignments for Natural Language Processing course by Higher School of Economics:
&lt;a href="https://www.coursera.org/learn/language-processing" rel="nofollow"&gt;https://www.coursera.org/learn/language-processing&lt;/a&gt;.
In this course you will learn how to solve common NLP problems using classical and deep learning approaches.&lt;/p&gt;
&lt;p&gt;From a practical side, we expect your familiarity with Python, since we will use it for all assignments in the course. Two of the assignments will also involve TensorFlow. You will work with many other libraries, including NLTK, Scikit-learn, and Gensim. You have several options on how to set it up.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-1-running-on-google-colab" class="anchor" aria-hidden="true" href="#1-running-on-google-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Running on Google Colab&lt;/h2&gt;
&lt;p&gt;Google has released its own flavour of Jupyter called Colab, which has free GPUs!&lt;/p&gt;
&lt;p&gt;Here's how you can use it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;https://colab.research.google.com&lt;/a&gt;, click &lt;strong&gt;Sign in&lt;/strong&gt; in the upper right corner, use your Google credentials to sign in.&lt;/li&gt;
&lt;li&gt;Click &lt;strong&gt;GITHUB&lt;/strong&gt; tab, paste &lt;a href="https://github.com/hse-aml/natural-language-processing"&gt;https://github.com/hse-aml/natural-language-processing&lt;/a&gt; and press Enter&lt;/li&gt;
&lt;li&gt;Choose the notebook you want to open, e.g. week1/week1-MultilabelClassification.ipynb&lt;/li&gt;
&lt;li&gt;Click &lt;strong&gt;File -&amp;gt; Save a copy in Drive...&lt;/strong&gt; to save your progress in Google Drive&lt;/li&gt;
&lt;li&gt;&lt;em&gt;If you need a GPU&lt;/em&gt;, click &lt;strong&gt;Runtime -&amp;gt; Change runtime type&lt;/strong&gt; and select &lt;strong&gt;GPU&lt;/strong&gt; in Hardware accelerator box&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Execute&lt;/strong&gt; the following code in the first cell that downloads dependencies (change for your week number):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;! wget https:&lt;span class="pl-k"&gt;//&lt;/span&gt;raw.githubusercontent.com&lt;span class="pl-k"&gt;/&lt;/span&gt;hse&lt;span class="pl-k"&gt;-&lt;/span&gt;aml&lt;span class="pl-k"&gt;/&lt;/span&gt;natural&lt;span class="pl-k"&gt;-&lt;/span&gt;language&lt;span class="pl-k"&gt;-&lt;/span&gt;processing&lt;span class="pl-k"&gt;/&lt;/span&gt;master&lt;span class="pl-k"&gt;/&lt;/span&gt;setup_google_colab.py &lt;span class="pl-k"&gt;-&lt;/span&gt;O setup_google_colab.py
&lt;span class="pl-k"&gt;import&lt;/span&gt; setup_google_colab
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; please, uncomment the week you're working on&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; setup_google_colab.setup_week1()  &lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; setup_google_colab.setup_week2()&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; setup_google_colab.setup_week3()&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; setup_google_colab.setup_week4()&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; setup_google_colab.setup_project()&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; setup_google_colab.setup_honor()&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="7"&gt;
&lt;li&gt;If you run many notebooks on Colab, they can continue to eat up memory,
you can kill them with &lt;code&gt;! pkill -9 python3&lt;/code&gt; and check with &lt;code&gt;! nvidia-smi&lt;/code&gt; that GPU memory is freed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Known issues:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No support for &lt;code&gt;ipywidgets&lt;/code&gt;, so we cannot use fancy &lt;code&gt;tqdm&lt;/code&gt; progress bars.
For now, we use a simplified version of a progress bar suitable for Colab.&lt;/li&gt;
&lt;li&gt;Blinking animation with &lt;code&gt;IPython.display.clear_output()&lt;/code&gt;.
It's usable, but still looking for a workaround.&lt;/li&gt;
&lt;li&gt;If you see an error "No module named 'common'", make sure you've uncommented the assignment-specific line in step 6, restart your kernel and execute all cells again&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-running-locally" class="anchor" aria-hidden="true" href="#2-running-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Running locally&lt;/h2&gt;
&lt;p&gt;Two options here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Use the Docker container of our course. It already has all libraries, that you will need. The setup for you is very simple: install Docker application depending on your OS, download our container image, run everything within the container. Please, see this &lt;a href="(Docker-tutorial.md)"&gt;detailed Docker tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Manually install all the libraries depending on your OS (each task contains a list of needed libraries in the very beginning). If you use Windows/MacOS you might find useful Anaconda distribution which allows to install easily most of the needed libraries. However, some tools, like StarSpace for week 2, are not compatible with Windows, so it's likely that you will have to use Docker anyways, if you go for these tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It might take a significant amount of time and resources to run the assignments code, but we expect that an average laptop is enough to accomplish the tasks. All assignments were tested in the Docker on Mac with 8GB RAM. If you have memory errors, that could be caused by not tested configurations or inefficient code. Consider reporting these cases or double-checking your code.&lt;/p&gt;
&lt;p&gt;For the final project, you will need to set up AWS machine - see &lt;a href="AWS-tutorial.md"&gt;AWS tutorial here&lt;/a&gt;. You are also welcome to try it out earlier during the course.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>hse-aml</author><guid isPermaLink="false">https://github.com/hse-aml/natural-language-processing</guid><pubDate>Mon, 20 Jan 2020 00:25:00 GMT</pubDate></item></channel></rss>