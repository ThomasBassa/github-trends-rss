<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, Today</title><link>https://github.com/trending/jupyter-notebook?since=daily</link><description>The top repositories on GitHub for jupyter-notebook, measured daily</description><pubDate>Mon, 06 Jan 2020 01:08:18 GMT</pubDate><lastBuildDate>Mon, 06 Jan 2020 01:08:18 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>Pierian-Data/Complete-Python-3-Bootcamp #1 in Jupyter Notebook, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Mon, 06 Jan 2020 00:01:00 GMT</pubDate></item><item><title>DataScienceResearchPeru/OpenSource-RoadMap-DataScience #2 in Jupyter Notebook, Today</title><link>https://github.com/DataScienceResearchPeru/OpenSource-RoadMap-DataScience</link><description>&lt;p&gt;&lt;i&gt;¡Camino a una educación autodidacta en Ciencia de Datos!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt; 
&lt;a target="_blank" rel="noopener noreferrer" href="images/foto-github.png"&gt;&lt;img src="images/foto-github.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-open-source-road-map-data-science" class="anchor" aria-hidden="true" href="#open-source-road-map-data-science"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Open Source Road Map Data Science&lt;/h3&gt;
&lt;p align="center"&gt;
  &lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; ¡Camino a una educación autodidacta en &lt;strong&gt;Data Science&lt;/strong&gt;!
  &lt;br&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-contenido" class="anchor" aria-hidden="true" href="#star-contenido"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Contenido&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#C%C3%B3mo-contribuir"&gt;Cómo contribuir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#De-qu%C3%A9-trata-esto"&gt;De qué trata esto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conviertete-en-un-estudiante-de-data-science-research-per%C3%BA"&gt;Conviertete en un estudiante de Data Science Research Perú&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#Motivaci%C3%B3n-y-Preparaci%C3%B3n"&gt;Motivación y Preparación&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#curr%C3%ADcula"&gt;Currícula&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#c%C3%B3mo-usar-est%C3%A1-gu%C3%ADa"&gt;Cómo usar está guía&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-star-cómo-contribuir" class="anchor" aria-hidden="true" href="#star-cómo-contribuir"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Cómo contribuir&lt;/h2&gt;
&lt;p&gt;El objetivo de este repositorio es contribuir a la formación de los profesionales interesados en Ciencia de Datos e Inteligencia Artificial.
Esto ayudará a incrementar los profesionales peruanos e hispanohablantes y así tener mas Data Scientist, Data Engineer, Machine Learning Engineer, Data Architects y demás perfiles existentes.
Puede hacer un Pull Request y agregar más contenido que crea necesario.
Aquí un &lt;a href="https://blog.desdelinux.net/tutorial-simple-primer-pr-pull-request/" rel="nofollow"&gt;Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-de-qué-trata-esto" class="anchor" aria-hidden="true" href="#star-de-qué-trata-esto"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; De qué trata esto&lt;/h2&gt;
&lt;p&gt;Esto es un camino sólido para aquellos que desean completar un curso de Ciencia de datos en su propio tiempo,
con cursos de las &lt;strong&gt;mejores universidades&lt;/strong&gt; en el mundo. En nuestro plan de estudios, damos preferencia a los
cursos de estilo MOOC (Massive Open Online Course) porque estos cursos se crearon teniendo en cuenta nuestro
estilo de aprendizaje.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-conviertete-en-un-estudiante-de-data-science-research-perú" class="anchor" aria-hidden="true" href="#star-conviertete-en-un-estudiante-de-data-science-research-perú"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Conviertete en un estudiante de Data Science Research Perú&lt;/h2&gt;
&lt;p&gt;Pueden enviarnos sugerencias y unirse a nuestros grupos de WhatsApp mediante el siguiente &lt;a href="https://wa.me/51931534817" rel="nofollow"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[Pendiente]&lt;/strong&gt; Para inscribirse oficialmente en este curso, debe crear un perfil en nuestra &lt;a href="https://datascience.pe" rel="nofollow"&gt;web&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-motivación-y-preparación" class="anchor" aria-hidden="true" href="#star-motivación-y-preparación"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Motivación y Preparación&lt;/h2&gt;
&lt;p&gt;Aquí hay dos enlaces interesantes que pueden marcar &lt;strong&gt;toda&lt;/strong&gt; la diferencia en su viaje.&lt;/p&gt;
&lt;p&gt;El primero es un video motivacional que muestra a un chico que pasó por el "Desafío MIT",
que consiste en aprender todo el currículo MIT &lt;strong&gt;de 4 años&lt;/strong&gt; para Ciencias de la Computación en &lt;strong&gt;1 año&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.scotthyoung.com/blog/myprojects/mit-challenge-2/" rel="nofollow"&gt;MIT Challenge&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El segundo enlace es un MOOC que le enseñará técnicas de aprendizaje utilizadas por expertos en arte, música,
literatura, matemáticas, ciencias, deportes y muchas otras disciplinas. Estas son &lt;strong&gt;habilidades fundamentales&lt;/strong&gt;
para tener éxito.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/learning-how-to-learn" rel="nofollow"&gt;Learning How to Learn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;¿Estas listo para empezar?&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-currícula" class="anchor" aria-hidden="true" href="#star-currícula"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Currícula&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#algebra-lineal"&gt;Algebra Lineal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#calculo"&gt;Cálculo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#calculo-multivariable"&gt;Cálculo multivariable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#base-de-datos"&gt;Base de Datos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilidad-y-estadistica"&gt;Probabilidad y Estadística&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#introduccion-a-la-ciencia-de-datos"&gt;Introducción a la Ciencia de Datos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#machine-learning"&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimizacion-convexa"&gt;Optimización Convexa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#manipulacion-y-recuperacion-de-datos"&gt;Manipulación y recuperación de datos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#big-data"&gt;Big Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#natural-language-processing"&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-learning"&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#especializacion"&gt;Especialización&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-star-algebra-lineal" class="anchor" aria-hidden="true" href="#star-algebra-lineal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Algebra Lineal&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/linear-algebra-foundations-to-frontiers" rel="nofollow"&gt;Algebra Lineal - Fundamentos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;15 semanas&lt;/td&gt;
&lt;td align="center"&gt;8 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/advanced-linear-algebra-foundations-to-frontiers" rel="nofollow"&gt;Algebra Lineal - Avanzado&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;15 semanas&lt;/td&gt;
&lt;td align="center"&gt;8 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/applications-linear-algebra-part-1-davidsonx-d003x-1" rel="nofollow"&gt;Aplicaciones de Álgebra Lineal Part 1&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;4 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/applications-linear-algebra-part-2-davidsonx-d003x-2" rel="nofollow"&gt;Aplicaciones de Álgebra Lineal Part 2&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 semanas&lt;/td&gt;
&lt;td align="center"&gt;5 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-cálculo" class="anchor" aria-hidden="true" href="#star-cálculo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Cálculo&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/calculus-1a-differentiation-mitx-18-01-1x" rel="nofollow"&gt;Calculus 1A: Diferenciación&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;13 semanas&lt;/td&gt;
&lt;td align="center"&gt;6-10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/calculus-1b-integration-mitx-18-01-2x" rel="nofollow"&gt;Calculus 1B: Integración&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;13 semanas&lt;/td&gt;
&lt;td align="center"&gt;5-10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/calculus-1c-coordinate-systems-infinite-mitx-18-01-3x" rel="nofollow"&gt;Calculus 1C: Sistemas de coordenadas y series infinitas&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;13 semanas&lt;/td&gt;
&lt;td align="center"&gt;6-10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-cálculo-multivariable" class="anchor" aria-hidden="true" href="#star-cálculo-multivariable"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Cálculo multivariable&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/index.htm" rel="nofollow"&gt;MIT Cálculo multivariable&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;15 semanas&lt;/td&gt;
&lt;td align="center"&gt;8 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;MIT&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-base-de-datos" class="anchor" aria-hidden="true" href="#star-base-de-datos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Base de Datos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about" rel="nofollow"&gt;Curso Base de Datos de Stanford&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;8-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Stanford&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-probabilidad-y-estadística" class="anchor" aria-hidden="true" href="#star-probabilidad-y-estadística"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Probabilidad y Estadística&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-probability-science-mitx-6-041x-1#.U3yb762SzIo" rel="nofollow"&gt;Introducción a la Probabilidad&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;16 semanas&lt;/td&gt;
&lt;td align="center"&gt;12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/OLI/StatReasoning/Open/about" rel="nofollow"&gt;Razonamiento Estadístico&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Standford&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-statistics-descriptive-uc-berkeleyx-stat2-1x" rel="nofollow"&gt;Introducción a la Estadística: Descriptiva&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-statistics-probability-uc-berkeleyx-stat2-2x" rel="nofollow"&gt;Introducción a la Estadística: Probabilística&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-statistics-inference-uc-berkeleyx-stat2-3x" rel="nofollow"&gt;Introducción a la Estadística: Inferencia&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;5 semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-python" class="anchor" aria-hidden="true" href="#star-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Python&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-7" rel="nofollow"&gt;Introducción a Computer Science y Programación usando Python&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;9 semanas&lt;/td&gt;
&lt;td align="center"&gt;15 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/introduction-computational-thinking-data-mitx-6-00-2x-3" rel="nofollow"&gt;Introducción al Pensamiento Computacional y Ciencia de Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;10 semanas&lt;/td&gt;
&lt;td align="center"&gt;15 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://prod-edx-mktg-edit.edx.org/course/introduction-python-data-science-microsoft-dat208x-1" rel="nofollow"&gt;Introducción a Python para Ciencia&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;6 semanas&lt;/td&gt;
&lt;td align="center"&gt;2-4 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/programming-python-data-science-microsoft-dat210x" rel="nofollow"&gt;Programación con Python para Ciencia de Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;6 semanas&lt;/td&gt;
&lt;td align="center"&gt;3-4 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-r" class="anchor" aria-hidden="true" href="#star-r"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; R&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/specializations/data-science-foundations-r" rel="nofollow"&gt;Programa especializado Data Science: Foundations using R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 meses&lt;/td&gt;
&lt;td align="center"&gt;6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/learn/r-programming" rel="nofollow"&gt;Programación R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;2 semanas&lt;/td&gt;
&lt;td align="center"&gt;10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/specializations/statistics" rel="nofollow"&gt;Programa especializado Statistics with R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;7 meses&lt;/td&gt;
&lt;td align="center"&gt;5 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/learn/intro-data-science-programacion-estadistica-r" rel="nofollow"&gt;Introducción a Data Science: Programación Estadística con R&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 semanas&lt;/td&gt;
&lt;td align="center"&gt;3-5 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-introducción-a-la-ciencia-de-datos" class="anchor" aria-hidden="true" href="#star-introducción-a-la-ciencia-de-datos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Introducción a la Ciencia de Datos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/course/datasci" rel="nofollow"&gt;Introducción a la Ciencia de Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;8 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://cs109.github.io/2015/" rel="nofollow"&gt;Ciencia de Datos - CS109 de Harvard&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;12 semanas&lt;/td&gt;
&lt;td align="center"&gt;5-6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Harvard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/analytics-edge-mitx-15-071x-2" rel="nofollow"&gt;La Ventaja de Analítica&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;12 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-15 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-machine-learning" class="anchor" aria-hidden="true" href="#star-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Machine Learning&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.edx.org/course/learning-data-introductory-machine-caltechx-cs1156x" rel="nofollow"&gt;Introducción a Machine Learning&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;10 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-20 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Edx&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://work.caltech.edu/lectures.html" rel="nofollow"&gt;Aprendiendo de los Datos&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;10 semanas&lt;/td&gt;
&lt;td align="center"&gt;10-20 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;California Institute of Technology&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about" rel="nofollow"&gt;Aprendizaje estadístico&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;3 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Standford&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;Curso Machine Learning de Stanford&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;8-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Coursera&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-optimización-convexa" class="anchor" aria-hidden="true" href="#star-optimización-convexa"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Optimización Convexa&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about" rel="nofollow"&gt;Optimización Convexa&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;9 semanas&lt;/td&gt;
&lt;td align="center"&gt;10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Standford&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-manipulación-y-recuperación-de-datos" class="anchor" aria-hidden="true" href="#star-manipulación-y-recuperación-de-datos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Manipulación y recuperación de datos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/data-wrangling-with-mongodb--ud032" rel="nofollow"&gt;Manipulación y recuperación de datos con MongoDB&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;8 semanas&lt;/td&gt;
&lt;td align="center"&gt;10 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-big-data" class="anchor" aria-hidden="true" href="#star-big-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Big Data&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617" rel="nofollow"&gt;Introducción a Hadoop y MapReduce&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;4 semanas&lt;/td&gt;
&lt;td align="center"&gt;6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/deploying-a-hadoop-cluster--ud1000" rel="nofollow"&gt;Despliegue a Hadoop Cluster&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;3 semanas&lt;/td&gt;
&lt;td align="center"&gt;6 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-natural-language-processing" class="anchor" aria-hidden="true" href="#star-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Natural Language Processing&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="http://cs224d.stanford.edu/" rel="nofollow"&gt;Deep Learning for Natural Language Processing&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;- semanas&lt;/td&gt;
&lt;td align="center"&gt;- horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Stanford&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-deep-learning" class="anchor" aria-hidden="true" href="#star-deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Deep Learning&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Cursos&lt;/th&gt;
&lt;th align="center"&gt;Duración&lt;/th&gt;
&lt;th align="center"&gt;Esfuerzo&lt;/th&gt;
&lt;th align="center"&gt;Plataforma&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://www.udacity.com/course/deep-learning--ud730" rel="nofollow"&gt;Deep Learning&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;12 semanas&lt;/td&gt;
&lt;td align="center"&gt;8-12 horas/semana&lt;/td&gt;
&lt;td align="center"&gt;Udacity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-star-especialización" class="anchor" aria-hidden="true" href="#star-especialización"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Especialización&lt;/h3&gt;
&lt;p&gt;Después de terminar los cursos anteriores, comience sus especializaciones en los temas que le interesan más.
Puede ver una lista de especializaciones disponibles. &lt;a href="https://github.com/DataScienceResearchPeru/OpenSource-RoadMap-DataScience/tree/master/especializacion"&gt;Aquí&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/15a5e1c20a0a4a22f1db4c752e6629c81614ab7e/687474703a2f2f692e696d6775722e636f6d2f5245514b3056552e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/15a5e1c20a0a4a22f1db4c752e6629c81614ab7e/687474703a2f2f692e696d6775722e636f6d2f5245514b3056552e6a7067" alt="keep learning" data-canonical-src="http://i.imgur.com/REQK0VU.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-star-cómo-usar-está-guía" class="anchor" aria-hidden="true" href="#star-cómo-usar-está-guía"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; Cómo usar está guía&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-orden-de-las-clases" class="anchor" aria-hidden="true" href="#orden-de-las-clases"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Orden de las clases&lt;/h3&gt;
&lt;p&gt;Esta guía fue desarrollada para ser consumida en un enfoque lineal. ¿Qué significa esto? Que debes completar un curso a la vez.&lt;/p&gt;
&lt;p&gt;Los cursos ya están en el orden en que debe completarse.
Simplemente comience en la sección &lt;a href="#%C3%A1lgebra-lineal"&gt;Álgebra lineal&lt;/a&gt; y después de terminar el primer curso, comience el siguiente.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Si el curso no está abierto, hágalo de todos modos con los recursos de la clase anterior.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-debo-tomar-todos-los-cursos" class="anchor" aria-hidden="true" href="#debo-tomar-todos-los-cursos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;¿Debo tomar todos los cursos?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Si!&lt;/strong&gt; ¡La intención es concluir &lt;strong&gt;todos&lt;/strong&gt; los cursos listados aquí!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-duración" class="anchor" aria-hidden="true" href="#duración"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Duración&lt;/h3&gt;
&lt;p&gt;¡Puede llevar más tiempo completar todas las clases en comparación con un curso regular de Ciencias de Datos, pero podemos &lt;strong&gt;garantizar&lt;/strong&gt; que su &lt;strong&gt;recompensa&lt;/strong&gt; será proporcional a &lt;strong&gt;su motivación / dedicación&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Debes concentrarte en tu &lt;strong&gt;hábito&lt;/strong&gt; y &lt;strong&gt;olvidarte&lt;/strong&gt; de los objetivos. Intenta invertir 1 ~ 2 horas &lt;strong&gt;todos los días&lt;/strong&gt; estudiando este plan de estudios. Si haces esto, &lt;strong&gt;inevitablemente&lt;/strong&gt; terminarás este plan de estudios.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-créditos" class="anchor" aria-hidden="true" href="#créditos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Créditos&lt;/h2&gt;
&lt;p&gt;El repositorio que nos sirvió de inspiración: &lt;a href="https://github.com/ossu/data-science"&gt;OSSU &lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>DataScienceResearchPeru</author><guid isPermaLink="false">https://github.com/DataScienceResearchPeru/OpenSource-RoadMap-DataScience</guid><pubDate>Mon, 06 Jan 2020 00:02:00 GMT</pubDate></item><item><title>rasbt/deeplearning-models #3 in Jupyter Notebook, Today</title><link>https://github.com/rasbt/deeplearning-models</link><description>&lt;p&gt;&lt;i&gt;A collection of various deep learning architectures, models, and tips&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.7" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning Models&lt;/h1&gt;
&lt;p&gt;A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-traditional-machine-learning" class="anchor" aria-hidden="true" href="#traditional-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Traditional Machine Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/perceptron.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/perceptron.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/logistic-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/logistic-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression (Multinomial Logistic Regression)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Softmax Regression with MLxtend's plot_decision_regions on Iris&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/basic-ml/softmax-regression-mlxtend-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-multilayer-perceptrons" class="anchor" aria-hidden="true" href="#multilayer-perceptrons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multilayer Perceptrons&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multilayer Perceptron&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Dropout&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-dropout.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-dropout.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Batch Normalization&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer Perceptron with Backpropagation from Scratch&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mlp/mlp-lowlevel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mlp/mlp-fromscratch__sigmoid-mse.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-basic" class="anchor" aria-hidden="true" href="#basic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network with He Initialization&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-he-init.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-he-init.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-concepts" class="anchor" aria-hidden="true" href="#concepts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Concepts&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Replacing Fully-Connnected by Equivalent Convolutional Layers&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/fc-to-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/fc-to-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-fully-convolutional" class="anchor" aria-hidden="true" href="#fully-convolutional"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully Convolutional&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Fully Convolutional Neural Network&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-allconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-allconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-lenet" class="anchor" aria-hidden="true" href="#lenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LeNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;LeNet-5 on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;LeNet-5 on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-lenet5-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-alexnet" class="anchor" aria-hidden="true" href="#alexnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AlexNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;AlexNet on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-alexnet-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-vgg" class="anchor" aria-hidden="true" href="#vgg"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VGG&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Network VGG-16&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;VGG-16 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Neural Network VGG-19&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg19.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg19.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-densenet" class="anchor" aria-hidden="true" href="#densenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DenseNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DenseNet-121 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;DenseNet-121 Image Classifier Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-densenet121-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-resnet" class="anchor" aria-hidden="true" href="#resnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ResNet&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ResNet and Residual Blocks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/resnet-ex-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/resnet-ex-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-18 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet18-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Object Classifier Trained on QuickDraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-34 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Digit Classifier Trained on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-mnist.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-50 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet50-celeba-dataparallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-101 Trained on CIFAR-10&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet101-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;ResNet-152 Gender Classifier Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet152-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-network-in-network" class="anchor" aria-hidden="true" href="#network-in-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network in Network&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Network in Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-normalization-layers" class="anchor" aria-hidden="true" href="#normalization-layers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Normalization Layers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BatchNorm before and after Activation for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_batchnorm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Filter Response Normalization for Network-in-Network CIFAR-10 Classifier&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/nin-cifar10_filter-response-norm.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-metric-learning" class="anchor" aria-hidden="true" href="#metric-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Metric Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Siamese Network with Multilayer Perceptrons&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/metric/siamese-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/metric/siamese-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-autoencoders" class="anchor" aria-hidden="true" href="#autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autoencoders&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-fully-connected-autoencoders" class="anchor" aria-hidden="true" href="#fully-connected-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fully-connected Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoder (MNIST)&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Autoencoder (MNIST) + Scikit-Learn Random Forest Classifier&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-basic-with-rf.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-basic.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-convolutional-autoencoders" class="anchor" aria-hidden="true" href="#convolutional-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions / Transposed Convolutions&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions and Continuous Jaccard Distance&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-jaccard.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Deconvolutions (without pooling operations)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-deconv-nopool.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Autoencoder with Nearest-neighbor Interpolation -- Trained on Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-nneighbor-quickdraw-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-variational-autoencoders" class="anchor" aria-hidden="true" href="#variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Variational Autoencoder&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-conv-var.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-conv-var.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-conditional-variational-autoencoders" class="anchor" aria-hidden="true" href="#conditional-variational-autoencoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional Variational Autoencoders&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (with labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Conditional Variational Autoencoder (without labels in reconstruction loss)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/autoencoder/ae-cnn-cvae_no-out-concat.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-generative-adversarial-networks-gans" class="anchor" aria-hidden="true" href="#generative-adversarial-networks-gans"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks (GANs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fully Connected GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Fully Connected Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional GAN on MNIST with Label Smoothing&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/gan-conv-smoothing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/gan-conv-smoothing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Convolutional Wasserstein GAN on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gan/dc-wgan-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gan/dc-wgan-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph-neural-networks-gnns" class="anchor" aria-hidden="true" href="#graph-neural-networks-gnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph Neural Networks (GNNs)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most Basic Graph Neural Network with Gaussian Filter on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Edge Prediction on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-edge-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Basic Graph Neural Network with Spectral Graph Convolution on MNIST&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/gnn/gnn-basic-graph-spectral-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-recurrent-neural-networks-rnns" class="anchor" aria-hidden="true" href="#recurrent-neural-networks-rnns"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-one-sentiment-analysis--classification" class="anchor" aria-hidden="true" href="#many-to-one-sentiment-analysis--classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-one: Sentiment Analysis / Classification&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple single-layer RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A simple single-layer RNN with packed sequences to ignore padding characters (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_simple_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells (IMDB) and pre-trained GloVe word vectors&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb-glove.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with LSTM cells and Own Dataset in CSV Format (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;RNN with GRU cells (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Multilayer bi-directional RNN (IMDB)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_gru_packed_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (AG News)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_agnews.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Yelp Review Polarity)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_yelp-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Bidirectional Multi-layer RNN with LSTM with Own Dataset in CSV Format (Amazon Review Polarity)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_bi_multilayer_lstm_own_csv_amazon-polarity.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-many-to-many--sequence-to-sequence" class="anchor" aria-hidden="true" href="#many-to-many--sequence-to-sequence"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Many-to-Many / Sequence-to-Sequence&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A simple character RNN to generate new text (Charles Dickens)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ordinal-regression" class="anchor" aria-hidden="true" href="#ordinal-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ordinal Regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Ordinal Regression CNN -- CORAL w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-coral-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Niu et al. 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-niu-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Ordinal Regression CNN -- Beckham and Pal 2016 w. ResNet34 on AFAD-Lite&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/ordinal/ordinal-cnn-beckham2016-afadlite.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tips-and-tricks" class="anchor" aria-hidden="true" href="#tips-and-tricks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tips and Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cyclical Learning Rate&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cyclical-learning-rate.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cyclical-learning-rate.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Annealing with Increasing the Batch Size (w. CIFAR-10 &amp;amp; AlexNet)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/cnn-alexnet-cifar10-batchincrease.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Gradient Clipping (w. MLP on MNIST)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/tricks/gradclipping_mlp.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/tricks/gradclipping_mlp.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-transfer-learning" class="anchor" aria-hidden="true" href="#transfer-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transfer Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transfer Learning Example (VGG16 pre-trained on ImageNet for Cifar-10)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;   [PyTorch: &lt;a href="pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/transfer/transferlearning-vgg16-cifar10-1.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#pytorch-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Custom Data Loader Example for PNG Files&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-dataloader-png/custom-dataloader-example.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- CSV files converted to HDF5&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Face Images from CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from Quickdraw&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-quickdraw.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Drawings from the Street View House Number (SVHN) Dataset&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-svhn.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Asian Face Dataset (AFAD)&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-afad.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using PyTorch Dataset Loading Utilities for Custom Datasets -- Dating Historical Color Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader_dating-historical-color-images.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing" class="anchor" aria-hidden="true" href="#training-and-preprocessing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generating Validation Set Splits&lt;br&gt;
[PyTorch]: &lt;a href="pytorch_ipynb/mechanics/validation-splits.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/validation-splits.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Dataloading with Pinned Memory&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-cifar10-pinmem.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Standardizing Images&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-standardized.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-standardized.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Image Transformation Examples&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/torchvision-transform-examples.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Char-RNN with Own Text File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/char_rnn-charlesdickens.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Sentiment Classification RNN with Own CSV File&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_own_csv_imdb.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-parallel-computing" class="anchor" aria-hidden="true" href="#parallel-computing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parallel Computing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Using Multiple GPUs with DataParallel -- VGG-16 Gender Classifier on CelebA&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-vgg16-celeba-data-parallel.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-other" class="anchor" aria-hidden="true" href="#other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sequential API and hooks&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/mlp-sequential.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/mlp-sequential.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Weight Sharing Within a Layer&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/cnn-weight-sharing.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Plotting Live Training Performance in Jupyter Notebooks with just Matplotlib&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/plot-jupyter-matplotlib.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-autograd" class="anchor" aria-hidden="true" href="#autograd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Autograd&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Getting Gradients of an Intermediate Variable in PyTorch&lt;br&gt;
   [PyTorch: &lt;a href="pytorch_ipynb/mechanics/manual-gradients.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/manual-gradients.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tensorflow-workflows-and-mechanics" class="anchor" aria-hidden="true" href="#tensorflow-workflows-and-mechanics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Workflows and Mechanics&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-custom-datasets-1" class="anchor" aria-hidden="true" href="#custom-datasets-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-npz.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Storing an Image Dataset for Minibatch Training using HDF5&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/image-data-chunking-hdf5.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Input Pipelines to Read Data from TFRecords Files&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/tfrecords.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/tfrecords.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using Queue Runners to Feed Images Directly from Disk&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/file-queues.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/file-queues.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Using TensorFlow's Dataset API&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/dataset-api.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/dataset-api.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-training-and-preprocessing-1" class="anchor" aria-hidden="true" href="#training-and-preprocessing-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Preprocessing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Saving and Loading Trained Models -- from TensorFlow Checkpoint Files and NumPy NPZ Archives&lt;br&gt;
   [TensorFlow 1: &lt;a href="tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb"&gt;GitHub&lt;/a&gt; | &lt;a href="https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/tensorflow1_ipynb/mechanics/saving-and-reloading-models.ipynb" rel="nofollow"&gt;Nbviewer&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/deeplearning-models</guid><pubDate>Mon, 06 Jan 2020 00:03:00 GMT</pubDate></item><item><title>TrickyGo/Dive-into-DL-TensorFlow2.0 #4 in Jupyter Notebook, Today</title><link>https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0</link><description>&lt;p&gt;&lt;i&gt;本项目将《动手学深度学习》(Dive into Deep Learning)原书中的MXNet实现改为TensorFlow 2.0实现，项目已得到李沐老师的同意&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="docs/README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="img/cover.png"&gt;&lt;img width="700" src="img/cover.png" alt="封面" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="https://TrickyGo.github.io/Dive-into-DL-TensorFlow2.0" rel="nofollow"&gt;本项目&lt;/a&gt;将&lt;a href="http://zh.d2l.ai/" rel="nofollow"&gt;《动手学深度学习》&lt;/a&gt; 原书中MXNet代码实现改为TensorFlow2.0实现。经过我的导师咨询李沐老师，这个项目的实施已得到李沐老师的同意。原书作者：阿斯顿·张、李沐、扎卡里 C. 立顿、亚历山大 J. 斯莫拉以及其他社区贡献者，GitHub地址：&lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;此书的&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;中&lt;/a&gt;、&lt;a href="https://d2l.ai/" rel="nofollow"&gt;英&lt;/a&gt;版本存在一些不同，本项目主要针对此书的中文版进行TensorFlow2.0重构。另外，本项目也参考了对此书中文版进行PyTorch重构的项目&lt;a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch"&gt;Dive-into-DL-PyTorch&lt;/a&gt;，在此表示感谢。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;现已更新到第六章，持续更新中。。。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;本仓库主要包含code和docs两个文件夹（外加一些数据存放在data中）。其中code文件夹就是每章相关jupyter notebook代码（基于TensorFlow2.0）；docs文件夹就是markdown格式的《动手学深度学习》书中的相关内容，然后利用&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;将网页文档部署到GitHub Pages上，由于原书使用的是MXNet框架，所以docs内容可能与原书略有不同，但是整体内容是一样的。欢迎对本项目做出贡献或提出issue。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-面向人群" class="anchor" aria-hidden="true" href="#面向人群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向人群&lt;/h2&gt;
&lt;p&gt;本项目面向对深度学习感兴趣，尤其是想使用TensorFlow2.0进行深度学习的童鞋。本项目并不要求你有任何深度学习或者机器学习的背景知识，你只需了解基础的数学和编程，如基础的线性代数、微分和概率，以及基础的Python编程。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-食用方法" class="anchor" aria-hidden="true" href="#食用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;食用方法&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-方法一" class="anchor" aria-hidden="true" href="#方法一"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;方法一&lt;/h3&gt;
&lt;p&gt;本仓库包含一些latex公式，但github的markdown原生是不支持公式显示的，而docs文件夹已经利用&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;被部署到了GitHub Pages上，所以查看文档最简便的方法就是直接访问&lt;a href="https://TrickyGo.github.io/Dive-into-DL-TensorFlow2.0" rel="nofollow"&gt;本项目网页版&lt;/a&gt;。当然如果你还想跑一下运行相关代码的话还是得把本项目clone下来，然后运行code文件夹下相关代码。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-方法二" class="anchor" aria-hidden="true" href="#方法二"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;方法二&lt;/h3&gt;
&lt;p&gt;你还可以在本地访问文档，先安装&lt;code&gt;docsify-cli&lt;/code&gt;工具:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm i docsify-cli -g&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后将本项目clone到本地:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; Dive-into-DL-TensorFlow2.0&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后运行一个本地服务器，这样就可以很方便的在&lt;code&gt;http://localhost:3000&lt;/code&gt;实时访问文档网页渲染效果。&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docsify serve docs&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="read_guide.md"&gt;阅读指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter01_DL-intro/deep-learning-intro.md"&gt;1. 深度学习简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2. 预备知识
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.1_install.md"&gt;2.1 环境配置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.2_tensor.md"&gt;2.2 数据操作&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.3_autograd.md"&gt;2.3 自动求梯度&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.4_document.md"&gt;2.4 查阅文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3. 深度学习基础
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.1_linear-regression.md"&gt;3.1 线性回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.2_linear-regression-scratch.md"&gt;3.2 线性回归的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.3_linear-regression-tensorflow2.0.md"&gt;3.3 线性回归的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.4_softmax-regression.md"&gt;3.4 softmax回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.5_fashion-mnist.md"&gt;3.5 图像分类数据集（Fashion-MNIST）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.6_softmax-regression-scratch.md"&gt;3.6 softmax回归的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.7_softmax-regression-tensorflow2.0.md"&gt;3.7 softmax回归的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.8_mlp.md"&gt;3.8 多层感知机&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.9_mlp-scratch.md"&gt;3.9 多层感知机的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.10_mlp-tensorflow2.0.md"&gt;3.10 多层感知机的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.11_underfit-overfit.md"&gt;3.11 模型选择、欠拟合和过拟合&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.12_weight-decay.md"&gt;3.12 权重衰减&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.13_dropout.md"&gt;3.13 丢弃法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.14_backprop.md"&gt;3.14 正向传播、反向传播和计算图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.15_numerical-stability-and-init.md"&gt;3.15 数值稳定性和模型初始化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.16_kaggle-house-price.md"&gt;3.16 实战Kaggle比赛：房价预测&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4. 深度学习计算
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.1_model-construction.md"&gt;4.1 模型构造&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.2_parameters.md"&gt;4.2 模型参数的访问、初始化和共享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.3_deferred-init.md"&gt;4.3 模型参数的延后初始化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.4_custom-layer.md"&gt;4.4 自定义层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.5_read-write.md"&gt;4.5 读取和存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL-computation/4.6_use-gpu.md"&gt;4.6 GPU计算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;5. 卷积神经网络
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.1_conv-layer.md"&gt;5.1 二维卷积层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.2_padding-and-strides.md"&gt;5.2 填充和步幅&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.3_channels.md"&gt;5.3 多输入通道和多输出通道&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.4_pooling.md"&gt;5.4 池化层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.5_lenet.md"&gt;5.5 卷积神经网络（LeNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.6_alexnet.md"&gt;5.6 深度卷积神经网络（AlexNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.7_vgg.md"&gt;5.7 使用重复元素的网络（VGG）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.8_nin.md"&gt;5.8 网络中的网络（NiN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.9_googlenet.md"&gt;5.9 含并行连结的网络（GoogLeNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.10_batch-norm.md"&gt;5.10 批量归一化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.11_resnet.md"&gt;5.11 残差网络（ResNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.12_densenet.md"&gt;5.12 稠密连接网络（DenseNet）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;6. 循环神经网络
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.1_lang-model.md"&gt;6.1 语言模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.2_rnn.md"&gt;6.2 循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.3_lang-model-dataset.md"&gt;6.3 语言模型数据集（周杰伦专辑歌词）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.4_rnn-scratch.md"&gt;6.4 循环神经网络的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.5_rnn-pytorch.md"&gt;6.5 循环神经网络的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.6_bptt.md"&gt;6.6 通过时间反向传播&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.7_gru.md"&gt;6.7 门控循环单元（GRU）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.8_lstm.md"&gt;6.8 长短期记忆（LSTM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.9_deep-rnn.md"&gt;6.9 深度循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.10_bi-rnn.md"&gt;6.10 双向循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;7. 优化算法
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.1_optimization-intro.md"&gt;7.1 优化与深度学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.2_gd-sgd.md"&gt;7.2 梯度下降和随机梯度下降&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.3_minibatch-sgd.md"&gt;7.3 小批量随机梯度下降&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.4_momentum.md"&gt;7.4 动量法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.5_adagrad.md"&gt;7.5 AdaGrad算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.6_rmsprop.md"&gt;7.6 RMSProp算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.7_adadelta.md"&gt;7.7 AdaDelta算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.8_adam.md"&gt;7.8 Adam算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;8. 计算性能
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.1_hybridize.md"&gt;8.1 命令式和符号式混合编程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.2_async-computation.md"&gt;8.2 异步计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.3_auto-parallelism.md"&gt;8.3 自动并行计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.4_multiple-gpus.md"&gt;8.4 多GPU计算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9. 计算机视觉
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.1_image-augmentation.md"&gt;9.1 图像增广&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.2_fine-tuning.md"&gt;9.2 微调&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.3_bounding-box.md"&gt;9.3 目标检测和边界框&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.4_anchor.md"&gt;9.4 锚框&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.5_multiscale-object-detection.md"&gt;9.5 多尺度目标检测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.6_object-detection-dataset.md"&gt;9.6 目标检测数据集（皮卡丘）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;待更新...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;10. 自然语言处理
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.1_word2vec.md"&gt;10.1 词嵌入（word2vec）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.2_approx-training.md"&gt;10.2 近似训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.3_word2vec-pytorch.md"&gt;10.3 word2vec的实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.4_fasttext.md"&gt;10.4 子词嵌入（fastText）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.5_glove.md"&gt;10.5 全局向量的词嵌入（GloVe）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.6_similarity-analogy.md"&gt;10.6 求近义词和类比词&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md"&gt;10.7 文本情感分类：使用循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md"&gt;10.8 文本情感分类：使用卷积神经网络（textCNN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.9_seq2seq.md"&gt;10.9 编码器—解码器（seq2seq）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.10_beam-search.md"&gt;10.10 束搜索&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.11_attention.md"&gt;10.11 注意力机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.12_machine-translation.md"&gt;10.12 机器翻译&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;持续更新中......&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-原书地址" class="anchor" aria-hidden="true" href="#原书地址"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;原书地址&lt;/h2&gt;
&lt;p&gt;中文版：&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;动手学深度学习&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;Github仓库&lt;/a&gt;&lt;br&gt;
English Version: &lt;a href="https://d2l.ai/" rel="nofollow"&gt;Dive into Deep Learning&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-en"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-引用" class="anchor" aria-hidden="true" href="#引用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;引用&lt;/h2&gt;
&lt;p&gt;如果您在研究中使用了这个项目请引用原书:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>TrickyGo</author><guid isPermaLink="false">https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0</guid><pubDate>Mon, 06 Jan 2020 00:04:00 GMT</pubDate></item><item><title>guipsamora/pandas_exercises #5 in Jupyter Notebook, Today</title><link>https://github.com/guipsamora/pandas_exercises</link><description>&lt;p&gt;&lt;i&gt;Practice your pandas skills!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pandas-exercises" class="anchor" aria-hidden="true" href="#pandas-exercises"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pandas Exercises&lt;/h1&gt;
&lt;p&gt;Fed up with a ton of tutorials but no easy way to find exercises I decided to create a repo just with exercises to practice pandas.
Don't get me wrong, tutorials are great resources, but to learn is to do. So unless you practice you won't learn.&lt;/p&gt;
&lt;p&gt;There will be three different types of files:&lt;br&gt;
      1. Exercise instructions&lt;br&gt;
      2. Solutions without code&lt;br&gt;
      3. Solutions with code and comments&lt;/p&gt;
&lt;p&gt;My suggestion is that you learn a topic in a tutorial, video or documentation and then do the first exercises.
Learn one more topic and do more exercises. If you are stuck, don't go directly to the solution with code files. Check the solutions only and try to get the correct answer.&lt;/p&gt;
&lt;p&gt;Suggestions and collaborations are more than welcome.&lt;g-emoji class="g-emoji" alias="slightly_smiling_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f642.png"&gt;🙂&lt;/g-emoji&gt; Please open an issue or make a PR indicating the exercise and your problem/solution.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-lessons" class="anchor" aria-hidden="true" href="#lessons"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lessons&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#getting-and-knowing"&gt;Getting and knowing&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#merge"&gt;Merge&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#time-series"&gt;Time Series&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#filtering-and-sorting"&gt;Filtering and Sorting&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#stats"&gt;Stats&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#deleting"&gt;Deleting&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#grouping"&gt;Grouping&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#visualization"&gt;Visualization&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Indexing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="#apply"&gt;Apply&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;a href="#creating-series-and-dataframes"&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;Exporting&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-getting-and-knowing" class="anchor" aria-hidden="true" href="#getting-and-knowing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data"&gt;Getting and knowing&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/Occupation"&gt;Occupation&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/01_Getting_%26_Knowing_Your_Data/World%20Food%20Facts"&gt;World Food Facts&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-filtering-and-sorting" class="anchor" aria-hidden="true" href="#filtering-and-sorting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting"&gt;Filtering and Sorting&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Euro12"&gt;Euro12&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/02_Filtering_%26_Sorting/Fictional%20Army"&gt;Fictional Army&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-grouping" class="anchor" aria-hidden="true" href="#grouping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping"&gt;Grouping&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Alcohol_Consumption"&gt;Alcohol Consumption&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Occupation"&gt;Occupation&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/03_Grouping/Regiment"&gt;Regiment&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-apply" class="anchor" aria-hidden="true" href="#apply"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply"&gt;Apply&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/Students_Alcohol_Consumption"&gt;Students Alcohol Consumption&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/04_Apply/US_Crime_Rates"&gt;US_Crime_Rates&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-merge" class="anchor" aria-hidden="true" href="#merge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge"&gt;Merge&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Auto_MPG"&gt;Auto_MPG&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Fictitous%20Names"&gt;Fictitious Names&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/05_Merge/Housing%20Market"&gt;House Market&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-stats" class="anchor" aria-hidden="true" href="#stats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats"&gt;Stats&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/US_Baby_Names"&gt;US_Baby_Names&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/06_Stats/Wind_Stats"&gt;Wind_Stats&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visualization" class="anchor" aria-hidden="true" href="#visualization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization"&gt;Visualization&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Chipotle"&gt;Chipotle&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Titanic_Desaster"&gt;Titanic Disaster&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Scores"&gt;Scores&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Online_Retail"&gt;Online Retail&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/07_Visualization/Tips"&gt;Tips&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-creating-series-and-dataframes" class="anchor" aria-hidden="true" href="#creating-series-and-dataframes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames"&gt;Creating Series and DataFrames&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/08_Creating_Series_and_DataFrames/Pokemon"&gt;Pokemon&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-time-series" class="anchor" aria-hidden="true" href="#time-series"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series"&gt;Time Series&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Apple_Stock"&gt;Apple_Stock&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data"&gt;Getting_Financial_Data&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/09_Time_Series/Getting_Financial_Data"&gt;Investor_Flow_of_Funds_US&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deleting" class="anchor" aria-hidden="true" href="#deleting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting"&gt;Deleting&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Iris"&gt;Iris&lt;/a&gt;&lt;br&gt;
&lt;a href="https://github.com/guipsamora/pandas_exercises/tree/master/10_Deleting/Wine"&gt;Wine&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-video-solutions" class="anchor" aria-hidden="true" href="#video-solutions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video Solutions&lt;/h1&gt;
&lt;p&gt;Video tutorials of data scientists working through the above exercises:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=pu3IpU937xs&amp;amp;list=PLgJhDSE2ZLxaY_DigHeiIDC1cD09rXgJv" rel="nofollow"&gt;Data Talks - Pandas Learning By Doing&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>guipsamora</author><guid isPermaLink="false">https://github.com/guipsamora/pandas_exercises</guid><pubDate>Mon, 06 Jan 2020 00:05:00 GMT</pubDate></item><item><title>AllenDowney/ThinkStats2 #6 in Jupyter Notebook, Today</title><link>https://github.com/AllenDowney/ThinkStats2</link><description>&lt;p&gt;&lt;i&gt;Text and supporting code for Think Stats, 2nd Edition&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-thinkstats2" class="anchor" aria-hidden="true" href="#thinkstats2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ThinkStats2&lt;/h1&gt;
&lt;p&gt;Text and supporting code for &lt;a href="http://greenteapress.com/thinkstats2/index.html" rel="nofollow"&gt;Think Stats, 2nd Edition&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AllenDowney</author><guid isPermaLink="false">https://github.com/AllenDowney/ThinkStats2</guid><pubDate>Mon, 06 Jan 2020 00:06:00 GMT</pubDate></item><item><title>ageron/handson-ml2 #7 in Jupyter Notebook, Today</title><link>https://github.com/ageron/handson-ml2</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in the second edition of my O'Reilly book &lt;a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/bdec1a5ed5a56e2ab3fc0c4decda7081bd62d662/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067" title="book" width="150" data-canonical-src="https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href="https://github.com/ageron/handson-ml"&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-play-with-these-notebooks-online-without-having-to-install-anything" class="anchor" aria-hidden="true" href="#want-to-play-with-these-notebooks-online-without-having-to-install-anything"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt;
&lt;p&gt;Use any of the following services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: open this repository in &lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;Colaboratory&lt;/a&gt;:
&lt;a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e69988217d15707bdd8b6b27f1d7d53a0dd00af7/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f696d672f636f6c61625f66617669636f6e2e69636f" width="90" data-canonical-src="https://colab.research.google.com/img/colab_favicon.ico" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;Binder&lt;/a&gt;:
&lt;a href="https://mybinder.org/v2/gh/ageron/handson-ml2/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/69ea8abed4df43bca4c671b965aeffef2c4f897a/68747470733a2f2f6d61747468696173627573736f6e6e6965722e636f6d2f706f7374732f696d672f62696e6465725f6c6f676f5f313238783132382e706e67" width="90" data-canonical-src="https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: Most of the time, Binder starts up quickly and works great, but when handson-ml2 is updated, Binder creates a new environment from scratch, and this can take quite some time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or open it in &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;Deepnote&lt;/a&gt;:
&lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3fae03be31b768100aa2a800d2cc3b6650c6cd48/68747470733a2f2f7777772e646565706e6f74652e636f6d2f7374617469632f696c6c757374726174696f6e2e706e67" width="150" data-canonical-src="https://www.deepnote.com/static/illustration.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-just-want-to-quickly-look-at-some-notebooks-without-executing-any-code" class="anchor" aria-hidden="true" href="#just-want-to-quickly-look-at-some-notebooks-without-executing-any-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt;
&lt;p&gt;Browse this repository using &lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;:
&lt;a href="https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/079030b4c39b76eafa0c6c3a5bd18112aafe42dd/68747470733a2f2f6a7570797465722e6f72672f6173736574732f6e61765f6c6f676f2e737667" width="150" data-canonical-src="https://jupyter.org/assets/nav_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;a href="index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math equations are not always displayed correctly.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-run-this-project-using-a-docker-image" class="anchor" aria-hidden="true" href="#want-to-run-this-project-using-a-docker-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to run this project using a Docker image?&lt;/h3&gt;
&lt;p&gt;Read the &lt;a href="https://github.com/ageron/handson-ml2/tree/master/docker"&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-want-to-install-this-project-on-your-own-machine" class="anchor" aria-hidden="true" href="#want-to-install-this-project-on-your-own-machine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want to install this project on your own machine?&lt;/h3&gt;
&lt;p&gt;Start by installing &lt;a href="https://www.anaconda.com/distribution/" rel="nofollow"&gt;Anaconda&lt;/a&gt; (or &lt;a href="https://docs.conda.io/en/latest/miniconda.html" rel="nofollow"&gt;Miniconda&lt;/a&gt;), &lt;a href="https://git-scm.com/downloads" rel="nofollow"&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href="https://www.nvidia.com/Download/index.aspx" rel="nofollow"&gt;GPU driver&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git
$ cd handson-ml2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to use a GPU, then edit &lt;code&gt;environment.yml&lt;/code&gt; (or &lt;code&gt;environment-windows.yml&lt;/code&gt; on Windows) and replace &lt;code&gt;tensorflow=2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-gpu=2.0.0&lt;/code&gt;. Also replace &lt;code&gt;tensorflow-serving-api==2.0.0&lt;/code&gt; with &lt;code&gt;tensorflow-serving-api-gpu==2.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml # or environment-windows.yml on Windows
$ conda activate tf2
$ python -m ipykernel install --user --name=python3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then if you're on Windows, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, start Jupyter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need further instructions, read the &lt;a href="INSTALL.md"&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park who helped on some of the exercise solutions, and to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml2</guid><pubDate>Mon, 06 Jan 2020 00:07:00 GMT</pubDate></item><item><title>dataquestio/solutions #8 in Jupyter Notebook, Today</title><link>https://github.com/dataquestio/solutions</link><description>&lt;p&gt;&lt;i&gt;Solutions for projects.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dataquest-project-solutions" class="anchor" aria-hidden="true" href="#dataquest-project-solutions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dataquest Project Solutions&lt;/h1&gt;
&lt;p&gt;This repository is a series of notebooks that show solutions for the &lt;a href="https://www.dataquest.io/apply" rel="nofollow"&gt;projects&lt;/a&gt; at &lt;a href="https://www.dataquest.io/" rel="nofollow"&gt;Dataquest.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, there are always going to be multiple ways to solve any one problem, so these notebooks just show one possible solution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission9Solutions.ipynb"&gt;Guided Project: Explore U.S. Births&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission103Solutions.ipynb"&gt;Guided Project: Customizing Data Visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission201Solution.ipynb"&gt;Guided Project: Star Wars survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission202Solution.ipynb"&gt;Guided Project: Police killings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission205Solutions.ipynb"&gt;Guided Project: Visualizing Pixar's Roller Coaster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission207Solutions.ipynb"&gt;Guided Project: Using Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission209Solution.ipynb"&gt;Guided Project: Analyzing movie reviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission210Solution.ipynb"&gt;Guided Project: Winning Jeopardy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission211Solution.ipynb"&gt;Guided Project: Predicting board game reviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission213Solution.ipynb"&gt;Guided Project: Predicting bike rentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission215Solutions.ipynb"&gt;Guided Project: Preparing data for SQLite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission216Solutions.ipynb"&gt;Guided Project: Creating relations in SQLite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission217Solutions.ipynb"&gt;Guided Project: Analyzing NYC High School Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission146Solutions.ipynb"&gt;Guided Project: Visualizing Earnings Based On College Majors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission218Solution.ipynb"&gt;Guided Project: Exploring Gun Deaths in the US&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission219Solution.ipynb"&gt;Guided Project: Analyzing Thanksgiving Dinner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission227Solutions.ipynb"&gt;Guided Project: Analyzing Wikipedia Pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission177Solutions.ipynb"&gt;Guided Project: Analyzing Stock Prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission188Solution.ipynb"&gt;Guided Project: Creating A Kaggle Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission167Solutions.ipynb"&gt;Guided Project: Analyzing Startup Fundraising Deals from Crunchbase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission240Solutions.ipynb"&gt;Guided Project: Predicting House Sale Prices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission191Solutions.ipynb"&gt;Guided Project: Answering Business Questions using SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission193Solutions.ipynb"&gt;Guided Project: Designing and Creating a Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission288Solutions.ipynb"&gt;Guided Project: Investigating Fandango's Movie Rating System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission277Solutions.Rmd"&gt;Guided Project: Forest Fires Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission327Solutions.Rmd"&gt;Guided Project: NYC Schools Perceptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission348Solutions.ipynb"&gt;Guided Project: Clean and Analyze Employee Exit Surveys&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dataquestio/solutions/blob/master/Mission449Solutions.Rmd"&gt;Guided Project: Finding the Best Markets to Advertise In&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dataquestio</author><guid isPermaLink="false">https://github.com/dataquestio/solutions</guid><pubDate>Mon, 06 Jan 2020 00:08:00 GMT</pubDate></item><item><title>CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers #9 in Jupyter Notebook, Today</title><link>https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers</link><description>&lt;p&gt;&lt;i&gt;aka "Bayesian Methods for Hackers": An introduction to Bayesian methods + probabilistic programming with a computation/understanding-first, mathematics-second point of view. All in pure Python ;)  &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bayesian-methods-for-hackers" class="anchor" aria-hidden="true" href="#bayesian-methods-for-hackers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/" rel="nofollow"&gt;Bayesian Methods for Hackers&lt;/a&gt;&lt;/h1&gt;
&lt;h4&gt;&lt;a id="user-content-using-python-and-pymc" class="anchor" aria-hidden="true" href="#using-python-and-pymc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Using Python and PyMC&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;The Bayesian method is the natural approach to inference, yet it is hidden from readers behind chapters of slow, mathematical analysis. The typical text on Bayesian inference involves two to three chapters on probability theory, then enters what Bayesian inference is. Unfortunately, due to mathematical intractability of most Bayesian models, the reader is only shown simple, artificial examples. This can leave the user with a &lt;em&gt;so-what&lt;/em&gt; feeling about Bayesian inference. In fact, this was the author's own prior opinion.&lt;/p&gt;
&lt;p&gt;After some recent success of Bayesian methods in machine-learning competitions, I decided to investigate the subject again. Even with my mathematical background, it took me three straight-days of reading examples and trying to put the pieces together to understand the methods. There was simply not enough literature bridging theory to practice. The problem with my misunderstanding was the disconnect between Bayesian mathematics and probabilistic programming. That being said, I suffered then so the reader would not have to now. This book attempts to bridge the gap.&lt;/p&gt;
&lt;p&gt;If Bayesian inference is the destination, then mathematical analysis is a particular path towards it. On the other hand, computing power is cheap enough that we can afford to take an alternate route via probabilistic programming. The latter path is much more useful, as it denies the necessity of mathematical intervention at each step, that is, we remove often-intractable mathematical analysis as a prerequisite to Bayesian inference. Simply put, this latter computational path proceeds via small intermediate jumps from beginning to end, where as the first path proceeds by enormous leaps, often landing far away from our target. Furthermore, without a strong mathematical background, the analysis required by the first path cannot even take place.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bayesian Methods for Hackers&lt;/em&gt; is designed as an introduction to Bayesian inference from a computational/understanding-first, and mathematics-second, point of view. Of course as an introductory book, we can only leave it at that: an introductory book. For the mathematically trained, they may cure the curiosity this text generates with other texts designed with mathematical analysis in mind. For the enthusiast with less mathematical background, or one who is not interested in the mathematics but simply the practice of Bayesian methods, this text should be sufficient and entertaining.&lt;/p&gt;
&lt;p&gt;The choice of PyMC as the probabilistic programming language is two-fold. As of this writing, there is currently no central resource for examples and explanations in the PyMC universe. The official documentation assumes prior knowledge of Bayesian inference and probabilistic programming. We hope this book encourages users at every level to look at PyMC. Secondly, with recent core developments and popularity of the scientific stack in Python, PyMC is likely to become a core component soon enough.&lt;/p&gt;
&lt;p&gt;PyMC does have dependencies to run, namely NumPy and (optionally) SciPy. To not limit the user, the examples in this book will rely only on PyMC, NumPy, SciPy and Matplotlib.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-printed-version-by-addison-wesley" class="anchor" aria-hidden="true" href="#printed-version-by-addison-wesley"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Printed Version by Addison-Wesley&lt;/h2&gt;
&lt;div&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e991336533f43ff762cbf7713516f4215640d402/687474703a2f2f7777772d66702e70656172736f6e68696768657265642e636f6d2f6173736574732f6869702f696d616765732f626967636f766572732f303133333930323833382e6a7067"&gt;&lt;img title="Bayesian Methods for Hackersg" src="https://camo.githubusercontent.com/e991336533f43ff762cbf7713516f4215640d402/687474703a2f2f7777772d66702e70656172736f6e68696768657265642e636f6d2f6173736574732f6869702f696d616765732f626967636f766572732f303133333930323833382e6a7067" align="right" height="200" data-canonical-src="http://www-fp.pearsonhighered.com/assets/hip/images/bigcovers/0133902838.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Bayesian Methods for Hackers is now available as a printed book!&lt;/strong&gt; You can pick up a copy on &lt;a href="http://www.amazon.com/Bayesian-Methods-Hackers-Probabilistic-Addison-Wesley/dp/0133902838" rel="nofollow"&gt;Amazon&lt;/a&gt;. What are the differences between the online version and the printed version?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Additional Chapter on Bayesian A/B testing&lt;/li&gt;
&lt;li&gt;Updated examples&lt;/li&gt;
&lt;li&gt;Answers to the end of chapter questions&lt;/li&gt;
&lt;li&gt;Additional explanation, and rewritten sections to aid the reader.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;p&gt;See the project homepage &lt;a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/" rel="nofollow"&gt;here&lt;/a&gt; for examples, too.&lt;/p&gt;
&lt;p&gt;The below chapters are rendered via the &lt;em&gt;nbviewer&lt;/em&gt; at
&lt;a href="http://nbviewer.jupyter.org/" rel="nofollow"&gt;nbviewer.jupyter.org/&lt;/a&gt;, and is read-only and rendered in real-time.
Interactive notebooks + examples can be downloaded by cloning!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pymc2" class="anchor" aria-hidden="true" href="#pymc2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyMC2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Prologue/Prologue.ipynb" rel="nofollow"&gt;&lt;strong&gt;Prologue:&lt;/strong&gt;&lt;/a&gt; Why we do it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 1: Introduction to Bayesian Methods&lt;/strong&gt;&lt;/a&gt;
Introduction to the philosophy and practice of Bayesian methods and answering the question, "What is probabilistic programming?" Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inferring human behaviour changes from text message rates&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC2.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 2: A little more on PyMC&lt;/strong&gt;&lt;/a&gt;
We explore modeling Bayesian problems using Python's PyMC library through examples. How do we create Bayesian models? Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting the frequency of cheating students, while avoiding liars&lt;/li&gt;
&lt;li&gt;Calculating probabilities of the Challenger space-shuttle disaster&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC2.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 3: Opening the Black Box of MCMC&lt;/strong&gt;&lt;/a&gt;
We discuss how MCMC operates and diagnostic tools. Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian clustering with mixture models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC2.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 4: The Greatest Theorem Never Told&lt;/strong&gt;&lt;/a&gt;
We explore an incredibly useful, and dangerous, theorem: The Law of Large Numbers. Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exploring a Kaggle dataset and the pitfalls of naive analysis&lt;/li&gt;
&lt;li&gt;How to sort Reddit comments from best to worst (not as easy as you think)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter5_LossFunctions/Ch5_LossFunctions_PyMC2.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 5: Would you rather lose an arm or a leg?&lt;/strong&gt;&lt;/a&gt;
The introduction of loss functions and their (awesome) use in Bayesian methods.  Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solving the &lt;em&gt;Price is Right&lt;/em&gt;'s Showdown&lt;/li&gt;
&lt;li&gt;Optimizing financial predictions&lt;/li&gt;
&lt;li&gt;Winning solution to the Kaggle Dark World's competition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter6_Priorities/Ch6_Priors_PyMC2.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 6: Getting our &lt;em&gt;prior&lt;/em&gt;-ities straight&lt;/strong&gt;&lt;/a&gt;
Probably the most important chapter. We draw on expert opinions to answer questions. Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-Armed Bandits and the Bayesian Bandit solution.&lt;/li&gt;
&lt;li&gt;What is the relationship between data sample size and prior?&lt;/li&gt;
&lt;li&gt;Estimating financial unknowns using expert priors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We explore useful tips to be objective in analysis as well as common pitfalls of priors.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pymc3" class="anchor" aria-hidden="true" href="#pymc3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyMC3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Prologue/Prologue.ipynb" rel="nofollow"&gt;&lt;strong&gt;Prologue:&lt;/strong&gt;&lt;/a&gt; Why we do it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/Ch1_Introduction_PyMC3.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 1: Introduction to Bayesian Methods&lt;/strong&gt;&lt;/a&gt;
Introduction to the philosophy and practice of Bayesian methods and answering the question, "What is probabilistic programming?" Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inferring human behaviour changes from text message rates&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/Ch2_MorePyMC_PyMC3.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 2: A little more on PyMC&lt;/strong&gt;&lt;/a&gt;
We explore modeling Bayesian problems using Python's PyMC library through examples. How do we create Bayesian models? Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detecting the frequency of cheating students, while avoiding liars&lt;/li&gt;
&lt;li&gt;Calculating probabilities of the Challenger space-shuttle disaster&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/Ch3_IntroMCMC_PyMC3.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 3: Opening the Black Box of MCMC&lt;/strong&gt;&lt;/a&gt;
We discuss how MCMC operates and diagnostic tools. Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian clustering with mixture models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC3.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 4: The Greatest Theorem Never Told&lt;/strong&gt;&lt;/a&gt;
We explore an incredibly useful, and dangerous, theorem: The Law of Large Numbers. Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exploring a Kaggle dataset and the pitfalls of naive analysis&lt;/li&gt;
&lt;li&gt;How to sort Reddit comments from best to worst (not as easy as you think)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter5_LossFunctions/Ch5_LossFunctions_PyMC3.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 5: Would you rather lose an arm or a leg?&lt;/strong&gt;&lt;/a&gt;
The introduction of loss functions and their (awesome) use in Bayesian methods.  Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solving the &lt;em&gt;Price is Right&lt;/em&gt;'s Showdown&lt;/li&gt;
&lt;li&gt;Optimizing financial predictions&lt;/li&gt;
&lt;li&gt;Winning solution to the Kaggle Dark World's competition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter6_Priorities/Ch6_Priors_PyMC3.ipynb" rel="nofollow"&gt;&lt;strong&gt;Chapter 6: Getting our &lt;em&gt;prior&lt;/em&gt;-ities straight&lt;/strong&gt;&lt;/a&gt;
Probably the most important chapter. We draw on expert opinions to answer questions. Examples include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-Armed Bandits and the Bayesian Bandit solution.&lt;/li&gt;
&lt;li&gt;What is the relationship between data sample size and prior?&lt;/li&gt;
&lt;li&gt;Estimating financial unknowns using expert priors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We explore useful tips to be objective in analysis as well as common pitfalls of priors.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;More questions about PyMC?&lt;/strong&gt;
Please post your modeling, convergence, or any other PyMC question on &lt;a href="http://stats.stackexchange.com/" rel="nofollow"&gt;cross-validated&lt;/a&gt;, the statistics stack-exchange.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-the-book" class="anchor" aria-hidden="true" href="#using-the-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using the book&lt;/h2&gt;
&lt;p&gt;The book can be read in three different ways, starting from most recommended to least recommended:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The most recommended option is to clone the repository to download the .ipynb files to your local machine. If you have Jupyter installed, you can view the
chapters in your browser &lt;em&gt;plus&lt;/em&gt; edit and run the code provided (and try some practice questions). This is the preferred option to read
this book, though it comes with some dependencies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jupyter is a requirement to view the ipynb files. It can be downloaded &lt;a href="http://jupyter.org/" rel="nofollow"&gt;here&lt;/a&gt;. Jupyter notebooks can be run by &lt;code&gt;(your-virtualenv) ~/path/to/the/book/Chapter1_Introduction $ jupyter notebook&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;For Linux users, you should not have a problem installing NumPy, SciPy, Matplotlib and PyMC. For Windows users, check out &lt;a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/" rel="nofollow"&gt;pre-compiled versions&lt;/a&gt; if you have difficulty.&lt;/li&gt;
&lt;li&gt;In the styles/ directory are a number of files (.matplotlirc) that used to make things pretty. These are not only designed for the book, but they offer many improvements over the default settings of matplotlib.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second, preferred, option is to use the nbviewer.jupyter.org site, which display Jupyter notebooks in the browser (&lt;a href="http://nbviewer.jupyter.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/Ch1_Introduction_PyMC2.ipynb" rel="nofollow"&gt;example&lt;/a&gt;).
The contents are updated synchronously as commits are made to the book. You can use the Contents section above to link to the chapters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PDFs are the least-preferred method to read the book, as PDFs are static and non-interactive. If PDFs are desired, they can be created dynamically using the &lt;a href="https://github.com/jupyter/nbconvert"&gt;nbconvert&lt;/a&gt; utility.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-configuration" class="anchor" aria-hidden="true" href="#installation-and-configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and configuration&lt;/h2&gt;
&lt;p&gt;If you would like to run the Jupyter notebooks locally, (option 1. above), you'll need to install the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Jupyter is a requirement to view the ipynb files. It can be downloaded &lt;a href="http://jupyter.org/install.html" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Necessary packages are PyMC, NumPy, SciPy and Matplotlib.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For Linux/OSX users, you should not have a problem installing the above, &lt;a href="http://www.penandpants.com/2012/02/24/install-python/" rel="nofollow"&gt;&lt;em&gt;except for Matplotlib on OSX&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Windows users, check out &lt;a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/" rel="nofollow"&gt;pre-compiled versions&lt;/a&gt; if you have difficulty.&lt;/li&gt;
&lt;li&gt;also recommended, for data-mining exercises, are &lt;a href="https://github.com/praw-dev/praw"&gt;PRAW&lt;/a&gt; and &lt;a href="https://github.com/kennethreitz/requests"&gt;requests&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New to Python or Jupyter, and help with the namespaces? Check out &lt;a href="http://stackoverflow.com/questions/12987624/confusion-between-numpy-scipy-matplotlib-and-pylab" rel="nofollow"&gt;this answer&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the styles/ directory are a number of files that are customized for the notebook.
These are not only designed for the book, but they offer many improvements over the
default settings of matplotlib and the Jupyter notebook. The in notebook style has not been finalized yet.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;This book has an unusual development design. The content is open-sourced, meaning anyone can be an author.
Authors submit content or revisions using the GitHub interface.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-contribute" class="anchor" aria-hidden="true" href="#how-to-contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to contribute&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-what-to-contribute" class="anchor" aria-hidden="true" href="#what-to-contribute"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What to contribute?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The current chapter list is not finalized. If you see something that is missing (MCMC, MAP, Bayesian networks, good prior choices, Potential classes etc.),
feel free to start there.&lt;/li&gt;
&lt;li&gt;Cleaning up Python code and making code more PyMC-esque&lt;/li&gt;
&lt;li&gt;Giving better explanations&lt;/li&gt;
&lt;li&gt;Spelling/grammar mistakes&lt;/li&gt;
&lt;li&gt;Suggestions&lt;/li&gt;
&lt;li&gt;Contributing to the Jupyter notebook styles&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-commiting" class="anchor" aria-hidden="true" href="#commiting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Commiting&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;All commits are welcome, even if they are minor ;)&lt;/li&gt;
&lt;li&gt;If you are unfamiliar with Github, you can email me contributions to the email below.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-reviews" class="anchor" aria-hidden="true" href="#reviews"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reviews&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;these are satirical, but real&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;"No, but it looks good" - &lt;a href="https://twitter.com/JohnDCook/status/359672133695184896" rel="nofollow"&gt;John D. Cook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"I ... read this book ... I like it!" - &lt;a href="http://www.andrewgelman.com/2013/07/21/bayes-related" rel="nofollow"&gt;Andrew Gelman&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"This book is a godsend, and a direct refutation to that 'hmph! you don't know maths, piss off!' school of thought...
The publishing model is so unusual. Not only is it open source but it relies on pull requests from anyone in order to progress the book. This is ingenious and heartening" - &lt;a href="http://www.reddit.com/r/Python/comments/1alnal/probabilistic_programming_and_bayesian_methods/" rel="nofollow"&gt;excited Reddit user&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributions-and-thanks" class="anchor" aria-hidden="true" href="#contributions-and-thanks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions and Thanks&lt;/h2&gt;
&lt;p&gt;Thanks to all our contributing authors, including (in chronological order):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Authors&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.camdp.com" rel="nofollow"&gt;Cameron Davidson-Pilon&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://stefgibson.com" rel="nofollow"&gt;Stef Gibson&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://bigsnarf.wordpress.com/" rel="nofollow"&gt;Vincent Ohprecio&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/larsman"&gt;Lars Buitinck&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://github.com/pmagwene"&gt;Paul Magwene&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Carreau"&gt;Matthias Bussonnier&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/JensRantil"&gt;Jens Rantil&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/y-p"&gt;y-p&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://www.etano.net/" rel="nofollow"&gt;Ethan Brown&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://jonathanwhitmore.com/" rel="nofollow"&gt;Jonathan Whitmore&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/matrig"&gt;Mattia Rigotti&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/colibius"&gt;Colby Lemon&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/gustavdelius"&gt;Gustav W Delius&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.mathisonian.com/" rel="nofollow"&gt;Matthew Conlen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/radford"&gt;Jim Radford&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://baniverso.com/" rel="nofollow"&gt;Vannessa Sabino&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/thomasbratt"&gt;Thomas Bratt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/nisanharamati"&gt;Nisan Haramati&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/bgrant"&gt;Robert Grant&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/xcthulhu"&gt;Matthew Wampler-Doty&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/yarikoptic"&gt;Yaroslav Halchenko&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/alexgarel"&gt;Alex Garel&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://twitter.com/sash_ko" rel="nofollow"&gt;Oleksandr Lysenko&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/liori"&gt;liori&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ducky427"&gt;ducky427&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/pablooliveira"&gt;Pablo de Oliveira Castro&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/sergeyfogelson"&gt;sergeyfogelson&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://neurotheory.columbia.edu/~mrigotti/" rel="nofollow"&gt;Mattia Rigotti&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/mbauman"&gt;Matt Bauman&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.andrewduberstein.com/" rel="nofollow"&gt;Andrew Duberstein&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://cebe.cc/" rel="nofollow"&gt;Carsten Brandt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://web2docx.com" rel="nofollow"&gt;Bob Jansen&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/ugurthemaster"&gt;ugurthemaster&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/williamscott"&gt;William Scott&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://twitter.com/minrk" rel="nofollow"&gt;Min RK&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Bulwersator"&gt;Bulwersator&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/elpres"&gt;elpres&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/hackaugusto"&gt;Augusto Hack&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/michaf"&gt;Michael Feldmann&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Youki"&gt;Youki&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://jensrantil.github.io" rel="nofollow"&gt;Jens Rantil&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://kyleam.com" rel="nofollow"&gt;Kyle Meyer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://ericmart.in" rel="nofollow"&gt;Eric Martin&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Inconditus"&gt;Inconditus&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/Kleptine"&gt;Kleptine&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/slayton"&gt;Stuart Layton&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/tritemio"&gt;Antonino Ingargiola&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/vsl9"&gt;vsl9&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/tom-christie"&gt;Tom Christie&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/bclow"&gt;bclow&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://sjp.co.nz/" rel="nofollow"&gt;Simon Potter&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/GarthSnyder"&gt;Garth Snyder&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://twitter.com/pushmatrix" rel="nofollow"&gt;Daniel Beauchamp&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://www.philippsinger.info" rel="nofollow"&gt;Philipp Singer&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/gbenmartin"&gt;gbenmartin&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://twitter.com/Springcoil" rel="nofollow"&gt;Peadar Coyle&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We would like to thank the Python community for building an amazing architecture. We would like to thank the
statistics community for building an amazing architecture.&lt;/p&gt;
&lt;p&gt;Similarly, the book is only possible because of the &lt;a href="http://github.com/pymc-devs/pymc"&gt;PyMC&lt;/a&gt; library. A big thanks to the core devs of PyMC: Chris Fonnesbeck, Anand Patil, David Huard and John Salvatier.&lt;/p&gt;
&lt;p&gt;One final thanks. This book was generated by Jupyter Notebook, a wonderful tool for developing in Python. We thank the IPython/Jupyter
community for developing the Notebook interface. All Jupyter notebook files are available for download on the GitHub repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h4&gt;
&lt;p&gt;Contact the main author, Cam Davidson-Pilon at &lt;a href="mailto:cam.davidson.pilon@gmail.com"&gt;cam.davidson.pilon@gmail.com&lt;/a&gt; or &lt;a href="https://twitter.com/cmrn_dp" rel="nofollow"&gt;@cmrndp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/090a62cab61c2a7115a3d8f4f99bffe5f1c75a26/687474703a2f2f692e696d6775722e636f6d2f5a623739515a622e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/090a62cab61c2a7115a3d8f4f99bffe5f1c75a26/687474703a2f2f692e696d6775722e636f6d2f5a623739515a622e706e67" alt="Imgur" data-canonical-src="http://i.imgur.com/Zb79QZb.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CamDavidsonPilon</author><guid isPermaLink="false">https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers</guid><pubDate>Mon, 06 Jan 2020 00:09:00 GMT</pubDate></item><item><title>ageron/handson-ml #10 in Jupyter Notebook, Today</title><link>https://github.com/ageron/handson-ml</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in my O'Reilly book &lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8e10a44b0ddbb9530cc27d877f06db68d9fa1c7d/687474703a2f2f616b616d6169636f766572732e6f7265696c6c792e636f6d2f696d616765732f393738313439313936323238322f6361742e676966" alt="book" data-canonical-src="http://akamaicovers.oreilly.com/images/9781491962282/cat.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simply open the &lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter&lt;/a&gt; notebooks you are interested in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using &lt;a href="http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;note: &lt;a href="https://github.com/ageron/handson-ml/blob/master/index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math formulas are not displayed correctly,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;by cloning this repository and running Jupyter locally. This option lets you play around with the code. In this case, follow the installation instructions below,&lt;/li&gt;
&lt;li&gt;or by running the notebooks in &lt;a href="https://beta.deepnote.com" rel="nofollow"&gt;Deepnote&lt;/a&gt;. This allows you to play around with the code online in your browser. For example, here's a link to the first chapter: &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb" rel="nofollow"&gt;&lt;img height="22" src="https://camo.githubusercontent.com/c3b9bd12a99f8de3301018192105256209bcf800/68747470733a2f2f626574612e646565706e6f74652e636f6d2f627574746f6e732f6c61756e63682d696e2d646565706e6f74652e737667" data-canonical-src="https://beta.deepnote.com/buttons/launch-in-deepnote.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;First, you will need to install &lt;a href="https://git-scm.com/" rel="nofollow"&gt;git&lt;/a&gt;, if you don't have it already.&lt;/p&gt;
&lt;p&gt;Next, clone this repository by opening a terminal and typing the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd $HOME  # or any other development directory you prefer
$ git clone https://github.com/ageron/handson-ml.git
$ cd handson-ml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not want to install git, you can instead download &lt;a href="https://github.com/ageron/handson-ml/archive/master.zip"&gt;master.zip&lt;/a&gt;, unzip it, rename the resulting directory to &lt;code&gt;handson-ml&lt;/code&gt; and move it to your development directory.&lt;/p&gt;
&lt;p&gt;If you want to go through chapter 16 on Reinforcement Learning, you will need to &lt;a href="https://gym.openai.com/docs" rel="nofollow"&gt;install OpenAI gym&lt;/a&gt; and its dependencies for Atari simulations.&lt;/p&gt;
&lt;p&gt;If you are familiar with Python and you know how to install Python libraries, go ahead and install the libraries listed in &lt;code&gt;requirements.txt&lt;/code&gt; and jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section. If you need detailed instructions, please read on.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python--required-libraries" class="anchor" aria-hidden="true" href="#python--required-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python &amp;amp; Required Libraries&lt;/h2&gt;
&lt;p&gt;Of course, you obviously need Python. Python 3 is already preinstalled on many systems nowadays. You can check which version you have by typing the following command (you may need to replace &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 --version  # for Python 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any Python 3 version should be fine, preferably 3.5 or above. If you don't have Python 3, I recommend installing it. To do so, you have several options: on Windows or MacOSX, you can just download it from &lt;a href="https://www.python.org/downloads/" rel="nofollow"&gt;python.org&lt;/a&gt;. On MacOSX, you can alternatively use &lt;a href="https://www.macports.org/" rel="nofollow"&gt;MacPorts&lt;/a&gt; or &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt;. If you are using Python 3.6 on MacOSX, you need to run the following command to install the &lt;code&gt;certifi&lt;/code&gt; package of certificates because Python 3.6 on MacOSX has no certificates to validate SSL connections (see this &lt;a href="https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error" rel="nofollow"&gt;StackOverflow question&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ /Applications/Python\ 3.6/Install\ Certificates.command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Linux, unless you know what you are doing, you should use your system's packaging system. For example, on Debian or Ubuntu, type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get update
$ sudo apt-get install python3 python3-pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to download and install &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt;. This is a package that includes both Python and many scientific libraries. You should prefer the Python 3 version.&lt;/p&gt;
&lt;p&gt;If you choose to use Anaconda, read the next section, or else jump to the &lt;a href="#using-pip"&gt;Using pip&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-anaconda" class="anchor" aria-hidden="true" href="#using-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Anaconda&lt;/h2&gt;
&lt;p&gt;Once you have &lt;a href="https://docs.anaconda.com/anaconda/install/" rel="nofollow"&gt;installed Anaconda&lt;/a&gt; (or Miniconda), you can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give you a conda environment named &lt;code&gt;mlbook&lt;/code&gt;, ready to use! Just activate it and you will have everything setup
for you:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda activate mlbook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You are all set! Next, jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-pip" class="anchor" aria-hidden="true" href="#using-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using pip&lt;/h2&gt;
&lt;p&gt;If you are not using Anaconda, you need to install several scientific Python libraries that are necessary for this project, in particular NumPy, Matplotlib, Pandas, Jupyter and TensorFlow (and a few others). For this, you can either use Python's integrated packaging system, pip, or you may prefer to use your system's own packaging system (if available, e.g. on Linux, or on MacOSX when using MacPorts or Homebrew). The advantage of using pip is that it is easy to create multiple isolated Python environments with different libraries and different library versions (e.g. one environment for each project). The advantage of using your system's packaging system is that there is less risk of having conflicts between your Python libraries and your system's other packages. Since I have many projects with different library requirements, I prefer to use pip with isolated environments. Moreover, the pip packages are usually the most recent ones available, while Anaconda and system packages often lag behind a bit.&lt;/p&gt;
&lt;p&gt;These are the commands you need to type in a terminal if you want to use pip to install the required libraries. Note: in all the following commands, if you chose to use Python 2 rather than Python 3, you must replace &lt;code&gt;pip3&lt;/code&gt; with &lt;code&gt;pip&lt;/code&gt;, and &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First you need to make sure you have the latest version of pip installed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;--user&lt;/code&gt; option will install the latest version of pip only for the current user. If you prefer to install it system wide (i.e. for all users), you must have administrator rights (e.g. use &lt;code&gt;sudo python3&lt;/code&gt; instead of &lt;code&gt;python3&lt;/code&gt; on Linux), and you should remove the &lt;code&gt;--user&lt;/code&gt; option. The same is true of the command below that uses the &lt;code&gt;--user&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Next, you can optionally create an isolated environment. This is recommended as it makes it possible to have a different environment for each project (e.g. one for this project), with potentially very different libraries, and different versions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade virtualenv
$ python3 -m virtualenv -p `which python3` env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a new directory called &lt;code&gt;env&lt;/code&gt; in the current directory, containing an isolated Python environment based on Python 3. If you installed multiple versions of Python 3 on your system, you can replace &lt;code&gt;`which python3`&lt;/code&gt; with the path to the Python executable you prefer to use.&lt;/p&gt;
&lt;p&gt;Now you must activate this environment. You will need to run this command every time you want to use this environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ source ./env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Windows, the command is slightly different:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ .\env\Scripts\activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, use pip to install the required python packages. If you are not using virtualenv, you should add the &lt;code&gt;--user&lt;/code&gt; option (alternatively you could install the libraries system-wide, but this will probably require administrator rights, e.g. using &lt;code&gt;sudo pip3&lt;/code&gt; instead of &lt;code&gt;pip3&lt;/code&gt; on Linux).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! You're all set, you just need to start Jupyter now.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-jupyter" class="anchor" aria-hidden="true" href="#starting-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting Jupyter&lt;/h2&gt;
&lt;p&gt;Okay! You can now start Jupyter, simply type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should open up your browser, and you should see Jupyter's tree view, with the contents of the current directory. If your browser does not open automatically, visit &lt;a href="http://127.0.0.1:8888/tree" rel="nofollow"&gt;127.0.0.1:8888&lt;/a&gt;. Click on &lt;code&gt;index.ipynb&lt;/code&gt; to get started!&lt;/p&gt;
&lt;p&gt;Congrats! You are ready to learn Machine Learning, hands on!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h1&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml</guid><pubDate>Mon, 06 Jan 2020 00:10:00 GMT</pubDate></item><item><title>YunYang1994/TensorFlow2.0-Examples #11 in Jupyter Notebook, Today</title><link>https://github.com/YunYang1994/TensorFlow2.0-Examples</link><description>&lt;p&gt;&lt;i&gt;🙄 difficult algorithm, simple code.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2 align="center"&gt;&lt;a id="user-content-tensorflow20-examples" class="anchor" aria-hidden="true" href="#tensorflow20-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;🎉TensorFlow2.0-Examples🎉!&lt;/code&gt;&lt;/h2&gt;
&lt;p align="center"&gt;"&lt;i&gt;Talk is cheap, show me the code.&lt;/i&gt;" 
  ----- Linus Torvalds&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master"&gt;
    &lt;img src="https://camo.githubusercontent.com/588410b32491be114a084c302282529d4759412a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4272616e63682d6d61737465722d677265656e2e7376673f6c6f6e6743616368653d74727565" alt="Branch" data-canonical-src="https://img.shields.io/badge/Branch-master-green.svg?longCache=true" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/stargazers"&gt;
    &lt;img src="https://camo.githubusercontent.com/9a6823f33eb16c4b8050789afcd2e248d043f8e7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f59756e59616e67313939342f54656e736f72466c6f77322e302d4578616d706c65732e7376673f6c6162656c3d5374617273267374796c653d736f6369616c" alt="Stars" data-canonical-src="https://img.shields.io/github/stars/YunYang1994/TensorFlow2.0-Examples.svg?label=Stars&amp;amp;style=social" style="max-width:100%;"&gt;
  &lt;/a&gt;
    &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/network/members"&gt;
    &lt;img src="https://camo.githubusercontent.com/0abf80c6579aab48b8fa89e521d75254b1ac1c58/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f59756e59616e67313939342f54656e736f72466c6f77322e302d4578616d706c65732e7376673f6c6162656c3d466f726b73267374796c653d736f6369616c" alt="Forks" data-canonical-src="https://img.shields.io/github/forks/YunYang1994/TensorFlow2.0-Examples.svg?label=Forks&amp;amp;style=social" style="max-width:100%;"&gt;
  &lt;/a&gt;
  
   &lt;a href="https://github.com/sindresorhus/awesome"&gt;
   &lt;img src="https://camo.githubusercontent.com/13c4e50d88df7178ae1882a203ed57b641674f94/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f643733303566333864323966656437386661383536353265336136336531353464643865383832392f6d656469612f62616467652e737667" alt="Awesome" data-canonical-src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" style="max-width:100%;"&gt;
  &lt;/a&gt;
  
   &lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/LICENSE"&gt;
   &lt;img src="https://camo.githubusercontent.com/72b8fa08522b87c996b58d36be5132a346d434c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6173686170652f6170697374617475732e7376673f6d61784167653d32353932303030" alt="Awesome" data-canonical-src="https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/p&gt;&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/LICENSE"&gt;
&lt;/a&gt;&lt;div align="center"&gt;&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/LICENSE"&gt;
  &lt;sub&gt;Created by
  &lt;/sub&gt;&lt;/a&gt;&lt;a href="https://github.com/YunYang1994"&gt;YunYang1994&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;p&gt;This tutorial was designed for easily diving into TensorFlow2.0.  it includes both notebooks and source codes with explanation. &lt;strong&gt;It will be continuously updated !&lt;/strong&gt; &lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-1---introduction" class="anchor" aria-hidden="true" href="#1---introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1 - Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hello World&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/helloworld.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/helloworld.py"&gt;code&lt;/a&gt;). Very simple example to learn how to print "hello world" using TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variable&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/variable.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/variable.py"&gt;code&lt;/a&gt;). Learn to use variable in tensorflow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Basical operation&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/basic_operations.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/basic_operations.py"&gt;code&lt;/a&gt;). A simple example that covers TensorFlow basic operations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Activation&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/activation.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/activation.py"&gt;code&lt;/a&gt;). Start to know some activation functions in tensorflow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GradientTape&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/1-Introduction/GradientTape.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="1-Introduction/GradientTape.py"&gt;code&lt;/a&gt;). Introduce a key technique for automatic differentiation&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-2---basical-models" class="anchor" aria-hidden="true" href="#2---basical-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2 - Basical Models&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Regression&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/2-Basical_Models/Linear_Regression.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/Linear_Regression.py"&gt;code&lt;/a&gt;). Implement a Linear Regression with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/2-Basical_Models/Logistic_Regression.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/Logistic_Regression.py"&gt;code&lt;/a&gt;). Implement a Logistic Regression with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multilayer Perceptron Layer&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/2-Basical_Models/Multilayer_Perceptron.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/Multilayer_Perceptron.py"&gt;code&lt;/a&gt;). Implement Multi-Layer Perceptron Model with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNN&lt;/strong&gt; (&lt;a href="https://tensorflow.google.cn/tutorials/quickstart/advanced" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="2-Basical_Models/CNN.py"&gt;code&lt;/a&gt;). Implement CNN Model with TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-3---neural-network-architecture" class="anchor" aria-hidden="true" href="#3---neural-network-architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3 - Neural Network Architecture&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VGG16&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/vgg16.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1409.1556.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). VGG16: Very Deep Convolutional Networks for Large-Scale Image Recognition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resnet&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/resnet.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1512.03385.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). Resnet: Deep Residual Learning for Image Recognition. &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AutoEncoder&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/autoencoder.py"&gt;code&lt;/a&gt;)(&lt;a href="http://www.cs.toronto.edu/~hinton/science.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). AutoEncoder: Reducing the Dimensionality of Data with Neural Networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68206851-b08d2580-0008-11ea-8b51-061e0cbead62.gif"&gt;&lt;img width="65%" src="https://user-images.githubusercontent.com/30433053/68206851-b08d2580-0008-11ea-8b51-061e0cbead62.gif" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FPN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/3-Neural_Network_Architecture/fpn.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1612.03144" rel="nofollow"&gt;paper&lt;/a&gt;). FPN: Feature Pyramid Networks for Object Detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-4---object-detection" class="anchor" aria-hidden="true" href="#4---object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4 - Object Detection&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RPN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/RPN"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/RPN/rpn.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1703.06283.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). RPN:  a Region Proposal Network &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67913231-4e2ac400-fbc7-11e9-9995-94ed6f7181d4.png"&gt;&lt;img width="70%" src="https://user-images.githubusercontent.com/30433053/67913231-4e2ac400-fbc7-11e9-9995-94ed6f7181d4.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MTCNN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/MTCNN"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/MTCNN/mtcnn.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1604.02878" rel="nofollow"&gt;paper&lt;/a&gt;). MTCNN: Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks. &lt;em&gt;(Face detection and Alignment)&lt;/em&gt; &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68547531-7e6f2f80-041d-11ea-8cfb-0c5a22af0921.jpg"&gt;&lt;img width="40%" src="https://user-images.githubusercontent.com/30433053/68547531-7e6f2f80-041d-11ea-8cfb-0c5a22af0921.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;YOLOv3&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/YOLOV3/core/yolov3.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1804.02767.pdf" rel="nofollow"&gt;paper&lt;/a&gt;). YOLOv3: An Incremental Improvement.&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67914531-656bb080-fbcb-11e9-9775-302a25faf747.png"&gt;&lt;img width="65%" src="https://user-images.githubusercontent.com/30433053/67914531-656bb080-fbcb-11e9-9775-302a25faf747.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/SSD"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/SSD/ssd.py"&gt;code&lt;/a&gt;)(&lt;a href="http://arxiv.org/abs/1512.02325" rel="nofollow"&gt;paper&lt;/a&gt;). SSD: Single Shot MultiBox Detector.&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt; 【TO DO】&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68290134-5f416c80-00c2-11ea-8cbc-d6010ced4efd.png"&gt;&lt;img width="56%" src="https://user-images.githubusercontent.com/30433053/68290134-5f416c80-00c2-11ea-8cbc-d6010ced4efd.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster R-CNN&lt;/strong&gt; (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/Faster-RCNN"&gt;notebook&lt;/a&gt;) (&lt;a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/Faster-RCNN/frcnn.py"&gt;code&lt;/a&gt;)(&lt;a href="http://arxiv.org/abs/1506.01497" rel="nofollow"&gt;paper&lt;/a&gt;). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt; 【TO DO】&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/68546623-54187480-0413-11ea-9396-0a698c5a2580.png"&gt;&lt;img width="65%" src="https://user-images.githubusercontent.com/30433053/68546623-54187480-0413-11ea-9396-0a698c5a2580.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-5---image-segmentation" class="anchor" aria-hidden="true" href="#5---image-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5 - Image Segmentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FCN&lt;/strong&gt; (&lt;a href="5-Image_Segmentation/FCN"&gt;notebook&lt;/a&gt;) (&lt;a href="5-Image_Segmentation/FCN/fcn8s.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1411.4038" rel="nofollow"&gt;paper&lt;/a&gt;). FCN: Fully Convolutional Networks for Semantic Segmentation. &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67917411-e62eaa80-fbd3-11e9-9fe1-95550cf559d7.png"&gt;&lt;img width="60%" src="https://user-images.githubusercontent.com/30433053/67917411-e62eaa80-fbd3-11e9-9fe1-95550cf559d7.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unet&lt;/strong&gt; (&lt;a href="5-Image_Segmentation/Unet"&gt;notebook&lt;/a&gt;) (&lt;a href="5-Image_Segmentation/Unet/train.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/abs/1505.04597" rel="nofollow"&gt;paper&lt;/a&gt;). U-Net: Convolutional Networks for Biomedical Image Segmentation. &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/30433053/67922238-2ba7a380-fbe5-11e9-96a0-55c6827bd024.png"&gt;&lt;img width="50%" src="https://user-images.githubusercontent.com/30433053/67922238-2ba7a380-fbe5-11e9-96a0-55c6827bd024.png" style="max-width:100%;"&gt;&lt;/a&gt;
    
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-6---generative-adversarial-networks" class="anchor" aria-hidden="true" href="#6---generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6 - Generative Adversarial Networks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DCGAN&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/6-Generative_Adversarial_Networks/dcgan.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="6-Generative_Adversarial_Networks/dcgan.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1511.06434.pdf" rel="nofollow"&gt;paper&lt;/a&gt;).  Deep Convolutional Generative Adversarial Network.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pix2Pix&lt;/strong&gt; (&lt;a href="https://nbviewer.jupyter.org/github/YunYang1994/tensorflow2.0-examples/blob/master/6-Generative_Adversarial_Networks/Pix2Pix.ipynb" rel="nofollow"&gt;notebook&lt;/a&gt;) (&lt;a href="6-Generative_Adversarial_Networks/Pix2Pix.py"&gt;code&lt;/a&gt;)(&lt;a href="https://arxiv.org/pdf/1611.07004.pdf" rel="nofollow"&gt;paper&lt;/a&gt;).  Image-to-Image Translation with Conditional Adversarial Networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-7---reinforcement-learning" class="anchor" aria-hidden="true" href="#7---reinforcement-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;7 - Reinforcement Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DQN&lt;/strong&gt; (&lt;a href="6-Reinforcement_Learning/YOLOV2.ipynb"&gt;notebook&lt;/a&gt;) (&lt;a href="6-Reinforcement_Learning/YOLOV2.py"&gt;code&lt;/a&gt;). deep Q-network (DQN).&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>YunYang1994</author><guid isPermaLink="false">https://github.com/YunYang1994/TensorFlow2.0-Examples</guid><pubDate>Mon, 06 Jan 2020 00:11:00 GMT</pubDate></item><item><title>nianticlabs/monodepth2 #12 in Jupyter Notebook, Today</title><link>https://github.com/nianticlabs/monodepth2</link><description>&lt;p&gt;&lt;i&gt;Monocular depth estimation from a single image&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-monodepth2" class="anchor" aria-hidden="true" href="#monodepth2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monodepth2&lt;/h1&gt;
&lt;p&gt;This is the reference PyTorch implementation for training and testing depth estimation models using the method described in&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Digging into Self-Supervised Monocular Depth Prediction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/C.Godard/" rel="nofollow"&gt;Clément Godard&lt;/a&gt;, &lt;a href="http://vision.caltech.edu/~macaodha/" rel="nofollow"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://www.michaelfirman.co.uk" rel="nofollow"&gt;Michael Firman&lt;/a&gt; and &lt;a href="http://www0.cs.ucl.ac.uk/staff/g.brostow/" rel="nofollow"&gt;Gabriel J. Brostow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1806.01260" rel="nofollow"&gt;ICCV 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="assets/teaser.gif"&gt;&lt;img src="assets/teaser.gif" alt="example input output gif" width="600" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;This code is for non-commercial use; please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{monodepth2,
  title     = {Digging into Self-Supervised Monocular Depth Prediction},
  author    = {Cl{\'{e}}ment Godard and
               Oisin {Mac Aodha} and
               Michael Firman and
               Gabriel J. Brostow},
  booktitle = {The International Conference on Computer Vision (ICCV)},
  month = {October},
year = {2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-️-setup" class="anchor" aria-hidden="true" href="#️-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; Setup&lt;/h2&gt;
&lt;p&gt;Assuming a fresh &lt;a href="https://www.anaconda.com/download/" rel="nofollow"&gt;Anaconda&lt;/a&gt; distribution, you can install the dependencies with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch
pip install tensorboardX==1.4
conda install opencv=3.3.1   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; just needed for evaluation&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We ran our experiments with PyTorch 0.4.1, CUDA 9.1, Python 3.6.6 and Ubuntu 18.04.
We have also successfully trained models with PyTorch 1.0, and our code is compatible with Python 2.7. You may have issues installing OpenCV version 3.3.1 if you use Python 3.7, we recommend to create a virtual environment with Python 3.6.6 &lt;code&gt;conda create -n monodepth2 python=3.6.6 anaconda &lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-️-prediction-for-a-single-image" class="anchor" aria-hidden="true" href="#️-prediction-for-a-single-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Prediction for a single image&lt;/h2&gt;
&lt;p&gt;You can predict depth for a single image with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On its first run this will download the &lt;code&gt;mono+stereo_640x192&lt;/code&gt; pretrained model (99MB) into the &lt;code&gt;models/&lt;/code&gt; folder.
We provide the following  options for &lt;code&gt;--model_name&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--model_name&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Imagenet pretrained?&lt;/th&gt;
&lt;th&gt;Model resolution&lt;/th&gt;
&lt;th&gt;KITTI abs. rel. error&lt;/th&gt;
&lt;th&gt;delta &amp;lt; 1.25&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.877&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.109&lt;/td&gt;
&lt;td&gt;0.864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.879&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.107&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.132&lt;/td&gt;
&lt;td&gt;0.845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.130&lt;/td&gt;
&lt;td&gt;0.831&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.127&lt;/td&gt;
&lt;td&gt;0.836&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can also download models trained on the odometry split with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_odom_640x192.zip" rel="nofollow"&gt;monocular&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_odom_640x192.zip" rel="nofollow"&gt;mono+stereo&lt;/a&gt; training modalities.
Finally, we provide resnet 50 depth estimation models trained with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_640x192.zip" rel="nofollow"&gt;ImageNet pretrained weights&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_no_pt_640x192.zip" rel="nofollow"&gt;trained from scratch&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-training-data" class="anchor" aria-hidden="true" href="#-kitti-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png"&gt;💾&lt;/g-emoji&gt; KITTI training data&lt;/h2&gt;
&lt;p&gt;You can download the entire &lt;a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" rel="nofollow"&gt;raw KITTI dataset&lt;/a&gt; by running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;wget -i splits/kitti_archives_to_download.txt -P kitti_data/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then unzip with&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; kitti_data
unzip &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;*.zip&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; ..&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; it weighs about &lt;strong&gt;175GB&lt;/strong&gt;, so make sure you have enough space to unzip too!&lt;/p&gt;
&lt;p&gt;Our default settings expect that you have converted the png images to jpeg with this command, &lt;strong&gt;which also deletes the raw KITTI &lt;code&gt;.png&lt;/code&gt; files&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;find kitti_data/ -name &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;*.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;|&lt;/span&gt; parallel &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg &amp;amp;&amp;amp; rm {}&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;or&lt;/strong&gt; you can skip this conversion step and train from raw png files by adding the flag &lt;code&gt;--png&lt;/code&gt; when training, at the expense of slower load times.&lt;/p&gt;
&lt;p&gt;The above conversion command creates images which match our experiments, where KITTI &lt;code&gt;.png&lt;/code&gt; images were converted to &lt;code&gt;.jpg&lt;/code&gt; on Ubuntu 16.04 with default chroma subsampling &lt;code&gt;2x2,1x1,1x1&lt;/code&gt;.
We found that Ubuntu 18.04 defaults to &lt;code&gt;2x2,2x2,2x2&lt;/code&gt;, which gives different results, hence the explicit parameter in the conversion command.&lt;/p&gt;
&lt;p&gt;You can also place the KITTI dataset wherever you like and point towards it with the &lt;code&gt;--data_path&lt;/code&gt; flag during training and evaluation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Splits&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The train/test/validation splits are defined in the &lt;code&gt;splits/&lt;/code&gt; folder.
By default, the code will train a depth model using &lt;a href="https://github.com/tinghuiz/SfMLearner"&gt;Zhou's subset&lt;/a&gt; of the standard Eigen split of KITTI, which is designed for monocular training.
You can also train a model using the new &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;benchmark split&lt;/a&gt; or the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;odometry split&lt;/a&gt; by setting the &lt;code&gt;--split&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can train on a custom monocular or stereo dataset by writing a new dataloader class which inherits from &lt;code&gt;MonoDataset&lt;/code&gt; – see the &lt;code&gt;KITTIDataset&lt;/code&gt; class in &lt;code&gt;datasets/kitti_dataset.py&lt;/code&gt; for an example.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--training" class="anchor" aria-hidden="true" href="#-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="hourglass_flowing_sand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png"&gt;⏳&lt;/g-emoji&gt; Training&lt;/h2&gt;
&lt;p&gt;By default models and tensorboard event files are saved to &lt;code&gt;~/tmp/&amp;lt;model_name&amp;gt;&lt;/code&gt;.
This can be changed with the &lt;code&gt;--log_dir&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monocular training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our code defaults to using Zhou's subsampled Eigen training data. For stereo-only training we have to specify that we want to use the full Eigen training set – see paper for details.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name stereo_model \
  --frame_ids 0 --use_stereo --split eigen_full&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Monocular + stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono+stereo_model \
  --frame_ids 0 -1 1 --use_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-gpus" class="anchor" aria-hidden="true" href="#gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPUs&lt;/h3&gt;
&lt;p&gt;The code can only be run on a single GPU.
You can specify which GPU to use with the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment variable:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;CUDA_VISIBLE_DEVICES=2 python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All our experiments were performed on a single NVIDIA Titan Xp.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Approximate GPU memory&lt;/th&gt;
&lt;th&gt;Approximate training time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;9GB&lt;/td&gt;
&lt;td&gt;12 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;6GB&lt;/td&gt;
&lt;td&gt;8 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;11GB&lt;/td&gt;
&lt;td&gt;15 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content--finetuning-a-pretrained-model" class="anchor" aria-hidden="true" href="#-finetuning-a-pretrained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="minidisc" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bd.png"&gt;💽&lt;/g-emoji&gt; Finetuning a pretrained model&lt;/h3&gt;
&lt;p&gt;Add the following to the training command to load an existing model for finetuning:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name finetuned_mono --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content--other-training-options" class="anchor" aria-hidden="true" href="#-other-training-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png"&gt;🔧&lt;/g-emoji&gt; Other training options&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;python train.py -h&lt;/code&gt; (or look at &lt;code&gt;options.py&lt;/code&gt;) to see the range of other training options, such as learning rates and ablation settings.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-evaluation" class="anchor" aria-hidden="true" href="#-kitti-evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; KITTI evaluation&lt;/h2&gt;
&lt;p&gt;To prepare the ground truth depth maps run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python export_gt_depth.py --data_path kitti_data --split eigen
python export_gt_depth.py --data_path kitti_data --split eigen_benchmark&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;...assuming that you have placed the KITTI dataset in the default location of &lt;code&gt;./kitti_data/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The following example command evaluates the epoch 19 weights of a model named &lt;code&gt;mono_model&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19/ --eval_mono&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For stereo models, you must use the &lt;code&gt;--eval_stereo&lt;/code&gt; flag (see note below):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/stereo_model/models/weights_19/ --eval_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you train your own model with our code you are likely to see slight differences to the publication results due to randomization in the weights initialization and data loading.&lt;/p&gt;
&lt;p&gt;An additional parameter &lt;code&gt;--eval_split&lt;/code&gt; can be set.
The three different values possible for &lt;code&gt;eval_split&lt;/code&gt; are explained here:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--eval_split&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Test set size&lt;/th&gt;
&lt;th&gt;For models trained with...&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;697&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The standard Eigen test files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen_benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;652&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Evaluate with the improved ground truth from the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split benchmark&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt; test files.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because no ground truth is available for the new KITTI depth benchmark, no scores will be reported  when &lt;code&gt;--eval_split benchmark&lt;/code&gt; is set.
Instead, a set of &lt;code&gt;.png&lt;/code&gt; images will be saved to disk ready for upload to the evaluation server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;External disparities evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally you can also use &lt;code&gt;evaluate_depth.py&lt;/code&gt; to evaluate raw disparities (or inverse depth) from other methods by using the &lt;code&gt;--ext_disp_to_eval&lt;/code&gt; flag:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --ext_disp_to_eval &lt;span class="pl-k"&gt;~&lt;/span&gt;/other_method_disp.npy&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt; Note on stereo evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our stereo models are trained with an effective baseline of &lt;code&gt;0.1&lt;/code&gt; units, while the actual KITTI stereo rig has a baseline of &lt;code&gt;0.54m&lt;/code&gt;. This means a scaling of &lt;code&gt;5.4&lt;/code&gt; must be applied for evaluation.
In addition, for models trained with stereo supervision we disable median scaling.
Setting the &lt;code&gt;--eval_stereo&lt;/code&gt; flag when evaluating will automatically disable median scaling and scale predicted depths by &lt;code&gt;5.4&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2934.png"&gt;⤴️&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png"&gt;⤵️&lt;/g-emoji&gt; Odometry evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We include code for evaluating poses predicted by models trained with &lt;code&gt;--split odom --dataset kitti_odom --data_path /path/to/kitti/odometry/dataset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this evaluation, the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;KITTI odometry dataset&lt;/a&gt; &lt;strong&gt;(color, 65GB)&lt;/strong&gt; and &lt;strong&gt;ground truth poses&lt;/strong&gt; zip files must be downloaded.
As above, we assume that the pngs have been converted to jpgs.&lt;/p&gt;
&lt;p&gt;If this data has been unzipped to folder &lt;code&gt;kitti_odom&lt;/code&gt;, a model can be evaluated with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_pose.py --eval_split odom_9 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/
python evaluate_pose.py --eval_split odom_10 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content--precomputed-results" class="anchor" aria-hidden="true" href="#-precomputed-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png"&gt;📦&lt;/g-emoji&gt; Precomputed results&lt;/h2&gt;
&lt;p&gt;You can download our precomputed disparity predictions from the following links:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Input size&lt;/th&gt;
&lt;th&gt;&lt;code&gt;.npy&lt;/code&gt; filesize&lt;/th&gt;
&lt;th&gt;Eigen disparities&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-️-license" class="anchor" aria-hidden="true" href="#️-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="woman_judge" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-2696.png"&gt;👩‍⚖️&lt;/g-emoji&gt; License&lt;/h2&gt;
&lt;p&gt;Copyright © Niantic, Inc. 2019. Patent Pending.
All rights reserved.
Please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nianticlabs</author><guid isPermaLink="false">https://github.com/nianticlabs/monodepth2</guid><pubDate>Mon, 06 Jan 2020 00:12:00 GMT</pubDate></item><item><title>practicalAI/practicalAI #13 in Jupyter Notebook, Today</title><link>https://github.com/practicalAI/practicalAI</link><description>&lt;p&gt;&lt;i&gt;📚 A practical approach to machine learning to enable everyone to learn, explore and build.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a href="https://practicalai.me" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/practicalAI/images/master/images/logo.png" width="200" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;A &lt;i&gt;&lt;b&gt;practical&lt;/b&gt;&lt;/i&gt; approach to machine learning.&lt;/p&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;
&lt;img src="https://camo.githubusercontent.com/c1b6c20adc52e06a1c58218665169097a63bd549/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726163746963616c41492f70726163746963616c41492e7376673f7374796c653d736f6369616c266c6162656c3d53746172" data-canonical-src="https://img.shields.io/github/stars/practicalAI/practicalAI.svg?style=social&amp;amp;label=Star" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/19c0cf9ba93aa446aa855a0203c46ee39841cba9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d2d3565626130302e7376673f6c6162656c3d4c696e6b6564496e266c6f676f3d6c696e6b6564696e267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://twitter.com/practicalAIme" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/1a44bef694d0cd085f8365eac5ff9b5f85568043/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f70726163746963616c41496d652e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/practicalAIme.svg?label=Follow&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;sub&gt;Created by
&lt;a href="https://goku.me" rel="nofollow"&gt;Goku Mohandas&lt;/a&gt; and
&lt;a href="https://github.com/practicalAI/practicalAI/graphs/contributors"&gt;
contributors
&lt;/a&gt;
&lt;/sub&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-notebooks" class="anchor" aria-hidden="true" href="#notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notebooks&lt;/h2&gt;
&lt;ul&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="earth_americas" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30e.png"&gt;🌎&lt;/g-emoji&gt; → &lt;a href="https://practicalai.me" rel="nofollow"&gt;https://practicalai.me&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt; Illustrative ML notebooks in &lt;a href="https://tensorflow.org" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt;.
    &lt;/li&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="hammer_and_pick" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2692.png"&gt;⚒️&lt;/g-emoji&gt; Build robust models using the functional API w/ custom components
    &lt;/li&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png"&gt;📦&lt;/g-emoji&gt; Train using simple yet highly customizable loops to build products fast
    &lt;/li&gt;
    &lt;li&gt;
        If you prefer Jupyter Notebooks or want to add/fix content, check out the &lt;a href="https://github.com/practicalAI/practicalAI/tree/master/notebooks"&gt;notebooks&lt;/a&gt; directory.
    &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;
        &lt;h4 align="center"&gt;&lt;a id="user-content-basic-ml" class="anchor" aria-hidden="true" href="#basic-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic ML&lt;/h4&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Basics&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Machine Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Deep Learning&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn Python basics with notebooks.&lt;/li&gt;
            &lt;li&gt;Use data science libraries like &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/" rel="nofollow"&gt;Pandas&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Implement basic ML models in &lt;a href="https://www.tensorflow.org/overview/" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Create deep learning models for improved performance.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/00_Notebooks.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="notebook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d3.png"&gt;📓&lt;/g-emoji&gt; Notebooks&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/04_Linear_Regression.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;📈&lt;/g-emoji&gt; Linear Regression&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/07_Data_and_Models.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="mag_right" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50e.png"&gt;🔎&lt;/g-emoji&gt; Data &amp;amp; Models&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/10_Convolutional_Neural_Networks.ipynb" rel="nofollow"&gt;️&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼&lt;/g-emoji&gt; Convolutional Neural Networks&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/01_Python.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; Python&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/05_Logistic_Regression.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; Logistic Regression&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/08_Utilities.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="hammer_and_wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png"&gt;🛠&lt;/g-emoji&gt; Utilities&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/11_Embeddings.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="crown" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f451.png"&gt;👑&lt;/g-emoji&gt; Embeddings&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/02_NumPy.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;🔢&lt;/g-emoji&gt; NumPy&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/06_Multilayer_Perceptrons.ipynb" rel="nofollow"&gt;️&lt;g-emoji class="g-emoji" alias="control_knobs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f39b.png"&gt;🎛&lt;/g-emoji&gt; Multilayer Perceptrons&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/09_Preprocessing.ipynb" rel="nofollow"&gt;️&lt;g-emoji class="g-emoji" alias="scissors" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2702.png"&gt;✂️&lt;/g-emoji&gt; Preprocessing&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/12_Recurrent_Neural_Networks.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/03_Pandas.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="panda_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png"&gt;🐼&lt;/g-emoji&gt; Pandas&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-production-ml" class="anchor" aria-hidden="true" href="#production-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Production ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Local&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Applications&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Scale&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Setup your local environment for ML.&lt;/li&gt;
            &lt;li&gt;Wrap your ML in RESTful APIs using &lt;a href="http://flask.pocoo.org/" rel="nofollow"&gt;Flask&lt;/a&gt; to create applications.&lt;/li&gt;
            &lt;li&gt;Standardize and scale your ML applications with &lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt; and &lt;a href="https://kubernetes.io/" rel="nofollow"&gt;Kubernetes&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Deploy simple and scalable ML workflows using &lt;a href="https://www.kubeflow.org/" rel="nofollow"&gt;Kubeflow&lt;/a&gt;.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; Local Setup&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="evergreen_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f332.png"&gt;🌲&lt;/g-emoji&gt; Logging&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="whale" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f433.png"&gt;🐳&lt;/g-emoji&gt; Docker&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="handshake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f91d.png"&gt;🤝&lt;/g-emoji&gt; Distributed Training&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; ML Scripts&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="funeral_urn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png"&gt;⚱️&lt;/g-emoji&gt; Flask Applications&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ship" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a2.png"&gt;🚢&lt;/g-emoji&gt; Kubernetes&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="battery" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50b.png"&gt;🔋&lt;/g-emoji&gt; Databases&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png"&gt;✅&lt;/g-emoji&gt; Unit Tests&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ocean" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png"&gt;🌊&lt;/g-emoji&gt; Kubeflow&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="closed_lock_with_key" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f510.png"&gt;🔐&lt;/g-emoji&gt; Authentication&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-advanced-ml" class="anchor" aria-hidden="true" href="#advanced-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;General&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Sequential&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Popular&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Dive into architectural and interpretable advancements in neural networks.&lt;/li&gt;
            &lt;li&gt;Implement state-of-the-art NLP techniques.&lt;/li&gt;
            &lt;li&gt;Learn about popular deep learning algorithms used for generation, time-series, etc.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;🧐 Attention&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bee" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41d.png"&gt;🐝&lt;/g-emoji&gt; Transformers&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="performing_arts" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ad.png"&gt;🎭&lt;/g-emoji&gt; Generative Adversarial Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crystal_ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52e.png"&gt;🔮&lt;/g-emoji&gt; Autoencoders&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="racing_car" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ce.png"&gt;🏎️&lt;/g-emoji&gt; Highway Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="japanese_ogre" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f479.png"&gt;👹&lt;/g-emoji&gt; BERT, GPT2, XLNet&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="8ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b1.png"&gt;🎱&lt;/g-emoji&gt; Bayesian Deep Learning&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="spider" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f577.png"&gt;🕷️&lt;/g-emoji&gt; Graph Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="droplet" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a7.png"&gt;💧&lt;/g-emoji&gt; Residual Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="clock9" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f558.png"&gt;🕘&lt;/g-emoji&gt; Temporal CNNs&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="cherries" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f352.png"&gt;🍒&lt;/g-emoji&gt; Reinforcement Learning&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-topics" class="anchor" aria-hidden="true" href="#topics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Topics&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Computer Vision&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Natural Language&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Unsupervised Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn how to use deep learning for computer vision tasks.&lt;/li&gt;
            &lt;li&gt;Implement techniques for natural language tasks.&lt;/li&gt;
            &lt;li&gt;Derive insights from unlabeled data using unsupervised learning.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="camera_flash" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f8.png"&gt;📸&lt;/g-emoji&gt; Image Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; Text classification&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dango" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f361.png"&gt;🍡&lt;/g-emoji&gt; Clustering&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="alarm_clock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f0.png"&gt;⏰&lt;/g-emoji&gt; Time-series Analysis&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Image Segmentation&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;💬&lt;/g-emoji&gt; Named Entity Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="houses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3d8.png"&gt;🏘️&lt;/g-emoji&gt; Topic Modeling&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="shopping_cart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6d2.png"&gt;🛒&lt;/g-emoji&gt; Recommendation Systems&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="art" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a8.png"&gt;🎨&lt;/g-emoji&gt; Image Generation&lt;/td&gt;
        &lt;td&gt;🧠 Knowledge Graphs&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png"&gt;🎯&lt;/g-emoji&gt; One-shot Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="card_file_box" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5c3.png"&gt;🗃️&lt;/g-emoji&gt; Interpretability&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://practicalai.me/#newsletter" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="mailbox_with_mail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ec.png"&gt;📬&lt;/g-emoji&gt; Newsletter&lt;/a&gt; - Subscribe to get updates on new content.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>practicalAI</author><guid isPermaLink="false">https://github.com/practicalAI/practicalAI</guid><pubDate>Mon, 06 Jan 2020 00:13:00 GMT</pubDate></item><item><title>leandromoreira/digital_video_introduction #14 in Jupyter Notebook, Today</title><link>https://github.com/leandromoreira/digital_video_introduction</link><description>&lt;p&gt;&lt;i&gt;A hands-on introduction to video technology: image, video, codec (av1, vp9, h265) and more (ffmpeg encoding).&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="/README-cn.md" title="Simplified Chinese"&gt;&lt;g-emoji class="g-emoji" alias="cn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1e8-1f1f3.png"&gt;🇨🇳&lt;/g-emoji&gt;&lt;/a&gt;
&lt;a href="/README-ja.md" title="Japanese"&gt;&lt;g-emoji class="g-emoji" alias="jp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ef-1f1f5.png"&gt;🇯🇵&lt;/g-emoji&gt;&lt;/a&gt;
&lt;a href="/README-it.md" title="Italian"&gt;&lt;g-emoji class="g-emoji" alias="it" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1ee-1f1f9.png"&gt;🇮🇹&lt;/g-emoji&gt;&lt;/a&gt;
&lt;a href="/README-ko.md" title="Korean"&gt;&lt;g-emoji class="g-emoji" alias="kr" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f0-1f1f7.png"&gt;🇰🇷&lt;/g-emoji&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://img.shields.io/badge/license-BSD--3--Clause-blue.svg" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/274d07206c413193cf01e32de7f897d98da66ca2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d436c617573652d626c75652e737667" alt="license" data-canonical-src="https://img.shields.io/badge/license-BSD--3--Clause-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-intro" class="anchor" aria-hidden="true" href="#intro"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Intro&lt;/h1&gt;
&lt;p&gt;A gentle introduction to video technology, although it's aimed at software developers / engineers, we want to make it easy &lt;strong&gt;for anyone to learn&lt;/strong&gt;. This idea was born during a &lt;a href="https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p" rel="nofollow"&gt;mini workshop for newcomers to video technology&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal is to introduce some digital video concepts with a &lt;strong&gt;simple vocabulary, lots of visual elements and practical examples&lt;/strong&gt; when possible, and make this knowledge available everywhere. Please, feel free to send corrections, suggestions and improve it.&lt;/p&gt;
&lt;p&gt;There will be &lt;strong&gt;hands-on&lt;/strong&gt; sections which require you to have &lt;strong&gt;docker installed&lt;/strong&gt; and this repository cloned.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/leandromoreira/digital_video_introduction.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; digital_video_introduction
./setup.sh&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: when you see a &lt;code&gt;./s/ffmpeg&lt;/code&gt; or &lt;code&gt;./s/mediainfo&lt;/code&gt; command, it means we're running a &lt;strong&gt;containerized version&lt;/strong&gt; of that program, which already includes all the needed requirements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All the &lt;strong&gt;hands-on should be performed from the folder you cloned&lt;/strong&gt; this repository. For the &lt;strong&gt;jupyter examples&lt;/strong&gt; you must start the server &lt;code&gt;./s/start_jupyter.sh&lt;/code&gt; and copy the URL and use it in your browser.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-changelog" class="anchor" aria-hidden="true" href="#changelog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Changelog&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;added DRM system&lt;/li&gt;
&lt;li&gt;released version 1.0.0&lt;/li&gt;
&lt;li&gt;added simplified Chinese translation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-index" class="anchor" aria-hidden="true" href="#index"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#intro"&gt;Intro&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#index"&gt;Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#basic-terminology"&gt;Basic terminology&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#other-ways-to-encode-a-color-image"&gt;Other ways to encode a color image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-play-around-with-image-and-color"&gt;Hands-on: play around with image and color&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dvd-is-dar-43"&gt;DVD is DAR 4:3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-check-video-properties"&gt;Hands-on: Check video properties&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#redundancy-removal"&gt;Redundancy removal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#colors-luminance-and-our-eyes"&gt;Colors, Luminance and our eyes&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#color-model"&gt;Color model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#converting-between-ycbcr-and-rgb"&gt;Converting between YCbCr and RGB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#chroma-subsampling"&gt;Chroma subsampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-check-ycbcr-histogram"&gt;Hands-on: Check YCbCr histogram&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#frame-types"&gt;Frame types&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#i-frame-intra-keyframe"&gt;I Frame (intra, keyframe)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#p-frame-predicted"&gt;P Frame (predicted)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-a-video-with-a-single-i-frame"&gt;Hands-on: A video with a single I-frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#b-frame-bi-predictive"&gt;B Frame (bi-predictive)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-compare-videos-with-b-frame"&gt;Hands-on: Compare videos with B-frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#temporal-redundancy-inter-prediction"&gt;Temporal redundancy (inter prediction)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-see-the-motion-vectors"&gt;Hands-on: See the motion vectors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#spatial-redundancy-intra-prediction"&gt;Spatial redundancy (intra prediction)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-check-intra-predictions"&gt;Hands-on: Check intra predictions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-does-a-video-codec-work"&gt;How does a video codec work?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-why-how"&gt;What? Why? How?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#history"&gt;History&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#the-birth-of-av1"&gt;The birth of AV1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-generic-codec"&gt;A generic codec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#1st-step---picture-partitioning"&gt;1st step - picture partitioning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-check-partitions"&gt;Hands-on: Check partitions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#2nd-step---predictions"&gt;2nd step - predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#3rd-step---transform"&gt;3rd step - transform&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-throwing-away-different-coefficients"&gt;Hands-on: throwing away different coefficients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#4th-step---quantization"&gt;4th step - quantization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#hands-on-quantization"&gt;Hands-on: quantization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#5th-step---entropy-coding"&gt;5th step - entropy coding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#vlc-coding"&gt;VLC coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#arithmetic-coding"&gt;Arithmetic coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-cabac-vs-cavlc"&gt;Hands-on: CABAC vs CAVLC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#6th-step---bitstream-format"&gt;6th step - bitstream format&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#h264-bitstream"&gt;H.264 bitstream&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hands-on-inspect-the-h264-bitstream"&gt;Hands-on: Inspect the H.264 bitstream&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#review"&gt;Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-does-h265-achieve-a-better-compression-ratio-than-h264"&gt;How does H.265 achieve a better compression ratio than H.264?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#online-streaming"&gt;Online streaming&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#general-architecture"&gt;General architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#progressive-download-and-adaptive-streaming"&gt;Progressive download and adaptive streaming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#content-protection"&gt;Content protection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use-jupyter"&gt;How to use jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conferences"&gt;Conferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-basic-terminology" class="anchor" aria-hidden="true" href="#basic-terminology"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic terminology&lt;/h1&gt;
&lt;p&gt;An &lt;strong&gt;image&lt;/strong&gt; can be thought of as a &lt;strong&gt;2D matrix&lt;/strong&gt;. If we think about &lt;strong&gt;colors&lt;/strong&gt;, we can extrapolate this idea seeing this image as a &lt;strong&gt;3D matrix&lt;/strong&gt; where the &lt;strong&gt;additional dimensions&lt;/strong&gt; are used to provide &lt;strong&gt;color data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If we chose to represent these colors using the &lt;a href="https://en.wikipedia.org/wiki/Primary_color" rel="nofollow"&gt;primary colors (red, green and blue)&lt;/a&gt;, we define three planes: the first one for &lt;strong&gt;red&lt;/strong&gt;, the second for &lt;strong&gt;green&lt;/strong&gt;, and the last one for the &lt;strong&gt;blue&lt;/strong&gt; color.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/image_3d_matrix_rgb.png"&gt;&lt;img src="/i/image_3d_matrix_rgb.png" alt="an image is a 3d matrix RGB" title="An image is a 3D matrix" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We'll call each point in this matrix &lt;strong&gt;a pixel&lt;/strong&gt; (picture element). One pixel represents the &lt;strong&gt;intensity&lt;/strong&gt; (usually a numeric value) of a given color. For example, a &lt;strong&gt;red pixel&lt;/strong&gt; means 0 of green, 0 of blue and maximum of red. The &lt;strong&gt;pink color pixel&lt;/strong&gt; can be formed with a combination of the three colors. Using a representative numeric range from 0 to 255, the pink pixel is defined by &lt;strong&gt;Red=255, Green=192 and Blue=203&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-other-ways-to-encode-a-color-image" class="anchor" aria-hidden="true" href="#other-ways-to-encode-a-color-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other ways to encode a color image&lt;/h4&gt;
&lt;p&gt;Many other possible models may be used to represent the colors that make up an image. We could, for instance, use an indexed palette where we'd only need a single byte to represent each pixel instead of the 3 needed when using the RGB model. In such a model we could use a 2D matrix instead of a 3D matrix to represent our color, this would save on memory but yield fewer color options.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/nes-color-palette.png"&gt;&lt;img src="/i/nes-color-palette.png" alt="NES palette" title="NES palette" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For instance, look at the picture down below. The first face is fully colored. The others are the red, green, and blue planes (shown as gray tones).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/rgb_channels_intensity.png"&gt;&lt;img src="/i/rgb_channels_intensity.png" alt="RGB channels intensity" title="RGB channels intensity" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see that the &lt;strong&gt;red color&lt;/strong&gt; will be the one that &lt;strong&gt;contributes more&lt;/strong&gt; (the brightest parts in the second face) to the final color while the &lt;strong&gt;blue color&lt;/strong&gt; contribution can be mostly &lt;strong&gt;only seen in Mario's eyes&lt;/strong&gt; (last face) and part of his clothes, see how &lt;strong&gt;all planes contribute less&lt;/strong&gt; (darkest parts) to the &lt;strong&gt;Mario's mustache&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;And each color intensity requires a certain amount of bits, this quantity is known as &lt;strong&gt;bit depth&lt;/strong&gt;. Let's say we spend &lt;strong&gt;8 bits&lt;/strong&gt; (accepting values from 0 to 255) per color (plane), therefore we have a &lt;strong&gt;color depth&lt;/strong&gt; of &lt;strong&gt;24 bits&lt;/strong&gt; (8 bits * 3 planes R/G/B), and we can also infer that we could use 2 to the power of 24 different colors.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;It's great&lt;/strong&gt; to learn &lt;a href="http://www.cambridgeincolour.com/tutorials/camera-sensors.htm" rel="nofollow"&gt;how an image is captured from the world to the bits&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another property of an image is the &lt;strong&gt;resolution&lt;/strong&gt;, which is the number of pixels in one dimension. It is often presented as width × height, for example, the &lt;strong&gt;4×4&lt;/strong&gt; image below.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/resolution.png"&gt;&lt;img src="/i/resolution.png" alt="image resolution" title="image resolution" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-play-around-with-image-and-color" class="anchor" aria-hidden="true" href="#hands-on-play-around-with-image-and-color"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: play around with image and color&lt;/h4&gt;
&lt;p&gt;You can &lt;a href="/image_as_3d_array.ipynb"&gt;play around with image and colors&lt;/a&gt; using &lt;a href="#how-to-use-jupyter"&gt;jupyter&lt;/a&gt; (python, numpy, matplotlib and etc).&lt;/p&gt;
&lt;p&gt;You can also learn &lt;a href="/filters_are_easy.ipynb"&gt;how image filters (edge detection, sharpen, blur...) work&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another property we can see while working with images or video is the &lt;strong&gt;aspect ratio&lt;/strong&gt; which simply describes the proportional relationship between width and height of an image or pixel.&lt;/p&gt;
&lt;p&gt;When people says this movie or picture is &lt;strong&gt;16x9&lt;/strong&gt; they usually are referring to the &lt;strong&gt;Display Aspect Ratio (DAR)&lt;/strong&gt;, however we also can have different shapes of individual pixels, we call this &lt;strong&gt;Pixel Aspect Ratio (PAR)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/DAR.png"&gt;&lt;img src="/i/DAR.png" alt="display aspect ratio" title="display aspect ratio" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/PAR.png"&gt;&lt;img src="/i/PAR.png" alt="pixel aspect ratio" title="pixel aspect ratio" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-dvd-is-dar-43" class="anchor" aria-hidden="true" href="#dvd-is-dar-43"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DVD is DAR 4:3&lt;/h4&gt;
&lt;p&gt;Although the real resolution of a DVD is 704x480 it still keeps a 4:3 aspect ratio because it has a PAR of 10:11 (704x10/480x11)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finally, we can define a &lt;strong&gt;video&lt;/strong&gt; as a &lt;strong&gt;succession of &lt;em&gt;n&lt;/em&gt; frames&lt;/strong&gt; in &lt;strong&gt;time&lt;/strong&gt; which can be seen as another dimension, &lt;em&gt;n&lt;/em&gt; is the frame rate or frames per second (FPS).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/video.png"&gt;&lt;img src="/i/video.png" alt="video" title="video" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The number of bits per second needed to show a video is its &lt;strong&gt;bit rate&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bit rate = width * height * bit depth * frames per second&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, a video with 30 frames per second, 24 bits per pixel, resolution of 480x240 will need &lt;strong&gt;82,944,000 bits per second&lt;/strong&gt; or 82.944 Mbps (30x480x240x24) if we don't employ any kind of compression.&lt;/p&gt;
&lt;p&gt;When the &lt;strong&gt;bit rate&lt;/strong&gt; is nearly constant it's called constant bit rate (&lt;strong&gt;CBR&lt;/strong&gt;) but it also can vary then called variable bit rate (&lt;strong&gt;VBR&lt;/strong&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This graph shows a constrained VBR which doesn't spend too many bits while the frame is black.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/vbr.png"&gt;&lt;img src="/i/vbr.png" alt="constrained vbr" title="constrained vbr" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the early days, engineers came up with a technique for doubling the perceived frame rate of a video display &lt;strong&gt;without consuming extra bandwidth&lt;/strong&gt;. This technique is known as &lt;strong&gt;interlaced video&lt;/strong&gt;; it basically sends half of the screen in 1 "frame" and the other half in the next "frame".&lt;/p&gt;
&lt;p&gt;Today screens render mostly using &lt;strong&gt;progressive scan technique&lt;/strong&gt;. Progressive is a way of displaying, storing, or transmitting moving images in which all the lines of each frame are drawn in sequence.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/interlaced_vs_progressive.png"&gt;&lt;img src="/i/interlaced_vs_progressive.png" alt="interlaced vs progressive" title="interlaced vs progressive" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now we have an idea about how an &lt;strong&gt;image&lt;/strong&gt; is represented digitally, how its &lt;strong&gt;colors&lt;/strong&gt; are arranged, how many &lt;strong&gt;bits per second&lt;/strong&gt; do we spend to show a video, if it's constant (CBR)  or variable (VBR), with a given &lt;strong&gt;resolution&lt;/strong&gt; using a given &lt;strong&gt;frame rate&lt;/strong&gt; and many other terms such as interlaced, PAR and others.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-check-video-properties" class="anchor" aria-hidden="true" href="#hands-on-check-video-properties"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check video properties&lt;/h4&gt;
&lt;p&gt;You can &lt;a href="https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream"&gt;check most of the  explained properties with ffmpeg or mediainfo.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-redundancy-removal" class="anchor" aria-hidden="true" href="#redundancy-removal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Redundancy removal&lt;/h1&gt;
&lt;p&gt;We learned that it's not feasible to use video without any compression; &lt;strong&gt;a single one hour video&lt;/strong&gt; at 720p resolution with 30fps would &lt;strong&gt;require 278GB&lt;sup&gt;*&lt;/sup&gt;&lt;/strong&gt;. Since &lt;strong&gt;using solely lossless data compression algorithms&lt;/strong&gt; like DEFLATE (used in PKZIP, Gzip, and PNG), &lt;strong&gt;won't&lt;/strong&gt; decrease the required bandwidth sufficiently we need to find other ways to compress the video.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; We found this number by multiplying 1280 x 720 x 24 x 30 x 3600 (width, height, bits per pixel, fps and time in seconds)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In order to do this, we can &lt;strong&gt;exploit how our vision works&lt;/strong&gt;. We're better at distinguishing brightness than colors, the &lt;strong&gt;repetitions in time&lt;/strong&gt;, a video contains a lot of images with few changes, and the &lt;strong&gt;repetitions within the image&lt;/strong&gt;, each frame also contains many areas using the same or similar color.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-colors-luminance-and-our-eyes" class="anchor" aria-hidden="true" href="#colors-luminance-and-our-eyes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Colors, Luminance and our eyes&lt;/h2&gt;
&lt;p&gt;Our eyes are &lt;a href="http://vanseodesign.com/web-design/color-luminance/" rel="nofollow"&gt;more sensitive to brightness than colors&lt;/a&gt;, you can test it for yourself, look at this picture.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/luminance_vs_color.png"&gt;&lt;img src="/i/luminance_vs_color.png" alt="luminance vs color" title="luminance vs color" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are unable to see that the colors of the &lt;strong&gt;squares A and B are identical&lt;/strong&gt; on the left side, that's fine, it's our brain playing tricks on us to &lt;strong&gt;pay more attention to light and dark than color&lt;/strong&gt;. There is a connector, with the same color, on the right side so we (our brain) can easily spot that in fact, they're the same color.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Simplistic explanation of how our eyes work&lt;/strong&gt;
The &lt;a href="http://www.biologymad.com/nervoussystem/eyenotes.htm" rel="nofollow"&gt;eye is a complex organ&lt;/a&gt;, it is composed of many parts but we are mostly interested in the cones and rods cells. The eye &lt;a href="https://en.wikipedia.org/wiki/Photoreceptor_cell" rel="nofollow"&gt;contains about 120 million rod cells and 6 million cone cells&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To &lt;strong&gt;oversimplify&lt;/strong&gt;, let's try to put colors and brightness in the eye's parts function. The &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Rod_cell" rel="nofollow"&gt;rod cells&lt;/a&gt; are mostly responsible for brightness&lt;/strong&gt; while the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Cone_cell" rel="nofollow"&gt;cone cells&lt;/a&gt; are responsible for color&lt;/strong&gt;, there are three types of cones, each with different pigment, namely: &lt;a href="https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg" rel="nofollow"&gt;S-cones (Blue), M-cones (Green) and L-cones (Red)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since we have many more rod cells (brightness) than cone cells (color), one can infer that we are more capable of distinguishing dark and light than colors.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/eyes.jpg"&gt;&lt;img src="/i/eyes.jpg" alt="eyes composition" title="eyes composition" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contrast sensitivity functions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Researchers of experimental psychology and many other fields have developed many theories on human vision. And one of them is called Contrast sensitivity functions. They are related to spatio and temporal of the light and their value presents at given init light, how much change is required before an observer reported there was a change. Notice the plural of the word "function", this is for the reason that we can measure Contrast sensitivity functions with not only black-white but also colors. The result of these experiments shows that in most cases our eyes are more sensitive to brightness than color.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once we know that we're more sensitive to &lt;strong&gt;luma&lt;/strong&gt; (the brightness in an image) we can try to exploit it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-color-model" class="anchor" aria-hidden="true" href="#color-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Color model&lt;/h3&gt;
&lt;p&gt;We first learned &lt;a href="#basic-terminology"&gt;how to color images&lt;/a&gt; work using the &lt;strong&gt;RGB model&lt;/strong&gt;, but there are other models too. In fact, there is a model that separates luma (brightness) from  chrominance (colors) and it is known as &lt;strong&gt;YCbCr&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; there are more models which do the same separation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This color model uses &lt;strong&gt;Y&lt;/strong&gt; to represent the brightness and two color channels &lt;strong&gt;Cb&lt;/strong&gt; (chroma blue) and &lt;strong&gt;Cr&lt;/strong&gt; (chroma red). The &lt;a href="https://en.wikipedia.org/wiki/YCbCr" rel="nofollow"&gt;YCbCr&lt;/a&gt; can be derived from RGB and it also can be converted back to RGB. Using this model we can create full colored images as we can see down below.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/ycbcr.png"&gt;&lt;img src="/i/ycbcr.png" alt="ycbcr example" title="ycbcr example" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-converting-between-ycbcr-and-rgb" class="anchor" aria-hidden="true" href="#converting-between-ycbcr-and-rgb"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Converting between YCbCr and RGB&lt;/h3&gt;
&lt;p&gt;Some may argue, how can we produce all the &lt;strong&gt;colors without using the green&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;To answer this question, we'll walk through a conversion from RGB to YCbCr. We'll use the coefficients from the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Rec._601" rel="nofollow"&gt;standard BT.601&lt;/a&gt;&lt;/strong&gt; that was recommended by the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/ITU-R" rel="nofollow"&gt;group ITU-R&lt;sup&gt;*&lt;/sup&gt;&lt;/a&gt;&lt;/strong&gt; . The first step is to &lt;strong&gt;calculate the luma&lt;/strong&gt;, we'll use the constants suggested by ITU and replace the RGB values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Y = 0.299R + 0.587G + 0.114B
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we had the luma, we can &lt;strong&gt;split the colors&lt;/strong&gt; (chroma blue and red):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Cb = 0.564(B - Y)
Cr = 0.713(R - Y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can also &lt;strong&gt;convert it back&lt;/strong&gt; and even get the &lt;strong&gt;green by using YCbCr&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R = Y + 1.402Cr
B = Y + 1.772Cb
G = Y - 0.344Cb - 0.714Cr
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; groups and standards are common in digital video, they usually define what are the standards, for instance, &lt;a href="https://en.wikipedia.org/wiki/Rec._2020" rel="nofollow"&gt;what is 4K? what frame rate should we use? resolution? color model?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Generally, &lt;strong&gt;displays&lt;/strong&gt; (monitors, TVs, screens and etc) utilize &lt;strong&gt;only the RGB model&lt;/strong&gt;, organized in different manners, see some of them magnified below:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/new_pixel_geometry.jpg"&gt;&lt;img src="/i/new_pixel_geometry.jpg" alt="pixel geometry" title="pixel geometry" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-chroma-subsampling" class="anchor" aria-hidden="true" href="#chroma-subsampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Chroma subsampling&lt;/h3&gt;
&lt;p&gt;With the image represented as luma and chroma components, we can take advantage of the human visual system's greater sensitivity for luma resolution rather than chroma to selectively remove information. &lt;strong&gt;Chroma subsampling&lt;/strong&gt; is the technique of encoding images using &lt;strong&gt;less resolution for chroma than for luma&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/ycbcr_subsampling_resolution.png"&gt;&lt;img src="/i/ycbcr_subsampling_resolution.png" alt="ycbcr subsampling resolutions" title="ycbcr subsampling resolutions" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How much should we reduce the chroma resolution?! It turns out that there are already some schemas that describe how to handle resolution and the merge (&lt;code&gt;final color = Y + Cb + Cr&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;These schemas are known as subsampling systems and are expressed as a 3 part ratio - &lt;code&gt;a:x:y&lt;/code&gt; which defines the chroma resolution in relation to a &lt;code&gt;a x 2&lt;/code&gt; block of luma pixels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a&lt;/code&gt; is the horizontal sampling reference (usually 4)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is the number of chroma samples in the first row of &lt;code&gt;a&lt;/code&gt; pixels (horizontal resolution in relation to &lt;code&gt;a&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is the number of changes of chroma samples between the first and seconds rows of &lt;code&gt;a&lt;/code&gt; pixels.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;An exception to this exists with 4:1:0, which provides a single chroma sample within each &lt;code&gt;4 x 4&lt;/code&gt; block of luma resolution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Common schemes used in modern codecs are: &lt;strong&gt;4:4:4&lt;/strong&gt; &lt;em&gt;(no subsampling)&lt;/em&gt;, &lt;strong&gt;4:2:2, 4:1:1, 4:2:0, 4:1:0 and 3:1:1&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;YCbCr 4:2:0 merge&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here's a merged piece of an image using YCbCr 4:2:0, notice that we only spend 12 bits per pixel.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/ycbcr_420_merge.png"&gt;&lt;img src="/i/ycbcr_420_merge.png" alt="YCbCr 4:2:0 merge" title="YCbCr 4:2:0 merge" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can see the same image encoded by the main chroma subsampling types, images in the first row are the final YCbCr while the last row of images shows the chroma resolution. It's indeed a great win for such small loss.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/chroma_subsampling_examples.jpg"&gt;&lt;img src="/i/chroma_subsampling_examples.jpg" alt="chroma subsampling examples" title="chroma subsampling examples" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Previously we had calculated that we needed &lt;a href="#redundancy-removal"&gt;278GB of storage to keep a video file with one hour at 720p resolution and 30fps&lt;/a&gt;. If we use &lt;strong&gt;YCbCr 4:2:0&lt;/strong&gt; we can cut &lt;strong&gt;this size in half (139 GB)&lt;/strong&gt;&lt;sup&gt;*&lt;/sup&gt; but it is still far from ideal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; we found this value by multiplying width, height, bits per pixel and fps. Previously we needed 24 bits, now we only need 12.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-check-ycbcr-histogram" class="anchor" aria-hidden="true" href="#hands-on-check-ycbcr-histogram"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check YCbCr histogram&lt;/h3&gt;
&lt;p&gt;You can &lt;a href="/encoding_pratical_examples.md#generates-yuv-histogram"&gt;check the YCbCr histogram with ffmpeg.&lt;/a&gt; This scene has a higher blue contribution, which is showed by the &lt;a href="https://en.wikipedia.org/wiki/Histogram" rel="nofollow"&gt;histogram&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/yuv_histogram.png"&gt;&lt;img src="/i/yuv_histogram.png" alt="ycbcr color histogram" title="ycbcr color histogram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-color-luma-luminance-gama-video-review" class="anchor" aria-hidden="true" href="#color-luma-luminance-gama-video-review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Color, luma, luminance, gama video review&lt;/h3&gt;
&lt;p&gt;Watch this incredible video explaining what is luma and learn about luminance, gamma, and color.
&lt;a href="http://www.youtube.com/watch?v=Ymt47wXUDEU" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0b7ba49561d9021d22f18aa453ccd4626714bcd9/687474703a2f2f696d672e796f75747562652e636f6d2f76692f596d7434377758554445552f302e6a7067" alt="Analog Luma - A history and explanation of video" data-canonical-src="http://img.youtube.com/vi/Ymt47wXUDEU/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-frame-types" class="anchor" aria-hidden="true" href="#frame-types"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Frame types&lt;/h2&gt;
&lt;p&gt;Now we can move on and try to eliminate the &lt;strong&gt;redundancy in time&lt;/strong&gt; but before that let's establish some basic terminology. Suppose we have a movie with 30fps, here are its first 4 frames.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_2.png"&gt;&lt;img src="/i/smw_background_ball_2.png" alt="ball 2" title="ball 2" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_3.png"&gt;&lt;img src="/i/smw_background_ball_3.png" alt="ball 3" title="ball 3" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_4.png"&gt;&lt;img src="/i/smw_background_ball_4.png" alt="ball 4" title="ball 4" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see &lt;strong&gt;lots of repetitions&lt;/strong&gt; within frames like &lt;strong&gt;the blue background&lt;/strong&gt;, it doesn't change from frame 0 to frame 3. To tackle this problem, we can &lt;strong&gt;abstractly categorize&lt;/strong&gt; them as three types of frames.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-frame-intra-keyframe" class="anchor" aria-hidden="true" href="#i-frame-intra-keyframe"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I Frame (intra, keyframe)&lt;/h3&gt;
&lt;p&gt;An I-frame (reference, keyframe, intra) is a &lt;strong&gt;self-contained frame&lt;/strong&gt;. It doesn't rely on anything to be rendered, an I-frame looks similar to a static photo. The first frame is usually an I-frame but we'll see I-frames inserted regularly among other types of frames.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-p-frame-predicted" class="anchor" aria-hidden="true" href="#p-frame-predicted"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;P Frame (predicted)&lt;/h3&gt;
&lt;p&gt;A P-frame takes advantage of the fact that almost always the current picture can be &lt;strong&gt;rendered using the previous frame.&lt;/strong&gt; For instance, in the second frame, the only change was the ball that moved forward. We can &lt;strong&gt;rebuild frame 1, only using the difference and referencing to the previous frame&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt; &amp;lt;-  &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_2_diff.png"&gt;&lt;img src="/i/smw_background_ball_2_diff.png" alt="ball 2" title="ball 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-a-video-with-a-single-i-frame" class="anchor" aria-hidden="true" href="#hands-on-a-video-with-a-single-i-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: A video with a single I-frame&lt;/h4&gt;
&lt;p&gt;Since a P-frame uses less data why can't we encode an entire &lt;a href="/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames"&gt;video with a single I-frame and all the rest being P-frames?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After you encoded this video, start to watch it and do a &lt;strong&gt;seek for an advanced&lt;/strong&gt; part of the video, you'll notice &lt;strong&gt;it takes some time&lt;/strong&gt; to really move to that part. That's because a &lt;strong&gt;P-frame needs a reference frame&lt;/strong&gt; (I-frame for instance) to be rendered.&lt;/p&gt;
&lt;p&gt;Another quick test you can do is to encode a video using a single I-Frame and then &lt;a href="/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second"&gt;encode it inserting an I-frame each 2s&lt;/a&gt; and &lt;strong&gt;check the size of each rendition&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-b-frame-bi-predictive" class="anchor" aria-hidden="true" href="#b-frame-bi-predictive"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;B Frame (bi-predictive)&lt;/h3&gt;
&lt;p&gt;What about referencing the past and future frames to provide even a better compression?! That's basically what a B-frame is.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_1.png"&gt;&lt;img src="/i/smw_background_ball_1.png" alt="ball 1" title="ball 1" style="max-width:100%;"&gt;&lt;/a&gt; &amp;lt;-  &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_2_diff.png"&gt;&lt;img src="/i/smw_background_ball_2_diff.png" alt="ball 2" title="ball 2" style="max-width:100%;"&gt;&lt;/a&gt; -&amp;gt; &lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_background_ball_3.png"&gt;&lt;img src="/i/smw_background_ball_3.png" alt="ball 3" title="ball 3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-compare-videos-with-b-frame" class="anchor" aria-hidden="true" href="#hands-on-compare-videos-with-b-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Compare videos with B-frame&lt;/h4&gt;
&lt;p&gt;You can generate two renditions, first with B-frames and other with &lt;a href="/encoding_pratical_examples.md#no-b-frames-at-all"&gt;no B-frames at all&lt;/a&gt; and check the size of the file as well as the quality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-summary" class="anchor" aria-hidden="true" href="#summary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h3&gt;
&lt;p&gt;These frames types are used to &lt;strong&gt;provide better compression&lt;/strong&gt;. We'll look how this happens in the next section, but for now we can think of &lt;strong&gt;I-frame as expensive while P-frame is cheaper but the cheapest is the B-frame.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/frame_types.png"&gt;&lt;img src="/i/frame_types.png" alt="frame types example" title="frame types example" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-temporal-redundancy-inter-prediction" class="anchor" aria-hidden="true" href="#temporal-redundancy-inter-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Temporal redundancy (inter prediction)&lt;/h2&gt;
&lt;p&gt;Let's explore the options we have to reduce the &lt;strong&gt;repetitions in time&lt;/strong&gt;, this type of redundancy can be solved with techniques of &lt;strong&gt;inter prediction&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We will try to &lt;strong&gt;spend fewer bits&lt;/strong&gt; to encode the sequence of frames 0 and 1.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/original_frames.png"&gt;&lt;img src="/i/original_frames.png" alt="original frames" title="original frames" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One thing we can do it's a subtraction, we simply &lt;strong&gt;subtract frame 1 from frame 0&lt;/strong&gt; and we get just what we need to &lt;strong&gt;encode the residual&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/difference_frames.png"&gt;&lt;img src="/i/difference_frames.png" alt="delta frames" title="delta frames" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But what if I tell you that there is a &lt;strong&gt;better method&lt;/strong&gt; which uses even fewer bits?! First, let's treat the &lt;code&gt;frame 0&lt;/code&gt; as a collection of well-defined partitions and then we'll try to match the blocks from &lt;code&gt;frame 0&lt;/code&gt; on &lt;code&gt;frame 1&lt;/code&gt;. We can think of it as &lt;strong&gt;motion estimation&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-wikipedia---block-motion-compensation" class="anchor" aria-hidden="true" href="#wikipedia---block-motion-compensation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wikipedia - block motion compensation&lt;/h3&gt;
&lt;p&gt;"&lt;strong&gt;Block motion compensation&lt;/strong&gt; divides up the current frame into non-overlapping blocks, and the motion compensation vector &lt;strong&gt;tells where those blocks come from&lt;/strong&gt; (a common misconception is that the previous frame is divided up into non-overlapping blocks, and the motion compensation vectors tell where those blocks move to). The source blocks typically overlap in the source frame. Some video compression algorithms assemble the current frame out of pieces of several different previously-transmitted frames."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/original_frames_motion_estimation.png"&gt;&lt;img src="/i/original_frames_motion_estimation.png" alt="delta frames" title="delta frames" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We could estimate that the ball moved from &lt;code&gt;x=0, y=25&lt;/code&gt; to &lt;code&gt;x=6, y=26&lt;/code&gt;, the &lt;strong&gt;x&lt;/strong&gt; and &lt;strong&gt;y&lt;/strong&gt; values are the &lt;strong&gt;motion vectors&lt;/strong&gt;. One &lt;strong&gt;further step&lt;/strong&gt; we can do to save bits is to &lt;strong&gt;encode only the motion vector difference&lt;/strong&gt; between the last block position and the predicted, so the final motion vector would be &lt;code&gt;x=6 (6-0), y=1 (26-25)&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a real-world situation, this &lt;strong&gt;ball would be sliced into n partitions&lt;/strong&gt; but the process is the same.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The objects on the frame &lt;strong&gt;move in a 3D way&lt;/strong&gt;, the ball can become smaller when it moves to the background. It's normal that &lt;strong&gt;we won't find the perfect match&lt;/strong&gt; to the block we tried to find a match. Here's a superposed view of our estimation vs the real picture.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/motion_estimation.png"&gt;&lt;img src="/i/motion_estimation.png" alt="motion estimation" title="motion estimation" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But we can see that when we apply &lt;strong&gt;motion estimation&lt;/strong&gt; the &lt;strong&gt;data to encode is smaller&lt;/strong&gt; than using simply delta frame techniques.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/comparison_delta_vs_motion_estimation.png"&gt;&lt;img src="/i/comparison_delta_vs_motion_estimation.png" alt="motion estimation vs delta " title="motion estimation delta" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-how-real-motion-compensation-would-look" class="anchor" aria-hidden="true" href="#how-real-motion-compensation-would-look"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How real motion compensation would look&lt;/h3&gt;
&lt;p&gt;This technique is applied to all blocks, very often a ball would be partitioned in more than one block.
&lt;a target="_blank" rel="noopener noreferrer" href="/i/real_world_motion_compensation.png"&gt;&lt;img src="/i/real_world_motion_compensation.png" alt="real world motion compensation" title="real world motion compensation" style="max-width:100%;"&gt;&lt;/a&gt;
Source: &lt;a href="https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf" rel="nofollow"&gt;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can &lt;a href="/frame_difference_vs_motion_estimation_plus_residual.ipynb"&gt;play around with these concepts using jupyter&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-see-the-motion-vectors" class="anchor" aria-hidden="true" href="#hands-on-see-the-motion-vectors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: See the motion vectors&lt;/h4&gt;
&lt;p&gt;We can &lt;a href="/encoding_pratical_examples.md#generate-debug-video"&gt;generate a video with the inter prediction (motion vectors)  with ffmpeg.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/motion_vectors_ffmpeg.png"&gt;&lt;img src="/i/motion_vectors_ffmpeg.png" alt="inter prediction (motion vectors) with ffmpeg" title="inter prediction (motion vectors) with ffmpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or we can use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only the first 10 frames).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/inter_prediction_intel_video_pro_analyzer.png"&gt;&lt;img src="/i/inter_prediction_intel_video_pro_analyzer.png" alt="inter prediction intel video pro analyzer" title="inter prediction intel video pro analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-spatial-redundancy-intra-prediction" class="anchor" aria-hidden="true" href="#spatial-redundancy-intra-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spatial redundancy (intra prediction)&lt;/h2&gt;
&lt;p&gt;If we analyze &lt;strong&gt;each frame&lt;/strong&gt; in a video we'll see that there are also &lt;strong&gt;many areas that are correlated&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/repetitions_in_space.png"&gt;&lt;img src="/i/repetitions_in_space.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let's walk through an example. This scene is mostly composed of blue and white colors.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_bg.png"&gt;&lt;img src="/i/smw_bg.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is an &lt;code&gt;I-frame&lt;/code&gt; and we &lt;strong&gt;can't use previous frames&lt;/strong&gt; to predict from but we still can compress it. We will encode the red block selection. If we &lt;strong&gt;look at its neighbors&lt;/strong&gt;, we can &lt;strong&gt;estimate&lt;/strong&gt; that there is a &lt;strong&gt;trend of colors around it&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_bg_block.png"&gt;&lt;img src="/i/smw_bg_block.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We will &lt;strong&gt;predict&lt;/strong&gt; that the frame will continue to &lt;strong&gt;spread the colors vertically&lt;/strong&gt;, it means that the colors of the &lt;strong&gt;unknown pixels will hold the values of its neighbors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_bg_prediction.png"&gt;&lt;img src="/i/smw_bg_prediction.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our &lt;strong&gt;prediction can be wrong&lt;/strong&gt;, for that reason we need to apply this technique (&lt;strong&gt;intra prediction&lt;/strong&gt;) and then &lt;strong&gt;subtract the real values&lt;/strong&gt; which gives us the residual block, resulting in a much more compressible matrix compared to the original.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/smw_residual.png"&gt;&lt;img src="/i/smw_residual.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-hands-on-check-intra-predictions" class="anchor" aria-hidden="true" href="#hands-on-check-intra-predictions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check intra predictions&lt;/h4&gt;
&lt;p&gt;You can &lt;a href="/encoding_pratical_examples.md#generate-debug-video"&gt;generate a video with macro blocks and their predictions with ffmpeg.&lt;/a&gt; Please check the ffmpeg documentation to understand the &lt;a href="https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors#AnalyzingMacroblockTypes" rel="nofollow"&gt;meaning of each block color&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/macro_blocks_ffmpeg.png"&gt;&lt;img src="/i/macro_blocks_ffmpeg.png" alt="intra prediction (macro blocks) with ffmpeg" title="inter prediction (motion vectors) with ffmpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or we can use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only the first 10 frames).&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/intra_prediction_intel_video_pro_analyzer.png"&gt;&lt;img src="/i/intra_prediction_intel_video_pro_analyzer.png" alt="intra prediction intel video pro analyzer" title="intra prediction intel video pro analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-how-does-a-video-codec-work" class="anchor" aria-hidden="true" href="#how-does-a-video-codec-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does a video codec work?&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-what-why-how" class="anchor" aria-hidden="true" href="#what-why-how"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What? Why? How?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;What?&lt;/strong&gt; It's a piece of software / hardware that compresses or decompresses digital video. &lt;strong&gt;Why?&lt;/strong&gt; Market and society demands higher quality videos with limited bandwidth or storage. Remember when we &lt;a href="#basic-terminology"&gt;calculated the needed bandwidth&lt;/a&gt; for 30 frames per second, 24 bits per pixel, resolution of a 480x240 video? It was &lt;strong&gt;82.944 Mbps&lt;/strong&gt; with no compression applied. It's the only way to deliver HD/FullHD/4K in TVs and the Internet. &lt;strong&gt;How?&lt;/strong&gt; We'll take a brief look at the major techniques here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CODEC vs Container&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One common mistake that beginners often do is to confuse digital video CODEC and &lt;a href="https://en.wikipedia.org/wiki/Digital_container_format" rel="nofollow"&gt;digital video container&lt;/a&gt;. We can think of &lt;strong&gt;containers&lt;/strong&gt; as a wrapper format which contains metadata of the video (and possible audio too), and the &lt;strong&gt;compressed video&lt;/strong&gt; can be seen as its payload.&lt;/p&gt;
&lt;p&gt;Usually, the extension of a video file defines its video container. For instance, the file &lt;code&gt;video.mp4&lt;/code&gt; is probably a &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/MPEG-4_Part_14" rel="nofollow"&gt;MPEG-4 Part 14&lt;/a&gt;&lt;/strong&gt; container and a file named &lt;code&gt;video.mkv&lt;/code&gt; it's probably a &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Matroska" rel="nofollow"&gt;matroska&lt;/a&gt;&lt;/strong&gt;. To be completely sure about the codec and container format we can use &lt;a href="/encoding_pratical_examples.md#inspect-stream"&gt;ffmpeg or mediainfo&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-history" class="anchor" aria-hidden="true" href="#history"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;History&lt;/h2&gt;
&lt;p&gt;Before we jump into the inner workings of a generic codec, let's look back to understand a little better about some old video codecs.&lt;/p&gt;
&lt;p&gt;The video codec &lt;a href="https://en.wikipedia.org/wiki/H.261" rel="nofollow"&gt;H.261&lt;/a&gt;  was born in 1990 (technically 1988), and it was designed to work with &lt;strong&gt;data rates of 64 kbit/s&lt;/strong&gt;. It already uses ideas such as chroma subsampling, macro block, etc. In the year of 1995, the &lt;strong&gt;H.263&lt;/strong&gt; video codec standard was published and continued to be extended until 2001.&lt;/p&gt;
&lt;p&gt;In 2003 the first version of &lt;strong&gt;H.264/AVC&lt;/strong&gt; was completed. In the same year, a company called &lt;strong&gt;TrueMotion&lt;/strong&gt; released their video codec as a &lt;strong&gt;royalty-free&lt;/strong&gt; lossy video compression called &lt;strong&gt;VP3&lt;/strong&gt;. In 2008, &lt;strong&gt;Google bought&lt;/strong&gt; this company, releasing &lt;strong&gt;VP8&lt;/strong&gt; in the same year. In December of 2012, Google released the &lt;strong&gt;VP9&lt;/strong&gt; and it's  &lt;strong&gt;supported by roughly ¾ of the browser market&lt;/strong&gt; (mobile included).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/AOMedia_Video_1" rel="nofollow"&gt;AV1&lt;/a&gt;&lt;/strong&gt; is a new &lt;strong&gt;royalty-free&lt;/strong&gt; and open source video codec that's being designed by the &lt;a href="http://aomedia.org/" rel="nofollow"&gt;Alliance for Open Media (AOMedia)&lt;/a&gt;, which is composed of the &lt;strong&gt;companies: Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel and Cisco&lt;/strong&gt; among others. The &lt;strong&gt;first version&lt;/strong&gt; 0.1.0 of the reference codec was &lt;strong&gt;published on April 7, 2016&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/codec_history_timeline.png"&gt;&lt;img src="/i/codec_history_timeline.png" alt="codec history timeline" title="codec history timeline" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-the-birth-of-av1" class="anchor" aria-hidden="true" href="#the-birth-of-av1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The birth of AV1&lt;/h4&gt;
&lt;p&gt;Early 2015, Google was working on &lt;a href="https://en.wikipedia.org/wiki/VP9#Successor:_from_VP10_to_AV1" rel="nofollow"&gt;VP10&lt;/a&gt;, Xiph (Mozilla) was working on &lt;a href="https://xiph.org/daala/" rel="nofollow"&gt;Daala&lt;/a&gt; and Cisco open-sourced its royalty-free video codec called &lt;a href="https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03" rel="nofollow"&gt;Thor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then MPEG LA first announced annual caps for HEVC (H.265) and fees 8 times higher than H.264 but soon they changed the rules again:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;no annual cap&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;content fee&lt;/strong&gt; (0.5% of revenue) and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;per-unit fees about 10 times higher than h264&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;a href="http://aomedia.org/about-us/" rel="nofollow"&gt;alliance for open media&lt;/a&gt; was created by companies from hardware manufacturer (Intel, AMD, ARM , Nvidia, Cisco), content delivery (Google, Netflix, Amazon), browser maintainers (Google, Mozilla), and others.&lt;/p&gt;
&lt;p&gt;The companies had a common goal, a royalty-free video codec and then AV1 was born with a much &lt;a href="http://aomedia.org/license/patent/" rel="nofollow"&gt;simpler patent license&lt;/a&gt;. &lt;strong&gt;Timothy B. Terriberry&lt;/strong&gt; did an awesome presentation, which is the source of this section, about the &lt;a href="https://www.youtube.com/watch?v=lzPaldsmJbk" rel="nofollow"&gt;AV1 conception, license model and its current state&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You'll be surprised to know that you can &lt;strong&gt;analyze the AV1 codec through your browser&lt;/strong&gt;, go to &lt;a href="http://aomanalyzer.org/" rel="nofollow"&gt;http://aomanalyzer.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/av1_browser_analyzer.png"&gt;&lt;img src="/i/av1_browser_analyzer.png" alt="av1 browser analyzer" title="av1 browser analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PS: If you want to learn more about the history of the codecs you must learn the basics behind &lt;a href="https://www.vcodex.com/video-compression-patents/" rel="nofollow"&gt;video compression patents&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-a-generic-codec" class="anchor" aria-hidden="true" href="#a-generic-codec"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A generic codec&lt;/h2&gt;
&lt;p&gt;We're going to introduce the &lt;strong&gt;main mechanics behind a generic video codec&lt;/strong&gt; but most of these concepts are useful and used in modern codecs such as VP9, AV1 and HEVC. Be sure to understand that we're going to simplify things a LOT. Sometimes we'll use a real example (mostly H.264) to demonstrate a technique.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-1st-step---picture-partitioning" class="anchor" aria-hidden="true" href="#1st-step---picture-partitioning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1st step - picture partitioning&lt;/h2&gt;
&lt;p&gt;The first step is to &lt;strong&gt;divide the frame&lt;/strong&gt; into several &lt;strong&gt;partitions, sub-partitions&lt;/strong&gt; and beyond.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/picture_partitioning.png"&gt;&lt;img src="/i/picture_partitioning.png" alt="picture partitioning" title="picture partitioning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But why?&lt;/strong&gt; There are many reasons, for instance, when we split the picture we can work the predictions more precisely, using small partitions for the small moving parts while using bigger partitions to a static background.&lt;/p&gt;
&lt;p&gt;Usually, the CODECs &lt;strong&gt;organize these partitions&lt;/strong&gt; into slices (or tiles), macro (or coding tree units) and many sub-partitions. The max size of these partitions varies, HEVC sets 64x64 while AVC uses 16x16 but the sub-partitions can reach sizes of 4x4.&lt;/p&gt;
&lt;p&gt;Remember that we learned how &lt;strong&gt;frames are typed&lt;/strong&gt;?! Well, you can &lt;strong&gt;apply those ideas to blocks&lt;/strong&gt; too, therefore we can have I-Slice, B-Slice, I-Macroblock and etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-check-partitions" class="anchor" aria-hidden="true" href="#hands-on-check-partitions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Check partitions&lt;/h3&gt;
&lt;p&gt;We can also use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; (which is paid but there is a free trial version which limits you to only the first 10 frames). Here are &lt;a href="/encoding_pratical_examples.md#transcoding"&gt;VP9 partitions&lt;/a&gt; analyzed.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/paritions_view_intel_video_pro_analyzer.png"&gt;&lt;img src="/i/paritions_view_intel_video_pro_analyzer.png" alt="VP9 partitions view intel video pro analyzer " title="VP9 partitions view intel video pro analyzer" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-2nd-step---predictions" class="anchor" aria-hidden="true" href="#2nd-step---predictions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2nd step - predictions&lt;/h2&gt;
&lt;p&gt;Once we have the partitions, we can make predictions over them. For the &lt;a href="#temporal-redundancy-inter-prediction"&gt;inter prediction&lt;/a&gt; we need &lt;strong&gt;to send the motion vectors and the residual&lt;/strong&gt; and the &lt;a href="#spatial-redundancy-intra-prediction"&gt;intra prediction&lt;/a&gt; we'll &lt;strong&gt;send the prediction direction and the residual&lt;/strong&gt; as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-3rd-step---transform" class="anchor" aria-hidden="true" href="#3rd-step---transform"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3rd step - transform&lt;/h2&gt;
&lt;p&gt;After we get the residual block (&lt;code&gt;predicted partition - real partition&lt;/code&gt;), we can &lt;strong&gt;transform&lt;/strong&gt; it in a way that lets us know which &lt;strong&gt;pixels we can discard&lt;/strong&gt; while keeping the &lt;strong&gt;overall quality&lt;/strong&gt;. There are some transformations for this exact behavior.&lt;/p&gt;
&lt;p&gt;Although there are &lt;a href="https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms" rel="nofollow"&gt;other transformations&lt;/a&gt;, we'll look more closely at the discrete cosine transform (DCT). The &lt;a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform" rel="nofollow"&gt;&lt;strong&gt;DCT&lt;/strong&gt;&lt;/a&gt; main features are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;converts&lt;/strong&gt; blocks of &lt;strong&gt;pixels&lt;/strong&gt; into  same-sized blocks of &lt;strong&gt;frequency coefficients&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;compacts&lt;/strong&gt; energy, making it easy to eliminate spatial redundancy.&lt;/li&gt;
&lt;li&gt;is &lt;strong&gt;reversible&lt;/strong&gt;, a.k.a. you can reverse to pixels.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;On 2 Feb 2017, Cintra, R. J. and Bayer, F. M have published their paper &lt;a href="https://arxiv.org/abs/1702.00817" rel="nofollow"&gt;DCT-like Transform for Image Compression
Requires 14 Additions Only&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Don't worry if you didn't understand the benefits from every bullet point, we'll try to make some experiments in order to see the real value from it.&lt;/p&gt;
&lt;p&gt;Let's take the following &lt;strong&gt;block of pixels&lt;/strong&gt; (8x8):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/pixel_matrice.png"&gt;&lt;img src="/i/pixel_matrice.png" alt="pixel values matrix" title="pixel values matrix" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which renders to the following block image (8x8):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/gray_image.png"&gt;&lt;img src="/i/gray_image.png" alt="pixel values matrix" title="pixel values matrix" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When we &lt;strong&gt;apply the DCT&lt;/strong&gt; over this block of pixels and we get the &lt;strong&gt;block of coefficients&lt;/strong&gt; (8x8):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_values.png"&gt;&lt;img src="/i/dct_coefficient_values.png" alt="coefficients values" title="coefficients values" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And if we render this block of coefficients, we'll get this image:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_image.png"&gt;&lt;img src="/i/dct_coefficient_image.png" alt="dct coefficients image" title="dct coefficients image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As you can see it looks nothing like the original image, we might notice that the &lt;strong&gt;first coefficient&lt;/strong&gt; is very different from all the others. This first coefficient is known as the DC coefficient which represents of &lt;strong&gt;all the samples&lt;/strong&gt; in the input array, something &lt;strong&gt;similar to an average&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This block of coefficients has an interesting property which is that it separates the high-frequency components from the low frequency.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dctfrequ.jpg"&gt;&lt;img src="/i/dctfrequ.jpg" alt="dct frequency coefficients property" title="dct frequency coefficients property" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In an image, &lt;strong&gt;most of the energy&lt;/strong&gt; will be concentrated in the &lt;a href="https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm" rel="nofollow"&gt;&lt;strong&gt;lower frequencies&lt;/strong&gt;&lt;/a&gt;, so if we transform an image into its frequency components and &lt;strong&gt;throw away the higher frequency coefficients&lt;/strong&gt;, we can &lt;strong&gt;reduce the amount of data&lt;/strong&gt; needed to describe the image without sacrificing too much image quality.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;frequency means how fast a signal is changing&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's try to apply the knowledge we acquired in the test by converting the original image to its frequency (block of coefficients) using DCT and then throwing away part of the least important coefficients.&lt;/p&gt;
&lt;p&gt;First, we convert it to its &lt;strong&gt;frequency domain&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_values.png"&gt;&lt;img src="/i/dct_coefficient_values.png" alt="coefficients values" title="coefficients values" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next, we discard part (67%) of the coefficients, mostly the bottom right part of it.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/dct_coefficient_zeroed.png"&gt;&lt;img src="/i/dct_coefficient_zeroed.png" alt="zeroed coefficients" title="zeroed coefficients" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Finally, we reconstruct the image from this discarded block of coefficients (remember, it needs to be reversible) and compare it to the original.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/original_vs_quantized.png"&gt;&lt;img src="/i/original_vs_quantized.png" alt="original vs quantized" title="original vs quantized" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we can see it resembles the original image but it introduced lots of differences from the original, we &lt;strong&gt;throw away 67.1875%&lt;/strong&gt; and we still were able to get at least something similar to the original. We could more intelligently discard the coefficients to have a better image quality but that's the next topic.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Each coefficient is formed using all the pixels&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It's important to note that each coefficient doesn't directly map to a single pixel but it's a weighted sum of all pixels. This amazing graph shows how the first and second coefficient is calculated, using weights which are unique for each index.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/applicat.jpg"&gt;&lt;img src="/i/applicat.jpg" alt="dct calculation" title="dct calculation" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Source: &lt;a href="https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm" rel="nofollow"&gt;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also try to &lt;a href="/dct_better_explained.ipynb"&gt;visualize the DCT by looking at a simple image&lt;/a&gt; formation over the DCT basis. For instance, here's the &lt;a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT" rel="nofollow"&gt;A character being formed&lt;/a&gt; using each coefficient weight.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ac2f3ff3a6e29112f3e7f51325cb7d3a2f08e377/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35652f496463742d616e696d6174696f6e2e676966"&gt;&lt;img src="https://camo.githubusercontent.com/ac2f3ff3a6e29112f3e7f51325cb7d3a2f08e377/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35652f496463742d616e696d6174696f6e2e676966" alt="" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-throwing-away-different-coefficients" class="anchor" aria-hidden="true" href="#hands-on-throwing-away-different-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: throwing away different coefficients&lt;/h3&gt;
&lt;p&gt;You can play around with the &lt;a href="/uniform_quantization_experience.ipynb"&gt;DCT transform&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-4th-step---quantization" class="anchor" aria-hidden="true" href="#4th-step---quantization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4th step - quantization&lt;/h2&gt;
&lt;p&gt;When we throw away some of the coefficients, in the last step (transform), we kinda did some form of quantization. This step is where we chose to lose information (the &lt;strong&gt;lossy part&lt;/strong&gt;) or in simple terms, we'll &lt;strong&gt;quantize coefficients to achieve compression&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;How can we quantize a block of coefficients? One simple method would be a uniform quantization, where we take a block, &lt;strong&gt;divide it by a single value&lt;/strong&gt; (10) and round this value.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/quantize.png"&gt;&lt;img src="/i/quantize.png" alt="quantize" title="quantize" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How can we &lt;strong&gt;reverse&lt;/strong&gt; (re-quantize) this block of coefficients? We can do that by &lt;strong&gt;multiplying the same value&lt;/strong&gt; (10) we divide it first.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/re-quantize.png"&gt;&lt;img src="/i/re-quantize.png" alt="re-quantize" title="re-quantize" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This &lt;strong&gt;approach isn't the best&lt;/strong&gt; because it doesn't take into account the importance of each coefficient, we could use a &lt;strong&gt;matrix of quantizers&lt;/strong&gt; instead of a single value, this matrix can exploit the property of the DCT, quantizing most the bottom right and less the upper left, the &lt;a href="https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html" rel="nofollow"&gt;JPEG uses a similar approach&lt;/a&gt;, you can check &lt;a href="https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40"&gt;source code to see this matrix&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-quantization" class="anchor" aria-hidden="true" href="#hands-on-quantization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: quantization&lt;/h3&gt;
&lt;p&gt;You can play around with the &lt;a href="/dct_experiences.ipynb"&gt;quantization&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-5th-step---entropy-coding" class="anchor" aria-hidden="true" href="#5th-step---entropy-coding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5th step - entropy coding&lt;/h2&gt;
&lt;p&gt;After we quantized the data (image blocks/slices/frames) we still can compress it in a lossless way. There are many ways (algorithms) to compress data. We're going to briefly experience some of them, for a deeper understanding you can read the amazing book &lt;a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/" rel="nofollow"&gt;Understanding Compression: Data Compression for Modern Developers&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-vlc-coding" class="anchor" aria-hidden="true" href="#vlc-coding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;VLC coding:&lt;/h3&gt;
&lt;p&gt;Let's suppose we have a stream of the symbols: &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;r&lt;/strong&gt; and &lt;strong&gt;t&lt;/strong&gt; and their probability (from 0 to 1) is represented by this table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;a&lt;/th&gt;
&lt;th&gt;e&lt;/th&gt;
&lt;th&gt;r&lt;/th&gt;
&lt;th&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;probability&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can assign unique binary codes (preferable small) to the most probable and bigger codes to the least probable ones.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;a&lt;/th&gt;
&lt;th&gt;e&lt;/th&gt;
&lt;th&gt;r&lt;/th&gt;
&lt;th&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;probability&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;binary code&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;td&gt;1110&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let's compress the stream &lt;strong&gt;eat&lt;/strong&gt;, assuming we would spend 8 bits for each symbol, we would spend &lt;strong&gt;24 bits&lt;/strong&gt; without any compression. But in case we replace each symbol for its code we can save space.&lt;/p&gt;
&lt;p&gt;The first step is to encode the symbol &lt;strong&gt;e&lt;/strong&gt; which is &lt;code&gt;10&lt;/code&gt; and the second symbol is &lt;strong&gt;a&lt;/strong&gt; which is added (not in a mathematical way) &lt;code&gt;[10][0]&lt;/code&gt; and finally the third symbol &lt;strong&gt;t&lt;/strong&gt; which makes our final compressed bitstream to be &lt;code&gt;[10][0][1110]&lt;/code&gt; or &lt;code&gt;1001110&lt;/code&gt; which only requires &lt;strong&gt;7 bits&lt;/strong&gt; (3.4 times less space than the original).&lt;/p&gt;
&lt;p&gt;Notice that each code must be a unique prefixed code &lt;a href="https://en.wikipedia.org/wiki/Huffman_coding" rel="nofollow"&gt;Huffman can help you to find these numbers&lt;/a&gt;. Though it has some issues there are &lt;a href="https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding" rel="nofollow"&gt;video codecs that still offers&lt;/a&gt; this method and it's the  algorithm for many applications which requires compression.&lt;/p&gt;
&lt;p&gt;Both encoder and decoder &lt;strong&gt;must know&lt;/strong&gt; the symbol table with its code, therefore, you need to send the table too.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-arithmetic-coding" class="anchor" aria-hidden="true" href="#arithmetic-coding"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arithmetic coding:&lt;/h3&gt;
&lt;p&gt;Let's suppose we have a stream of the symbols: &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;r&lt;/strong&gt;, &lt;strong&gt;s&lt;/strong&gt; and &lt;strong&gt;t&lt;/strong&gt; and their probability is represented by this table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;a&lt;/th&gt;
&lt;th&gt;e&lt;/th&gt;
&lt;th&gt;r&lt;/th&gt;
&lt;th&gt;s&lt;/th&gt;
&lt;th&gt;t&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;probability&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.15&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With this table in mind, we can build ranges containing all the possible symbols sorted by the most frequents.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/range.png"&gt;&lt;img src="/i/range.png" alt="initial arithmetic range" title="initial arithmetic range" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now let's encode the stream &lt;strong&gt;eat&lt;/strong&gt;, we pick the first symbol &lt;strong&gt;e&lt;/strong&gt; which is located within the subrange &lt;strong&gt;0.3 to 0.6&lt;/strong&gt; (but not included) and we take this subrange and split it again using the same proportions used before but within this new range.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/second_subrange.png"&gt;&lt;img src="/i/second_subrange.png" alt="second sub range" title="second sub range" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let's continue to encode our stream &lt;strong&gt;eat&lt;/strong&gt;, now we take the second symbol &lt;strong&gt;a&lt;/strong&gt; which is within the new subrange &lt;strong&gt;0.3 to 0.39&lt;/strong&gt; and then we take our last symbol &lt;strong&gt;t&lt;/strong&gt; and we do the same process again and we get the last subrange &lt;strong&gt;0.354 to 0.372&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/arithimetic_range.png"&gt;&lt;img src="/i/arithimetic_range.png" alt="final arithmetic range" title="final arithmetic range" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We just need to pick a number within the last subrange &lt;strong&gt;0.354 to 0.372&lt;/strong&gt;, let's choose &lt;strong&gt;0.36&lt;/strong&gt; but we could choose any number within this subrange. With &lt;strong&gt;only&lt;/strong&gt; this number we'll be able to recover our original stream &lt;strong&gt;eat&lt;/strong&gt;. If you think about it, it's like if we were drawing a line within ranges of ranges to encode our stream.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/range_show.png"&gt;&lt;img src="/i/range_show.png" alt="final range traverse" title="final range traverse" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;reverse process&lt;/strong&gt; (A.K.A. decoding) is equally easy, with our number &lt;strong&gt;0.36&lt;/strong&gt; and our original range we can run the same process but now using this number to reveal the stream encoded behind this number.&lt;/p&gt;
&lt;p&gt;With the first range, we notice that our number fits at the slice, therefore, it's our first symbol, now we split this subrange again, doing the same process as before, and we'll notice that &lt;strong&gt;0.36&lt;/strong&gt; fits the symbol &lt;strong&gt;a&lt;/strong&gt; and after we repeat the process we came to the last symbol &lt;strong&gt;t&lt;/strong&gt; (forming our original encoded stream &lt;em&gt;eat&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Both encoder and decoder &lt;strong&gt;must know&lt;/strong&gt; the symbol probability table, therefore you need to send the table.&lt;/p&gt;
&lt;p&gt;Pretty neat, isn't it? People are damn smart to come up with a such solution, some &lt;a href="https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding" rel="nofollow"&gt;video codecs use&lt;/a&gt; this technique (or at least offer it as an option).&lt;/p&gt;
&lt;p&gt;The idea is to lossless compress the quantized bitstream, for sure this article is missing tons of details, reasons, trade-offs and etc. But &lt;a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/" rel="nofollow"&gt;you should learn more&lt;/a&gt; as a developer. Newer codecs are trying to use different &lt;a href="https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems" rel="nofollow"&gt;entropy coding algorithms like ANS.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-cabac-vs-cavlc" class="anchor" aria-hidden="true" href="#hands-on-cabac-vs-cavlc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: CABAC vs CAVLC&lt;/h3&gt;
&lt;p&gt;You can &lt;a href="https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc"&gt;generate two streams, one with CABAC and other with CAVLC&lt;/a&gt; and &lt;strong&gt;compare the time&lt;/strong&gt; it took to generate each of them as well as &lt;strong&gt;the final size&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-6th-step---bitstream-format" class="anchor" aria-hidden="true" href="#6th-step---bitstream-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6th step - bitstream format&lt;/h2&gt;
&lt;p&gt;After we did all these steps we need to &lt;strong&gt;pack the compressed frames and context to these steps&lt;/strong&gt;. We need to explicitly inform to the decoder about &lt;strong&gt;the decisions taken by the encoder&lt;/strong&gt;, such as bit depth, color space, resolution, predictions info (motion vectors, intra prediction direction), profile, level, frame rate, frame type, frame number and much more.&lt;/p&gt;
&lt;p&gt;We're going to study, superficially, the H.264 bitstream. Our first step is to &lt;a href="/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream"&gt;generate a minimal  H.264 &lt;sup&gt;*&lt;/sup&gt; bitstream&lt;/a&gt;, we can do that using our own repository and &lt;a href="http://ffmpeg.org/" rel="nofollow"&gt;ffmpeg&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; ffmpeg adds, by default, all the encoding parameter as a &lt;strong&gt;SEI NAL&lt;/strong&gt;, soon we'll define what is a NAL.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This command will generate a raw h264 bitstream with a &lt;strong&gt;single frame&lt;/strong&gt;, 64x64, with color space yuv420 and using the following image as the frame.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/minimal.png"&gt;&lt;img src="/i/minimal.png" alt="used frame to generate minimal h264 bitstream" title="used frame to generate minimal h264 bitstream" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-h264-bitstream" class="anchor" aria-hidden="true" href="#h264-bitstream"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;H.264 bitstream&lt;/h3&gt;
&lt;p&gt;The AVC (H.264) standard defines that the information will be sent in &lt;strong&gt;macro frames&lt;/strong&gt; (in the network sense), called &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Network_Abstraction_Layer" rel="nofollow"&gt;NAL&lt;/a&gt;&lt;/strong&gt; (Network Abstraction Layer). The main goal of the NAL is the provision of a "network-friendly" video representation, this standard must work on TVs (stream based), the Internet (packet based) among others.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/nal_units.png"&gt;&lt;img src="/i/nal_units.png" alt="NAL units H.264" title="NAL units H.264" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is a &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Frame_synchronization" rel="nofollow"&gt;synchronization marker&lt;/a&gt;&lt;/strong&gt; to define the boundaries of the NAL's units. Each synchronization marker holds a value of &lt;code&gt;0x00 0x00 0x01&lt;/code&gt; except to the very first one which is &lt;code&gt;0x00 0x00 0x00 0x01&lt;/code&gt;. If we run the &lt;strong&gt;hexdump&lt;/strong&gt; on the generated h264 bitstream, we can identify at least three NALs in the beginning of the file.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/minimal_yuv420_hex.png"&gt;&lt;img src="/i/minimal_yuv420_hex.png" alt="synchronization marker on NAL units" title="synchronization marker on NAL units" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As we said before, the decoder needs to know not only the picture data but also the details of the video, frame, colors, used parameters, and others. The &lt;strong&gt;first byte&lt;/strong&gt; of each NAL defines its category and &lt;strong&gt;type&lt;/strong&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAL type id&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Undefined&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Coded slice of a non-IDR picture&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Coded slice data partition A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Coded slice data partition B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Coded slice data partition C&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;IDR&lt;/strong&gt; Coded slice of an IDR picture&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SEI&lt;/strong&gt; Supplemental enhancement information&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SPS&lt;/strong&gt; Sequence parameter set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;PPS&lt;/strong&gt; Picture parameter set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;Access unit delimiter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;End of sequence&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;End of stream&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Usually, the first NAL of a bitstream is a &lt;strong&gt;SPS&lt;/strong&gt;, this type of NAL is responsible for informing the general encoding variables like &lt;strong&gt;profile&lt;/strong&gt;, &lt;strong&gt;level&lt;/strong&gt;, &lt;strong&gt;resolution&lt;/strong&gt; and others.&lt;/p&gt;
&lt;p&gt;If we skip the first synchronization marker we can decode the &lt;strong&gt;first byte&lt;/strong&gt; to know what &lt;strong&gt;type of NAL&lt;/strong&gt; is the first one.&lt;/p&gt;
&lt;p&gt;For instance the first byte after the synchronization marker is &lt;code&gt;01100111&lt;/code&gt;, where the first bit (&lt;code&gt;0&lt;/code&gt;) is to the field &lt;strong&gt;forbidden_zero_bit&lt;/strong&gt;, the next 2 bits (&lt;code&gt;11&lt;/code&gt;) tell us the field &lt;strong&gt;nal_ref_idc&lt;/strong&gt; which indicates whether this NAL is a reference field or not and the rest 5 bits (&lt;code&gt;00111&lt;/code&gt;) inform us the field &lt;strong&gt;nal_unit_type&lt;/strong&gt;, in this case, it's a &lt;strong&gt;SPS&lt;/strong&gt; (7) NAL unit.&lt;/p&gt;
&lt;p&gt;The second byte (&lt;code&gt;binary=01100100, hex=0x64, dec=100&lt;/code&gt;) of an SPS NAL is the field &lt;strong&gt;profile_idc&lt;/strong&gt; which shows the profile that the encoder has used, in this case, we used  the &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles" rel="nofollow"&gt;constrained high-profile&lt;/a&gt;&lt;/strong&gt;, it's a high profile without the support of B (bi-predictive) slices.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/minimal_yuv420_bin.png"&gt;&lt;img src="/i/minimal_yuv420_bin.png" alt="SPS binary view" title="SPS binary view" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When we read the H.264 bitstream spec for an SPS NAL we'll find many values for the &lt;strong&gt;parameter name&lt;/strong&gt;, &lt;strong&gt;category&lt;/strong&gt; and a &lt;strong&gt;description&lt;/strong&gt;, for instance, let's look at &lt;code&gt;pic_width_in_mbs_minus_1&lt;/code&gt; and &lt;code&gt;pic_height_in_map_units_minus_1&lt;/code&gt; fields.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter name&lt;/th&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pic_width_in_mbs_minus_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;ue(v)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pic_height_in_map_units_minus_1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;ue(v)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ue(v)&lt;/strong&gt;: unsigned integer &lt;a href="https://pythonhosted.org/bitstring/exp-golomb.html" rel="nofollow"&gt;Exp-Golomb-coded&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If we do some math with the value of these fields we will end up with the &lt;strong&gt;resolution&lt;/strong&gt;. We can represent a &lt;code&gt;1920 x 1080&lt;/code&gt; using a &lt;code&gt;pic_width_in_mbs_minus_1&lt;/code&gt; with the value of &lt;code&gt;119 ( (119 + 1) * macroblock_size = 120 * 16 = 1920) &lt;/code&gt;, again saving space, instead of encode &lt;code&gt;1920&lt;/code&gt; we did it with &lt;code&gt;119&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we continue to examine our created video with a binary view (ex: &lt;code&gt;xxd -b -c 11 v/minimal_yuv420.h264&lt;/code&gt;), we can skip to the last NAL which is the frame itself.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/slice_nal_idr_bin.png"&gt;&lt;img src="/i/slice_nal_idr_bin.png" alt="h264 idr slice header" title="h264 idr slice header" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see its first 6 bytes values: &lt;code&gt;01100101 10001000 10000100 00000000 00100001 11111111&lt;/code&gt;. As we already know the first byte tell us about what type of NAL it is, in this case, (&lt;code&gt;00101&lt;/code&gt;) it's an &lt;strong&gt;IDR Slice (5)&lt;/strong&gt; and we can further inspect it:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/slice_header.png"&gt;&lt;img src="/i/slice_header.png" alt="h264 slice header spec" title="h264 slice header spec" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Using the spec info we can decode what type of slice (&lt;strong&gt;slice_type&lt;/strong&gt;), the frame number (&lt;strong&gt;frame_num&lt;/strong&gt;) among others important fields.&lt;/p&gt;
&lt;p&gt;In order to get the values of some fields (&lt;code&gt;ue(v), me(v), se(v) or te(v)&lt;/code&gt;) we need to decode it using a special decoder called &lt;a href="https://pythonhosted.org/bitstring/exp-golomb.html" rel="nofollow"&gt;Exponential-Golomb&lt;/a&gt;, this method is &lt;strong&gt;very efficient to encode variable values&lt;/strong&gt;, mostly when there are many default values.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The values of &lt;strong&gt;slice_type&lt;/strong&gt; and &lt;strong&gt;frame_num&lt;/strong&gt; of this video are 7 (I slice) and 0 (the first frame).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can see the &lt;strong&gt;bitstream as a protocol&lt;/strong&gt; and if you want or need to learn more about this bitstream please refer to the &lt;a href="http://www.itu.int/rec/T-REC-H.264-201610-I" rel="nofollow"&gt;ITU H.264 spec.&lt;/a&gt; Here's a macro diagram which shows where the picture data (compressed YUV) resides.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/h264_bitstream_macro_diagram.png"&gt;&lt;img src="/i/h264_bitstream_macro_diagram.png" alt="h264 bitstream macro diagram" title="h264 bitstream macro diagram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can explore others bitstreams like the &lt;a href="https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf" rel="nofollow"&gt;VP9 bitstream&lt;/a&gt;, &lt;a href="http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf" rel="nofollow"&gt;H.265 (HEVC)&lt;/a&gt; or even our &lt;strong&gt;new best friend&lt;/strong&gt; &lt;a href="https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8" rel="nofollow"&gt;&lt;strong&gt;AV1&lt;/strong&gt; bitstream&lt;/a&gt;, &lt;a href="http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/" rel="nofollow"&gt;do they all look similar? No&lt;/a&gt;, but once you learned one you can easily get the others.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-hands-on-inspect-the-h264-bitstream" class="anchor" aria-hidden="true" href="#hands-on-inspect-the-h264-bitstream"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hands-on: Inspect the H.264 bitstream&lt;/h3&gt;
&lt;p&gt;We can &lt;a href="https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video"&gt;generate a single frame video&lt;/a&gt; and use  &lt;a href="https://en.wikipedia.org/wiki/MediaInfo" rel="nofollow"&gt;mediainfo&lt;/a&gt; to inspect its H.264 bitstream. In fact, you can even see the &lt;a href="https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp"&gt;source code that parses h264 (AVC)&lt;/a&gt; bitstream.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/mediainfo_details_1.png"&gt;&lt;img src="/i/mediainfo_details_1.png" alt="mediainfo details h264 bitstream" title="mediainfo details h264 bitstream" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can also use the &lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;Intel Video Pro Analyzer&lt;/a&gt; which is paid but there is a free trial version which limits you to only the first 10 frames but that's okay for learning purposes.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/intel-video-pro-analyzer.png"&gt;&lt;img src="/i/intel-video-pro-analyzer.png" alt="intel video pro analyzer details h264 bitstream" title="intel video pro analyzer details h264 bitstream" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-review" class="anchor" aria-hidden="true" href="#review"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Review&lt;/h2&gt;
&lt;p&gt;We'll notice that many of the &lt;strong&gt;modern codecs uses this same model we learned&lt;/strong&gt;. In fact, let's look at the Thor video codec block diagram, it contains all the steps we studied. The idea is that you now should be able to at least understand better the innovations and papers for the area.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/thor_codec_block_diagram.png"&gt;&lt;img src="/i/thor_codec_block_diagram.png" alt="thor_codec_block_diagram" title="thor_codec_block_diagram" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Previously we had calculated that we needed &lt;a href="#chroma-subsampling"&gt;139GB of storage to keep a video file with one hour at 720p resolution and 30fps&lt;/a&gt; if we use the techniques we learned here, like &lt;strong&gt;inter and intra prediction, transform, quantization, entropy coding and other&lt;/strong&gt; we can achieve, assuming we are spending &lt;strong&gt;0.031 bit per pixel&lt;/strong&gt;, the same perceivable quality video &lt;strong&gt;requiring only 367.82MB vs 139GB&lt;/strong&gt; of store.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We choose to use &lt;strong&gt;0.031 bit per pixel&lt;/strong&gt; based on the example video provided here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-how-does-h265-achieve-a-better-compression-ratio-than-h264" class="anchor" aria-hidden="true" href="#how-does-h265-achieve-a-better-compression-ratio-than-h264"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does H.265 achieve a better compression ratio than H.264?&lt;/h2&gt;
&lt;p&gt;Now that we know more about how codecs work, then it is easy to understand how new codecs are able to deliver higher resolutions with fewer bits.&lt;/p&gt;
&lt;p&gt;We will compare AVC and HEVC, let's keep in mind that it is almost always a trade-off between more CPU cycles (complexity) and compression rate.&lt;/p&gt;
&lt;p&gt;HEVC has bigger and more &lt;strong&gt;partitions&lt;/strong&gt; (and &lt;strong&gt;sub-partitions&lt;/strong&gt;) options than AVC, more &lt;strong&gt;intra predictions directions&lt;/strong&gt;, &lt;strong&gt;improved entropy coding&lt;/strong&gt; and more, all these improvements made H.265 capable to compress 50% more than H.264.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/avc_vs_hevc.png"&gt;&lt;img src="/i/avc_vs_hevc.png" alt="h264 vs h265" title="H.264 vs H.265" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-online-streaming" class="anchor" aria-hidden="true" href="#online-streaming"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online streaming&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-general-architecture" class="anchor" aria-hidden="true" href="#general-architecture"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;General architecture&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/general_architecture.png"&gt;&lt;img src="/i/general_architecture.png" alt="general architecture" title="general architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TODO]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-progressive-download-and-adaptive-streaming" class="anchor" aria-hidden="true" href="#progressive-download-and-adaptive-streaming"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Progressive download and adaptive streaming&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/progressive_download.png"&gt;&lt;img src="/i/progressive_download.png" alt="progressive download" title="progressive download" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/adaptive_streaming.png"&gt;&lt;img src="/i/adaptive_streaming.png" alt="adaptive streaming" title="adaptive streaming" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TODO]&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-content-protection" class="anchor" aria-hidden="true" href="#content-protection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Content protection&lt;/h2&gt;
&lt;p&gt;We can use &lt;strong&gt;a simple token system&lt;/strong&gt; to protect the content. The user without a token tries to request a video and the CDN forbids her or him while a user with a valid token can play the content, it works pretty similarly to most of the web authentication systems.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/token_protection.png"&gt;&lt;img src="/i/token_protection.png" alt="token protection" title="token_protection" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The sole use of this token system still allows a user to download a video and distribute it. Then the &lt;strong&gt;DRM (digital rights management)&lt;/strong&gt; systems can be used to try to avoid this.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/drm.png"&gt;&lt;img src="/i/drm.png" alt="drm" title="drm" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In real life production systems, people often use both techniques to provide authorization and authentication.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-drm" class="anchor" aria-hidden="true" href="#drm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DRM&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-main-systems" class="anchor" aria-hidden="true" href="#main-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main systems&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;FPS - &lt;a href="https://developer.apple.com/streaming/fps/" rel="nofollow"&gt;&lt;strong&gt;FairPlay Streaming&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PR - &lt;a href="https://www.microsoft.com/playready/" rel="nofollow"&gt;&lt;strong&gt;PlayReady&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WV - &lt;a href="http://www.widevine.com/" rel="nofollow"&gt;&lt;strong&gt;Widevine&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-what" class="anchor" aria-hidden="true" href="#what"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What?&lt;/h4&gt;
&lt;p&gt;DRM means Digital rights management, it's a way &lt;strong&gt;to provide copyright protection for digital media&lt;/strong&gt;, for instance, digital video and audio. Although it's used in many places &lt;a href="https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works" rel="nofollow"&gt;it's not universally accepted&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-why" class="anchor" aria-hidden="true" href="#why"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why?&lt;/h4&gt;
&lt;p&gt;Content creator (mostly studios) want to protect its intelectual property against copy to prevent unauthorized redistribution of digital media.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how" class="anchor" aria-hidden="true" href="#how"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How?&lt;/h4&gt;
&lt;p&gt;We're going to describe an abstract and generic form of DRM in a very simplified way.&lt;/p&gt;
&lt;p&gt;Given a &lt;strong&gt;content C1&lt;/strong&gt; (i.e. an hls or dash video streaming), with a &lt;strong&gt;player P1&lt;/strong&gt; (i.e. shaka-clappr, exo-player or ios) in a &lt;strong&gt;device D1&lt;/strong&gt; (i.e. a smartphone, TV, tablet or desktop/notebook) using a &lt;strong&gt;DRM system DRM1&lt;/strong&gt; (widevine, playready or FairPlay).&lt;/p&gt;
&lt;p&gt;The content C1 is encrypted with a &lt;strong&gt;symmetric-key K1&lt;/strong&gt; from the system DRM1, generating the &lt;strong&gt;encrypted content C'1&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/drm_general_flow.jpeg"&gt;&lt;img src="/i/drm_general_flow.jpeg" alt="drm general flow" title="drm general flow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The player P1, of a device D1, has two keys (asymmetric), a &lt;strong&gt;private key PRK1&lt;/strong&gt; (this key is protected&lt;sup&gt;1&lt;/sup&gt; and only known by &lt;strong&gt;D1&lt;/strong&gt;) and a &lt;strong&gt;public key PUK1&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;sup&gt;1&lt;/sup&gt;protected&lt;/strong&gt;: this protection can be &lt;strong&gt;via hardware&lt;/strong&gt;, for instance, this key can be stored inside a special (read-only) chip that works like &lt;a href="https://en.wikipedia.org/wiki/Black_box" rel="nofollow"&gt;a black-box&lt;/a&gt; to provide decryption, or &lt;strong&gt;by software&lt;/strong&gt; (less safe), the DRM system provides means to know which type of protection a given device has.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When the &lt;strong&gt;player P1 wants to play&lt;/strong&gt; the &lt;strong&gt;content C'1&lt;/strong&gt;, it needs to deal with the &lt;strong&gt;DRM system DRM1&lt;/strong&gt;, giving its public key &lt;strong&gt;PUK1&lt;/strong&gt;. The DRM system DRM1 returns the &lt;strong&gt;key K1 encrypted&lt;/strong&gt; with the client''s public key &lt;strong&gt;PUK1&lt;/strong&gt;. In theory, this response is something that &lt;strong&gt;only D1 is capable of decrypting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;K1P1D1 = enc(K1, PUK1)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P1&lt;/strong&gt; uses its DRM local system (it could be a &lt;a href="https://en.wikipedia.org/wiki/System_on_a_chip" rel="nofollow"&gt;SOC&lt;/a&gt;, a specialized hardware or software), this system is &lt;strong&gt;able to decrypt&lt;/strong&gt; the content using its private key PRK1, it can decrypt &lt;strong&gt;the symmetric-key K1 from the K1P1D1&lt;/strong&gt; and &lt;strong&gt;play C'1&lt;/strong&gt;. At best case, the keys are not exposed through RAM.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;K1 = dec(K1P1D1, PRK1)

P1.play(dec(C'1, K1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="/i/drm_decoder_flow.jpeg"&gt;&lt;img src="/i/drm_decoder_flow.jpeg" alt="drm decoder flow" title="drm decoder flow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use-jupyter" class="anchor" aria-hidden="true" href="#how-to-use-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use jupyter&lt;/h1&gt;
&lt;p&gt;Make sure you have &lt;strong&gt;docker installed&lt;/strong&gt; and just run &lt;code&gt;./s/start_jupyter.sh&lt;/code&gt; and follow the instructions on the terminal.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-conferences" class="anchor" aria-hidden="true" href="#conferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conferences&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://demuxed.com/" rel="nofollow"&gt;DEMUXED&lt;/a&gt; - you can &lt;a href="https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA" rel="nofollow"&gt;check the last 2 events presentations.&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h1&gt;
&lt;p&gt;The richest content is here, it's where all the info we saw in this text was extracted, based or inspired by. You can deepen your knowledge with these amazing links, books, videos and etc.&lt;/p&gt;
&lt;p&gt;Online Courses and Tutorials:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/digital/" rel="nofollow"&gt;https://www.coursera.org/learn/digital/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf" rel="nofollow"&gt;https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://xiph.org/video/vid1.shtml" rel="nofollow"&gt;https://xiph.org/video/vid1.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://xiph.org/video/vid2.shtml" rel="nofollow"&gt;https://xiph.org/video/vid2.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/ffmpeg-encoding-course" rel="nofollow"&gt;http://slhck.info/ffmpeg-encoding-course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cambridgeincolour.com/tutorials/camera-sensors.htm" rel="nofollow"&gt;http://www.cambridgeincolour.com/tutorials/camera-sensors.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/vcodex/a-short-history-of-video-coding" rel="nofollow"&gt;http://www.slideshare.net/vcodex/a-short-history-of-video-coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338" rel="nofollow"&gt;http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.android.com/guide/topics/media/media-formats.html" rel="nofollow"&gt;https://developer.android.com/guide/topics/media/media-formats.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/MadhawaKasun/audio-compression-23398426" rel="nofollow"&gt;http://www.slideshare.net/MadhawaKasun/audio-compression-23398426&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf" rel="nofollow"&gt;http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Books:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486395327&amp;amp;sr=1-1" rel="nofollow"&gt;https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486395327&amp;amp;sr=1-1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925" rel="nofollow"&gt;https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396914&amp;amp;sr=1-3&amp;amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO" rel="nofollow"&gt;https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396914&amp;amp;sr=1-3&amp;amp;keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396940&amp;amp;sr=1-1&amp;amp;keywords=jan+ozer" rel="nofollow"&gt;https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1486396940&amp;amp;sr=1-1&amp;amp;keywords=jan+ozer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Onboarding material:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Eyevinn/streaming-onboarding"&gt;https://github.com/Eyevinn/streaming-onboarding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://howvideo.works/" rel="nofollow"&gt;https://howvideo.works/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aws.training/Details/eLearning?id=17775" rel="nofollow"&gt;https://www.aws.training/Details/eLearning?id=17775&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aws.training/Details/eLearning?id=17887" rel="nofollow"&gt;https://www.aws.training/Details/eLearning?id=17887&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aws.training/Details/Video?id=24750" rel="nofollow"&gt;https://www.aws.training/Details/Video?id=24750&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bitstream Specifications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.itu.int/rec/T-REC-H.264-201610-I" rel="nofollow"&gt;http://www.itu.int/rec/T-REC-H.264-201610-I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;amp;lang=en" rel="nofollow"&gt;http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&amp;amp;lang=en&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf" rel="nofollow"&gt;https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf" rel="nofollow"&gt;http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243" rel="nofollow"&gt;http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html" rel="nofollow"&gt;http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.doom9.org/showthread.php?t=167081" rel="nofollow"&gt;https://forum.doom9.org/showthread.php?t=167081&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.doom9.org/showthread.php?t=168947" rel="nofollow"&gt;https://forum.doom9.org/showthread.php?t=168947&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Software:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/" rel="nofollow"&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/ffmpeg-all.html" rel="nofollow"&gt;https://ffmpeg.org/ffmpeg-all.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/ffprobe.html" rel="nofollow"&gt;https://ffmpeg.org/ffprobe.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://trac.ffmpeg.org/wiki/" rel="nofollow"&gt;https://trac.ffmpeg.org/wiki/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://software.intel.com/en-us/intel-video-pro-analyzer" rel="nofollow"&gt;https://software.intel.com/en-us/intel-video-pro-analyzer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8" rel="nofollow"&gt;https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Non-ITU Codecs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://aomedia.googlesource.com/" rel="nofollow"&gt;https://aomedia.googlesource.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/webmproject/libvpx/tree/master/vp9"&gt;https://github.com/webmproject/libvpx/tree/master/vp9&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml" rel="nofollow"&gt;https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.xiph.org/~jm/daala/revisiting/" rel="nofollow"&gt;https://people.xiph.org/~jm/daala/revisiting/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=lzPaldsmJbk" rel="nofollow"&gt;https://www.youtube.com/watch?v=lzPaldsmJbk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fosdem.org/2017/schedule/event/om_av1/" rel="nofollow"&gt;https://fosdem.org/2017/schedule/event/om_av1/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jmvalin.ca/papers/AV1_tools.pdf" rel="nofollow"&gt;https://jmvalin.ca/papers/AV1_tools.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Encoding Concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://x265.org/hevc-h265/" rel="nofollow"&gt;http://x265.org/hevc-h265/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/video/2017/03/01/rate-control.html" rel="nofollow"&gt;http://slhck.info/video/2017/03/01/rate-control.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/video/2017/02/24/vbr-settings.html" rel="nofollow"&gt;http://slhck.info/video/2017/02/24/vbr-settings.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://slhck.info/video/2017/02/24/crf-guide.html" rel="nofollow"&gt;http://slhck.info/video/2017/02/24/crf-guide.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1702.00817v1.pdf" rel="nofollow"&gt;https://arxiv.org/pdf/1702.00817v1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors" rel="nofollow"&gt;https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html" rel="nofollow"&gt;http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html" rel="nofollow"&gt;http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/" rel="nofollow"&gt;https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/" rel="nofollow"&gt;https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/" rel="nofollow"&gt;https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Video Sequences for Testing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://bbb3d.renderfarming.net/download.html" rel="nofollow"&gt;http://bbb3d.renderfarming.net/download.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx" rel="nofollow"&gt;https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Miscellaneous:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Eyevinn/streaming-onboarding"&gt;https://github.com/Eyevinn/streaming-onboarding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/a/24890903" rel="nofollow"&gt;http://stackoverflow.com/a/24890903&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264" rel="nofollow"&gt;http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html" rel="nofollow"&gt;http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vanseodesign.com/web-design/color-luminance/" rel="nofollow"&gt;http://vanseodesign.com/web-design/color-luminance/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.biologymad.com/nervoussystem/eyenotes.htm" rel="nofollow"&gt;http://www.biologymad.com/nervoussystem/eyenotes.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf" rel="nofollow"&gt;http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.csc.villanova.edu/~rschumey/csc4800/dct.html" rel="nofollow"&gt;http://www.csc.villanova.edu/~rschumey/csc4800/dct.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.explainthatstuff.com/digitalcameras.html" rel="nofollow"&gt;http://www.explainthatstuff.com/digitalcameras.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.hkvstar.com" rel="nofollow"&gt;http://www.hkvstar.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.hometheatersound.com/" rel="nofollow"&gt;http://www.hometheatersound.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.lighterra.com/papers/videoencodingh264/" rel="nofollow"&gt;http://www.lighterra.com/papers/videoencodingh264/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.red.com/learn/red-101/video-chroma-subsampling" rel="nofollow"&gt;http://www.red.com/learn/red-101/video-chroma-subsampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/ManoharKuse/hevc-intra-coding" rel="nofollow"&gt;http://www.slideshare.net/ManoharKuse/hevc-intra-coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/mwalendo/h264vs-hevc" rel="nofollow"&gt;http://www.slideshare.net/mwalendo/h264vs-hevc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/rvarun7777/final-seminar-46117193" rel="nofollow"&gt;http://www.slideshare.net/rvarun7777/final-seminar-46117193&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf" rel="nofollow"&gt;http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx" rel="nofollow"&gt;http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;amp;PageNum=1" rel="nofollow"&gt;http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&amp;amp;PageNum=1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/" rel="nofollow"&gt;http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/" rel="nofollow"&gt;https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/" rel="nofollow"&gt;https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://codesequoia.wordpress.com/category/video/" rel="nofollow"&gt;https://codesequoia.wordpress.com/category/video/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.apple.com/library/content/technotes/tn2224/_index.html" rel="nofollow"&gt;https://developer.apple.com/library/content/technotes/tn2224/_index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikibooks.org/wiki/MeGUI/x264_Settings" rel="nofollow"&gt;https://en.wikibooks.org/wiki/MeGUI/x264_Settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/AOMedia_Video_1" rel="nofollow"&gt;https://en.wikipedia.org/wiki/AOMedia_Video_1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Cone_cell" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Cone_cell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg" rel="nofollow"&gt;https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Inter_frame" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Inter_frame&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Intra-frame_coding" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Intra-frame_coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Photoreceptor_cell" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Photoreceptor_cell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Pixel_aspect_ratio" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Pixel_aspect_ratio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Presentation_timestamp" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Presentation_timestamp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Rod_cell" rel="nofollow"&gt;https://en.wikipedia.org/wiki/Rod_cell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg" rel="nofollow"&gt;https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/" rel="nofollow"&gt;https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping" rel="nofollow"&gt;https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/" rel="nofollow"&gt;https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03" rel="nofollow"&gt;https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.encoding.com/android/" rel="nofollow"&gt;https://www.encoding.com/android/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.encoding.com/http-live-streaming-hls/" rel="nofollow"&gt;https://www.encoding.com/http-live-streaming-hls/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm" rel="nofollow"&gt;https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lifewire.com/cmos-image-sensor-493271" rel="nofollow"&gt;https://www.lifewire.com/cmos-image-sensor-493271&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ" rel="nofollow"&gt;https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar" rel="nofollow"&gt;https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.vcodex.com/h264avc-intra-precition/" rel="nofollow"&gt;https://www.vcodex.com/h264avc-intra-precition/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=9vgtJJ2wwMA" rel="nofollow"&gt;https://www.youtube.com/watch?v=9vgtJJ2wwMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=LFXN9PiOGtY" rel="nofollow"&gt;https://www.youtube.com/watch?v=LFXN9PiOGtY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6" rel="nofollow"&gt;https://www.youtube.com/watch?v=Lto-ajuqW3w&amp;amp;list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=LWxu4rkZBLw" rel="nofollow"&gt;https://www.youtube.com/watch?v=LWxu4rkZBLw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf" rel="nofollow"&gt;https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>leandromoreira</author><guid isPermaLink="false">https://github.com/leandromoreira/digital_video_introduction</guid><pubDate>Mon, 06 Jan 2020 00:14:00 GMT</pubDate></item><item><title>abhinavsagar/Cryptocurrency-Price-Prediction #15 in Jupyter Notebook, Today</title><link>https://github.com/abhinavsagar/Cryptocurrency-Price-Prediction</link><description>&lt;p&gt;&lt;i&gt;Predicting cryptocurrency prices using LSTM neural network&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cryptocurrency-price-prediction" class="anchor" aria-hidden="true" href="#cryptocurrency-price-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cryptocurrency-Price-Prediction&lt;/h1&gt;
&lt;p&gt;Predicting Cryptocurrency Price using LSTM neural network&lt;/p&gt;
&lt;p&gt;Check out the corresponding medium blog post &lt;a href="https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a?source=friends_link&amp;amp;sk=331d27e1be556a0803f34b746f505467" rel="nofollow"&gt;https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cryptocurrency-line-plot" class="anchor" aria-hidden="true" href="#cryptocurrency-line-plot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cryptocurrency line plot&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="i13.png"&gt;&lt;img src="i13.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-actual-vs-predicted-prices" class="anchor" aria-hidden="true" href="#actual-vs-predicted-prices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Actual vs predicted prices&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="i14.png"&gt;&lt;img src="i14.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@misc{Abhinav:2019,
  Author = {Abhinav Sagar},
  Title = {Cryptocurrency-Price-Prediction},
  Year = {2019},
  Publisher = {GitHub},
  Journal = {GitHub repository},
  Howpublished = {\url{https://github.com/abhinavsagar/Cryptocurrency-Price-Prediction}}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>abhinavsagar</author><guid isPermaLink="false">https://github.com/abhinavsagar/Cryptocurrency-Price-Prediction</guid><pubDate>Mon, 06 Jan 2020 00:15:00 GMT</pubDate></item><item><title>udacity/deep-learning-v2-pytorch #16 in Jupyter Notebook, Today</title><link>https://github.com/udacity/deep-learning-v2-pytorch</link><description>&lt;p&gt;&lt;i&gt;Projects and exercises for the latest Deep Learning ND program https://www.udacity.com/course/deep-learning-nanodegree--nd101&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-pytorch" class="anchor" aria-hidden="true" href="#deep-learning-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning (PyTorch)&lt;/h1&gt;
&lt;p&gt;This repository contains material related to Udacity's &lt;a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="nofollow"&gt;Deep Learning Nanodegree program&lt;/a&gt;. It consists of a bunch of tutorial notebooks for various deep learning topics. In most cases, the notebooks lead you through implementing models such as convolutional networks, recurrent networks, and GANs. There are other topics covered such as weight initialization and batch normalization.&lt;/p&gt;
&lt;p&gt;There are also notebooks used as projects for the Nanodegree program. In the program itself, the projects are reviewed by real people (Udacity reviewers), but the starting code is available here, as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table Of Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-neural-networks" class="anchor" aria-hidden="true" href="#introduction-to-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks"&gt;Introduction to Neural Networks&lt;/a&gt;: Learn how to implement gradient descent and apply it to predicting patterns in student admissions data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-analysis-network"&gt;Sentiment Analysis with NumPy&lt;/a&gt;: &lt;a href="http://iamtrask.github.io/" rel="nofollow"&gt;Andrew Trask&lt;/a&gt; leads you through building a sentiment analysis model, predicting if some text is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch"&gt;Introduction to PyTorch&lt;/a&gt;: Learn how to build neural networks in PyTorch and use pre-trained networks for state-of-the-art image classifiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks"&gt;Convolutional Neural Networks&lt;/a&gt;: Visualize the output of layers that make up a CNN. Learn how to define and train a CNN for classifying &lt;a href="https://en.wikipedia.org/wiki/MNIST_database" rel="nofollow"&gt;MNIST data&lt;/a&gt;, a handwritten digit database that is notorious in the fields of machine and deep learning. Also, define and train a CNN for classifying images in the &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"&gt;CIFAR10 dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/transfer-learning"&gt;Transfer Learning&lt;/a&gt;. In practice, most people don't train their own networks on huge datasets; they use &lt;strong&gt;pre-trained&lt;/strong&gt; networks such as VGGnet. Here you'll use VGGnet to help classify images of flowers without training an end-to-end network from scratch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/weight-initialization"&gt;Weight Initialization&lt;/a&gt;: Explore how initializing network weights affects performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder"&gt;Autoencoders&lt;/a&gt;: Build models for image compression and de-noising, using feedforward and convolutional networks in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/style-transfer"&gt;Style Transfer&lt;/a&gt;: Extract style and content features from images, using a pre-trained network. Implement style transfer according to the paper, &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="nofollow"&gt;Image Style Transfer Using Convolutional Neural Networks&lt;/a&gt; by Gatys et. al. Define appropriate losses for iteratively creating a target, style-transferred image of your own design!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/recurrent-neural-networks"&gt;Intro to Recurrent Networks (Time series &amp;amp; Character-level RNN)&lt;/a&gt;: Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text; learn how to implement these in PyTorch for a variety of tasks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"&gt;Embeddings (Word2Vec)&lt;/a&gt;: Implement the Word2Vec model to find semantic representations of words for use in natural language processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn"&gt;Sentiment Analysis RNN&lt;/a&gt;: Implement a recurrent neural network that can predict if the text of a moview review is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/attention"&gt;Attention&lt;/a&gt;: Implement attention and apply it to annotation vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/gan-mnist"&gt;Generative Adversarial Network on MNIST&lt;/a&gt;: Train a simple generative adversarial network on the MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/batch-norm"&gt;Batch Normalization&lt;/a&gt;: Learn how to improve training rates and network stability with batch normalizations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/dcgan-svhn"&gt;Deep Convolutional GAN (DCGAN)&lt;/a&gt;: Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/cycle-gan"&gt;CycleGAN&lt;/a&gt;: Implement a CycleGAN that is designed to learn from unpaired and unlabeled data; use trained generators to transform images from summer to winter and vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deploying-a-model-with-aws-sagemaker" class="anchor" aria-hidden="true" href="#deploying-a-model-with-aws-sagemaker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploying a Model (with AWS SageMaker)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/sagemaker-deployment"&gt;All exercise and project notebooks&lt;/a&gt; for the lessons on model deployment can be found in the linked, Github repo. Learn to deploy pre-trained models using AWS SageMaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-bikesharing"&gt;Predicting Bike-Sharing Patterns&lt;/a&gt;: Implement a neural network in NumPy to predict bike rentals.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification"&gt;Dog Breed Classifier&lt;/a&gt;: Build a convolutional neural network with PyTorch to classify any image (even an image of a face) as a specific dog breed.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-tv-script-generation"&gt;TV Script Generation&lt;/a&gt;: Train a recurrent neural network to generate scripts in the style of dialogue from Seinfeld.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-face-generation"&gt;Face Generation&lt;/a&gt;: Use a DCGAN on the CelebA dataset to generate images of new and realistic human faces.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-elective-material" class="anchor" aria-hidden="true" href="#elective-material"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Elective Material&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/tensorflow/intro-to-tensorflow"&gt;Intro to TensorFlow&lt;/a&gt;: Starting building neural networks with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/keras"&gt;Keras&lt;/a&gt;: Learn to build neural networks and convolutional neural networks with Keras.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-configure-and-manage-your-environment-with-anaconda" class="anchor" aria-hidden="true" href="#configure-and-manage-your-environment-with-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure and Manage Your Environment with Anaconda&lt;/h2&gt;
&lt;p&gt;Per the Anaconda &lt;a href="http://conda.pydata.org/docs" rel="nofollow"&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conda is an open source package management system and environment management system
for installing multiple versions of software packages and their dependencies and
switching easily between them. It works on Linux, OS X and Windows, and was created
for Python programs but can package and distribute any software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Using Anaconda consists of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt; on your computer, by selecting the latest Python version for your operating system. If you already have &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;miniconda&lt;/code&gt; installed, you should be able to skip this step and move on to step 2.&lt;/li&gt;
&lt;li&gt;Create and activate * a new &lt;code&gt;conda&lt;/code&gt; &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;environment&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;* Each time you wish to work on any exercises, activate your &lt;code&gt;conda&lt;/code&gt; environment!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-1-installation" class="anchor" aria-hidden="true" href="#1-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the latest version of &lt;code&gt;miniconda&lt;/code&gt; that matches your system.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Linux&lt;/th&gt;
&lt;th&gt;Mac&lt;/th&gt;
&lt;th&gt;Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe" rel="nofollow"&gt;64-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh" rel="nofollow"&gt;32-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe" rel="nofollow"&gt;32-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt; on your machine. Detailed instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-create-and-activate-the-environment" class="anchor" aria-hidden="true" href="#2-create-and-activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Create and Activate the Environment&lt;/h2&gt;
&lt;p&gt;For Windows users, these following commands need to be executed from the &lt;strong&gt;Anaconda prompt&lt;/strong&gt; as opposed to a Windows terminal window. For Mac, a normal terminal window will work.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-git-and-version-control" class="anchor" aria-hidden="true" href="#git-and-version-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git and version control&lt;/h4&gt;
&lt;p&gt;These instructions also assume you have &lt;code&gt;git&lt;/code&gt; installed for working with Github from a terminal window, but if you do not, you can download that first with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you'd like to learn more about version control and using &lt;code&gt;git&lt;/code&gt; from the command line, take a look at our &lt;a href="https://www.udacity.com/course/version-control-with-git--ud123" rel="nofollow"&gt;free course: Version Control with Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, we're ready to create our local environment!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository, and navigate to the downloaded folder. This may take a minute or two to clone due to the included image data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/udacity/deep-learning-v2-pytorch.git
cd deep-learning-v2-pytorch
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Create (and activate) a new environment, named &lt;code&gt;deep-learning&lt;/code&gt; with Python 3.6. If prompted to proceed with the install &lt;code&gt;(Proceed [y]/n)&lt;/code&gt; type y.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create -n deep-learning python=3.6
source activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create --name deep-learning python=3.6
activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point your command line should look something like: &lt;code&gt;(deep-learning) &amp;lt;User&amp;gt;:deep-learning-v2-pytorch &amp;lt;user&amp;gt;$&lt;/code&gt;. The &lt;code&gt;(deep-learning)&lt;/code&gt; indicates that your environment has been activated, and you can proceed with further package installations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install PyTorch and torchvision; this should install the latest version of PyTorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch -c pytorch
pip install torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install a few required pip packages, which are specified in the requirements text file (including OpenCV).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="7"&gt;
&lt;li&gt;That's it!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now most of the &lt;code&gt;deep-learning&lt;/code&gt; libraries are available to you. Very occasionally, you will see a repository with an addition requirements file, which exists should you want to use TensorFlow and Keras, for example. In this case, you're encouraged to install another library to your existing environment, or create a new environment for a specific project.&lt;/p&gt;
&lt;p&gt;Now, assuming your &lt;code&gt;deep-learning&lt;/code&gt; environment is still activated, you can navigate to the main repo and start looking at the notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd
cd deep-learning-v2-pytorch
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit the environment when you have completed your work session, simply close the terminal window.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>udacity</author><guid isPermaLink="false">https://github.com/udacity/deep-learning-v2-pytorch</guid><pubDate>Mon, 06 Jan 2020 00:16:00 GMT</pubDate></item><item><title>zergtant/pytorch-handbook #17 in Jupyter Notebook, Today</title><link>https://github.com/zergtant/pytorch-handbook</link><description>&lt;p&gt;&lt;i&gt;pytorch handbook是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门，其中包含的Pytorch教程全部通过测试保证可以成功运行&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-中文手册pytorch-handbook" class="anchor" aria-hidden="true" href="#pytorch-中文手册pytorch-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch 中文手册（pytorch handbook）&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png"&gt;&lt;img src="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png" alt="pytorch" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-书籍介绍" class="anchor" aria-hidden="true" href="#书籍介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;书籍介绍&lt;/h2&gt;
&lt;p&gt;这是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门。&lt;/p&gt;
&lt;p&gt;由于本人水平有限，在写此教程的时候参考了一些网上的资料，在这里对他们表示敬意，我会在每个引用中附上原文地址，方便大家参考。&lt;/p&gt;
&lt;p&gt;深度学习的技术在飞速的发展，同时PyTorch也在不断更新，且本人会逐步完善相关内容。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本说明" class="anchor" aria-hidden="true" href="#版本说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本说明&lt;/h2&gt;
&lt;p&gt;由于PyTorch版本更迭，教程的版本会与PyTorch版本，保持一致。&lt;/p&gt;
&lt;p&gt;2019.10.10 PyTorch已经发布1.3的稳定版。&lt;/p&gt;
&lt;p&gt;已经全部测试完毕 代码可完全兼容1.3&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-qq-3群" class="anchor" aria-hidden="true" href="#qq-3群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QQ 3群&lt;/h2&gt;
&lt;p&gt;群号：773681699&lt;/p&gt;
&lt;p&gt;扫描二维码&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Pytorch-Handbook-3.png"&gt;&lt;img src="Pytorch-Handbook-3.png" alt="QR" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="//shang.qq.com/wpa/qunwpa?idkey=ee402d5f0e7732b2171e643d729177ce55ac404eafda5edd9b740d73fabe6a96" rel="nofollow"&gt;点击链接加入群聊 『PyTorch Handbook 交流3群』&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1群(985896536)已满，2群(681980831)已满&lt;/p&gt;
&lt;p&gt;不要再加了&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-说明" class="anchor" aria-hidden="true" href="#说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;说明&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;修改错别字请直接提issue或PR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PR时请注意版本&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;有问题也请直接提issue&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感谢&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-第一章pytorch-入门" class="anchor" aria-hidden="true" href="#第一章pytorch-入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一章：PyTorch 入门&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter1/1.1-pytorch-introduction.md"&gt;PyTorch 简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.2-pytorch-installation.md"&gt;PyTorch 环境搭建&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md"&gt;PyTorch 深度学习：60分钟快速入门（官方）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter1/1_tensor_tutorial.ipynb"&gt;张量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/2_autograd_tutorial.ipynb"&gt;Autograd：自动求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/3_neural_networks_tutorial.ipynb"&gt;神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/4_cifar10_tutorial.ipynb"&gt;训练一个分类器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/5_data_parallel_tutorial.ipynb"&gt;选读：数据并行处理（多GPU）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.4-pytorch-resource.md"&gt;相关资源介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-第二章-基础" class="anchor" aria-hidden="true" href="#第二章-基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二章 基础&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-pytorch-基础" class="anchor" aria-hidden="true" href="#第一节-pytorch-基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 PyTorch 基础&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.1.pytorch-basics-tensor.ipynb"&gt;张量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.2-pytorch-basics-autograd.ipynb"&gt;自动求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.3-pytorch-basics-nerual-network.ipynb"&gt;神经网络包nn和优化器optm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.4-pytorch-basics-data-loader.ipynb"&gt;数据的加载和预处理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-深度学习基础及数学原理" class="anchor" aria-hidden="true" href="#第二节-深度学习基础及数学原理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 深度学习基础及数学原理&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.2-deep-learning-basic-mathematics.ipynb"&gt;深度学习基础及数学原理&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-神经网络简介" class="anchor" aria-hidden="true" href="#第三节-神经网络简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 神经网络简介&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.3-deep-learning-neural-network-introduction.ipynb"&gt;神经网络简介&lt;/a&gt;  注：本章在本地使用微软的Edge打开会崩溃，请使Chrome Firefox打开查看&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-卷积神经网络" class="anchor" aria-hidden="true" href="#第四节-卷积神经网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 卷积神经网络&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.4-cnn.ipynb"&gt;卷积神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-循环神经网络" class="anchor" aria-hidden="true" href="#第五节-循环神经网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 循环神经网络&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.5-rnn.ipynb"&gt;循环神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第三章-实践" class="anchor" aria-hidden="true" href="#第三章-实践"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三章 实践&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-logistic回归二元分类" class="anchor" aria-hidden="true" href="#第一节-logistic回归二元分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 logistic回归二元分类&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.1-logistic-regression.ipynb"&gt;logistic回归二元分类&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-cnnmnist数据集手写数字识别" class="anchor" aria-hidden="true" href="#第二节-cnnmnist数据集手写数字识别"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 CNN:MNIST数据集手写数字识别&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.2-mnist.ipynb"&gt;CNN:MNIST数据集手写数字识别&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-rnn实例通过sin预测cos" class="anchor" aria-hidden="true" href="#第三节-rnn实例通过sin预测cos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 RNN实例：通过Sin预测Cos&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.3-rnn.ipynb"&gt;RNN实例：通过Sin预测Cos&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第四章-提高" class="anchor" aria-hidden="true" href="#第四章-提高"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四章 提高&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-fine-tuning" class="anchor" aria-hidden="true" href="#第一节-fine-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 Fine-tuning&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.1-fine-tuning.ipynb"&gt;Fine-tuning&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-可视化" class="anchor" aria-hidden="true" href="#第二节-可视化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 可视化&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.1-visdom.ipynb"&gt;visdom&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.2-tensorboardx.ipynb"&gt;tensorboardx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.3-cnn-visualizing.ipynb"&gt;可视化理解卷积神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-fastai" class="anchor" aria-hidden="true" href="#第三节-fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 Fast.ai&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.3-fastai.ipynb"&gt;Fast.ai&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-训练的一些技巧" class="anchor" aria-hidden="true" href="#第四节-训练的一些技巧"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 训练的一些技巧&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-多gpu并行训练" class="anchor" aria-hidden="true" href="#第五节-多gpu并行训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 多GPU并行训练&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.5-multiply-gpu-parallel-training.ipynb"&gt;多GPU并行计算&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第五章-应用" class="anchor" aria-hidden="true" href="#第五章-应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五章 应用&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-kaggle介绍" class="anchor" aria-hidden="true" href="#第一节-kaggle介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 Kaggle介绍&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.1-kaggle.md"&gt;Kaggle介绍&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-结构化数据" class="anchor" aria-hidden="true" href="#第二节-结构化数据"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 结构化数据&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-计算机视觉" class="anchor" aria-hidden="true" href="#第三节-计算机视觉"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 计算机视觉&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.3-Fashion-MNIST.ipynb"&gt;Fashion MNIST 图像分类&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-自然语言处理" class="anchor" aria-hidden="true" href="#第四节-自然语言处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 自然语言处理&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-协同过滤" class="anchor" aria-hidden="true" href="#第五节-协同过滤"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 协同过滤&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-第六章-资源" class="anchor" aria-hidden="true" href="#第六章-资源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第六章 资源&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-第七章-附录" class="anchor" aria-hidden="true" href="#第七章-附录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第七章 附录&lt;/h3&gt;
&lt;p&gt;transforms的常用操作总结&lt;/p&gt;
&lt;p&gt;pytorch的损失函数总结&lt;/p&gt;
&lt;p&gt;pytorch的优化器总结&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67" alt="" data-canonical-src="https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn" rel="nofollow"&gt;本作品采用知识共享署名-非商业性使用-相同方式共享 3.0  中国大陆许可协议进行许可&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zergtant</author><guid isPermaLink="false">https://github.com/zergtant/pytorch-handbook</guid><pubDate>Mon, 06 Jan 2020 00:17:00 GMT</pubDate></item><item><title>huseinzol05/Stock-Prediction-Models #18 in Jupyter Notebook, Today</title><link>https://github.com/huseinzol05/Stock-Prediction-Models</link><description>&lt;p&gt;&lt;i&gt;Gathers machine learning and deep learning models for Stock forecasting including trading bots and simulations&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a href="#readme"&gt;
        &lt;img alt="logo" width="50%" src="output/evolution-strategy.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models/blob/master/LICENSE"&gt;&lt;img alt="MIT License" src="https://camo.githubusercontent.com/65c6912b68aeea16375c94041820c27b3d8d9c8b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368652d2d4c6963656e73652d2d322e302d79656c6c6f772e737667" data-canonical-src="https://img.shields.io/badge/License-Apache--License--2.0-yellow.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;&lt;img src="https://camo.githubusercontent.com/a109e6ff3099b0fe22cf4964a86453b3992990d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646565706c6561726e696e672d33302d2d6d6f64656c732d737563636573732e737667" data-canonical-src="https://img.shields.io/badge/deeplearning-30--models-success.svg" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="#"&gt;&lt;img src="https://camo.githubusercontent.com/49a8b81e45fcd97bb447147c89e1dc8d7ba5db83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6167656e742d32332d2d6d6f64656c732d737563636573732e737667" data-canonical-src="https://img.shields.io/badge/agent-23--models-success.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Stock-Prediction-Models&lt;/strong&gt;, Gathers machine learning and deep learning models for Stock forecasting, included trading bots and simulations.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#models"&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#agents"&gt;Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="realtime-agent"&gt;Realtime Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#data-explorations"&gt;Data Explorations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#simulations"&gt;Simulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#tensorflow-js"&gt;Tensorflow-js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#misc"&gt;Misc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results"&gt;Results&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-agent"&gt;Results Agent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-signal-prediction"&gt;Results signal prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-analysis"&gt;Results analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huseinzol05/Stock-Prediction-Models#results-simulation"&gt;Results simulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-models" class="anchor" aria-hidden="true" href="#models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-deep-learning-models" class="anchor" aria-hidden="true" href="#deep-learning-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="deep-learning"&gt;Deep-learning models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;LSTM Bidirectional&lt;/li&gt;
&lt;li&gt;LSTM 2-Path&lt;/li&gt;
&lt;li&gt;GRU&lt;/li&gt;
&lt;li&gt;GRU Bidirectional&lt;/li&gt;
&lt;li&gt;GRU 2-Path&lt;/li&gt;
&lt;li&gt;Vanilla&lt;/li&gt;
&lt;li&gt;Vanilla Bidirectional&lt;/li&gt;
&lt;li&gt;Vanilla 2-Path&lt;/li&gt;
&lt;li&gt;LSTM Seq2seq&lt;/li&gt;
&lt;li&gt;LSTM Bidirectional Seq2seq&lt;/li&gt;
&lt;li&gt;LSTM Seq2seq VAE&lt;/li&gt;
&lt;li&gt;GRU Seq2seq&lt;/li&gt;
&lt;li&gt;GRU Bidirectional Seq2seq&lt;/li&gt;
&lt;li&gt;GRU Seq2seq VAE&lt;/li&gt;
&lt;li&gt;Attention-is-all-you-Need&lt;/li&gt;
&lt;li&gt;CNN-Seq2seq&lt;/li&gt;
&lt;li&gt;Dilated-CNN-Seq2seq&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to use one of the model to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href="deep-learning/how-to-forecast.ipynb"&gt;how-to-forecast.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consensus, how to use sentiment data to forecast &lt;code&gt;t + N&lt;/code&gt;, &lt;a href="deep-learning/sentiment-consensus.ipynb"&gt;sentiment-consensus.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-stacking-models" class="anchor" aria-hidden="true" href="#stacking-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="stacking"&gt;Stacking models&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Deep Feed-forward Auto-Encoder Neural Network to reduce dimension + Deep Recurrent Neural Network + ARIMA + Extreme Boosting Gradient Regressor&lt;/li&gt;
&lt;li&gt;Adaboost + Bagging + Extra Trees + Gradient Boosting + Random Forest + XGB&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-agents" class="anchor" aria-hidden="true" href="#agents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="agent"&gt;Agents&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Turtle-trading agent&lt;/li&gt;
&lt;li&gt;Moving-average agent&lt;/li&gt;
&lt;li&gt;Signal rolling agent&lt;/li&gt;
&lt;li&gt;Policy-gradient agent&lt;/li&gt;
&lt;li&gt;Q-learning agent&lt;/li&gt;
&lt;li&gt;Evolution-strategy agent&lt;/li&gt;
&lt;li&gt;Double Q-learning agent&lt;/li&gt;
&lt;li&gt;Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Double Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Duel Q-learning agent&lt;/li&gt;
&lt;li&gt;Double Duel Q-learning agent&lt;/li&gt;
&lt;li&gt;Duel Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Double Duel Recurrent Q-learning agent&lt;/li&gt;
&lt;li&gt;Actor-critic agent&lt;/li&gt;
&lt;li&gt;Actor-critic Duel agent&lt;/li&gt;
&lt;li&gt;Actor-critic Recurrent agent&lt;/li&gt;
&lt;li&gt;Actor-critic Duel Recurrent agent&lt;/li&gt;
&lt;li&gt;Curiosity Q-learning agent&lt;/li&gt;
&lt;li&gt;Recurrent Curiosity Q-learning agent&lt;/li&gt;
&lt;li&gt;Duel Curiosity Q-learning agent&lt;/li&gt;
&lt;li&gt;Neuro-evolution agent&lt;/li&gt;
&lt;li&gt;Neuro-evolution with Novelty search agent&lt;/li&gt;
&lt;li&gt;ABCD strategy agent&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-data-explorations" class="anchor" aria-hidden="true" href="#data-explorations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="misc"&gt;Data Explorations&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;stock market study on TESLA stock, &lt;a href="misc/tesla-study.ipynb"&gt;tesla-study.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock, &lt;a href="misc/outliers.ipynb"&gt;outliers.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Overbought-Oversold study on TESLA stock, &lt;a href="misc/overbought-oversold.ipynb"&gt;overbought-oversold.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Which stock you need to buy? &lt;a href="misc/which-stock.ipynb"&gt;which-stock.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-simulations" class="anchor" aria-hidden="true" href="#simulations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="simulation"&gt;Simulations&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Simple Monte Carlo, &lt;a href="simulation/monte-carlo-drift.ipynb"&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dynamic volatility Monte Carlo, &lt;a href="simulation/monte-carlo-dynamic-volatility.ipynb"&gt;monte-carlo-dynamic-volatility.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Drift Monte Carlo, &lt;a href="simulation/monte-carlo-drift.ipynb"&gt;monte-carlo-drift.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment, &lt;a href="simulation/multivariate-drift-monte-carlo.ipynb"&gt;multivariate-drift-monte-carlo.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Portfolio optimization, &lt;a href="simulation/portfolio-optimization.ipynb"&gt;portfolio-optimization.ipynb&lt;/a&gt;, inspired from &lt;a href="https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/" rel="nofollow"&gt;https://pythonforfinance.net/2017/01/21/investment-portfolio-optimisation-with-python/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-js" class="anchor" aria-hidden="true" href="#tensorflow-js"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="stock-forecasting-js"&gt;Tensorflow-js&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I code &lt;a href="deep-learning/1.lstm.ipynb"&gt;LSTM Recurrent Neural Network&lt;/a&gt; and &lt;a href="agent/simple-agent.ipynb"&gt;Simple signal rolling agent&lt;/a&gt; inside Tensorflow JS, you can try it here, &lt;a href="https://huseinhouse.com/stock-forecasting-js/" rel="nofollow"&gt;huseinhouse.com/stock-forecasting-js&lt;/a&gt;, you can download any historical CSV and upload dynamically.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-misc" class="anchor" aria-hidden="true" href="#misc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="misc"&gt;Misc&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;fashion trending prediction with cross-validation, &lt;a href="misc/fashion-forecasting.ipynb"&gt;fashion-forecasting.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bitcoin analysis with LSTM prediction, &lt;a href="misc/bitcoin-analysis-lstm.ipynb"&gt;bitcoin-analysis-lstm.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kijang Emas Bank Negara, &lt;a href="misc/kijang-emas-bank-negara.ipynb"&gt;kijang-emas-bank-negara.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-results-agent" class="anchor" aria-hidden="true" href="#results-agent"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results Agent&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;This agent only able to buy or sell 1 unit per transaction.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Turtle-trading agent, &lt;a href="agent/1.turtle-agent.ipynb"&gt;turtle-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/turtle-agent.png"&gt;&lt;img src="output-agent/turtle-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Moving-average agent, &lt;a href="agent/2.moving-average-agent.ipynb"&gt;moving-average-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/moving-average-agent.png"&gt;&lt;img src="output-agent/moving-average-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Signal rolling agent, &lt;a href="agent/3.signal-rolling-agent.ipynb"&gt;signal-rolling-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/signal-rolling-agent.png"&gt;&lt;img src="output-agent/signal-rolling-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Policy-gradient agent, &lt;a href="agent/4.policy-gradient-agent.ipynb"&gt;policy-gradient-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/policy-gradient-agent.png"&gt;&lt;img src="output-agent/policy-gradient-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Q-learning agent, &lt;a href="agent/5.q-learning-agent.ipynb"&gt;q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/q-learning-agent.png"&gt;&lt;img src="output-agent/q-learning-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="6"&gt;
&lt;li&gt;Evolution-strategy agent, &lt;a href="agent/6.evolution-strategy-agent.ipynb"&gt;evolution-strategy-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/evolution-strategy-agent.png"&gt;&lt;img src="output-agent/evolution-strategy-agent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="7"&gt;
&lt;li&gt;Double Q-learning agent, &lt;a href="agent/7.double-q-learning-agent.ipynb"&gt;double-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-q-learning.png"&gt;&lt;img src="output-agent/double-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="8"&gt;
&lt;li&gt;Recurrent Q-learning agent, &lt;a href="agent/8.recurrent-q-learning-agent.ipynb"&gt;recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/recurrent-q-learning.png"&gt;&lt;img src="output-agent/recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="9"&gt;
&lt;li&gt;Double Recurrent Q-learning agent, &lt;a href="agent/9.double-recurrent-q-learning-agent.ipynb"&gt;double-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-recurrent-q-learning.png"&gt;&lt;img src="output-agent/double-recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="10"&gt;
&lt;li&gt;Duel Q-learning agent, &lt;a href="agent/10.duel-q-learning-agent.ipynb"&gt;duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-q-learning.png"&gt;&lt;img src="output-agent/double-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="11"&gt;
&lt;li&gt;Double Duel Q-learning agent, &lt;a href="agent/11.double-duel-q-learning-agent.ipynb"&gt;double-duel-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-duel-q-learning.png"&gt;&lt;img src="output-agent/double-duel-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="12"&gt;
&lt;li&gt;Duel Recurrent Q-learning agent, &lt;a href="agent/12.duel-recurrent-q-learning-agent.ipynb"&gt;duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/duel-recurrent-q-learning.png"&gt;&lt;img src="output-agent/duel-recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="13"&gt;
&lt;li&gt;Double Duel Recurrent Q-learning agent, &lt;a href="agent/13.double-duel-recurrent-q-learning-agent.ipynb"&gt;double-duel-recurrent-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/double-duel-recurrent-q-learning.png"&gt;&lt;img src="output-agent/double-duel-recurrent-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="14"&gt;
&lt;li&gt;Actor-critic agent, &lt;a href="agent/14.actor-critic-agent.ipynb"&gt;actor-critic-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic.png"&gt;&lt;img src="output-agent/actor-critic.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="15"&gt;
&lt;li&gt;Actor-critic Duel agent, &lt;a href="agent/14.actor-critic-duel-agent.ipynb"&gt;actor-critic-duel-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic-duel.png"&gt;&lt;img src="output-agent/actor-critic-duel.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="16"&gt;
&lt;li&gt;Actor-critic Recurrent agent, &lt;a href="agent/16.actor-critic-recurrent-agent.ipynb"&gt;actor-critic-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic-recurrent.png"&gt;&lt;img src="output-agent/actor-critic-recurrent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="17"&gt;
&lt;li&gt;Actor-critic Duel Recurrent agent, &lt;a href="agent/17.actor-critic-duel-recurrent-agent.ipynb"&gt;actor-critic-duel-recurrent-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/actor-critic-duel-recurrent.png"&gt;&lt;img src="output-agent/actor-critic-duel-recurrent.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="18"&gt;
&lt;li&gt;Curiosity Q-learning agent, &lt;a href="agent/18.curiosity-q-learning-agent.ipynb"&gt;curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/curiosity-q-learning.png"&gt;&lt;img src="output-agent/curiosity-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="19"&gt;
&lt;li&gt;Recurrent Curiosity Q-learning agent, &lt;a href="agent/19.recurrent-curiosity-q-learning-agent.ipynb"&gt;recurrent-curiosity-q-learning.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/recurrent-curiosity-q-learning.png"&gt;&lt;img src="output-agent/recurrent-curiosity-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="20"&gt;
&lt;li&gt;Duel Curiosity Q-learning agent, &lt;a href="agent/20.duel-curiosity-q-learning-agent.ipynb"&gt;duel-curiosity-q-learning-agent.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/duel-curiosity-q-learning.png"&gt;&lt;img src="output-agent/duel-curiosity-q-learning.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="21"&gt;
&lt;li&gt;Neuro-evolution agent, &lt;a href="agent/21.neuro-evolution-agent.ipynb"&gt;neuro-evolution.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/neuro-evolution.png"&gt;&lt;img src="output-agent/neuro-evolution.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="22"&gt;
&lt;li&gt;Neuro-evolution with Novelty search agent, &lt;a href="agent/22.neuro-evolution-novelty-search-agent.ipynb"&gt;neuro-evolution-novelty-search.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/neuro-evolution-novelty-search.png"&gt;&lt;img src="output-agent/neuro-evolution-novelty-search.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="23"&gt;
&lt;li&gt;ABCD strategy agent, &lt;a href="agent/23.abcd-strategy-agent.ipynb"&gt;abcd-strategy.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output-agent/abcd-strategy.png"&gt;&lt;img src="output-agent/abcd-strategy.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-results-signal-prediction" class="anchor" aria-hidden="true" href="#results-signal-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results signal prediction&lt;/h3&gt;
&lt;p&gt;I will cut the dataset to train and test datasets,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Train dataset derived from starting timestamp until last 30 days&lt;/li&gt;
&lt;li&gt;Test dataset derived from last 30 days until end of the dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So we will let the model do forecasting based on last 30 days, and we will going to repeat the experiment for 10 times. You can increase it locally if you want, and tuning parameters will help you by a lot.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;LSTM, accuracy 95.693%, time taken for 1 epoch 01:09&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm.png"&gt;&lt;img src="output/lstm.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;LSTM Bidirectional, accuracy 93.8%, time taken for 1 epoch 01:40&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-lstm.png"&gt;&lt;img src="output/bidirectional-lstm.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;LSTM 2-Path, accuracy 94.63%, time taken for 1 epoch 01:39&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm-2path.png"&gt;&lt;img src="output/lstm-2path.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;GRU, accuracy 94.63%, time taken for 1 epoch 02:10&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru.png"&gt;&lt;img src="output/gru.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;GRU Bidirectional, accuracy 92.5673%, time taken for 1 epoch 01:40&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-gru.png"&gt;&lt;img src="output/bidirectional-gru.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="6"&gt;
&lt;li&gt;GRU 2-Path, accuracy 93.2117%, time taken for 1 epoch 01:39&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru-2path.png"&gt;&lt;img src="output/gru-2path.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="7"&gt;
&lt;li&gt;Vanilla, accuracy 91.4686%, time taken for 1 epoch 00:52&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/vanilla.png"&gt;&lt;img src="output/vanilla.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="8"&gt;
&lt;li&gt;Vanilla Bidirectional, accuracy 88.9927%, time taken for 1 epoch 01:06&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-vanilla.png"&gt;&lt;img src="output/bidirectional-vanilla.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="9"&gt;
&lt;li&gt;Vanilla 2-Path, accuracy 91.5406%, time taken for 1 epoch 01:08&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/vanilla-2path.png"&gt;&lt;img src="output/vanilla-2path.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="10"&gt;
&lt;li&gt;LSTM Seq2seq, accuracy 94.9817%, time taken for 1 epoch 01:36&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm-seq2seq.png"&gt;&lt;img src="output/lstm-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="11"&gt;
&lt;li&gt;LSTM Bidirectional Seq2seq, accuracy 94.517%, time taken for 1 epoch 02:30&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-lstm-seq2seq.png"&gt;&lt;img src="output/bidirectional-lstm-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="12"&gt;
&lt;li&gt;LSTM Seq2seq VAE, accuracy 95.4190%, time taken for 1 epoch 01:48&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/lstm-seq2seq-vae.png"&gt;&lt;img src="output/lstm-seq2seq-vae.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="13"&gt;
&lt;li&gt;GRU Seq2seq, accuracy 90.8854%, time taken for 1 epoch 01:34&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru-seq2seq.png"&gt;&lt;img src="output/gru-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="14"&gt;
&lt;li&gt;GRU Bidirectional Seq2seq, accuracy 67.9915%, time taken for 1 epoch 02:30&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/bidirectional-gru-seq2seq.png"&gt;&lt;img src="output/bidirectional-gru-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="15"&gt;
&lt;li&gt;GRU Seq2seq VAE, accuracy 89.1321%, time taken for 1 epoch 01:48&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/gru-seq2seq-vae.png"&gt;&lt;img src="output/gru-seq2seq-vae.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="16"&gt;
&lt;li&gt;Attention-is-all-you-Need, accuracy 94.2482%, time taken for 1 epoch 01:41&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/attention-is-all-you-need.png"&gt;&lt;img src="output/attention-is-all-you-need.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="17"&gt;
&lt;li&gt;CNN-Seq2seq, accuracy 90.74%, time taken for 1 epoch 00:43&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/cnn-seq2seq.png"&gt;&lt;img src="output/cnn-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="18"&gt;
&lt;li&gt;Dilated-CNN-Seq2seq, accuracy 95.86%, time taken for 1 epoch 00:14&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/dilated-cnn-seq2seq.png"&gt;&lt;img src="output/dilated-cnn-seq2seq.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to forecast,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/how-to-forecast.png"&gt;&lt;img src="output/how-to-forecast.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Sentiment consensus,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="output/sentiment-consensus.png"&gt;&lt;img src="output/sentiment-consensus.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-results-analysis" class="anchor" aria-hidden="true" href="#results-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results analysis&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Outliers study using K-means, SVM, and Gaussian on TESLA stock&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="misc/outliers.png"&gt;&lt;img src="misc/outliers.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Overbought-Oversold study on TESLA stock&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="misc/overbought-oversold.png"&gt;&lt;img src="misc/overbought-oversold.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Which stock you need to buy?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="misc/which-stock.png"&gt;&lt;img src="misc/which-stock.png" width="40%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-results-simulation" class="anchor" aria-hidden="true" href="#results-simulation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results simulation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Simple Monte Carlo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/monte-carlo-simple.png"&gt;&lt;img src="simulation/monte-carlo-simple.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Dynamic volatity Monte Carlo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/monte-carlo-dynamic-volatility.png"&gt;&lt;img src="simulation/monte-carlo-dynamic-volatility.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Drift Monte Carlo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/monte-carlo-drift.png"&gt;&lt;img src="simulation/monte-carlo-drift.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Multivariate Drift Monte Carlo BTC/USDT with Bitcurate sentiment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/multivariate-drift-monte-carlo.png"&gt;&lt;img src="simulation/multivariate-drift-monte-carlo.png" width="70%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Portfolio optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="simulation/portfolio-optimization.png"&gt;&lt;img src="simulation/portfolio-optimization.png" width="40%" align="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huseinzol05</author><guid isPermaLink="false">https://github.com/huseinzol05/Stock-Prediction-Models</guid><pubDate>Mon, 06 Jan 2020 00:18:00 GMT</pubDate></item><item><title>jeffheaton/t81_558_deep_learning #19 in Jupyter Notebook, Today</title><link>https://github.com/jeffheaton/t81_558_deep_learning</link><description>&lt;p&gt;&lt;i&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-t81-558applications-of-deep-neural-networks" class="anchor" aria-hidden="true" href="#t81-558applications-of-deep-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.wustl.edu" rel="nofollow"&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instructor: &lt;a href="https://sites.wustl.edu/jeffheaton/" rel="nofollow"&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href="https://github.com/jeffheaton"&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Section 2. Spring 2020, Monday, 2:30 PM - 5:20 PM Online &amp;amp; Cupples I / 215&lt;/li&gt;
&lt;li&gt;Section 1. Spring 2020, Monday, 6:00 PM - 9:00 PM Online &amp;amp; Cupples I / 215&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-course-description" class="anchor" aria-hidden="true" href="#course-description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course Description&lt;/h1&gt;
&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-objectives" class="anchor" aria-hidden="true" href="#objectives"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Objectives&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt;
&lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt;
&lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus&lt;/h1&gt;
&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments.  &lt;a href="https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/pdf/t81_558_spring2020_syllabus.pdf" rel="nofollow"&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Module&lt;/th&gt;
&lt;th&gt;Content&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_01_1_overview.ipynb"&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 01/13/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (first meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_02_1_python_pandas.ipynb"&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 01/27/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;	Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class1.ipynb"&gt;Module 1 Assignment&lt;/a&gt; Due: 01/28/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_03_1_neural_net.ipynb"&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 02/03/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class2.ipynb"&gt;Module 2: Assignment&lt;/a&gt; due: 02/04/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_04_1_feature_encode.ipynb"&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 02/10/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class3.ipynb"&gt;Module 3 Assignment&lt;/a&gt; due: 02/11/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_05_1_reg_ridge_lasso.ipynb"&gt;Module 5&lt;/a&gt;&lt;br&gt;Week of 02/17/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class4.ipynb"&gt;Module 4 Assignment&lt;/a&gt; due: 02/18/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_06_1_python_images.ipynb"&gt;Module 6&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 02/24/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&lt;ul&gt;	Part 6.1: Image Processing in Python&lt;li&gt;Part 6.2: Keras Neural Networks for MINST and Fashion MINST&lt;/li&gt;&lt;li&gt;Part 6.3: Implementing a ResNet in Keras&lt;/li&gt;&lt;li&gt;Part 6.4: Computer Vision with OpenCV&lt;/li&gt;&lt;li&gt;Part 6.5: Recognizing Multiple Images with Darknet&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class5.ipynb"&gt;Module 5 Assignment&lt;/a&gt; due: 02/25/2020&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (2nd Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_07_1_gan_intro.ipynb"&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 03/02/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 7: GAN&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&lt;li&gt;Part 7.2: Implementing a GAN in Keras&lt;/li&gt;&lt;li&gt;Part 7.3: Face Generation with StyleGAN and Python&lt;/li&gt;&lt;li&gt;Part 7.4: GANS for Semi-Supervised Learning in Keras&lt;/li&gt;&lt;li&gt;Part 7.5: An Overview of GAN Research&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class6.ipynb"&gt;Module 6 Assignment&lt;/a&gt; due: 03/03/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_08_1_kaggle_intro.ipynb"&gt;Module 8&lt;/a&gt;&lt;br&gt;Week of 03/16/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&lt;li&gt;Part 8.5: Current Semester's Kaggle&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class7.ipynb"&gt;Module 7 Assignment&lt;/a&gt; due: 03/17/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_09_1_keras_transfer.ipynb"&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 03/23/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&lt;li&gt;Part 9.2: Popular Pretrained Neural Networks for Keras. &lt;/li&gt;&lt;li&gt;Part 9.3: Transfer Learning for Computer Vision and Keras&lt;/li&gt;&lt;li&gt;Part 9.4: Transfer Learning for Languages and Keras&lt;/li&gt;&lt;li&gt;Part 9.5: Transfer Learning for Keras Feature Engineering&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class8.ipynb"&gt;Module 8 Assignment&lt;/a&gt; due: 03/24/2020&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (3rd Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_10_1_timeseries.ipynb"&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 03/30/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, TensorFlow and Keras&lt;/li&gt;&lt;li&gt;Part 10.2: Programming LSTM with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.3: Image Captioning with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.4: Temporal CNN in Keras and TensorFlow&lt;/li&gt;&lt;li&gt;Part 10.5: Predicting the Stock Market with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class9.ipynb"&gt;Module 9 Assignment&lt;/a&gt; due: 03/31/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_11_01_spacy.ipynb"&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 04/06/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 11.1: Getting Started with Spacy in Python&lt;/li&gt;&lt;li&gt;Part 11.2: Word2Vec and Text Classification&lt;/li&gt;&lt;li&gt;Part 11.3: Natural Language Processing with Spacy and Keras&lt;/li&gt;&lt;li&gt;Part 11.4: What are Embedding Layers in Keras&lt;/li&gt;&lt;li&gt;Part 11.5: Learning English from Scratch with Keras and TensorFlow&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class10.ipynb"&gt;Module 10 Assignment&lt;/a&gt; due: 04/07/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_12_01_ai_gym.ipynb"&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 04/13/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Kaggle Assignment due: 04/13/2020 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&lt;li&gt;Part 12.5: How Alpha Zero used Reinforcement Learning to Master Chess&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_13_01_flask.ipynb"&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 04/20/2020&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 13.1: Deploying a Model to AWS&lt;/li&gt;&lt;li&gt;Part 13.2: Flask and Deep Learning Web Services&lt;/li&gt;&lt;li&gt;Part 13.3: AI at the Edge: Using Keras on a Mobile Device&lt;/li&gt;&lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&lt;li&gt;Part 13.5: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&lt;li&gt;&lt;strong&gt;We will meet on campus this week!&lt;/strong&gt; (4th Meeting)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="t81_558_class_14_01_automl.ipynb"&gt;Module 14&lt;/a&gt;&lt;br&gt;Week of 04/27/2020&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Module 14: Other Neural Network Techniques&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Part 14.1: What is AutoML&lt;/li&gt;&lt;li&gt;Part 14.2: Using Denoising AutoEncoders in Keras&lt;/li&gt;&lt;li&gt;Part 14.3: Training an Intrusion Detection System with KDD99&lt;/li&gt;&lt;li&gt;Part 14.4: Anomaly Detection in Keras&lt;/li&gt;&lt;li&gt;Part 14.5: New Technology in Deep Learning&lt;/li&gt;&lt;li&gt;Final Project due 05/04/2020&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://data.heatonresearch.com/data/t81-558/index.html" rel="nofollow"&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jeffheaton</author><guid isPermaLink="false">https://github.com/jeffheaton/t81_558_deep_learning</guid><pubDate>Mon, 06 Jan 2020 00:19:00 GMT</pubDate></item><item><title>tensorflow/examples #20 in Jupyter Notebook, Today</title><link>https://github.com/tensorflow/examples</link><description>&lt;p&gt;&lt;i&gt;TensorFlow examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-examples" class="anchor" aria-hidden="true" href="#tensorflow-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Examples&lt;/h1&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/0905c7d634421f8aa4ab3ddf19a582572df568e1/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f736f6369616c2e706e67" data-canonical-src="https://www.tensorflow.org/images/tf_logo_social.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-most-important-links" class="anchor" aria-hidden="true" href="#most-important-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Most important links!&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./community"&gt;Community examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="./courses/udacity_deep_learning"&gt;Course materials&lt;/a&gt; for the &lt;a href="https://www.udacity.com/course/deep-learning--ud730" rel="nofollow"&gt;Deep Learning&lt;/a&gt; class on Udacity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are looking to learn TensorFlow, don't miss the
&lt;a href="http://github.com/tensorflow/docs"&gt;core TensorFlow documentation&lt;/a&gt;
which is largely runnable code.
Those notebooks can be opened in Colab from
&lt;a href="https://tensorflow.org" rel="nofollow"&gt;tensorflow.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-this-repo" class="anchor" aria-hidden="true" href="#what-is-this-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this repo?&lt;/h2&gt;
&lt;p&gt;This is the TensorFlow example repo.  It has several classes of material:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Showcase examples and documentation for our fantastic &lt;a href="https://tensorflow.org/community" rel="nofollow"&gt;TensorFlow Community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Provide examples mentioned on TensorFlow.org&lt;/li&gt;
&lt;li&gt;Publish material supporting official TensorFlow courses&lt;/li&gt;
&lt;li&gt;Publish supporting material for the &lt;a href="https://blog.tensorflow.org" rel="nofollow"&gt;TensorFlow Blog&lt;/a&gt; and &lt;a href="https://youtube.com/tensorflow" rel="nofollow"&gt;TensorFlow YouTube Channel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We welcome community contributions, see &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; and, for style help,
&lt;a href="https://www.tensorflow.org/community/documentation" rel="nofollow"&gt;Writing TensorFlow documentation&lt;/a&gt;
guide.&lt;/p&gt;
&lt;p&gt;To file an issue, use the tracker in the
&lt;a href="https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md"&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/examples</guid><pubDate>Mon, 06 Jan 2020 00:20:00 GMT</pubDate></item><item><title>rasbt/python-machine-learning-book-3rd-edition #21 in Jupyter Notebook, Today</title><link>https://github.com/rasbt/python-machine-learning-book-3rd-edition</link><description>&lt;p&gt;&lt;i&gt;The "Python Machine Learning (3rd edition)" book code repository&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python-machine-learning-3rd-ed-code-repository" class="anchor" aria-hidden="true" href="#python-machine-learning-3rd-ed-code-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Machine Learning (3rd Ed.) Code Repository&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/61841a3590d58efb5f368ffb4d82ef16e216fd82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e372d626c75652e737667" alt="Python 3.6" data-canonical-src="https://img.shields.io/badge/Python-3.7-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667"&gt;&lt;img src="https://camo.githubusercontent.com/a0e2e02654c03ef5b20640e5d052b0b448e59313/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64652532304c6963656e73652d4d49542d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/Code%20License-MIT-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code repositories for the 1st and 2nd edition are available at&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/rasbt/python-machine-learning-book"&gt;https://github.com/rasbt/python-machine-learning-book&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition"&gt;https://github.com/rasbt/python-machine-learning-book-2nd-edition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Python Machine Learning, 3rd Ed.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;to be published December 12th, 2019&lt;/p&gt;
&lt;p&gt;Paperback: 770 pages&lt;br&gt;
Publisher: Packt Publishing&lt;br&gt;
Language: English&lt;/p&gt;
&lt;p&gt;ISBN-10: 1789955750&lt;br&gt;
ISBN-13: 978-1789955750&lt;br&gt;
Kindle ASIN: B07VBLX2W7&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/" rel="nofollow"&gt;&lt;img src="./.other/cover_1.jpg" width="248" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750/" rel="nofollow"&gt;Amazon Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.packtpub.com/data/python-machine-learning-third-edition" rel="nofollow"&gt;Packt Page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents-and-code-notebooks" class="anchor" aria-hidden="true" href="#table-of-contents-and-code-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents and Code Notebooks&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Helpful installation and setup instructions can be found in the &lt;a href="ch01/README.md"&gt;README.md file of Chapter 1&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note that these are just the code examples accompanying the book, which we uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Machine Learning - Giving Computers the Ability to Learn from Data [&lt;a href="ch01"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Training Machine Learning Algorithms for Classification [&lt;a href="ch02"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A Tour of Machine Learning Classifiers Using Scikit-Learn [&lt;a href="ch03"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Building Good Training Sets – Data Pre-Processing [&lt;a href="ch04"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Compressing Data via Dimensionality Reduction [&lt;a href="ch05"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Learning Best Practices for Model Evaluation and Hyperparameter Optimization [&lt;a href="ch06"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Combining Different Models for Ensemble Learning [&lt;a href="ch07"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Applying Machine Learning to Sentiment Analysis [&lt;a href="ch08"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Embedding a Machine Learning Model into a Web Application [&lt;a href="ch09"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Predicting Continuous Target Variables with Regression Analysis [&lt;a href="ch10"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Working with Unlabeled Data – Clustering Analysis [&lt;a href="ch11"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Implementing a Multi-layer Artificial Neural Network from Scratch [&lt;a href="ch12"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Parallelizing Neural Network Training with TensorFlow [&lt;a href="ch13"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Going Deeper: The Mechanics of TensorFlow [&lt;a href="ch14"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Classifying Images with Deep Convolutional Neural Networks [&lt;a href="ch15"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Modeling Sequential Data Using Recurrent Neural Networks [&lt;a href="ch16"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Generative Adversarial Networks for Synthesizing New Data [&lt;a href="ch17"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Reinforcement Learning for Decision Making in Complex Environments [&lt;a href="ch18"&gt;open dir&lt;/a&gt;]&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;Raschka, Sebastian, and Vahid Mirjalili. &lt;em&gt;Python Machine Learning, 3rd Ed&lt;/em&gt;. Packt Publishing, 2019.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{RaschkaMirjalili2019,  
address = {Birmingham, UK},  
author = {Raschka, Sebastian and Mirjalili, Vahid},  
edition = {3},  
isbn = {978-1789955750},   
publisher = {Packt Publishing},  
title = {{Python Machine Learning, 3rd Ed.}},  
year = {2019}  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasbt</author><guid isPermaLink="false">https://github.com/rasbt/python-machine-learning-book-3rd-edition</guid><pubDate>Mon, 06 Jan 2020 00:21:00 GMT</pubDate></item><item><title>aimacode/aima-python #22 in Jupyter Notebook, Today</title><link>https://github.com/aimacode/aima-python</link><description>&lt;p&gt;&lt;i&gt;Python implementation of algorithms from Russell And Norvig's "Artificial Intelligence - A Modern Approach"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
  &lt;a href="http://aima.cs.berkeley.edu/" rel="nofollow"&gt;&lt;img src="https://raw.githubusercontent.com/aimacode/aima-python/master/images/aima_logo.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;
&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-aima-python--" class="anchor" aria-hidden="true" href="#aima-python--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;aima-python&lt;/code&gt; &lt;a href="https://travis-ci.org/aimacode/aima-python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/01177a7ca5a96309e258148841f9c5c82ff80993/68747470733a2f2f7472617669732d63692e6f72672f61696d61636f64652f61696d612d707974686f6e2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/aimacode/aima-python.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="http://mybinder.org/repo/aimacode/aima-python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Python code for the book &lt;em&gt;&lt;a href="http://aima.cs.berkeley.edu" rel="nofollow"&gt;Artificial Intelligence: A Modern Approach&lt;/a&gt;.&lt;/em&gt; You can use this in conjunction with a course on AI, or for study on your own. We're looking for &lt;a href="https://github.com/aimacode/aima-python/blob/master/CONTRIBUTING.md"&gt;solid contributors&lt;/a&gt; to help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-structure-of-the-project" class="anchor" aria-hidden="true" href="#structure-of-the-project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Structure of the Project&lt;/h2&gt;
&lt;p&gt;When complete, this project will have Python implementations for all the pseudocode algorithms in the book, as well as tests and examples of use. For each major topic, such as &lt;code&gt;nlp&lt;/code&gt; (natural language processing), we provide the following  files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nlp.py&lt;/code&gt;: Implementations of all the pseudocode algorithms, and necessary support functions/classes/data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tests/test_nlp.py&lt;/code&gt;: A lightweight test suite, using &lt;code&gt;assert&lt;/code&gt; statements, designed for use with &lt;a href="http://pytest.org/latest/" rel="nofollow"&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/a&gt;, but also usable on their own.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp.ipynb&lt;/code&gt;: A Jupyter (IPython) notebook that explains and gives examples of how to use the code.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp_apps.ipynb&lt;/code&gt;: A Jupyter notebook that gives example applications of the code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-python-34-and-up" class="anchor" aria-hidden="true" href="#python-34-and-up"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python 3.4 and up&lt;/h2&gt;
&lt;p&gt;This code requires Python 3.4 or later, and does not run in Python 2. You can &lt;a href="https://www.python.org/downloads" rel="nofollow"&gt;install Python&lt;/a&gt; or use a browser-based Python interpreter such as &lt;a href="https://repl.it/languages/python3" rel="nofollow"&gt;repl.it&lt;/a&gt;.
You can run the code in an IDE, or from the command line with &lt;code&gt;python -i filename.py&lt;/code&gt; where the &lt;code&gt;-i&lt;/code&gt; option puts you in an interactive loop where you can run Python functions. All notebooks are available in a &lt;a href="http://mybinder.org/repo/aimacode/aima-python" rel="nofollow"&gt;binder environment&lt;/a&gt;. Alternatively, visit &lt;a href="http://jupyter.org/" rel="nofollow"&gt;jupyter.org&lt;/a&gt; for instructions on setting up your own Jupyter notebook environment.&lt;/p&gt;
&lt;p&gt;There is a sibling &lt;a href="https://github.com/rajatjain1997/aima-docker"&gt;aima-docker&lt;/a&gt; project that shows you how to use docker containers to run more complex problems in more complex software environments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation-guide" class="anchor" aria-hidden="true" href="#installation-guide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Guide&lt;/h2&gt;
&lt;p&gt;To download the repository:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git clone https://github.com/aimacode/aima-python.git&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then you need to install the basic dependencies to run the project on your system:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd aima-python
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to fetch the datasets from the &lt;a href="https://github.com/aimacode/aima-data"&gt;&lt;code&gt;aima-data&lt;/code&gt;&lt;/a&gt; repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git submodule init
git submodule update
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait for the datasets to download, it may take a while. Once they are downloaded, you need to install &lt;code&gt;pytest&lt;/code&gt;, so that you can run the test suite:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install pytest&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then to run the tests:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And you are good to go!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-index-of-algorithms" class="anchor" aria-hidden="true" href="#index-of-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index of Algorithms&lt;/h1&gt;
&lt;p&gt;Here is a table of algorithms, the figure, name of the algorithm in the book and in the repository, and the file where they are implemented in the repository. This chart was made for the third edition of the book and is being updated for the upcoming fourth edition. Empty implementations are a good place for contributors to look for an issue. The &lt;a href="https://github.com/aimacode/aima-pseudocode"&gt;aima-pseudocode&lt;/a&gt; project describes all the algorithms from the book. An asterisk next to the file name denotes the algorithm is not fully implemented. Another great place for contributors to start is by adding tests and writing on the notebooks. You can see which algorithms have tests and notebook sections below. If the algorithm you want to work on is covered, don't worry! You can still add more tests and provide some examples of use in the notebook!&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Name (in 3&lt;sup&gt;rd&lt;/sup&gt; edition)&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Tests&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Notebook&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;td align="left"&gt;Random-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;RandomVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;td align="left"&gt;Model-Based-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ModelBasedVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.1&lt;/td&gt;
&lt;td align="left"&gt;Environment&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Environment&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.1&lt;/td&gt;
&lt;td align="left"&gt;Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Agent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.3&lt;/td&gt;
&lt;td align="left"&gt;Table-Driven-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;TableDrivenVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.7&lt;/td&gt;
&lt;td align="left"&gt;Table-Driven-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;TableDrivenAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.8&lt;/td&gt;
&lt;td align="left"&gt;Reflex-Vacuum-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ReflexVacuumAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.10&lt;/td&gt;
&lt;td align="left"&gt;Simple-Reflex-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;SimpleReflexAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2.12&lt;/td&gt;
&lt;td align="left"&gt;Model-Based-Reflex-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ReflexAgentWithState&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/agents.py"&gt;&lt;code&gt;agents.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;Problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Problem&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;Node&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Node&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;Queue&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Queue&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/utils.py"&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;No Need&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.1&lt;/td&gt;
&lt;td align="left"&gt;Simple-Problem-Solving-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;SimpleProblemSolvingAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.2&lt;/td&gt;
&lt;td align="left"&gt;Romania&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;romania&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.7&lt;/td&gt;
&lt;td align="left"&gt;Tree-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;depth/breadth_first_tree_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.7&lt;/td&gt;
&lt;td align="left"&gt;Graph-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;depth/breadth_first_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.11&lt;/td&gt;
&lt;td align="left"&gt;Breadth-First-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;breadth_first_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.14&lt;/td&gt;
&lt;td align="left"&gt;Uniform-Cost-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;uniform_cost_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.17&lt;/td&gt;
&lt;td align="left"&gt;Depth-Limited-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;depth_limited_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.18&lt;/td&gt;
&lt;td align="left"&gt;Iterative-Deepening-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;iterative_deepening_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.22&lt;/td&gt;
&lt;td align="left"&gt;Best-First-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;best_first_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.24&lt;/td&gt;
&lt;td align="left"&gt;A*-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;astar_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.26&lt;/td&gt;
&lt;td align="left"&gt;Recursive-Best-First-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;recursive_best_first_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.2&lt;/td&gt;
&lt;td align="left"&gt;Hill-Climbing&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;hill_climbing&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.5&lt;/td&gt;
&lt;td align="left"&gt;Simulated-Annealing&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;simulated_annealing&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.8&lt;/td&gt;
&lt;td align="left"&gt;Genetic-Algorithm&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;genetic_algorithm&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.11&lt;/td&gt;
&lt;td align="left"&gt;And-Or-Graph-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;and_or_graph_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.21&lt;/td&gt;
&lt;td align="left"&gt;Online-DFS-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;online_dfs_agent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.24&lt;/td&gt;
&lt;td align="left"&gt;LRTA*-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;LRTAStarAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;5.3&lt;/td&gt;
&lt;td align="left"&gt;Minimax-Decision&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;minimax_decision&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/games.py"&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;5.7&lt;/td&gt;
&lt;td align="left"&gt;Alpha-Beta-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;alphabeta_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/games.py"&gt;&lt;code&gt;games.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6&lt;/td&gt;
&lt;td align="left"&gt;CSP&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;CSP&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.3&lt;/td&gt;
&lt;td align="left"&gt;AC-3&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;AC3&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.5&lt;/td&gt;
&lt;td align="left"&gt;Backtracking-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;backtracking_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.8&lt;/td&gt;
&lt;td align="left"&gt;Min-Conflicts&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;min_conflicts&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.11&lt;/td&gt;
&lt;td align="left"&gt;Tree-CSP-Solver&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;tree_csp_solver&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/csp.py"&gt;&lt;code&gt;csp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7&lt;/td&gt;
&lt;td align="left"&gt;KB&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;KB&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.1&lt;/td&gt;
&lt;td align="left"&gt;KB-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;KB_AgentProgram&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.7&lt;/td&gt;
&lt;td align="left"&gt;Propositional Logic Sentence&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Expr&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/utils.py"&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.10&lt;/td&gt;
&lt;td align="left"&gt;TT-Entails&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;tt_entails&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.12&lt;/td&gt;
&lt;td align="left"&gt;PL-Resolution&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;pl_resolution&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.14&lt;/td&gt;
&lt;td align="left"&gt;Convert to CNF&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;to_cnf&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.15&lt;/td&gt;
&lt;td align="left"&gt;PL-FC-Entails?&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;pl_fc_entails&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.17&lt;/td&gt;
&lt;td align="left"&gt;DPLL-Satisfiable?&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;dpll_satisfiable&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.18&lt;/td&gt;
&lt;td align="left"&gt;WalkSAT&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;WalkSAT&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.20&lt;/td&gt;
&lt;td align="left"&gt;Hybrid-Wumpus-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;HybridWumpusAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.22&lt;/td&gt;
&lt;td align="left"&gt;SATPlan&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;SAT_plan&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9&lt;/td&gt;
&lt;td align="left"&gt;Subst&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;subst&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9.1&lt;/td&gt;
&lt;td align="left"&gt;Unify&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;unify&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9.3&lt;/td&gt;
&lt;td align="left"&gt;FOL-FC-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;fol_fc_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9.6&lt;/td&gt;
&lt;td align="left"&gt;FOL-BC-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;fol_bc_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.1&lt;/td&gt;
&lt;td align="left"&gt;Air-Cargo-problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;air_cargo&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.2&lt;/td&gt;
&lt;td align="left"&gt;Spare-Tire-Problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;spare_tire&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.3&lt;/td&gt;
&lt;td align="left"&gt;Three-Block-Tower&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;three_block_tower&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.7&lt;/td&gt;
&lt;td align="left"&gt;Cake-Problem&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;have_cake_and_eat_cake_too&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.9&lt;/td&gt;
&lt;td align="left"&gt;Graphplan&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;GraphPlan&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;10.13&lt;/td&gt;
&lt;td align="left"&gt;Partial-Order-Planner&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;PartialOrderPlanner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.1&lt;/td&gt;
&lt;td align="left"&gt;Job-Shop-Problem-With-Resources&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;job_shop_problem&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.5&lt;/td&gt;
&lt;td align="left"&gt;Hierarchical-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;hierarchical_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.8&lt;/td&gt;
&lt;td align="left"&gt;Angelic-Search&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;angelic_search&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11.10&lt;/td&gt;
&lt;td align="left"&gt;Doubles-tennis&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;double_tennis_problem&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/planning.py"&gt;&lt;code&gt;planning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;13&lt;/td&gt;
&lt;td align="left"&gt;Discrete Probability Distribution&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;ProbDist&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;13.1&lt;/td&gt;
&lt;td align="left"&gt;DT-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;DTAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.9&lt;/td&gt;
&lt;td align="left"&gt;Enumeration-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;enumeration_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.11&lt;/td&gt;
&lt;td align="left"&gt;Elimination-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;elimination_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.13&lt;/td&gt;
&lt;td align="left"&gt;Prior-Sample&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;prior_sample&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.14&lt;/td&gt;
&lt;td align="left"&gt;Rejection-Sampling&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;rejection_sampling&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.15&lt;/td&gt;
&lt;td align="left"&gt;Likelihood-Weighting&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;likelihood_weighting&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14.16&lt;/td&gt;
&lt;td align="left"&gt;Gibbs-Ask&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;gibbs_ask&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15.4&lt;/td&gt;
&lt;td align="left"&gt;Forward-Backward&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;forward_backward&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15.6&lt;/td&gt;
&lt;td align="left"&gt;Fixed-Lag-Smoothing&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;fixed_lag_smoothing&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15.17&lt;/td&gt;
&lt;td align="left"&gt;Particle-Filtering&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;particle_filtering&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;16.9&lt;/td&gt;
&lt;td align="left"&gt;Information-Gathering-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;InformationGatheringAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.4&lt;/td&gt;
&lt;td align="left"&gt;Value-Iteration&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;value_iteration&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.7&lt;/td&gt;
&lt;td align="left"&gt;Policy-Iteration&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;policy_iteration&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.9&lt;/td&gt;
&lt;td align="left"&gt;POMDP-Value-Iteration&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;pomdp_value_iteration&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.5&lt;/td&gt;
&lt;td align="left"&gt;Decision-Tree-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;DecisionTreeLearner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.8&lt;/td&gt;
&lt;td align="left"&gt;Cross-Validation&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;cross_validation&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.11&lt;/td&gt;
&lt;td align="left"&gt;Decision-List-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;DecisionListLearner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;*&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.24&lt;/td&gt;
&lt;td align="left"&gt;Back-Prop-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;BackPropagationLearner&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.34&lt;/td&gt;
&lt;td align="left"&gt;AdaBoost&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;AdaBoost&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.2&lt;/td&gt;
&lt;td align="left"&gt;Current-Best-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;current_best_learning&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.3&lt;/td&gt;
&lt;td align="left"&gt;Version-Space-Learning&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;version_space_learning&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.8&lt;/td&gt;
&lt;td align="left"&gt;Minimal-Consistent-Det&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;minimal_consistent_det&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;19.12&lt;/td&gt;
&lt;td align="left"&gt;FOIL&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;FOIL_container&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="knowledge.py"&gt;&lt;code&gt;knowledge.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;21.2&lt;/td&gt;
&lt;td align="left"&gt;Passive-ADP-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;PassiveADPAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/rl.py"&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;21.4&lt;/td&gt;
&lt;td align="left"&gt;Passive-TD-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;PassiveTDAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/rl.py"&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;21.8&lt;/td&gt;
&lt;td align="left"&gt;Q-Learning-Agent&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;QLearningAgent&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/rl.py"&gt;&lt;code&gt;rl.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;22.1&lt;/td&gt;
&lt;td align="left"&gt;HITS&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;HITS&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/nlp.py"&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;23&lt;/td&gt;
&lt;td align="left"&gt;Chart-Parse&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;Chart&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/nlp.py"&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;23.5&lt;/td&gt;
&lt;td align="left"&gt;CYK-Parse&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;CYK_parse&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/nlp.py"&gt;&lt;code&gt;nlp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;25.9&lt;/td&gt;
&lt;td align="left"&gt;Monte-Carlo-Localization&lt;/td&gt;
&lt;td align="left"&gt;&lt;code&gt;monte_carlo_localization&lt;/code&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/probability.py"&gt;&lt;code&gt;probability.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Done&lt;/td&gt;
&lt;td align="left"&gt;Included&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-index-of-data-structures" class="anchor" aria-hidden="true" href="#index-of-data-structures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Index of data structures&lt;/h1&gt;
&lt;p&gt;Here is a table of the implemented data structures, the figure, name of the implementation in the repository, and the file where they are implemented.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;strong&gt;Figure&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;Name (in repository)&lt;/strong&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;strong&gt;File&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3.2&lt;/td&gt;
&lt;td align="left"&gt;romania_map&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.9&lt;/td&gt;
&lt;td align="left"&gt;vacumm_world&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4.23&lt;/td&gt;
&lt;td align="left"&gt;one_dim_state_space&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;6.1&lt;/td&gt;
&lt;td align="left"&gt;australia_map&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/search.py"&gt;&lt;code&gt;search.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.13&lt;/td&gt;
&lt;td align="left"&gt;wumpus_world_inference&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;7.16&lt;/td&gt;
&lt;td align="left"&gt;horn_clauses_KB&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/logic.py"&gt;&lt;code&gt;logic.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17.1&lt;/td&gt;
&lt;td align="left"&gt;sequential_decision_environment&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/mdp.py"&gt;&lt;code&gt;mdp.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;18.2&lt;/td&gt;
&lt;td align="left"&gt;waiting_decision_tree&lt;/td&gt;
&lt;td align="left"&gt;&lt;a href="../master/learning.py"&gt;&lt;code&gt;learning.py&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h1&gt;
&lt;p&gt;Many thanks for contributions over the years. I got bug reports, corrected code, and other support from Darius Bacon, Phil Ruggera, Peng Shao, Amit Patil, Ted Nienstedt, Jim Martin, Ben Catanzariti, and others. Now that the project is on GitHub, you can see the &lt;a href="https://github.com/aimacode/aima-python/graphs/contributors"&gt;contributors&lt;/a&gt; who are doing a great job of actively improving the project. Many thanks to all contributors, especially &lt;a href="https://github.com/darius"&gt;@darius&lt;/a&gt;, &lt;a href="https://github.com/SnShine"&gt;@SnShine&lt;/a&gt;, &lt;a href="https://github.com/reachtarunhere"&gt;@reachtarunhere&lt;/a&gt;, &lt;a href="https://github.com/antmarakis"&gt;@antmarakis&lt;/a&gt;, &lt;a href="https://github.com/Chipe1"&gt;@Chipe1&lt;/a&gt;, &lt;a href="https://github.com/ad71"&gt;@ad71&lt;/a&gt; and &lt;a href="https://github.com/MariannaSpyrakou"&gt;@MariannaSpyrakou&lt;/a&gt;.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>aimacode</author><guid isPermaLink="false">https://github.com/aimacode/aima-python</guid><pubDate>Mon, 06 Jan 2020 00:22:00 GMT</pubDate></item><item><title>robmarkcole/satellite-image-deep-learning #23 in Jupyter Notebook, Today</title><link>https://github.com/robmarkcole/satellite-image-deep-learning</link><description>&lt;p&gt;&lt;i&gt; Resources for deep learning with satellite &amp; aerial imagery&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://github.com/sponsors/robmarkcole"&gt;&lt;img src="https://camo.githubusercontent.com/71d78e24fcd75bb7723712c87325766d6b95acd6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73706f6e736f722d2546302539462539322539362d677265656e" alt="Sponsor" data-canonical-src="https://img.shields.io/badge/sponsor-%F0%9F%92%96-green" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This document primarily lists resources for performing deep learning (DL) on satellite imagery. To a lesser extent Machine learning (ML, e.g. random forests, stochastic gradient descent) are also discussed, as are classical image processing techniques.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-top-links" class="anchor" aria-hidden="true" href="#top-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Top links&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chrieke/awesome-satellite-imagery-datasets"&gt;https://github.com/chrieke/awesome-satellite-imagery-datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/jacquestardie/0d1c0cb413b3b9b06edf"&gt;A modern geospatial workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/deepVector/geospatial-machine-learning"&gt;geospatial-machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.satimagingcorp.com/satellite-sensors/" rel="nofollow"&gt;Long list of satellite missions with example imagery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://registry.opendata.aws/" rel="nofollow"&gt;AWS datasets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#datasets"&gt;Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#online-computing-resources"&gt;Online computing resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#interesting-dl-projects"&gt;Interesting dl projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#production"&gt;Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#image-formats--catalogues"&gt;Image formats and catalogues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#state-of-the-art"&gt;State of the art&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#online-platforms-for-geo-analysis"&gt;Online platforms for Geo analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#techniques"&gt;Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#useful-references"&gt;Useful references&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Warning&lt;/strong&gt; satellite image files can be LARGE, even a small data set may comprise 50 GB of imagery&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.maptiler.com/gallery/satellite/" rel="nofollow"&gt;Various datasets listed here&lt;/a&gt; and at &lt;a href="https://github.com/chrieke/awesome-satellite-imagery-datasets"&gt;awesome-satellite-imagery-datasets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-worldview---spacenet" class="anchor" aria-hidden="true" href="#worldview---spacenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WorldView - SpaceNet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/WorldView-3" rel="nofollow"&gt;https://en.wikipedia.org/wiki/WorldView-3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;0.3m PAN, 1.24 MS, 3.7m SWIR. Off-Nadir (stereo) available.&lt;/li&gt;
&lt;li&gt;Owned by &lt;a href="https://www.digitalglobe.com/" rel="nofollow"&gt;DigitalGlobe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@sumit.arora/getting-started-with-aws-spacenet-and-spacenet-dataset-visualization-basics-7ddd2e5809a2" rel="nofollow"&gt;Intro to SpaceNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spacenetchallenge.github.io/datasets/datasetHomePage.html" rel="nofollow"&gt;SpaceNet dataset on AWS&lt;/a&gt; -&amp;gt; see &lt;a href="https://medium.com/the-downlinq/getting-started-with-spacenet-data-827fd2ec9f53" rel="nofollow"&gt;this getting started notebook&lt;/a&gt; and this notebook on the &lt;a href="https://medium.com/the-downlinq/introducing-the-spacenet-off-nadir-imagery-and-buildings-dataset-e4a3c1cb4ce3" rel="nofollow"&gt;off-Nadir dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://menthe.ovh.hw.ipol.im/IARPA_data/cloud_optimized_geotif/" rel="nofollow"&gt;cloud_optimized_geotif here&lt;/a&gt; used in the 3D modelling notebook &lt;a href="https://gfacciol.github.io/IS18/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SpaceNetChallenge/utilities"&gt;Package of utilities&lt;/a&gt; to assist working with the SpaceNet dataset.&lt;/li&gt;
&lt;li&gt;For more Worldview imagery see Kaggle DSTL competition.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-sentinel" class="anchor" aria-hidden="true" href="#sentinel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentinel&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;As part of the &lt;a href="https://en.wikipedia.org/wiki/Copernicus_Programme" rel="nofollow"&gt;EU Copernicus program&lt;/a&gt;, multiple Sentinel satellites are capturing imagery -&amp;gt; see &lt;a href="https://en.wikipedia.org/wiki/Copernicus_Programme#Sentinel_missions" rel="nofollow"&gt;wikipedia&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;13 bands, Spatial resolution of 10 m, 20 m and 60 m, 290 km swath, the temporal resolution is 5 days&lt;/li&gt;
&lt;li&gt;&lt;a href="https://console.cloud.google.com/storage/browser/gcp-public-data-sentinel-2?prefix=tiles%2F31%2FT%2FCJ%2F" rel="nofollow"&gt;Open access data on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paid access via &lt;a href="https://www.sentinel-hub.com/" rel="nofollow"&gt;sentinel-hub&lt;/a&gt; and &lt;a href="https://github.com/sentinel-hub/sentinelhub-py"&gt;python-api&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/binder-examples/getting-data/blob/master/Sentinel2.ipynb"&gt;Example loading sentinel data in a notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/so2sat" rel="nofollow"&gt;so2sat on Tensorflow datasets&lt;/a&gt; - So2Sat LCZ42 is a dataset consisting of co-registered synthetic aperture radar and multispectral optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) label. The dataset is distributed over 42 cities across different continents and cultural regions of the world.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/eurosat" rel="nofollow"&gt;eurosat&lt;/a&gt; - EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/bigearthnet" rel="nofollow"&gt;bigearthnet&lt;/a&gt; - The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. The image patch size on the ground is 1.2 x 1.2 km with variable image size depending on the channel resolution. This is a multi-label dataset with 43 imbalanced labels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-landsat" class="anchor" aria-hidden="true" href="#landsat"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Landsat&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Long running US program -&amp;gt; see &lt;a href="https://en.wikipedia.org/wiki/Landsat_program" rel="nofollow"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;8 bands, 15 to 60 meters, 185km swath, the temporal resolution is 16 days&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/storage/docs/public-datasets/landsat" rel="nofollow"&gt;Imagery on GCP&lt;/a&gt;, see &lt;a href="https://console.cloud.google.com/storage/browser/gcp-public-data-landsat/" rel="nofollow"&gt;the GCP bucket here&lt;/a&gt;, with imagery analysed in &lt;a href="https://github.com/pangeo-data/pangeo-example-notebooks/blob/master/landsat8-cog-ndvi.ipynb"&gt;this notebook&lt;/a&gt; on Pangeo&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-shuttle-radar-topography-mission-digital-elevation-maps" class="anchor" aria-hidden="true" href="#shuttle-radar-topography-mission-digital-elevation-maps"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Shuttle Radar Topography Mission (digital elevation maps)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://srtm.csi.cgiar.org/srtmdata/" rel="nofollow"&gt;Data - open access&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aerial-imagery-drones" class="anchor" aria-hidden="true" href="#aerial-imagery-drones"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial imagery (drones)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-kaggle" class="anchor" aria-hidden="true" href="#kaggle"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle&lt;/h2&gt;
&lt;p&gt;Kaggle hosts several large satellite image datasets (&lt;a href="https://www.kaggle.com/datasets?sortBy=relevance&amp;amp;group=public&amp;amp;search=image&amp;amp;page=1&amp;amp;pageSize=20&amp;amp;size=large&amp;amp;filetype=all&amp;amp;license=all" rel="nofollow"&gt;&amp;gt; 1 GB&lt;/a&gt;). A list if general image datasets is &lt;a href="https://gisgeography.com/free-satellite-imagery-data-list/" rel="nofollow"&gt;here&lt;/a&gt;. A list of land-use datasets is &lt;a href="https://gisgeography.com/free-global-land-cover-land-use-data/" rel="nofollow"&gt;here&lt;/a&gt;. The &lt;a href="http://blog.kaggle.com" rel="nofollow"&gt;kaggle blog&lt;/a&gt; is an interesting read.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---amazon-from-space---classification-challenge" class="anchor" aria-hidden="true" href="#kaggle---amazon-from-space---classification-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Amazon from space - classification challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data" rel="nofollow"&gt;https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3-5 meter resolution GeoTIFF images from planet Dove satellite constellation&lt;/li&gt;
&lt;li&gt;12 classes including - &lt;strong&gt;cloudy, primary + waterway&lt;/strong&gt; etc&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2017/10/17/planet-understanding-the-amazon-from-space-1st-place-winners-interview/" rel="nofollow"&gt;1st place winner interview - used 11 custom CNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---dstl---segmentation-challenge" class="anchor" aria-hidden="true" href="#kaggle---dstl---segmentation-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - DSTL - segmentation challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection" rel="nofollow"&gt;https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rating - medium, many good examples (see the Discussion as well as kernels), but as this competition was run a couple of years ago many examples use python 2&lt;/li&gt;
&lt;li&gt;WorldView 3 - 45 satellite images covering 1km x 1km in both 3 (i.e. RGB) and 16-band (400nm - SWIR) images&lt;/li&gt;
&lt;li&gt;10 Labelled classes include - &lt;strong&gt;Buildings, Road, Trees, Crops, Waterway, Vehicles&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2017/04/26/dstl-satellite-imagery-competition-1st-place-winners-interview-kyle-lee/" rel="nofollow"&gt;Interview with 1st place winner who used segmentation networks&lt;/a&gt; - 40+ models, each tweaked for particular target (e.g. roads, trees)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/" rel="nofollow"&gt;Deepsense 4th place solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My analysis &lt;a href="https://github.com/robmarkcole/Useful-python/tree/master/Kaggle/dstl-satellite-imagery-feature-detection"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---airbus-ship-detection-challenge" class="anchor" aria-hidden="true" href="#kaggle---airbus-ship-detection-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Airbus Ship Detection Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/airbus-ship-detection/overview" rel="nofollow"&gt;https://www.kaggle.com/c/airbus-ship-detection/overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rating - medium, most solutions using deep-learning, many kernels, &lt;a href="https://www.kaggle.com/kmader/baseline-u-net-model-part-1" rel="nofollow"&gt;good example kernel&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I believe there was a problem with this dataset, which led to many complaints that the competition was ruined.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---draper---place-images-in-order-of-time" class="anchor" aria-hidden="true" href="#kaggle---draper---place-images-in-order-of-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Draper - place images in order of time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/draper-satellite-image-chronology/data" rel="nofollow"&gt;https://www.kaggle.com/c/draper-satellite-image-chronology/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rating - hard. Not many useful kernels.&lt;/li&gt;
&lt;li&gt;Images are grouped into sets of five, each of which have the same setId. Each image in a set was taken on a different day (but not necessarily at the same time each day). The images for each set cover approximately the same area but are not exactly aligned.&lt;/li&gt;
&lt;li&gt;Kaggle interviews for entrants who &lt;a href="http://blog.kaggle.com/2016/09/15/draper-satellite-image-chronology-machine-learning-solution-vicens-gaitan/" rel="nofollow"&gt;used XGBOOST&lt;/a&gt; and a &lt;a href="http://blog.kaggle.com/2016/09/08/draper-satellite-image-chronology-damien-soukhavong/" rel="nofollow"&gt;hybrid human/ML approach&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---deepsat---classification-challenge" class="anchor" aria-hidden="true" href="#kaggle---deepsat---classification-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Deepsat - classification challenge&lt;/h3&gt;
&lt;p&gt;Not satellite but airborne imagery. Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared. The training and test labels are one-hot encoded 1x6 vectors. Each image patch is size normalized to 28x28 pixels. Data in &lt;code&gt;.mat&lt;/code&gt; Matlab format. JPEG?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://csc.lsu.edu/~saikat/deepsat/" rel="nofollow"&gt;Imagery source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/crawford/deepsat-sat4" rel="nofollow"&gt;Sat4&lt;/a&gt; 500,000 image patches covering four broad land cover classes - &lt;strong&gt;barren land, trees, grassland and a class that consists of all land cover classes other than the above three&lt;/strong&gt; &lt;a href="https://www.kaggle.com/robmarkcole/satellite-image-classification" rel="nofollow"&gt;Example notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/crawford/deepsat-sat6" rel="nofollow"&gt;Sat6&lt;/a&gt; 405,000 image patches each of size 28x28 and covering 6 landcover classes - &lt;strong&gt;barren land, trees, grassland, roads, buildings and water bodies.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alan.do/deep-gradient-boosted-learning-4e33adaf2969" rel="nofollow"&gt;Deep Gradient Boosted Learning article&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---other" class="anchor" aria-hidden="true" href="#kaggle---other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - other&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Satellite + loan data -&amp;gt; &lt;a href="https://www.kaggle.com/reubencpereira/spatial-data-repo" rel="nofollow"&gt;https://www.kaggle.com/reubencpereira/spatial-data-repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-alternative-datasets" class="anchor" aria-hidden="true" href="#alternative-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternative datasets&lt;/h2&gt;
&lt;p&gt;There are a variety of datasets suitable for land classification problems.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-datasets" class="anchor" aria-hidden="true" href="#tensorflow-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tensorflow datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There are a number of remote sensing datasets&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/resisc45" rel="nofollow"&gt;resisc45&lt;/a&gt; - RESISC45 dataset is a publicly available benchmark for Remote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/eurosat" rel="nofollow"&gt;eurosat&lt;/a&gt; - EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/bigearthnet" rel="nofollow"&gt;bigearthnet&lt;/a&gt; - The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. The image patch size on the ground is 1.2 x 1.2 km with variable image size depending on the channel resolution. This is a multi-label dataset with 43 imbalanced labels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-uc-merced" class="anchor" aria-hidden="true" href="#uc-merced"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;UC Merced&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html" rel="nofollow"&gt;http://weegee.vision.ucmerced.edu/datasets/landuse.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Available as a Tensorflow dataset -&amp;gt; &lt;a href="https://www.tensorflow.org/datasets/catalog/uc_merced" rel="nofollow"&gt;https://www.tensorflow.org/datasets/catalog/uc_merced&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is a 21 class land use image dataset meant for research purposes.&lt;/li&gt;
&lt;li&gt;There are 100 RGB TIFF images for each class&lt;/li&gt;
&lt;li&gt;Each image measures 256x256 pixels with a pixel resolution of 1 foot&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aws-datasets" class="anchor" aria-hidden="true" href="#aws-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Landsat -&amp;gt; free viewer at &lt;a href="https://viewer.remotepixel.ca/#3/40/-70.5" rel="nofollow"&gt;remotepixel&lt;/a&gt; and &lt;a href="https://libra.developmentseed.org/" rel="nofollow"&gt;libra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optical, radar, segmented etc. &lt;a href="https://aws.amazon.com/earth/" rel="nofollow"&gt;https://aws.amazon.com/earth/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spacenetchallenge.github.io/datasets/datasetHomePage.html" rel="nofollow"&gt;SpaceNet - WorldView-3&lt;/a&gt; and &lt;a href="https://spark-in.me/post/spacenet-three-challenge" rel="nofollow"&gt;article here&lt;/a&gt;. Also example &lt;a href="https://docs.rastervision.io/en/0.8/quickstart.html" rel="nofollow"&gt;semantic segmentation using Raster Vision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-quilt" class="anchor" aria-hidden="true" href="#quilt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quilt&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Several people have uploaded datasets to &lt;a href="https://quiltdata.com/search/?q=satellite" rel="nofollow"&gt;Quilt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-google-earth-engine" class="anchor" aria-hidden="true" href="#google-earth-engine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Earth Engine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/earth-engine/" rel="nofollow"&gt;https://developers.google.com/earth-engine/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Various imagery and climate datasets, including Landsat &amp;amp; Sentinel imagery&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/earth-engine/python_install" rel="nofollow"&gt;Python API&lt;/a&gt; but  all compute happens on Googles servers&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-weather-datasets" class="anchor" aria-hidden="true" href="#weather-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Weather Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UK met-odffice -&amp;gt; &lt;a href="https://www.metoffice.gov.uk/datapoint" rel="nofollow"&gt;https://www.metoffice.gov.uk/datapoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NASA (make request and emailed when ready) -&amp;gt; &lt;a href="https://search.earthdata.nasa.gov" rel="nofollow"&gt;https://search.earthdata.nasa.gov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NOAA (requires BigQuery) -&amp;gt; &lt;a href="https://www.kaggle.com/noaa/goes16/home" rel="nofollow"&gt;https://www.kaggle.com/noaa/goes16/home&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Time series weather data for several US cities -&amp;gt; &lt;a href="https://www.kaggle.com/selfishgene/historical-hourly-weather-data" rel="nofollow"&gt;https://www.kaggle.com/selfishgene/historical-hourly-weather-data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-online-computing-resources" class="anchor" aria-hidden="true" href="#online-computing-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online computing resources&lt;/h1&gt;
&lt;p&gt;Generally a GPU is required for DL, and this section lists Jupyter environments with GPU available. There is a good overview of online Jupyter envs &lt;a href="https://course-v3.fast.ai/index.html" rel="nofollow"&gt;on the fast.at site&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-google-colab" class="anchor" aria-hidden="true" href="#google-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Colab&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Collaboratory &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;notebooks&lt;/a&gt; with GPU as a backend for free for 12 hours at a time. Note that the GPU may be shared with other users, so if you aren't getting good performance try reloading.&lt;/li&gt;
&lt;li&gt;Tensorflow available &amp;amp; pytorch can be installed, &lt;a href="https://towardsdatascience.com/fast-ai-lesson-1-on-google-colab-free-gpu-d2af89f53604" rel="nofollow"&gt;useful articles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---also-google" class="anchor" aria-hidden="true" href="#kaggle---also-google"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - also Google!&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Free to use&lt;/li&gt;
&lt;li&gt;GPU Kernels - may run for 1 hour&lt;/li&gt;
&lt;li&gt;Tensorflow, pytorch &amp;amp; fast.ai available&lt;/li&gt;
&lt;li&gt;Advantage that many datasets are already available&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@hortonhearsafoo/announcing-fast-ai-part-1-now-available-as-kaggle-kernels-8ef4ca3b9ce6" rel="nofollow"&gt;Read&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-floydhub" class="anchor" aria-hidden="true" href="#floydhub"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Floydhub&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.floydhub.com/" rel="nofollow"&gt;https://www.floydhub.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pricing -&amp;gt; &lt;a href="https://www.floydhub.com/pricing" rel="nofollow"&gt;https://www.floydhub.com/pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Free plan allows 1 job and 10GB storage, but pay for GPU.&lt;/li&gt;
&lt;li&gt;Cloud GPUs (AWS backend)&lt;/li&gt;
&lt;li&gt;Tensorboard&lt;/li&gt;
&lt;li&gt;Version Control for DL&lt;/li&gt;
&lt;li&gt;Deploy Models as REST APIs&lt;/li&gt;
&lt;li&gt;Public Datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;### Clouderizer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://clouderizer.com/" rel="nofollow"&gt;https://clouderizer.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clouderizer $5 month for 200 hours (Robbie plan)&lt;/li&gt;
&lt;li&gt;Run projects locally, on cloud or both.&lt;/li&gt;
&lt;li&gt;SSH terminal, Jupyter Notebooks and Tensorboard are securely accessible from Clouderizer Web Console.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-paperspace" class="anchor" aria-hidden="true" href="#paperspace"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Paperspace&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.paperspace.com/" rel="nofollow"&gt;https://www.paperspace.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1-Click Jupyter Notebooks, GPU on demand&lt;/li&gt;
&lt;li&gt;Pay as you go -&amp;gt; &lt;a href="https://www.paperspace.com/pricing" rel="nofollow"&gt;https://www.paperspace.com/pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Paperspace/paperspace-python"&gt;Python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-crestle" class="anchor" aria-hidden="true" href="#crestle"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Crestle&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.crestle.com/" rel="nofollow"&gt;https://www.crestle.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pricing -&amp;gt; &lt;a href="https://www.crestle.com/pricing" rel="nofollow"&gt;https://www.crestle.com/pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Min plan is $5 per month with 200 hours per month. Pay $0.59/hour for GPU and storage $0.014/GB/day&lt;/li&gt;
&lt;li&gt;Cloud GPU &amp;amp; persistent file store&lt;/li&gt;
&lt;li&gt;Fast.ai lessons pre-installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-interesting-dl-projects" class="anchor" aria-hidden="true" href="#interesting-dl-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interesting DL projects&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-raster-vision-by-azavea" class="anchor" aria-hidden="true" href="#raster-vision-by-azavea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Raster Vision by Azavea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.azavea.com/projects/raster-vision/" rel="nofollow"&gt;https://www.azavea.com/projects/raster-vision/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An open source Python framework for building computer vision models on aerial, satellite, and other large imagery sets.&lt;/li&gt;
&lt;li&gt;Accessible through the &lt;a href="https://www.rasterfoundry.com/" rel="nofollow"&gt;Raster Foundry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/azavea/raster-vision-examples"&gt;Example use cases on open data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-robosat" class="anchor" aria-hidden="true" href="#robosat"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RoboSat&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mapbox/robosat"&gt;https://github.com/mapbox/robosat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generic ecosystem for feature extraction from aerial and satellite imagery.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-robosatpink" class="anchor" aria-hidden="true" href="#robosatpink"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RoboSat.Pink&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A fork of robotsat&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datapink/robosat.pink"&gt;https://github.com/datapink/robosat.pink&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deeposm" class="anchor" aria-hidden="true" href="#deeposm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepOSM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/trailbehind/DeepOSM"&gt;https://github.com/trailbehind/DeepOSM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Train a deep learning net with OpenStreetMap features and satellite imagery.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deepnetsforeo---segmentation" class="anchor" aria-hidden="true" href="#deepnetsforeo---segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepNetsForEO - segmentation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nshaud/DeepNetsForEO"&gt;https://github.com/nshaud/DeepNetsForEO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Uses SegNET for working on remote sensing images using deep learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-skynet-data" class="anchor" aria-hidden="true" href="#skynet-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Skynet-data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/developmentseed/skynet-data"&gt;https://github.com/developmentseed/skynet-data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data pipeline for machine learning with OpenStreetMap&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-production" class="anchor" aria-hidden="true" href="#production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Production&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-custom-rest-api" class="anchor" aria-hidden="true" href="#custom-rest-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom REST API&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Basic &lt;a href="https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html" rel="nofollow"&gt;https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html&lt;/a&gt; with code &lt;a href="https://github.com/jrosebr1/simple-keras-rest-api"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Advanced &lt;a href="https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/" rel="nofollow"&gt;https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/galiboo/olympus"&gt;https://github.com/galiboo/olympus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-serving" class="anchor" aria-hidden="true" href="#tensorflow-serving"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tensorflow Serving&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/serving/" rel="nofollow"&gt;https://www.tensorflow.org/serving/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Official version is python 2 but python 3 build &lt;a href="https://github.com/illagrenan/tensorflow-serving-api-python3"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Another approach is &lt;a href="https://www.tensorflow.org/serving/docker" rel="nofollow"&gt;to use Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. Multiple models, or indeed multiple versions of the same model, can be served simultaneously.  TensorFlow Serving comes with a scheduler that groups individual inference requests into batches for joint execution on a GPU&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-floydhub-1" class="anchor" aria-hidden="true" href="#floydhub-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Floydhub&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Allows exposing model via rest API&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-modeldepot" class="anchor" aria-hidden="true" href="#modeldepot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;modeldepot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://modeldepot.io" rel="nofollow"&gt;https://modeldepot.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ML models hosted&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-image-formats--catalogues" class="anchor" aria-hidden="true" href="#image-formats--catalogues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image formats &amp;amp; catalogues&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We certainly want to consider cloud optimised GeoTiffs &lt;a href="https://www.cogeo.org/" rel="nofollow"&gt;https://www.cogeo.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://terria.io/" rel="nofollow"&gt;https://terria.io/&lt;/a&gt; for pretty catalogues&lt;/li&gt;
&lt;li&gt;&lt;a href="https://remotepixel.ca/projects/index.html#satsearch" rel="nofollow"&gt;Remote pixel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://apps.sentinel-hub.com/eo-browser/" rel="nofollow"&gt;Sentinel-hub eo-browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Large datasets may come in HDF5 format, can view with -&amp;gt; &lt;a href="https://www.hdfgroup.org/downloads/hdfview/" rel="nofollow"&gt;https://www.hdfgroup.org/downloads/hdfview/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Climate data is often in netcdf format, which can be &lt;a href="https://moderndata.plot.ly/weather-maps-in-python-with-mapbox-gl-xarray-and-netcdf4/" rel="nofollow"&gt;opened using xarray&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The xarray docs list a number of ways that data &lt;a href="http://xarray.pydata.org/en/latest/io.html#" rel="nofollow"&gt;can be stored and loaded&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stac---spatiotemporal-asset-catalog" class="anchor" aria-hidden="true" href="#stac---spatiotemporal-asset-catalog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;STAC - SpatioTemporal Asset Catalog&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Specification describing the layout of a catalogue comprising of static files. The aim is that the catalogue is crawlable so it can be indexed by a search engine and make imagery discoverable, without requiring yet another API interface.&lt;/li&gt;
&lt;li&gt;An initiative of &lt;a href="https://www.radiant.earth/" rel="nofollow"&gt;https://www.radiant.earth/&lt;/a&gt; in particular &lt;a href="https://github.com/cholmes"&gt;https://github.com/cholmes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spec at &lt;a href="https://github.com/radiantearth/stac-spec"&gt;https://github.com/radiantearth/stac-spec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browser at &lt;a href="https://github.com/radiantearth/stac-browser"&gt;https://github.com/radiantearth/stac-browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Talk at &lt;a href="https://docs.google.com/presentation/d/1O6W0lMeXyUtPLl-k30WPJIyH1ecqrcWk29Np3bi6rl0/edit#slide=id.p" rel="nofollow"&gt;https://docs.google.com/presentation/d/1O6W0lMeXyUtPLl-k30WPJIyH1ecqrcWk29Np3bi6rl0/edit#slide=id.p&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example catalogue at &lt;a href="https://landsat-stac.s3.amazonaws.com/catalog.json" rel="nofollow"&gt;https://landsat-stac.s3.amazonaws.com/catalog.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chat &lt;a href="https://gitter.im/SpatioTemporal-Asset-Catalog/Lobby" rel="nofollow"&gt;https://gitter.im/SpatioTemporal-Asset-Catalog/Lobby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Several useful repos on &lt;a href="https://github.com/sat-utils"&gt;https://github.com/sat-utils&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-state-of-the-art" class="anchor" aria-hidden="true" href="#state-of-the-art"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;State of the art&lt;/h1&gt;
&lt;p&gt;What are companies doing?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overall trend to using AWS S3 backend for image storage. There are a variety of tools for exploring and having teams collaborate on data on S3, e.g. &lt;a href="https://github.com/quiltdata/t4"&gt;T4&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Bucking the trend, &lt;a href="https://spacenews.com/descartes-labs-platform-adds-airbus-imagery/" rel="nofollow"&gt;Descartes &amp;amp; Airbus are using a google backend&lt;/a&gt; -&amp;gt; checkout &lt;a href="https://github.com/dask/gcsfs"&gt;gcsts for google cloud storage sile-system&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Just speculating, but a &lt;a href="https://github.com/aws-samples/amazon-rekognition-video-analyzer"&gt;serverless pipeline&lt;/a&gt; appears to be where companies are headed for routine compute tasks, whilst providing a Jupyter notebook approach for custom analysis.&lt;/li&gt;
&lt;li&gt;Traditional data formats aren't designed for processing, so new standards are developing such as &lt;a href="http://blog.digitalglobe.com/developers/cloud-optimized-geotiffs-and-the-path-to-accessible-satellite-imagery-analytics/" rel="nofollow"&gt;cloud optimised geotiffs&lt;/a&gt; and &lt;a href="https://github.com/zarr-developers/zarr"&gt;zarr&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-online-platforms-for-geo-analysis" class="anchor" aria-hidden="true" href="#online-platforms-for-geo-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online platforms for Geo analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/pangeo/cloud-native-geoprocessing-of-earth-observation-satellite-data-with-pangeo-997692d91ca2" rel="nofollow"&gt;This article discusses some of the available platforms&lt;/a&gt; -&amp;gt; TLDR Pangeo rocks, but must BYO imagery&lt;/li&gt;
&lt;li&gt;Pangeo - open source resources for parallel processing using Dask and Xarray &lt;a href="http://pangeo.io/index.html" rel="nofollow"&gt;http://pangeo.io/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sandbox.intelligence-airbusds.com/web/" rel="nofollow"&gt;Airbus Sandbox&lt;/a&gt; -&amp;gt; will provide access to imagery&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.descarteslabs.com/" rel="nofollow"&gt;Descartes Labs&lt;/a&gt; -&amp;gt; access to EO imagery from a variety of providers via python API -&amp;gt; not clear which imagery is available (Airbus + others?) or pricing&lt;/li&gt;
&lt;li&gt;DigitalGlobe have a cloud hosted Jupyter notebook platform called &lt;a href="https://platform.digitalglobe.com/gbdx/" rel="nofollow"&gt;GBDX&lt;/a&gt;. Cloud hosting means they can guarantee the infrastructure supports their algorithms, and they appear to be close/closer to deploying DL. &lt;a href="https://notebooks.geobigdata.io/hub/tutorials/list" rel="nofollow"&gt;Tutorial notebooks here&lt;/a&gt;. Only Sentinel-2 and Landsat data on free tier.&lt;/li&gt;
&lt;li&gt;Planet have a &lt;a href="https://developers.planet.com/" rel="nofollow"&gt;Jupyter notebook platform&lt;/a&gt; which can be deployed locally and requires an &lt;a href="https://developers.planet.com/docs/quickstart/getting-started/" rel="nofollow"&gt;API key&lt;/a&gt; (14 days free). They have a python wrapper (2.7..) to their rest API. No price after 14 day trial.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-techniques" class="anchor" aria-hidden="true" href="#techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Techniques&lt;/h1&gt;
&lt;p&gt;This section explores the different techniques (DL, ML &amp;amp; classical) people are applying to common problems in satellite imagery analysis. Classification problems are the most simply addressed via DL, object detection is harder, and cloud detection harder still (niche interest).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-land-classification" class="anchor" aria-hidden="true" href="#land-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Land classification&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very common problem, assign land classification to a pixel based on pixel value, can be addressed via &lt;a href="https://github.com/acgeospatial/Satellite_Imagery_Python/blob/master/Clustering_KMeans-Sentinel2.ipynb"&gt;simple sklearn cluster algorithm&lt;/a&gt; or &lt;a href="https://towardsdatascience.com/land-use-land-cover-classification-with-deep-learning-9a5041095ddb" rel="nofollow"&gt;deep learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Land use is related to classification, but we are trying to detect a scene, e.g. housing, forestry. I have tried CNN -&amp;gt; &lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning/tree/master/land_classification"&gt;See my notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tavgreen/landuse_classification"&gt;Land Use Classification using Convolutional Neural Network in Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1709.00201.pdf" rel="nofollow"&gt;Sea-Land segmentation using DL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Azure/pixel_level_land_classification"&gt;Pixel level segmentation on Azure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hantek/deeplearn_hsi"&gt;Deep Learning-Based Classification of Hyperspectral Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rogerxujiang/dstl_unet"&gt;A U-net based on Tensorflow for objection detection (or segmentation) of satellite images - DSTL dataset but python 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/whats-growing-there-a5618a2e6933" rel="nofollow"&gt;What’s growing there? Using eo-learn and fastai to identify crops from multi-spectral remote sensing data (Sentinel 2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-semantic-segmentation" class="anchor" aria-hidden="true" href="#semantic-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semantic segmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pixel-wise classification&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/matterport/Mask_RCNN"&gt;Instance segmentation with keras - links to satellite examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-change-detection" class="anchor" aria-hidden="true" href="#change-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Monitor water levels, coast lines, size of urban areas, wildfire damage. Note, clouds change often too..!&lt;/li&gt;
&lt;li&gt;Using PCA (python 2, requires updating) -&amp;gt; &lt;a href="https://appliedmachinelearning.blog/2017/11/25/unsupervised-changed-detection-in-multi-temporal-satellite-images-using-pca-k-means-python-code/" rel="nofollow"&gt;https://appliedmachinelearning.blog/2017/11/25/unsupervised-changed-detection-in-multi-temporal-satellite-images-using-pca-k-means-python-code/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Using CNN -&amp;gt; &lt;a href="https://github.com/vbhavank/Unstructured-change-detection-using-CNN"&gt;https://github.com/vbhavank/Unstructured-change-detection-using-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vbhavank/Siamese-neural-network-for-change-detection"&gt;Siamese neural network to detect changes in aerial images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.spaceknow.com/" rel="nofollow"&gt;https://www.spaceknow.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/agr-ayush/Landsat-Time-Series-Analysis-for-Multi-Temporal-Land-Cover-Classification"&gt;LANDSAT Time Series Analysis for Multi-temporal Land Cover Classification using Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.planet.com/pulse/publications/change-detection-in-3d-generating-digital-elevation-models-from-dove-imagery/" rel="nofollow"&gt;Change Detection in 3D: Generating Digital Elevation Models from Dove Imagery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mdpi.com/2072-4292/10/11/1827" rel="nofollow"&gt;Change Detection in Hyperspectral Images Using Recurrent 3D Fully Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hfattahi/PySAR"&gt;PySAR - InSAR (Interferometric Synthetic Aperture Radar) timeseries analysis in python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-image-registration" class="anchor" aria-hidden="true" href="#image-registration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image registration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Image_registration" rel="nofollow"&gt;Wikipedia article on registration&lt;/a&gt; -&amp;gt; register for change detection or &lt;a href="https://mono.software/2018/03/14/Image-stitching/" rel="nofollow"&gt;image stitching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Traditional approach -&amp;gt; define control points, employ RANSAC algorithm&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_correlation" rel="nofollow"&gt;Phase correlation&lt;/a&gt; used to estimate the translation between two images with sub-pixel accuracy, useful for &lt;a href="https://onlinelibrary.wiley.com/doi/10.1002/9781118724194.ch11" rel="nofollow"&gt;allows accurate registration of low resolution imagery onto high resolution imagery&lt;/a&gt;, or register a &lt;a href="https://www.mathworks.com/help/images/registering-an-image-using-normalized-cross-correlation.html" rel="nofollow"&gt;sub-image on a full image&lt;/a&gt; -&amp;gt; Unlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects. &lt;a href="https://github.com/JamieTurrin/Phase-Correlation"&gt;Applied to Landsat images here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-object-detection" class="anchor" aria-hidden="true" href="#object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A typical task is detecting boats on the ocean, which should be simpler than land based challenges owing to blank background in images, but is still challenging and no convincing robust solutions available.&lt;/li&gt;
&lt;li&gt;Intro articles &lt;a href="https://medium.com/earthcube-stories/how-hard-it-is-for-an-ai-to-detect-ships-on-satellite-images-7265e34aadf0" rel="nofollow"&gt;here&lt;/a&gt; and &lt;a href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gbdxstories.digitalglobe.com/boats/" rel="nofollow"&gt;DigitalGlobe article&lt;/a&gt; - they use a combination classical techniques (masks, erodes) to reduce the search space (identifying water via &lt;a href="https://en.wikipedia.org/wiki/Normalized_difference_water_index" rel="nofollow"&gt;NDWI&lt;/a&gt; which requires SWIR) then apply a binary DL classifier on candidate regions of interest. They deploy the final algo &lt;a href="https://github.com/platformstories/boat-detector"&gt;as a task&lt;/a&gt; on their GBDX platform. They propose that in the future an R-CNN may be suitable for the whole process.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/planetlabs/notebooks/blob/master/jupyter-notebooks/ship-detector/01_ship_detector.ipynb"&gt;Planet use non DL felzenszwalb algorithm to detect ships&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/kmader/synthetic-word-ocr/kernels" rel="nofollow"&gt;Segmentation of buildings on kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jyamaoka/LandUse"&gt;Identifying Buildings in Satellite Images with Machine Learning and Quilt&lt;/a&gt; -&amp;gt; NDVI &amp;amp; edge detection via gaussian blur as features, fed to TPOT for training with labels from OpenStreetMap, modelled as a two class problem, “Buildings” and “Nature”.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/" rel="nofollow"&gt;Deep learning for satellite imagery via image segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/the-downlinq/building-extraction-with-yolt2-and-spacenet-data-a926f9ffac4f" rel="nofollow"&gt;Building Extraction with YOLT2 and SpaceNet Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jremillard/images-to-osm"&gt;Find sports fields using Mask R-CNN and overlay on open-street-map&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-cloud-detection" class="anchor" aria-hidden="true" href="#cloud-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cloud detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A subset of the object detection problem, but surprisingly challenging&lt;/li&gt;
&lt;li&gt;From &lt;a href="https://medium.com/sentinel-hub/improving-cloud-detection-with-machine-learning-c09dc5d7cf13" rel="nofollow"&gt;this article on sentinelhub&lt;/a&gt; there are three popular classical algorithms that detects thresholds in multiple bands in order to identify clouds. In the same article they propose using semantic segmentation combined with a CNN for a cloud classifier (excellent review paper &lt;a href="https://arxiv.org/pdf/1704.06857.pdf" rel="nofollow"&gt;here&lt;/a&gt;), but state that this requires too much compute resources.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mdpi.com/2072-4292/8/8/666" rel="nofollow"&gt;This article&lt;/a&gt; compares a number of ML algorithms, random forests, stochastic gradient descent, support vector machines, Bayesian method.&lt;/li&gt;
&lt;li&gt;DL..&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-super-resolution" class="anchor" aria-hidden="true" href="#super-resolution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Super resolution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/the-downlinq/super-resolution-on-satellite-imagery-using-deep-learning-part-1-ec5c5cd3cd2" rel="nofollow"&gt;https://medium.com/the-downlinq/super-resolution-on-satellite-imagery-using-deep-learning-part-1-ec5c5cd3cd2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://modeldepot.io/joe/vdsr-for-super-resolution" rel="nofollow"&gt;https://modeldepot.io/joe/vdsr-for-super-resolution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pansharpening" class="anchor" aria-hidden="true" href="#pansharpening"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pansharpening&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Image fusion of low res multispectral with high res pan band. Several algorithms described &lt;a href="http://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/fundamentals-of-panchromatic-sharpening.htm" rel="nofollow"&gt;in the ArcGIS docs&lt;/a&gt;, with the simplest being taking the mean of the pan and RGB pixel value.&lt;/li&gt;
&lt;li&gt;Does not require DL, classical algos suffice, &lt;a href="http://nbviewer.jupyter.org/github/HyperionAnalytics/PyDataNYC2014/blob/master/panchromatic_sharpening.ipynb" rel="nofollow"&gt;see this notebook&lt;/a&gt; and &lt;a href="https://www.kaggle.com/resolut/panchromatic-sharpening" rel="nofollow"&gt;this kaggle kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mapbox/rio-pansharpen"&gt;https://github.com/mapbox/rio-pansharpen&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stereo-imaging-for-terrain-mapping--dems" class="anchor" aria-hidden="true" href="#stereo-imaging-for-terrain-mapping--dems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stereo imaging for terrain mapping &amp;amp; DEMs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Digital_elevation_model" rel="nofollow"&gt;Wikipedia DEM article&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Phase_correlation" rel="nofollow"&gt;phase correlation&lt;/a&gt; article&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/depth-from-stereo.md"&gt;Intro to depth from stereo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Map terrain from stereo images to produce a digital elevation model (DEM) -&amp;gt; high resolution &amp;amp; paired images required, typically 0.3 m, e.g. &lt;a href="https://dg-cms-uploads-production.s3.amazonaws.com/uploads/document/file/37/DG-WV2ELEVACCRCY-WP.pdf" rel="nofollow"&gt;Worldview&lt;/a&gt; or &lt;a href="https://www.pobonline.com/articles/100233-when-is-satellite-stereo-imagery-the-best-option-for-3d-modeling" rel="nofollow"&gt;GeoEye&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Process of creating a DEM &lt;a href="https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B1/327/2016/isprs-archives-XLI-B1-327-2016.pdf" rel="nofollow"&gt;here&lt;/a&gt; and &lt;a href="https://www.geoimage.com.au/media/brochure_pdfs/Geoimage_DEM_brochure_Oct10_LR.pdf" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pro.arcgis.com/en/pro-app/help/data/imagery/generate-elevation-data-using-the-dems-wizard.htm" rel="nofollow"&gt;ArcGIS can generate DEMs from stereo images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MISS3D/s2p"&gt;https://github.com/MISS3D/s2p&lt;/a&gt; -&amp;gt; produces elevation models from images taken by high resolution optical satellites -&amp;gt; demo code on &lt;a href="https://gfacciol.github.io/IS18/" rel="nofollow"&gt;https://gfacciol.github.io/IS18/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dev.ipol.im/~facciolo/pub/CVPRW2017.pdf" rel="nofollow"&gt;Automatic 3D Reconstruction from Multi-Date Satellite Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Seki_SGM-Nets_Semi-Global_Matching_CVPR_2017_paper.pdf" rel="nofollow"&gt;Semi-global matching with neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/geohackweek/glacierhack_2018"&gt;Predict the fate of glaciers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mrharicot/monodepth"&gt;monodepth - Unsupervised single image depth prediction with CNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jzbontar/mc-cnn"&gt;Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/lidar"&gt;Terrain and hydrological analysis based on LiDAR-derived digital elevation models (DEM) - Python package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-image.org/docs/dev/auto_examples/transform/plot_register_translation.html?highlight=cross%20correlation" rel="nofollow"&gt;Phase correlation in scikit-image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lidar" class="anchor" aria-hidden="true" href="#lidar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lidar&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/geoai/reconstructing-3d-buildings-from-aerial-lidar-with-ai-details-6a81cb3079c0" rel="nofollow"&gt;Reconstructing 3D buildings from aerial LiDAR with Mask R-CNN)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nvdi---vegetation-index" class="anchor" aria-hidden="true" href="#nvdi---vegetation-index"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVDI - vegetation index&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Simple band math &lt;code&gt;ndvi = np.true_divide((ir - r), (ir + r))&lt;/code&gt; but challenging due to the size of the imagery.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/HyperionAnalytics/PyDataNYC2014/blob/master/ndvi_calculation.ipynb" rel="nofollow"&gt;Example notebook local&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pangeo-data/pangeo-example-notebooks/blob/master/landsat8-cog-ndvi.ipynb"&gt;Landsat data in cloud optimised format analysed for NVDI&lt;/a&gt; with &lt;a href="https://medium.com/pangeo/cloud-native-geoprocessing-of-earth-observation-satellite-data-with-pangeo-997692d91ca2" rel="nofollow"&gt;medium article here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-sar" class="anchor" aria-hidden="true" href="#sar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SAR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/upstream/denoising-sentinel-1-radar-images-5f764faffb3e" rel="nofollow"&gt;Removing speckle noise from Sentinel-1 SAR using a CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A dataset which is specifically made for deep learning on SAR and optical imagery is the SEN1-2 dataset, which contains corresponding patch pairs of Sentinel 1 (VV) and 2 (RGB) data. It is the largest manually curated dataset of S1 and S2 products, with corresponding labels for land use/land cover mapping, SAR-optical fusion, segmentation and classification tasks. Paper: &lt;a href="https://elib.dlr.de/128117/1/SEN12MS_Preprint.pdf" rel="nofollow"&gt;https://elib.dlr.de/128117/1/SEN12MS_Preprint.pdf&lt;/a&gt; Data: &lt;a href="https://mediatum.ub.tum.de/1474000" rel="nofollow"&gt;https://mediatum.ub.tum.de/1474000&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/so2sat" rel="nofollow"&gt;so2sat on Tensorflow datasets&lt;/a&gt; - So2Sat LCZ42 is a dataset consisting of co-registered synthetic aperture radar and multispectral optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) label. The dataset is distributed over 42 cities across different continents and cultural regions of the world.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019JB017519" rel="nofollow"&gt;Using Machine Learning to Automatically Detect Volcanic Unrest in a Time Series of Interferograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-aerial-imagery-drones-1" class="anchor" aria-hidden="true" href="#aerial-imagery-drones-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial imagery (drones)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/pedestrian-detection-in-aerial-images-using-retinanet-9053e8a72c6" rel="nofollow"&gt;RetinaNet for pedestrian detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-for-fun" class="anchor" aria-hidden="true" href="#for-fun"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;For fun&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/jacquestardie/6227891818625e4c19c1b1d5bebe4fe4"&gt;Style transfer - see the world in a new way&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-useful-open-source-software" class="anchor" aria-hidden="true" href="#useful-open-source-software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful open source software&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://qgis.org/en/site/" rel="nofollow"&gt;QGIS&lt;/a&gt;- Create, edit, visualise, analyse and publish geospatial information. &lt;a href="https://docs.qgis.org/testing/en/docs/pyqgis_developer_cookbook/intro.html#scripting-in-the-python-console" rel="nofollow"&gt;Python scripting and plugins&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.orfeo-toolbox.org/" rel="nofollow"&gt;Orfeo toolbox&lt;/a&gt; - remote sensing toolbox with python API (just a wrapper to the C code). Do activites such as &lt;a href="https://www.orfeo-toolbox.org/CookBook/Applications/app_Pansharpening.html" rel="nofollow"&gt;pansharpening&lt;/a&gt;, ortho-rectification, image registration, image segmentation &amp;amp; classification. Not much documentation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://appliedimagery.com/download/" rel="nofollow"&gt;QUICK TERRAIN READER - view DEMS, Windows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-useful-github-repos" class="anchor" aria-hidden="true" href="#useful-github-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful github repos&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sshuair/torchvision-enhance"&gt;torchvision-enhance&lt;/a&gt; -&amp;gt; Enhance PyTorch vision for semantic segmentation, multi-channel images and TIF file,...&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sshuair/dl-satellite-docker"&gt;dl-satellite-docker&lt;/a&gt; -&amp;gt; docker files for geospatial analysis, including tensorflow, pytorch, gdal, xgboost...&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-useful-references" class="anchor" aria-hidden="true" href="#useful-references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0" rel="nofollow"&gt;https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/taspinar/sidl/blob/master/notebooks/2_Detecting_road_and_roadtypes_in_sattelite_images.ipynb"&gt;https://github.com/taspinar/sidl/blob/master/notebooks/2_Detecting_road_and_roadtypes_in_sattelite_images.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenGeoscience/geonotebook"&gt;Geonotebooks&lt;/a&gt; with &lt;a href="https://github.com/OpenGeoscience/geonotebook/tree/master/devops/docker"&gt;Docker container&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/acgeospatial/Sentinel-5P/blob/master/Sentinel_5P.ipynb"&gt;Sentinel NetCDF data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open Data Cube - serve up cubes of data &lt;a href="https://www.opendatacube.org/" rel="nofollow"&gt;https://www.opendatacube.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/RemotePixel/remotepixel-api"&gt;Process Satellite data using AWS Lambda functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenDroneMap/ODM"&gt;OpenDroneMap&lt;/a&gt; - generate maps, point clouds, 3D models and DEMs from drone, balloon or kite images.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content--support-this-work" class="anchor" aria-hidden="true" href="#-support-this-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt; Support this work&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/sponsors/robmarkcole"&gt;https://github.com/sponsors/robmarkcole&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you or your business find this work useful please consider becoming a sponsor at the link above, this really helps justify the time I invest in maintaining this repo. As we say in England, 'every little helps' - thanks in advance!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>robmarkcole</author><guid isPermaLink="false">https://github.com/robmarkcole/satellite-image-deep-learning</guid><pubDate>Mon, 06 Jan 2020 00:23:00 GMT</pubDate></item><item><title>simoninithomas/Deep_reinforcement_learning_Course #24 in Jupyter Notebook, Today</title><link>https://github.com/simoninithomas/Deep_reinforcement_learning_Course</link><description>&lt;p&gt;&lt;i&gt;Implementations from the free course Deep Reinforcement Learning with Tensorflow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-reinforcement-learning-course" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning-course"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="nofollow"&gt;Deep Reinforcement Learning Course&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-️-im-currently-updating-the-implementations-january-and-february-some-delay-due-to-job-interviews-with-tensorflow-and-pytorch" class="anchor" aria-hidden="true" href="#️-im-currently-updating-the-implementations-january-and-february-some-delay-due-to-job-interviews-with-tensorflow-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png"&gt;⚠️&lt;/g-emoji&gt; I'm currently updating the implementations (January and February (some delay due to job interviews)) with Tensorflow and PyTorch.&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png"&gt;&lt;img src="https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png" alt="Deep Reinforcement Course with Tensorflow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;  Deep Reinforcement Learning Course is a free series of blog posts and videos &lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;🆕&lt;/g-emoji&gt; about Deep Reinforcement Learning, where we'll &lt;b&gt;learn the main algorithms, and how to implement them with Tensorflow.&lt;/b&gt;
&lt;/p&gt;&lt;p&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt;The articles explain the concept from the big picture to the mathematical details behind it.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; The videos explain how to create the agent with Tensorflow &lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="nofollow"&gt;Syllabus&lt;/a&gt;&lt;br&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content--part-1-introduction-to-reinforcement-learning-article-" class="anchor" aria-hidden="true" href="#-part-1-introduction-to-reinforcement-learning-article-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; Part 1: Introduction to Reinforcement Learning &lt;a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419" rel="nofollow"&gt;ARTICLE&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-part-2-q-learning-with-frozenlake-" class="anchor" aria-hidden="true" href="#part-2-q-learning-with-frozenlake-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 2: Q-learning with FrozenLake &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article--frozenlake-implementation" class="anchor" aria-hidden="true" href="#-article--frozenlake-implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe" rel="nofollow"&gt;ARTICLE&lt;/a&gt; // &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb"&gt;FROZENLAKE IMPLEMENTATION&lt;/a&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--implementing-a-q-learning-agent-that-plays-taxi-v2--" class="anchor" aria-hidden="true" href="#-implementing-a-q-learning-agent-that-plays-taxi-v2--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/q2ZOEFAaaI0" rel="nofollow"&gt;Implementing a Q-learning agent that plays Taxi-v2 &lt;g-emoji class="g-emoji" alias="taxi" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f695.png"&gt;🚕&lt;/g-emoji&gt;&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-3-deep-q-learning-with-doom-" class="anchor" aria-hidden="true" href="#part-3-deep-q-learning-with-doom-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 3: Deep Q-learning with Doom &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article----doom-implementation" class="anchor" aria-hidden="true" href="#-article----doom-implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8" rel="nofollow"&gt;ARTICLE&lt;/a&gt;  //  &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb"&gt;DOOM IMPLEMENTATION&lt;/a&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-a-dqn-agent-that-learns-to-play-atari-space-invaders--" class="anchor" aria-hidden="true" href="#-create-a-dqn-agent-that-learns-to-play-atari-space-invaders--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/gCJyVX98KJ4" rel="nofollow"&gt;Create a DQN Agent that learns to play Atari Space Invaders &lt;g-emoji class="g-emoji" alias="space_invader" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f47e.png"&gt;👾&lt;/g-emoji&gt;&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-4-policy-gradients-with-doom-" class="anchor" aria-hidden="true" href="#part-4-policy-gradients-with-doom-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 4: Policy Gradients with Doom &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article---cartpole-implementation--doom-implementation-" class="anchor" aria-hidden="true" href="#-article---cartpole-implementation--doom-implementation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f" rel="nofollow"&gt;ARTICLE&lt;/a&gt; //  &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb"&gt;CARTPOLE IMPLEMENTATION&lt;/a&gt; // &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Doom/Doom%20REINFORCE%20Monte%20Carlo%20Policy%20gradients.ipynb"&gt;DOOM IMPLEMENTATION&lt;/a&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-doom-deathmatch-" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-doom-deathmatch-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://www.youtube.com/watch?v=wLTQRuizVyE" rel="nofollow"&gt;Create an Agent that learns to play Doom deathmatch&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-3-improvments-in-deep-q-learning-" class="anchor" aria-hidden="true" href="#part-3-improvments-in-deep-q-learning-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 3+: Improvments in Deep Q-Learning &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article--doom-deadly-corridor-implementation-" class="anchor" aria-hidden="true" href="#-article--doom-deadly-corridor-implementation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682" rel="nofollow"&gt;ARTICLE&lt;/a&gt;//  &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20(%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay).ipynb"&gt;Doom Deadly corridor IMPLEMENTATION&lt;/a&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-doom-deadly-corridor-" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-doom-deadly-corridor-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/-Ynjw0Vl3i4" rel="nofollow"&gt;Create an Agent that learns to play Doom Deadly corridor&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-5-advantage-advantage-actor-critic-a2c-" class="anchor" aria-hidden="true" href="#part-5-advantage-advantage-actor-critic-a2c-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 5: Advantage Advantage Actor Critic (A2C) &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article-" class="anchor" aria-hidden="true" href="#-article-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d" rel="nofollow"&gt;ARTICLE&lt;/a&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-sonic-" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-sonic-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/GCfUdkCL7FQ" rel="nofollow"&gt;Create an Agent that learns to play Sonic&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-6-proximal-policy-gradients-" class="anchor" aria-hidden="true" href="#part-6-proximal-policy-gradients-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 6: Proximal Policy Gradients &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article" class="anchor" aria-hidden="true" href="#-article"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e" rel="nofollow"&gt;ARTICLE&lt;/a&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-sonic-the-hedgehog-2-and-3--" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-sonic-the-hedgehog-2-and-3--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="man_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png"&gt;👨‍💻&lt;/g-emoji&gt; &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/PPO%20with%20Sonic%20the%20Hedgehog"&gt;Create an Agent that learns to play Sonic the Hedgehog 2 and 3 &lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-7-curiosity-driven-learning-made-easy-part-i--" class="anchor" aria-hidden="true" href="#part-7-curiosity-driven-learning-made-easy-part-i--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 7: Curiosity Driven Learning made easy Part I &lt;br&gt; &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article--" class="anchor" aria-hidden="true" href="#-article--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359" rel="nofollow"&gt;ARTICLE&lt;/a&gt; &lt;br&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-8-random-network-distillation-with-pytorch--" class="anchor" aria-hidden="true" href="#part-8-random-network-distillation-with-pytorch--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 8: Random Network Distillation with PyTorch  &lt;br&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content--a-trained-rnd-agent-that-learned-to-play-montezumas-revenge-21-hours-of-training-with-a-tesla-k80---" class="anchor" aria-hidden="true" href="#-a-trained-rnd-agent-that-learned-to-play-montezumas-revenge-21-hours-of-training-with-a-tesla-k80---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="man_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png"&gt;👨‍💻&lt;/g-emoji&gt; &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/RND%20Montezuma's%20revenge%20PyTorch"&gt;A trained RND agent that learned to play Montezuma's revenge (21 hours of training with a Tesla K80&lt;/a&gt;  &lt;br&gt; &lt;br&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-any-questions-" class="anchor" aria-hidden="true" href="#any-questions-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Any questions &lt;g-emoji class="g-emoji" alias="man_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png"&gt;👨‍💻&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt; If you have any questions, feel free to ask me: &lt;/p&gt;
&lt;p&gt; &lt;g-emoji class="g-emoji" alias="e-mail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e7.png"&gt;📧&lt;/g-emoji&gt;: &lt;a href="mailto:hello@simoninithomas.com"&gt;hello@simoninithomas.com&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt; Github: &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course"&gt;https://github.com/simoninithomas/Deep_reinforcement_learning_Course&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;🌐&lt;/g-emoji&gt; : &lt;a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="nofollow"&gt;https://simoninithomas.github.io/Deep_reinforcement_learning_Course/&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; Twitter: &lt;a href="https://twitter.com/ThomasSimonini" rel="nofollow"&gt;@ThomasSimonini&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; Don't forget to &lt;b&gt; follow me on &lt;a href="https://twitter.com/ThomasSimonini" rel="nofollow"&gt;twitter&lt;/a&gt;, &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course"&gt;github&lt;/a&gt; and &lt;a href="https://medium.com/@thomassimonini" rel="nofollow"&gt;Medium&lt;/a&gt; to be alerted of the new articles that I publish &lt;/b&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-help--" class="anchor" aria-hidden="true" href="#how-to-help--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to help  &lt;g-emoji class="g-emoji" alias="raised_hands" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64c.png"&gt;🙌&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt;3 ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Clap our articles and like our videos a lot&lt;/strong&gt;:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Share and speak about our articles and videos&lt;/strong&gt;: By sharing our articles and videos you help us to spread the word.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improve our notebooks&lt;/strong&gt;: if you found a bug or &lt;strong&gt;a better implementation&lt;/strong&gt; you can send a pull request.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>simoninithomas</author><guid isPermaLink="false">https://github.com/simoninithomas/Deep_reinforcement_learning_Course</guid><pubDate>Mon, 06 Jan 2020 00:24:00 GMT</pubDate></item><item><title>yandexdataschool/nlp_course #25 in Jupyter Notebook, Today</title><link>https://github.com/yandexdataschool/nlp_course</link><description>&lt;p&gt;&lt;i&gt;YSDA course in Natural Language Processing&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ysda-natural-language-processing-course-" class="anchor" aria-hidden="true" href="#ysda-natural-language-processing-course-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;YSDA Natural Language Processing course &lt;a href="https://mybinder.org/v2/gh/yandexdataschool/nlp_course/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;This is the 2019 version. For previous year' course materials, go to &lt;a href="https://github.com/yandexdataschool/nlp_course/tree/master"&gt;this branch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lecture and seminar materials for each week are in ./week* folders&lt;/li&gt;
&lt;li&gt;YSDA homework deadlines will be listed in Anytask (&lt;a href="https://github.com/yandexdataschool/nlp_course/wiki/Homeworks-and-grading"&gt;read more&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Any technical issues, ideas, bugs in course materials, contribution ideas - add an &lt;a href="https://github.com/yandexdataschool/nlp_course/issues"&gt;issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Installing libraries and troubleshooting: &lt;a href="https://github.com/yandexdataschool/nlp_course/issues/1"&gt;this thread&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week01_embeddings"&gt;&lt;strong&gt;week01&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Embeddings&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Word embeddings. Distributional semantics, LSA, Word2Vec, GloVe. Why and when we need them.&lt;/li&gt;
&lt;li&gt;Seminar: Playing with word and sentence embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week02_classification"&gt;&lt;strong&gt;week02&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Text classification&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Text classification. Classical approaches for text representation: BOW, TF-IDF. Neural approaches: embeddings, convolutions, RNNs&lt;/li&gt;
&lt;li&gt;Seminar: Salary prediction with convolutional neural networks; explaining network predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week03_lm"&gt;&lt;strong&gt;week03&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Language models: N-gram and neural approaches; visualizing trained models&lt;/li&gt;
&lt;li&gt;Seminar: Generating ArXiv papers with language models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week04_seq2seq"&gt;&lt;strong&gt;week04&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Seq2seq/Attention&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Seq2seq: encoder-decoder framework. Attention: Bahdanau model. Self-attention, Transformer. Analysis of attention heads in Transformer.&lt;/li&gt;
&lt;li&gt;Seminar: Machine translation of hotel and hostel descriptions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week05_em"&gt;&lt;strong&gt;week05&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Expectation-Maximization&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Expectation-Maximization and Hidden Markov Models&lt;/li&gt;
&lt;li&gt;Seminar: Implementing expectation maximization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week06_mt"&gt;&lt;strong&gt;week06&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Machine Translation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Word Alignment Models, Noisy Channel, Machine Translation.&lt;/li&gt;
&lt;li&gt;Seminar: Introduction to word alignment assignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-contributors--course-staff" class="anchor" aria-hidden="true" href="#contributors--course-staff"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors &amp;amp; course staff&lt;/h1&gt;
&lt;p&gt;Course materials and teaching performed by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://lena-voita.github.io" rel="nofollow"&gt;Elena Voita&lt;/a&gt; - course admin, lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kovarsky"&gt;Boris Kovarsky&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/drt7"&gt;David Talbot&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/esgv"&gt;Sergey Gubanov&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/justheuristic"&gt;Just Heuristic&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yandexdataschool</author><guid isPermaLink="false">https://github.com/yandexdataschool/nlp_course</guid><pubDate>Mon, 06 Jan 2020 00:25:00 GMT</pubDate></item></channel></rss>