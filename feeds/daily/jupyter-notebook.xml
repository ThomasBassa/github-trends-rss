<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, Today</title><link>https://github.com/trending/jupyter-notebook?since=daily</link><description>The top repositories on GitHub for jupyter-notebook, measured daily</description><pubDate>Fri, 08 Nov 2019 01:07:49 GMT</pubDate><lastBuildDate>Fri, 08 Nov 2019 01:07:49 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>chiphuyen/python-is-cool #1 in Jupyter Notebook, Today</title><link>https://github.com/chiphuyen/python-is-cool</link><description>&lt;p&gt;&lt;i&gt;Cool Python features for machine learning that I used to be too afraid to use&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-is-cool" class="anchor" aria-hidden="true" href="#python-is-cool"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;python-is-cool&lt;/h1&gt;
&lt;p&gt;A gentle guide to the Python features that I didn't know existed or was too afraid to use. This will be updated as I learn more and become less lazy.&lt;/p&gt;
&lt;p&gt;This uses &lt;code&gt;python &amp;gt;= 3.6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;GitHub has problem rendering Jupyter notebook so I copied the content here. I still keep the notebook in case you want to clone and run it on your machine, but you can also click the Binder badge below and run it in your browser.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/chiphuyen/python-is-cool/master?urlpath=lab/tree/cool-python-tips.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-1-lambda-map-filter-reduce" class="anchor" aria-hidden="true" href="#1-lambda-map-filter-reduce"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Lambda, map, filter, reduce&lt;/h2&gt;
&lt;p&gt;The lambda keyword is used to create inline functions. The functions&lt;code&gt;square_fn&lt;/code&gt; and &lt;code&gt;square_ld&lt;/code&gt; below are identical.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;square_fn&lt;/span&gt;(&lt;span class="pl-smi"&gt;x&lt;/span&gt;):
    &lt;span class="pl-k"&gt;return&lt;/span&gt; x &lt;span class="pl-k"&gt;*&lt;/span&gt; x

square_ld &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x &lt;span class="pl-k"&gt;*&lt;/span&gt; x

&lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;):
    &lt;span class="pl-k"&gt;assert&lt;/span&gt; square_fn(i) &lt;span class="pl-k"&gt;==&lt;/span&gt; square_ld(i)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its quick declaration makes &lt;code&gt;lambda&lt;/code&gt; functions ideal for use in callbacks, and when functions are to be passed as arguments to other functions. They are especially useful when used in conjunction with functions like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, and &lt;code&gt;reduce&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;map(fn, iterable)&lt;/code&gt; applies the &lt;code&gt;fn&lt;/code&gt; to all elements of the &lt;code&gt;iterable&lt;/code&gt; (e.g. list, set, dictionary, tuple, string) and returns a map object.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;nums &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;333&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;2323&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;2230&lt;/span&gt;, &lt;span class="pl-c1"&gt;40&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;34&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;]
nums_squared &lt;span class="pl-k"&gt;=&lt;/span&gt; [num &lt;span class="pl-k"&gt;*&lt;/span&gt; num &lt;span class="pl-k"&gt;for&lt;/span&gt; num &lt;span class="pl-k"&gt;in&lt;/span&gt; nums]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(nums_squared)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.1111111&lt;/span&gt;, &lt;span class="pl-c1"&gt;2263.04081632&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.085147&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.384083&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.44444444&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the same as calling using &lt;code&gt;map&lt;/code&gt; with a callback function.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;nums_squared_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;map&lt;/span&gt;(square_fn, nums)
nums_squared_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x &lt;span class="pl-k"&gt;*&lt;/span&gt; x, nums)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-c1"&gt;list&lt;/span&gt;(nums_squared_1))

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.1111111&lt;/span&gt;, &lt;span class="pl-c1"&gt;2263.04081632&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.085147&lt;/span&gt;, &lt;span class="pl-c1"&gt;1.384083&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.44444444&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also use &lt;code&gt;map&lt;/code&gt; with more than one iterable. For example, if you want to calculate the mean squared error of a simple linear function &lt;code&gt;f(x) = ax + b&lt;/code&gt; with the true label &lt;code&gt;labels&lt;/code&gt;, these two methods are equivalent:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;a, b &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;0.5&lt;/span&gt;
xs &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;]
labels &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;6.4&lt;/span&gt;, &lt;span class="pl-c1"&gt;8.9&lt;/span&gt;, &lt;span class="pl-c1"&gt;10.9&lt;/span&gt;, &lt;span class="pl-c1"&gt;15.3&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Method 1: using a loop&lt;/span&gt;
errors &lt;span class="pl-k"&gt;=&lt;/span&gt; []
&lt;span class="pl-k"&gt;for&lt;/span&gt; i, x &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;enumerate&lt;/span&gt;(xs):
    errors.append((a &lt;span class="pl-k"&gt;*&lt;/span&gt; x &lt;span class="pl-k"&gt;+&lt;/span&gt; b &lt;span class="pl-k"&gt;-&lt;/span&gt; labels[i]) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;)
result1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;sum&lt;/span&gt;(errors) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;0.5&lt;/span&gt; &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(xs)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Method 2: using map&lt;/span&gt;
diffs &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;map&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: (a &lt;span class="pl-k"&gt;*&lt;/span&gt; x &lt;span class="pl-k"&gt;+&lt;/span&gt; b &lt;span class="pl-k"&gt;-&lt;/span&gt; y) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt;, xs, labels)
result2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;sum&lt;/span&gt;(diffs) &lt;span class="pl-k"&gt;**&lt;/span&gt; &lt;span class="pl-c1"&gt;0.5&lt;/span&gt; &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(xs)

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(result1, result2)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.35089172119045514&lt;/span&gt; &lt;span class="pl-c1"&gt;0.35089172119045514&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that objects returned by &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;filter&lt;/code&gt; are iterators, which means that their values aren't stored but generated as needed. After you've called &lt;code&gt;sum(diffs)&lt;/code&gt;, &lt;code&gt;diffs&lt;/code&gt; becomes empty. If you want to keep all elements in &lt;code&gt;diffs&lt;/code&gt;, convert it to a list using &lt;code&gt;list(diffs)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;filter(fn, iterable)&lt;/code&gt; works the same way as &lt;code&gt;map&lt;/code&gt;, except that &lt;code&gt;fn&lt;/code&gt; returns a boolean value and &lt;code&gt;filter&lt;/code&gt; returns all the elements of the &lt;code&gt;iterable&lt;/code&gt; for which the &lt;code&gt;fn&lt;/code&gt; returns True.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;bad_preds &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;filter&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: x &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;, errors)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-c1"&gt;list&lt;/span&gt;(bad_preds))

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.8100000000000006&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.6400000000000011&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;reduce(fn, iterable, initializer)&lt;/code&gt; is used when we want to iteratively apply an operator to all elements in a list. For example, if we want to calculate the product of all elements in a list:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;product &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; num &lt;span class="pl-k"&gt;in&lt;/span&gt; nums:
    product &lt;span class="pl-k"&gt;*=&lt;/span&gt; num
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(product)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;12.95564683272412&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is equivalent to:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; functools &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-v"&gt;reduce&lt;/span&gt;
product &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-v"&gt;reduce&lt;/span&gt;(&lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;, &lt;span class="pl-smi"&gt;y&lt;/span&gt;: x &lt;span class="pl-k"&gt;*&lt;/span&gt; y, nums)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(product)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;12.95564683272412&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-note-on-the-performance-of-lambda-functions" class="anchor" aria-hidden="true" href="#note-on-the-performance-of-lambda-functions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note on the performance of lambda functions&lt;/h3&gt;
&lt;p&gt;Lambda functions are meant for one time use. Each time &lt;code&gt;lambda x: dosomething(x)&lt;/code&gt; is called, the function has to be created, which hurts the performance if you call &lt;code&gt;lambda x: dosomething(x)&lt;/code&gt; multiple times (e.g. when you pass it inside &lt;code&gt;reduce&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;When you assign a name to the lambda function as in &lt;code&gt;fn = lambda x: dosomething(x)&lt;/code&gt;, its performance is slightly slower than the same function defined using &lt;code&gt;def&lt;/code&gt;, but the difference is negligible. See &lt;a href="https://stackoverflow.com/questions/26540885/lambda-is-slower-than-function-call-in-python-why" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Even though I find lambdas cool, I personally recommend using named functions when you can for the sake of clarity.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-2-list-manipulation" class="anchor" aria-hidden="true" href="#2-list-manipulation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. List manipulation&lt;/h2&gt;
&lt;p&gt;Python lists are super cool.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-21-unpacking" class="anchor" aria-hidden="true" href="#21-unpacking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.1 Unpacking&lt;/h3&gt;
&lt;p&gt;We can unpack a list by each element like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;]
a, b, c, d &lt;span class="pl-k"&gt;=&lt;/span&gt; elems
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(a, b, c, d)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt; &lt;span class="pl-c1"&gt;2&lt;/span&gt; &lt;span class="pl-c1"&gt;3&lt;/span&gt; &lt;span class="pl-c1"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can also unpack a list like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;a, &lt;span class="pl-k"&gt;*&lt;/span&gt;new_elems, d &lt;span class="pl-k"&gt;=&lt;/span&gt; elems
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(a)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(new_elems)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(d)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
    [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;]
    &lt;span class="pl-c1"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-22-slicing" class="anchor" aria-hidden="true" href="#22-slicing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.2 Slicing&lt;/h3&gt;
&lt;p&gt;We know that we can reverse a list using &lt;code&gt;[::-1]&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems[::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;])

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;9&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The syntax &lt;code&gt;[x:y:z]&lt;/code&gt; means "take every &lt;code&gt;z&lt;/code&gt;th element of a list from index &lt;code&gt;x&lt;/code&gt; to index &lt;code&gt;y&lt;/code&gt;". When &lt;code&gt;z&lt;/code&gt; is negative, it indicates going backwards. When &lt;code&gt;x&lt;/code&gt; isn't specified, it defaults to the first element of the list in the direction you are traversing the list. When &lt;code&gt;y&lt;/code&gt; isn't specified, it defaults to the last element of the list. So if we want to take every 2th element of a list, we use &lt;code&gt;[::2]&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;evens &lt;span class="pl-k"&gt;=&lt;/span&gt; elems[::&lt;span class="pl-c1"&gt;2&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(evens)

reversed_evens &lt;span class="pl-k"&gt;=&lt;/span&gt; elems[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;::&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(reversed_evens)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;]
    [&lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;0&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can also use slicing to delete all the even numbers in the list.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;del&lt;/span&gt; elems[::&lt;span class="pl-c1"&gt;2&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-23-insertion" class="anchor" aria-hidden="true" href="#23-insertion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.3 Insertion&lt;/h3&gt;
&lt;p&gt;We can change the value of an element in a list to another value.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
elems[&lt;span class="pl-c1"&gt;1&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we want to replace the element at an index with multiple elements, e.g. replace the value &lt;code&gt;1&lt;/code&gt; with 3 values &lt;code&gt;20, 30, 40&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
elems[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;2&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;30&lt;/span&gt;, &lt;span class="pl-c1"&gt;40&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;20&lt;/span&gt;, &lt;span class="pl-c1"&gt;30&lt;/span&gt;, &lt;span class="pl-c1"&gt;40&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we want to insert 3 values &lt;code&gt;0.2, 0.3, 0.5&lt;/code&gt; between element at index 0 and element at index 1:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;elems &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(&lt;span class="pl-c1"&gt;range&lt;/span&gt;(&lt;span class="pl-c1"&gt;10&lt;/span&gt;))
elems[&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;1&lt;/span&gt;] &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.3&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;]
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(elems)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;0&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.2&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.3&lt;/span&gt;, &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;, &lt;span class="pl-c1"&gt;9&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-24-flattening" class="anchor" aria-hidden="true" href="#24-flattening"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.4 Flattening&lt;/h3&gt;
&lt;p&gt;We can flatten a list of lists using &lt;code&gt;sum&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;list_of_lists &lt;span class="pl-k"&gt;=&lt;/span&gt; [[&lt;span class="pl-c1"&gt;1&lt;/span&gt;], [&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;], [&lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;]]
&lt;span class="pl-c1"&gt;sum&lt;/span&gt;(list_of_lists, [])

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we have nested lists, we can recursively flatten it. That's another beauty of lambda functions -- we can use it in the same line as its creation.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;nested_lists &lt;span class="pl-k"&gt;=&lt;/span&gt; [[&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;], [[&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-c1"&gt;4&lt;/span&gt;], [&lt;span class="pl-c1"&gt;5&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;], [[&lt;span class="pl-c1"&gt;7&lt;/span&gt;, &lt;span class="pl-c1"&gt;8&lt;/span&gt;], [&lt;span class="pl-c1"&gt;9&lt;/span&gt;, &lt;span class="pl-c1"&gt;10&lt;/span&gt;], [[&lt;span class="pl-c1"&gt;11&lt;/span&gt;, [&lt;span class="pl-c1"&gt;12&lt;/span&gt;, &lt;span class="pl-c1"&gt;13&lt;/span&gt;]]]]]]
flatten &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-k"&gt;lambda&lt;/span&gt; &lt;span class="pl-smi"&gt;x&lt;/span&gt;: [y &lt;span class="pl-k"&gt;for&lt;/span&gt; l &lt;span class="pl-k"&gt;in&lt;/span&gt; x &lt;span class="pl-k"&gt;for&lt;/span&gt; y &lt;span class="pl-k"&gt;in&lt;/span&gt; flatten(l)] &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;type&lt;/span&gt;(x) &lt;span class="pl-k"&gt;is&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt; &lt;span class="pl-k"&gt;else&lt;/span&gt; [x]
flatten(nested_lists)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This line of code is from&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; https://github.com/sahands/python-by-example/blob/master/python-by-example.rst#flattening-lists&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-25-list-vs-generator" class="anchor" aria-hidden="true" href="#25-list-vs-generator"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.5 List vs generator&lt;/h3&gt;
&lt;p&gt;To illustrate the difference between a list and a generator, let's look at an example of creating n-grams out of a list of tokens.&lt;/p&gt;
&lt;p&gt;One way to create n-grams is to use a sliding window.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;ngrams&lt;/span&gt;(&lt;span class="pl-smi"&gt;tokens&lt;/span&gt;, &lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    length &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokens)
    grams &lt;span class="pl-k"&gt;=&lt;/span&gt; []
    &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(length &lt;span class="pl-k"&gt;-&lt;/span&gt; n &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;):
        grams.append(tokens[i:i&lt;span class="pl-k"&gt;+&lt;/span&gt;n])
    &lt;span class="pl-k"&gt;return&lt;/span&gt; grams

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngrams(tokens, &lt;span class="pl-c1"&gt;3&lt;/span&gt;))

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
     [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
     [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;],
     [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the above example, we have to store all the n-grams at the same time. If the text has m tokens, then the memory requirement is &lt;code&gt;O(nm)&lt;/code&gt;, which can be problematic when m is large.&lt;/p&gt;
&lt;p&gt;Instead of using a list to store all n-grams, we can use a generator that generates the next n-gram when it's asked for. This is known as lazy evaluation. We can make the function &lt;code&gt;ngrams&lt;/code&gt; returns a generator using the keyword &lt;code&gt;yield&lt;/code&gt;. Then the memory requirement is &lt;code&gt;O(m+n)&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;ngrams&lt;/span&gt;(&lt;span class="pl-smi"&gt;tokens&lt;/span&gt;, &lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    length &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokens)
    &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(length &lt;span class="pl-k"&gt;-&lt;/span&gt; n &lt;span class="pl-k"&gt;+&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;):
        &lt;span class="pl-k"&gt;yield&lt;/span&gt; tokens[i:i&lt;span class="pl-k"&gt;+&lt;/span&gt;n]

ngrams_generator &lt;span class="pl-k"&gt;=&lt;/span&gt; ngrams(tokens, &lt;span class="pl-c1"&gt;3&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngrams_generator)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;generator &lt;span class="pl-c1"&gt;object&lt;/span&gt; ngrams at &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;1069b26d0&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="pl-k"&gt;for&lt;/span&gt; ngram &lt;span class="pl-k"&gt;in&lt;/span&gt; ngrams_generator:
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngram)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
    [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Another way to generate n-grams is to use slices to create lists: &lt;code&gt;[0, 1, ..., -n]&lt;/code&gt;, &lt;code&gt;[1, 2, ..., -n+1]&lt;/code&gt;, ..., &lt;code&gt;[n-1, n, ..., -1]&lt;/code&gt;, and then &lt;code&gt;zip&lt;/code&gt; them together.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;ngrams&lt;/span&gt;(&lt;span class="pl-smi"&gt;tokens&lt;/span&gt;, &lt;span class="pl-smi"&gt;n&lt;/span&gt;):
    length &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokens)
    slices &lt;span class="pl-k"&gt;=&lt;/span&gt; (tokens[i:length&lt;span class="pl-k"&gt;-&lt;/span&gt;n&lt;span class="pl-k"&gt;+&lt;/span&gt;i&lt;span class="pl-k"&gt;+&lt;/span&gt;&lt;span class="pl-c1"&gt;1&lt;/span&gt;] &lt;span class="pl-k"&gt;for&lt;/span&gt; i &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;range&lt;/span&gt;(n))
    &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;zip&lt;/span&gt;(&lt;span class="pl-k"&gt;*&lt;/span&gt;slices)

ngrams_generator &lt;span class="pl-k"&gt;=&lt;/span&gt; ngrams(tokens, &lt;span class="pl-c1"&gt;3&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngrams_generator)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;&lt;span class="pl-c1"&gt;zip&lt;/span&gt; &lt;span class="pl-c1"&gt;object&lt;/span&gt; at &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;1069a7dc8&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; zip objects are generators&lt;/span&gt;

&lt;span class="pl-k"&gt;for&lt;/span&gt; ngram &lt;span class="pl-k"&gt;in&lt;/span&gt; ngrams_generator:
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(ngram)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;want&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;go&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;to&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;school&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that to create slices, we use &lt;code&gt;(tokens[...] for i in range(n))&lt;/code&gt; instead of &lt;code&gt;[tokens[...] for i in range(n)]&lt;/code&gt;. &lt;code&gt;[]&lt;/code&gt; is the normal list comprehension that returns a list. &lt;code&gt;()&lt;/code&gt; returns a generator.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-3-classes-and-magic-methods" class="anchor" aria-hidden="true" href="#3-classes-and-magic-methods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Classes and magic methods&lt;/h2&gt;
&lt;p&gt;In Python, magic methods are prefixed and suffixed with the double underscore &lt;code&gt;__&lt;/code&gt;, also known as dunder. The most wellknown magic method is probably &lt;code&gt;__init__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we try to print out a Node object, however, it's not very interpretable.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;root &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;5&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; &amp;lt;__main__.Node object at 0x1069c4518&amp;gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ideally, when user prints out a node, we want to print out the node's value and the values of its children if it has children. To do so, we use the magic method &lt;code&gt;__repr__&lt;/code&gt;, which must return a printable object, like string.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__repr__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;):
        strings &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;value: &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.value&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;]
        strings.append(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;left: &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.left.value&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;left: None&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
        strings.append(&lt;span class="pl-s"&gt;f&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-s"&gt;right: &lt;/span&gt;&lt;span class="pl-c1"&gt;{&lt;/span&gt;&lt;span class="pl-c1"&gt;self&lt;/span&gt;.right.value&lt;span class="pl-c1"&gt;}&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;right: None&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;, &lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;.join(strings)

left &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;4&lt;/span&gt;)
root &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;5&lt;/span&gt;, left)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; value: 5, left: 4, right: None&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We'd also like to compare two nodes by comparing their values. To do so, we overload the operator &lt;code&gt;==&lt;/code&gt; with &lt;code&gt;__eq__&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt; with &lt;code&gt;__lt__&lt;/code&gt;, and &lt;code&gt;&amp;gt;=&lt;/code&gt; with &lt;code&gt;__ge__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__eq__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;other&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;==&lt;/span&gt; other.value

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__lt__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;other&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; other.value

    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__ge__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;other&lt;/span&gt;):
        &lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;&amp;gt;=&lt;/span&gt; other.value


left &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;4&lt;/span&gt;)
root &lt;span class="pl-k"&gt;=&lt;/span&gt; Node(&lt;span class="pl-c1"&gt;5&lt;/span&gt;, left)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(left &lt;span class="pl-k"&gt;==&lt;/span&gt; root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; False&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(left &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; True&lt;/span&gt;
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(left &lt;span class="pl-k"&gt;&amp;gt;=&lt;/span&gt; root) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; False&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For a comprehensive list of supported magic methods &lt;a href="https://www.tutorialsteacher.com/python/magic-methods-in-python" rel="nofollow"&gt;here&lt;/a&gt; or see the official Python documentation &lt;a href="https://docs.python.org/3/reference/datamodel.html#special-method-names" rel="nofollow"&gt;here&lt;/a&gt; (slightly harder to read).&lt;/p&gt;
&lt;p&gt;Some of the methods that I highly recommend:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__len__&lt;/code&gt;: to overload the &lt;code&gt;len()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__str__&lt;/code&gt;: to overload the &lt;code&gt;str()&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;__iter__&lt;/code&gt;: if you want to your objects to be iterators. This also allows you to call &lt;code&gt;next()&lt;/code&gt; on your object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For classes like Node where we know for sure all the attributes they can support (in the case of Node, they are &lt;code&gt;value&lt;/code&gt;, &lt;code&gt;left&lt;/code&gt;, and &lt;code&gt;right&lt;/code&gt;), we might want to use &lt;code&gt;__slots__&lt;/code&gt; to denote those values for both performance boost and memory saving. For a comprehensive understanding of pros and cons of &lt;code&gt;__slots__&lt;/code&gt;, see this &lt;a href="https://stackoverflow.com/a/28059785/5029595" rel="nofollow"&gt;absolutely amazing answer by Aaron Hall on StackOverflow&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Node&lt;/span&gt;:
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt; A struct to denote the node of a binary tree.&lt;/span&gt;
&lt;span class="pl-s"&gt;    It contains a value and pointers to left and right children.&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-c1"&gt;__slots__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;value&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;left&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;right&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;value&lt;/span&gt;, &lt;span class="pl-smi"&gt;left&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;, &lt;span class="pl-smi"&gt;right&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;None&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.value &lt;span class="pl-k"&gt;=&lt;/span&gt; value
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.left &lt;span class="pl-k"&gt;=&lt;/span&gt; left
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.right &lt;span class="pl-k"&gt;=&lt;/span&gt; right&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-4-local-namespace-objects-attributes" class="anchor" aria-hidden="true" href="#4-local-namespace-objects-attributes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. local namespace, object's attributes&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;locals()&lt;/code&gt; function returns a dictionary containing the variables defined in the local namespace.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Model1&lt;/span&gt;:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;hidden_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-smi"&gt;num_layers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-smi"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-4&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-c1"&gt;locals&lt;/span&gt;())
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.hidden_size &lt;span class="pl-k"&gt;=&lt;/span&gt; hidden_size
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.num_layers &lt;span class="pl-k"&gt;=&lt;/span&gt; num_layers
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.learning_rate &lt;span class="pl-k"&gt;=&lt;/span&gt; learning_rate

model1 &lt;span class="pl-k"&gt;=&lt;/span&gt; Model1()

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;self&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;__main__.Model1 &lt;span class="pl-c1"&gt;object&lt;/span&gt; at &lt;span class="pl-c1"&gt;&lt;span class="pl-k"&gt;0x&lt;/span&gt;1069b1470&lt;/span&gt;&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All attributes of an object are stored in its &lt;code&gt;__dict__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;print&lt;/span&gt;(model1.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that manually assigning each of the arguments to an attribute can be quite tiring when the list of the arguments is large. To avoid this, we can directly assign the list of arguments to the object's &lt;code&gt;__dict__&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Model2&lt;/span&gt;:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-smi"&gt;hidden_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-smi"&gt;num_layers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-smi"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-4&lt;/span&gt;):
        params &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;locals&lt;/span&gt;()
        &lt;span class="pl-k"&gt;del&lt;/span&gt; params[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;self&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; params

model2 &lt;span class="pl-k"&gt;=&lt;/span&gt; Model2()
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(model2.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This can be especially convenient when the object is initiated using the catch-all &lt;code&gt;**kwargs&lt;/code&gt;, though the use of &lt;code&gt;**kwargs&lt;/code&gt; should be reduced to the minimum.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Model3&lt;/span&gt;:
    &lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-c1"&gt;__init__&lt;/span&gt;(&lt;span class="pl-smi"&gt;&lt;span class="pl-smi"&gt;self&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-k"&gt;**&lt;/span&gt;&lt;span class="pl-smi"&gt;kwargs&lt;/span&gt;):
        &lt;span class="pl-c1"&gt;self&lt;/span&gt;.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; kwargs

model3 &lt;span class="pl-k"&gt;=&lt;/span&gt; Model3(&lt;span class="pl-v"&gt;hidden_size&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;num_layers&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-4&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(model3.&lt;span class="pl-c1"&gt;__dict__&lt;/span&gt;)

&lt;span class="pl-k"&gt;==&amp;gt;&lt;/span&gt; {&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;hidden_size&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;num_layers&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;3&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;learning_rate&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;0.0003&lt;/span&gt;}&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-5-wild-import" class="anchor" aria-hidden="true" href="#5-wild-import"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5. Wild import&lt;/h2&gt;
&lt;p&gt;Often, you run into this wild import &lt;code&gt;*&lt;/code&gt; that looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;file.py&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;    &lt;span class="pl-k"&gt;from&lt;/span&gt; parts &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is irresponsible because it will import everything in module, even the imports of that module. For example, if &lt;code&gt;parts.py&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;parts.py&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Encoder&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Decoder&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Loss&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;helper&lt;/span&gt;(&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;args&lt;/span&gt;, &lt;span class="pl-k"&gt;**&lt;/span&gt;&lt;span class="pl-smi"&gt;kwargs&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;utils&lt;/span&gt;(&lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;args&lt;/span&gt;, &lt;span class="pl-k"&gt;**&lt;/span&gt;&lt;span class="pl-smi"&gt;kwargs&lt;/span&gt;):
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since &lt;code&gt;parts.py&lt;/code&gt; doesn't have &lt;code&gt;__all__&lt;/code&gt; specified, &lt;code&gt;file.py&lt;/code&gt; will import Encoder, Decoder, Loss, utils, helper together with numpy and tensorflow.&lt;/p&gt;
&lt;p&gt;If we intend that only Encoder, Decoder, and Loss are ever to be imported and used in another module, we should specify that in &lt;code&gt;parts.py&lt;/code&gt; using the &lt;code&gt;__all__&lt;/code&gt; keyword.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;parts.py&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt; &lt;span class="pl-c1"&gt;__all__&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Encoder&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Decoder&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Loss&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
&lt;span class="pl-k"&gt;import&lt;/span&gt; numpy
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow

&lt;span class="pl-k"&gt;class&lt;/span&gt; &lt;span class="pl-en"&gt;Encoder&lt;/span&gt;:
    &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, if some user irresponsibly does a wild import with &lt;code&gt;parts&lt;/code&gt;, they can only import Encoder, Decoder, Loss. Personally, I also find &lt;code&gt;__all__&lt;/code&gt; helpful as it gives me an overview of the module.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>chiphuyen</author><guid isPermaLink="false">https://github.com/chiphuyen/python-is-cool</guid><pubDate>Fri, 08 Nov 2019 00:01:00 GMT</pubDate></item><item><title>naganandy/graph-based-deep-learning-literature #2 in Jupyter Notebook, Today</title><link>https://github.com/naganandy/graph-based-deep-learning-literature</link><description>&lt;p&gt;&lt;i&gt;links to conference publications in graph-based deep learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-graph-based-deep-learning-literature" class="anchor" aria-hidden="true" href="#graph-based-deep-learning-literature"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph-based deep learning literature&lt;/h1&gt;
&lt;p&gt;The repository contains links to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md"&gt;conference publications&lt;/a&gt; and &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature#top-10-most-cited-publications"&gt;the top 10 most cited publications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature#relevant-workshops"&gt;related workshops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature#surveys--literature-reviews"&gt;surveys / literature reviews&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;in graph-based deep learning. The &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#conferences"&gt;links to conference publications&lt;/a&gt; are arranged in the reverse chronological order of conference dates from the conferences below. Please click on a year below beside a conference name to see publications of the conference in that year.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-machine-learning-conferences" class="anchor" aria-hidden="true" href="#machine-learning-conferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine learning conferences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-neurips----2019--2018--2017--2016" class="anchor" aria-hidden="true" href="#neurips----2019--2018--2017--2016"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://nips.cc/" rel="nofollow"&gt;NeurIPS&lt;/a&gt;  - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#neurips-2019-dec"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#neurips-2018-dec"&gt;2018&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#nips-2017"&gt;2017&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#nips-2016"&gt;2016&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-icml---2019--2018--2017" class="anchor" aria-hidden="true" href="#icml---2019--2018--2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://icml.cc/" rel="nofollow"&gt;ICML&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icml-2019-jun"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icml-2018-jul"&gt;2018&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icml-2017"&gt;2017&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-iclr---2019--2018--2017--2016--2014" class="anchor" aria-hidden="true" href="#iclr---2019--2018--2017--2016--2014"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://iclr.cc/" rel="nofollow"&gt;ICLR&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2019-may"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2018-may"&gt;2018&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2017"&gt;2017&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2016"&gt;2016&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iclr-2014"&gt;2014&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt; &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-computer-vision-conferences" class="anchor" aria-hidden="true" href="#computer-vision-conferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer vision conferences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-cvpr---2019--2018--2017" class="anchor" aria-hidden="true" href="#cvpr---2019--2018--2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://cvpr2020.thecvf.com/" rel="nofollow"&gt;CVPR&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#cvpr-2019-jun"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#cvpr-2018-jun"&gt;2018&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#cvpr-2017"&gt;2017&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-iccv---2019--2017" class="anchor" aria-hidden="true" href="#iccv---2019--2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://iccv2019.thecvf.com/" rel="nofollow"&gt;ICCV&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iccv-2019-nov"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#iccv-2017"&gt;2017&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-eccv---2018" class="anchor" aria-hidden="true" href="#eccv---2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://eccv2020.eu/" rel="nofollow"&gt;ECCV&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#eccv-2018-sep"&gt;2018&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt; &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-data-conferences" class="anchor" aria-hidden="true" href="#data-conferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data conferences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-kdd---2019--2018" class="anchor" aria-hidden="true" href="#kdd---2019--2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.kdd.org/kdd2020/" rel="nofollow"&gt;KDD&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#kdd-2019-aug"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#kdd-2018-aug"&gt;2018&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-www---2019--2018" class="anchor" aria-hidden="true" href="#www---2019--2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www2020.thewebconf.org/" rel="nofollow"&gt;WWW&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#www-2019-may"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#www-2018-april"&gt;2018&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-icdm---2019--2018" class="anchor" aria-hidden="true" href="#icdm---2019--2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://icdm2019.bigke.org/" rel="nofollow"&gt;ICDM&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icdm-2019-nov"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#icdm-2018-nov"&gt;2018&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt; &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-artificial-intelligence-conferences" class="anchor" aria-hidden="true" href="#artificial-intelligence-conferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Artificial intelligence conferences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-aaai---2019--2018--2017" class="anchor" aria-hidden="true" href="#aaai---2019--2018--2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://aaai.org/Conferences/AAAI-20/" rel="nofollow"&gt;AAAI&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#aaai-2019-feb"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#aaai-2018-feb"&gt;2018&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#aaai-2017"&gt;2017&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-ijcai---2019--2018--2017" class="anchor" aria-hidden="true" href="#ijcai---2019--2018--2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://ijcai20.org/" rel="nofollow"&gt;IJCAI&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#ijcai-2019-aug"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#ijcai-2018-jul"&gt;2018&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#ijcai-2017"&gt;2017&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-uai---2019--2018" class="anchor" aria-hidden="true" href="#uai---2019--2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://auai.org/uai2019/" rel="nofollow"&gt;UAI&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#uai-2019-jul"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#uai-2018-aug"&gt;2018&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt; &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-natural-language-processing-conferences" class="anchor" aria-hidden="true" href="#natural-language-processing-conferences"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural language processing conferences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-acl---2019--2018" class="anchor" aria-hidden="true" href="#acl---2019--2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://acl2020.org/" rel="nofollow"&gt;ACL&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#acl-2019-jul"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#acl-2018-jul"&gt;2018&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-emnlp---2019--2018--2017" class="anchor" aria-hidden="true" href="#emnlp---2019--2018--2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.emnlp-ijcnlp2019.org/" rel="nofollow"&gt;EMNLP&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#emnlp-2019-nov"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#emnlp-2018-nov"&gt;2018&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#emnlp-2017"&gt;2017&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-naacl---2019--2018" class="anchor" aria-hidden="true" href="#naacl---2019--2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://naacl2019.org/" rel="nofollow"&gt;NAACL&lt;/a&gt; - &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#naacl-2019-jun"&gt;2019&lt;/a&gt; | &lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/README.md#naacl-2018-jun"&gt;2018&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt; &lt;br&gt;
&lt;br&gt; &lt;br&gt;
&lt;br&gt; &lt;br&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-top-10-most-cited-publications" class="anchor" aria-hidden="true" href="#top-10-most-cited-publications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Top 10 most cited publications&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-semi-supervised-classification-with-graph-convolutional-networks" class="anchor" aria-hidden="true" href="#semi-supervised-classification-with-graph-convolutional-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gcn_iclr17/README.md"&gt;Semi-Supervised Classification with Graph Convolutional Networks&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering" class="anchor" aria-hidden="true" href="#convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/chebnet_nips16/README.md"&gt;Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-spectral-networks-and-locally-connected-networks-on-graphs" class="anchor" aria-hidden="true" href="#spectral-networks-and-locally-connected-networks-on-graphs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/graphcnn_iclr14/README.md"&gt;Spectral Networks and Locally Connected Networks on Graphs&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-networks-on-graphs-for-learning-molecular-fingerprints" class="anchor" aria-hidden="true" href="#convolutional-networks-on-graphs-for-learning-molecular-fingerprints"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/graphcnn_nips15/README.md"&gt;Convolutional Networks on Graphs for Learning Molecular Fingerprints&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-inductive-representation-learning-on-large-graphs" class="anchor" aria-hidden="true" href="#inductive-representation-learning-on-large-graphs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/graphsage_nips17/README.md"&gt;Inductive Representation Learning on Large Graphs&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-the-graph-neural-network-model" class="anchor" aria-hidden="true" href="#the-graph-neural-network-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnn_tnn09/README.md"&gt;The Graph Neural Network Model&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-gated-graph-sequence-neural-networks" class="anchor" aria-hidden="true" href="#gated-graph-sequence-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/ggnn_iclr16/README.md"&gt;Gated Graph Sequence Neural Networks&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-neural-message-passing-for-quantum-chemistry" class="anchor" aria-hidden="true" href="#neural-message-passing-for-quantum-chemistry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/mpnn_icml17/README.md"&gt;Neural Message Passing for Quantum Chemistry&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-graph-attention-networks" class="anchor" aria-hidden="true" href="#graph-attention-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gan_iclr18/README.md"&gt;Graph Attention Networks&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-learning-convolutional-neural-networks-for-graphs" class="anchor" aria-hidden="true" href="#learning-convolutional-neural-networks-for-graphs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gcn_icml16/README.md"&gt;Learning Convolutional Neural Networks for Graphs&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt; &lt;br&gt;
&lt;br&gt; &lt;br&gt;
&lt;br&gt; &lt;br&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-related-workshops" class="anchor" aria-hidden="true" href="#related-workshops"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related workshops&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-2020" class="anchor" aria-hidden="true" href="#2020"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2020&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning-on-graphs-methodologies-and-applications-aaai" class="anchor" aria-hidden="true" href="#deep-learning-on-graphs-methodologies-and-applications-aaai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://dlg2019.bitbucket.io/aaai20/" rel="nofollow"&gt;Deep Learning on Graphs: Methodologies and Applications (AAAI)&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-2019" class="anchor" aria-hidden="true" href="#2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-graph-representation-learning-neurips" class="anchor" aria-hidden="true" href="#graph-representation-learning-neurips"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://grlearning.github.io/" rel="nofollow"&gt;Graph Representation Learning (NeurIPS)&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-scene-graph-representation-and-learning-iccv" class="anchor" aria-hidden="true" href="#scene-graph-representation-and-learning-iccv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://cs.stanford.edu/people/ranjaykrishna/sgrl/index.html" rel="nofollow"&gt;Scene Graph Representation and Learning (ICCV)&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning-on-graphs-methods-and-applications-kdd" class="anchor" aria-hidden="true" href="#deep-learning-on-graphs-methods-and-applications-kdd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://dlg2019.bitbucket.io/" rel="nofollow"&gt;Deep Learning on Graphs: Methods and Applications (KDD)&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-learning-and-reasoning-with-graph-structured-representations-icml" class="anchor" aria-hidden="true" href="#learning-and-reasoning-with-graph-structured-representations-icml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://graphreason.github.io/" rel="nofollow"&gt;Learning and Reasoning with Graph-Structured Representations (ICML)&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-representation-learning-on-graphs-and-manifolds-iclr" class="anchor" aria-hidden="true" href="#representation-learning-on-graphs-and-manifolds-iclr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://rlgm.github.io/" rel="nofollow"&gt;Representation Learning on Graphs and Manifolds (ICLR)&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-2018" class="anchor" aria-hidden="true" href="#2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-relational-representation-learning-neurips" class="anchor" aria-hidden="true" href="#relational-representation-learning-neurips"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://r2learning.github.io/" rel="nofollow"&gt;Relational Representation Learning (NeurIPS)&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt; &lt;br&gt;
&lt;br&gt; &lt;br&gt;
&lt;br&gt; &lt;br&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-surveys--literature-reviews" class="anchor" aria-hidden="true" href="#surveys--literature-reviews"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Surveys / literature reviews&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-2019-1" class="anchor" aria-hidden="true" href="#2019-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2019&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-graph-neural-networks-for-small-graph-and-giant-network-representation-learning-an-overview" class="anchor" aria-hidden="true" href="#graph-neural-networks-for-small-graph-and-giant-network-representation-learning-an-overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnnaug_arxiv19/README.md"&gt;Graph Neural Networks for Small Graph and Giant Network Representation Learning: An Overview&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-learning-representations-of-graph-data----a-survey" class="anchor" aria-hidden="true" href="#learning-representations-of-graph-data----a-survey"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/lrg_arxiv19/README.md"&gt;Learning Representations of Graph Data -- A Survey&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-2018-1" class="anchor" aria-hidden="true" href="#2018-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2018&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-adversarial-attack-and-defense-on-graph-data-a-survey" class="anchor" aria-hidden="true" href="#adversarial-attack-and-defense-on-graph-data-a-survey"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/aagsurvey_arxiv18/README.md"&gt;Adversarial Attack and Defense on Graph Data: A Survey&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-graph-neural-networks-a-review-of-methods-and-applications" class="anchor" aria-hidden="true" href="#graph-neural-networks-a-review-of-methods-and-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnnreview_arxiv18/README.md"&gt;Graph Neural Networks: A Review of Methods and Applications&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-a-comprehensive-survey-on-graph-neural-networks" class="anchor" aria-hidden="true" href="#a-comprehensive-survey-on-graph-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnnsurvey_arxiv19/README.md"&gt;A Comprehensive Survey on Graph Neural Networks&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-deep-learning-on-graphs-a-survey" class="anchor" aria-hidden="true" href="#deep-learning-on-graphs-a-survey"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/dlgsurvey_arxiv18/README.md"&gt;Deep Learning on Graphs: A Survey&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-relational-inductive-biases-deep-learning-and-graph-networks" class="anchor" aria-hidden="true" href="#relational-inductive-biases-deep-learning-and-graph-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gnet_arXiv18/README.md"&gt;Relational inductive biases, deep learning, and graph networks&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h2&gt;&lt;a id="user-content-2017" class="anchor" aria-hidden="true" href="#2017"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2017&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-representation-learning-on-graphs-methods-and-applications" class="anchor" aria-hidden="true" href="#representation-learning-on-graphs-methods-and-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/grl_ideb17/README.md"&gt;Representation Learning on Graphs: Methods and Applications&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-geometric-deep-learning-going-beyond-euclidean-data" class="anchor" aria-hidden="true" href="#geometric-deep-learning-going-beyond-euclidean-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/gdl_isp17/README.md"&gt;Geometric Deep Learning: Going beyond Euclidean data&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>naganandy</author><guid isPermaLink="false">https://github.com/naganandy/graph-based-deep-learning-literature</guid><pubDate>Fri, 08 Nov 2019 00:02:00 GMT</pubDate></item><item><title>practicalAI/practicalAI #3 in Jupyter Notebook, Today</title><link>https://github.com/practicalAI/practicalAI</link><description>&lt;p&gt;&lt;i&gt;📚 A practical approach to machine learning. &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a href="https://practicalai.me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/369fcfdcd241a0324ffaee51715926a56c049302/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f70726163746963616c41492f6c6f676f2e706e67" width="200" data-canonical-src="https://practicalai.me/static/img/practicalAI/logo.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;p&gt;A &lt;i&gt;&lt;b&gt;practical&lt;/b&gt;&lt;/i&gt; approach to machine learning.&lt;/p&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;
&lt;img src="https://camo.githubusercontent.com/c1b6c20adc52e06a1c58218665169097a63bd549/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70726163746963616c41492f70726163746963616c41492e7376673f7374796c653d736f6369616c266c6162656c3d53746172" data-canonical-src="https://img.shields.io/github/stars/practicalAI/practicalAI.svg?style=social&amp;amp;label=Star" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/19c0cf9ba93aa446aa855a0203c46ee39841cba9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d2d3565626130302e7376673f6c6162656c3d4c696e6b6564496e266c6f676f3d6c696e6b6564696e267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&amp;amp;logo=linkedin&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://twitter.com/GokuMohandas" rel="nofollow"&gt;
&lt;img src="https://camo.githubusercontent.com/fd3346389d2255e7c7aa4395d6afb74e7a1df552/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f476f6b754d6f68616e6461732e7376673f6c6162656c3d466f6c6c6f77267374796c653d736f6369616c" data-canonical-src="https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&amp;amp;style=social" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;p&gt;&lt;sub&gt;Created by
&lt;a href="https://goku.me" rel="nofollow"&gt;Goku Mohandas&lt;/a&gt; and
&lt;a href="https://github.com/GokuMohandas/practicalAI/graphs/contributors"&gt;
contributors
&lt;/a&gt;
&lt;/sub&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-notebooks" class="anchor" aria-hidden="true" href="#notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notebooks&lt;/h2&gt;
&lt;ul&gt;
    &lt;li&gt;
        &lt;g-emoji class="g-emoji" alias="earth_americas" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30e.png"&gt;🌎&lt;/g-emoji&gt; → &lt;a href="https://practicalai.me" rel="nofollow"&gt;https://practicalai.me&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        All of these notebooks are in &lt;a href="https://tensorflow.org" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt; but you can find old PyTorch notebooks in the &lt;a href="https://github.com/practicalAI/practicalAI/tree/4ad626098aca25db5628fe67895e738d5a5c2c2a"&gt;v0.1&lt;/a&gt; release.
    &lt;/li&gt;
    &lt;li&gt;
        If you prefer Jupyter Notebooks or want to add/fix content, check out the &lt;a href="https://github.com/practicalAI/practicalAI/tree/master/notebooks"&gt;notebooks&lt;/a&gt; directory.
    &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;
        &lt;h4 align="center"&gt;&lt;a id="user-content-basic-ml" class="anchor" aria-hidden="true" href="#basic-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic ML&lt;/h4&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Basics&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Machine Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Deep Learning&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn Python basics with notebooks.&lt;/li&gt;
            &lt;li&gt;Use data science libraries like &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;NumPy&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/" rel="nofollow"&gt;Pandas&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Implement basic ML models in &lt;a href="https://www.tensorflow.org/overview/" rel="nofollow"&gt;TensorFlow 2.0 + Keras&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Create deep learning models for improved performance.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/00_Notebooks.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="notebook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d3.png"&gt;📓&lt;/g-emoji&gt; Notebooks&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="chart_with_upwards_trend" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png"&gt;📈&lt;/g-emoji&gt; Linear Regression&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="mag_right" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50e.png"&gt;🔎&lt;/g-emoji&gt; Data &amp;amp; Models&lt;/td&gt;
        &lt;td&gt;️&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼&lt;/g-emoji&gt; Convolutional Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/01_Python.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; Python&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; Logistic Regression&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="hammer_and_wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png"&gt;🛠&lt;/g-emoji&gt; Utilities&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crown" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f451.png"&gt;👑&lt;/g-emoji&gt; Embeddings&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/02_NumPy.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="1234" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f522.png"&gt;🔢&lt;/g-emoji&gt; NumPy&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;️&lt;g-emoji class="g-emoji" alias="control_knobs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f39b.png"&gt;🎛&lt;/g-emoji&gt; Multilayer Perceptrons&lt;/td&gt;
        &lt;td&gt;️&lt;g-emoji class="g-emoji" alias="scissors" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2702.png"&gt;✂️&lt;/g-emoji&gt; Preprocessing&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="green_book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d7.png"&gt;📗&lt;/g-emoji&gt; Recurrent Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/03_Pandas.ipynb" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="panda_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png"&gt;🐼&lt;/g-emoji&gt; Pandas&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-production-ml" class="anchor" aria-hidden="true" href="#production-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Production ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Local&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Applications&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Scale&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Setup your local environment for ML.&lt;/li&gt;
            &lt;li&gt;Wrap your ML in RESTful APIs using &lt;a href="http://flask.pocoo.org/" rel="nofollow"&gt;Flask&lt;/a&gt; to create applications.&lt;/li&gt;
            &lt;li&gt;Standardize and scale your ML applications with &lt;a href="https://www.docker.com/" rel="nofollow"&gt;Docker&lt;/a&gt; and &lt;a href="https://kubernetes.io/" rel="nofollow"&gt;Kubernetes&lt;/a&gt;.&lt;/li&gt;
            &lt;li&gt;Deploy simple and scalable ML workflows using &lt;a href="https://www.kubeflow.org/" rel="nofollow"&gt;Kubeflow&lt;/a&gt;.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; Local Setup&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="evergreen_tree" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f332.png"&gt;🌲&lt;/g-emoji&gt; Logging&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="whale" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f433.png"&gt;🐳&lt;/g-emoji&gt; Docker&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="handshake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f91d.png"&gt;🤝&lt;/g-emoji&gt; Distributed Training&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png"&gt;🐍&lt;/g-emoji&gt; ML Scripts&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="funeral_urn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png"&gt;⚱️&lt;/g-emoji&gt; Flask Applications&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ship" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a2.png"&gt;🚢&lt;/g-emoji&gt; Kubernetes&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="battery" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f50b.png"&gt;🔋&lt;/g-emoji&gt; Databases&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png"&gt;✅&lt;/g-emoji&gt; Unit Tests&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="ocean" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png"&gt;🌊&lt;/g-emoji&gt; Kubeflow&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="closed_lock_with_key" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f510.png"&gt;🔐&lt;/g-emoji&gt; Authentication&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-advanced-ml" class="anchor" aria-hidden="true" href="#advanced-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced ML&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;General&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Sequential&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Popular&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="3"&gt;
        &lt;ul&gt;
            &lt;li&gt;Dive into architectural and interpretable advancements in neural networks.&lt;/li&gt;
            &lt;li&gt;Implement state-of-the-art NLP techniques.&lt;/li&gt;
            &lt;li&gt;Learn about popular deep learning algorithms used for generation, time-series, etc.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;🧐 Attention&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="bee" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41d.png"&gt;🐝&lt;/g-emoji&gt; Transformers&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="performing_arts" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ad.png"&gt;🎭&lt;/g-emoji&gt; Generative Adversarial Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="crystal_ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f52e.png"&gt;🔮&lt;/g-emoji&gt; Autoencoders&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="racing_car" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ce.png"&gt;🏎️&lt;/g-emoji&gt; Highway Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="japanese_ogre" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f479.png"&gt;👹&lt;/g-emoji&gt; BERT, GPT2, XLNet&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="8ball" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b1.png"&gt;🎱&lt;/g-emoji&gt; Bayesian Deep Learning&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="spider" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f577.png"&gt;🕷️&lt;/g-emoji&gt; Graph Neural Networks&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="droplet" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a7.png"&gt;💧&lt;/g-emoji&gt; Residual Networks&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="clock9" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f558.png"&gt;🕘&lt;/g-emoji&gt; Temporal CNNs&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="cherries" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f352.png"&gt;🍒&lt;/g-emoji&gt; Reinforcement Learning&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="2"&gt;&lt;h4 align="center"&gt;&lt;a id="user-content-topics" class="anchor" aria-hidden="true" href="#topics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Topics&lt;/h4&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center"&gt;&lt;b&gt;Computer Vision&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Natural Language&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Unsupervised Learning&lt;/b&gt;&lt;/td&gt;
        &lt;td align="center"&gt;&lt;b&gt;Miscellaneous&lt;/b&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
        &lt;td colspan="1" rowspan="4"&gt;
        &lt;ul&gt;
            &lt;li&gt;Learn how to use deep learning for computer vision tasks.&lt;/li&gt;
            &lt;li&gt;Implement techniques for natural language tasks.&lt;/li&gt;
            &lt;li&gt;Derive insights from unlabeled data using unsupervised learning.&lt;/li&gt;
        &lt;/ul&gt;
        &lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="camera_flash" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f8.png"&gt;📸&lt;/g-emoji&gt; Image Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png"&gt;📖&lt;/g-emoji&gt; Text classification&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dango" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f361.png"&gt;🍡&lt;/g-emoji&gt; Clustering&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="alarm_clock" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f0.png"&gt;⏰&lt;/g-emoji&gt; Time-series Analysis&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Image Segmentation&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="speech_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png"&gt;💬&lt;/g-emoji&gt; Named Entity Recognition&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="houses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3d8.png"&gt;🏘️&lt;/g-emoji&gt; Topic Modeling&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="shopping_cart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6d2.png"&gt;🛒&lt;/g-emoji&gt; Recommendation Systems&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="art" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a8.png"&gt;🎨&lt;/g-emoji&gt; Image Generation&lt;/td&gt;
        &lt;td&gt;🧠 Knowledge Graphs&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png"&gt;🎯&lt;/g-emoji&gt; One-shot Learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;g-emoji class="g-emoji" alias="card_file_box" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5c3.png"&gt;🗃️&lt;/g-emoji&gt; Interpretability&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://practicalai.me/#newsletter" rel="nofollow"&gt;&lt;g-emoji class="g-emoji" alias="mailbox_with_mail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ec.png"&gt;📬&lt;/g-emoji&gt; Newsletter&lt;/a&gt; - Subscribe to get updates on new content&lt;/p&gt;
&lt;div&gt;
&lt;a href="https://twitter.com/practicalai_me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6729390835283dc393cb7f500d840be48cdac6c6/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f74772e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/tw.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.youtube.com/channel/UCgW4K2UDK21kHIzxpjNos7Q" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/786df4e8cd9d5421583bf063cdcf9e3d06d14de5/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f7974622e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/ytb.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/practicalAI/practicalAI"&gt;&lt;img src="https://camo.githubusercontent.com/082d52273e2945cb5126868e9cc44d4c91ffe912/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f677468622e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/gthb.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.linkedin.com/company/practicalai-me" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/045b09864f16b18059d20d17bfaf906b26ca3aae/68747470733a2f2f70726163746963616c61692e6d652f7374617469632f696d672f6d656469612f6c6e6b646e2e706e67" width="30" data-canonical-src="https://practicalai.me/static/img/media/lnkdn.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>practicalAI</author><guid isPermaLink="false">https://github.com/practicalAI/practicalAI</guid><pubDate>Fri, 08 Nov 2019 00:03:00 GMT</pubDate></item><item><title>jackfrued/Python-100-Days #4 in Jupyter Notebook, Today</title><link>https://github.com/jackfrued/Python-100-Days</link><description>&lt;p&gt;&lt;i&gt;Python - 100天从新手到大师&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python---100天从新手到大师" class="anchor" aria-hidden="true" href="#python---100天从新手到大师"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python - 100天从新手到大师&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;作者：骆昊&lt;/p&gt;
&lt;p&gt;最近有很多想学习Python的小伙伴陆陆续续加入我们的交流群，目前我们的交流群人数已经超过一万人。我们的目标是打造一个优质的Python交流社区，一方面为想学习Python的初学者扫平入门过程中的重重障碍；另一方为新入行的开发者提供问道的途径，帮助他们迅速成长为优秀的职业人；此外，有经验的开发者可以利用这个平台把自己的工作经验无偿分享或有偿提供出来，让大家都能够得到职业技能以及综合素质的全面提升。之前的公开课和线下技术交流活动因为工作的关系荒废了一段时间了，但是各位小伙伴仍然活跃在交流群并一如既往的支持我们，在此向大家表示感谢。近期开始持续更新前15天和最后10天的内容，前15天是写给初学者的，我希望把上手的难度进一步降低，例子程序更加简单清晰；最后10天是Python项目实战和面试相关的东西，我希望内容更详实和完整，尤其是第100天的面试题部分；创作不易，感谢大家的打赏支持，这些钱不会用于购买咖啡而是通过腾讯公益平台捐赠给需要帮助的人（&lt;a href="./%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.md"&gt;点击&lt;/a&gt;了解捐赠情况）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-qq-group.png"&gt;&lt;img src="./res/python-qq-group.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python应用领域和就业形势分析" class="anchor" aria-hidden="true" href="#python应用领域和就业形势分析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python应用领域和就业形势分析&lt;/h3&gt;
&lt;p&gt;简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学习曲线低，非专业人士也能上手&lt;/li&gt;
&lt;li&gt;开源系统，拥有强大的生态圈&lt;/li&gt;
&lt;li&gt;解释型语言，完美的平台可移植性&lt;/li&gt;
&lt;li&gt;支持面向对象和函数式编程&lt;/li&gt;
&lt;li&gt;能够通过调用C/C++代码扩展功能&lt;/li&gt;
&lt;li&gt;代码规范程度高，可读性强&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前几个比较流行的领域，Python都有用武之地。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;云基础设施 - Python / Java / Go&lt;/li&gt;
&lt;li&gt;DevOps - Python / Shell / Ruby / Go&lt;/li&gt;
&lt;li&gt;网络爬虫 - Python / PHP / C++&lt;/li&gt;
&lt;li&gt;数据分析挖掘 - Python / R / Scala / Matlab&lt;/li&gt;
&lt;li&gt;机器学习 - Python / R / Java / Lisp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作为一名Python开发者，主要的就业领域包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python服务器后台开发 / 游戏服务器开发 / 数据接口开发工程师&lt;/li&gt;
&lt;li&gt;Python自动化运维工程师&lt;/li&gt;
&lt;li&gt;Python数据分析 / 数据可视化 / 大数据工程师&lt;/li&gt;
&lt;li&gt;Python爬虫工程师&lt;/li&gt;
&lt;li&gt;Python聊天机器人开发 / 图像识别和视觉算法 / 深度学习工程师&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图显示了主要城市Python招聘需求量及薪资待遇排行榜（截止到2018年5月）。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-top-10.png"&gt;&lt;img src="./res/python-top-10.png" alt="Python招聘需求及薪资待遇Top 10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-bj-salary.png"&gt;&lt;img src="./res/python-bj-salary.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-salary-chengdu.png"&gt;&lt;img src="./res/python-salary-chengdu.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;给初学者的几个建议：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make English as your working language.&lt;/li&gt;
&lt;li&gt;Practice makes perfect.&lt;/li&gt;
&lt;li&gt;All experience comes from mistakes.&lt;/li&gt;
&lt;li&gt;Don't be one of the leeches.&lt;/li&gt;
&lt;li&gt;Either stand out or kicked out.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day0115---python语言基础" class="anchor" aria-hidden="true" href="#day0115---python语言基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01~15 - &lt;a href="./Day01-15"&gt;Python语言基础&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day01---初识python" class="anchor" aria-hidden="true" href="#day01---初识python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01 - &lt;a href="./Day01-15/01.%E5%88%9D%E8%AF%86Python.md"&gt;初识Python&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python简介 - Python的历史 / Python的优缺点 / Python的应用领域&lt;/li&gt;
&lt;li&gt;搭建编程环境 - Windows环境 / Linux环境 / MacOS环境&lt;/li&gt;
&lt;li&gt;从终端运行Python程序 - Hello, world / print函数 / 运行程序&lt;/li&gt;
&lt;li&gt;使用IDLE - 交互式环境(REPL) / 编写多行代码 / 运行程序 / 退出IDLE&lt;/li&gt;
&lt;li&gt;注释 - 注释的作用 / 单行注释 / 多行注释&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day02---语言元素" class="anchor" aria-hidden="true" href="#day02---语言元素"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day02 - &lt;a href="./Day01-15/02.%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0.md"&gt;语言元素&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;程序和进制 - 指令和程序 / 冯诺依曼机 / 二进制和十进制 / 八进制和十六进制&lt;/li&gt;
&lt;li&gt;变量和类型 - 变量的命名 / 变量的使用 / input函数 / 检查变量类型 / 类型转换&lt;/li&gt;
&lt;li&gt;数字和字符串 - 整数 / 浮点数 / 复数 / 字符串 / 字符串基本操作 / 字符编码&lt;/li&gt;
&lt;li&gt;运算符 - 数学运算符 / 赋值运算符 / 比较运算符 / 逻辑运算符 / 身份运算符 / 运算符的优先级&lt;/li&gt;
&lt;li&gt;应用案例 - 华氏温度转换成摄氏温度 / 输入圆的半径计算周长和面积 / 输入年份判断是否是闰年&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day03---分支结构" class="anchor" aria-hidden="true" href="#day03---分支结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day03 - &lt;a href="./Day01-15/03.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md"&gt;分支结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;分支结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图&lt;/li&gt;
&lt;li&gt;if语句 - 简单的if / if-else结构 / if-elif-else结构 / 嵌套的if&lt;/li&gt;
&lt;li&gt;应用案例 - 用户身份验证 / 英制单位与公制单位互换 / 掷骰子决定做什么 / 百分制成绩转等级制 / 分段函数求值 / 输入三条边的长度如果能构成三角形就计算周长和面积&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day04---循环结构" class="anchor" aria-hidden="true" href="#day04---循环结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day04 - &lt;a href="./Day01-15/04.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md"&gt;循环结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;循环结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图&lt;/li&gt;
&lt;li&gt;while循环 - 基本结构 / break语句 / continue语句&lt;/li&gt;
&lt;li&gt;for循环 - 基本结构 / range类型 / 循环中的分支结构 / 嵌套的循环 / 提前结束程序&lt;/li&gt;
&lt;li&gt;应用案例 - 1~100求和 / 判断素数 / 猜数字游戏 / 打印九九表 / 打印三角形图案 / 猴子吃桃 / 百钱百鸡&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day05---构造程序逻辑" class="anchor" aria-hidden="true" href="#day05---构造程序逻辑"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day05 - &lt;a href="./Day01-15/05.%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91.md"&gt;构造程序逻辑&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;经典案例：水仙花数 / 百钱百鸡 / Craps赌博游戏&lt;/li&gt;
&lt;li&gt;练习题目：斐波那契数列 / 完美数 / 素数&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day06---函数和模块的使用" class="anchor" aria-hidden="true" href="#day06---函数和模块的使用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day06 - &lt;a href="./Day01-15/06.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;函数和模块的使用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;函数的作用 - 代码的坏味道 / 用函数封装功能模块&lt;/li&gt;
&lt;li&gt;定义函数 - def语句 / 函数名 / 参数列表 / return语句 / 调用自定义函数&lt;/li&gt;
&lt;li&gt;调用函数 - Python内置函数 /  导入模块和函数&lt;/li&gt;
&lt;li&gt;函数的参数 - 默认参数 / 可变参数 / 关键字参数 / 命名关键字参数&lt;/li&gt;
&lt;li&gt;函数的返回值 - 没有返回值  / 返回单个值 / 返回多个值&lt;/li&gt;
&lt;li&gt;作用域问题 - 局部作用域 / 嵌套作用域 / 全局作用域 / 内置作用域 / 和作用域相关的关键字&lt;/li&gt;
&lt;li&gt;用模块管理函数 - 模块的概念 / 用自定义模块管理函数 / 命名冲突的时候会怎样（同一个模块和不同的模块）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day07---字符串和常用数据结构" class="anchor" aria-hidden="true" href="#day07---字符串和常用数据结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day07 - &lt;a href="./Day01-15/07.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md"&gt;字符串和常用数据结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;字符串的使用 - 计算长度 / 下标运算 / 切片 / 常用方法&lt;/li&gt;
&lt;li&gt;列表基本用法 - 定义列表 / 用下表访问元素 / 下标越界 / 添加元素 / 删除元素 / 修改元素 / 切片 / 循环遍历&lt;/li&gt;
&lt;li&gt;列表常用操作 - 连接 / 复制(复制元素和复制数组) / 长度 / 排序 / 倒转 / 查找&lt;/li&gt;
&lt;li&gt;生成列表 - 使用range创建数字列表 / 生成表达式 / 生成器&lt;/li&gt;
&lt;li&gt;元组的使用 - 定义元组 / 使用元组中的值 / 修改元组变量 / 元组和列表转换&lt;/li&gt;
&lt;li&gt;集合基本用法 - 集合和列表的区别 /  创建集合 / 添加元素 / 删除元素 /  清空&lt;/li&gt;
&lt;li&gt;集合常用操作 - 交集 / 并集 / 差集 / 对称差 / 子集 / 超集&lt;/li&gt;
&lt;li&gt;字典的基本用法 - 字典的特点 / 创建字典 / 添加元素 / 删除元素 / 取值 / 清空&lt;/li&gt;
&lt;li&gt;字典常用操作 - keys()方法 / values()方法 / items()方法 / setdefault()方法&lt;/li&gt;
&lt;li&gt;基础练习 - 跑马灯效果 / 列表找最大元素 / 统计考试成绩的平均分 / Fibonacci数列 / 杨辉三角&lt;/li&gt;
&lt;li&gt;综合案例 - 双色球选号 / 井字棋&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day08---面向对象编程基础" class="anchor" aria-hidden="true" href="#day08---面向对象编程基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day08 - &lt;a href="./Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80.md"&gt;面向对象编程基础&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;类和对象 - 什么是类 / 什么是对象 / 面向对象其他相关概念&lt;/li&gt;
&lt;li&gt;定义类 - 基本结构 / 属性和方法 / 构造器 / 析构器 / __str__方法&lt;/li&gt;
&lt;li&gt;使用对象 - 创建对象 / 给对象发消息&lt;/li&gt;
&lt;li&gt;面向对象的四大支柱 - 抽象 / 封装 / 继承 / 多态&lt;/li&gt;
&lt;li&gt;基础练习 - 定义学生类 / 定义时钟类 / 定义图形类 / 定义汽车类&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day09---面向对象进阶" class="anchor" aria-hidden="true" href="#day09---面向对象进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day09 - &lt;a href="./Day01-15/09.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6.md"&gt;面向对象进阶&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;属性 - 类属性 / 实例属性 / 属性访问器 / 属性修改器 / 属性删除器 / 使用__slots__&lt;/li&gt;
&lt;li&gt;类中的方法 - 实例方法 / 类方法 / 静态方法&lt;/li&gt;
&lt;li&gt;运算符重载 - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__&lt;/li&gt;
&lt;li&gt;类(的对象)之间的关系 - 关联 / 继承 / 依赖&lt;/li&gt;
&lt;li&gt;继承和多态 - 什么是继承 / 继承的语法 / 调用父类方法 / 方法重写 / 类型判定 / 多重继承 / 菱形继承(钻石继承)和C3算法&lt;/li&gt;
&lt;li&gt;综合案例 - 工资结算系统 / 图书自动折扣系统 / 自定义分数类&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day10---图形用户界面和游戏开发" class="anchor" aria-hidden="true" href="#day10---图形用户界面和游戏开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day10 - &lt;a href="./Day01-15/10.%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91.md"&gt;图形用户界面和游戏开发&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;使用tkinter开发GUI程序&lt;/li&gt;
&lt;li&gt;使用pygame三方库开发游戏应用&lt;/li&gt;
&lt;li&gt;“大球吃小球”游戏&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day11---文件和异常" class="anchor" aria-hidden="true" href="#day11---文件和异常"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day11 - &lt;a href="./Day01-15/11.%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8.md"&gt;文件和异常&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;读文件 - 读取整个文件 / 逐行读取 / 文件路径&lt;/li&gt;
&lt;li&gt;写文件 - 覆盖写入 / 追加写入 / 文本文件 / 二进制文件&lt;/li&gt;
&lt;li&gt;异常处理 - 异常机制的重要性 / try-except代码块 / else代码块 / finally代码块 / 内置异常类型 / 异常栈 / raise语句&lt;/li&gt;
&lt;li&gt;数据持久化 - CSV文件概述 / csv模块的应用 / JSON数据格式 / json模块的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day12---字符串和正则表达式" class="anchor" aria-hidden="true" href="#day12---字符串和正则表达式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day12 - &lt;a href="./Day01-15/12.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.md"&gt;字符串和正则表达式&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;字符串高级操作 - 转义字符 / 原始字符串 / 多行字符串 / in和 not in运算符 / is开头的方法 / join和split方法 / strip相关方法 / pyperclip模块 / 不变字符串和可变字符串 / StringIO的使用&lt;/li&gt;
&lt;li&gt;正则表达式入门 - 正则表达式的作用 / 元字符 / 转义 / 量词 / 分组 / 零宽断言 /贪婪匹配与惰性匹配懒惰 / 使用re模块实现正则表达式操作（匹配、搜索、替换、捕获）&lt;/li&gt;
&lt;li&gt;使用正则表达式 - re模块 / compile函数 / group和groups方法 / match方法 / search方法 / findall和finditer方法 / sub和subn方法 / split方法&lt;/li&gt;
&lt;li&gt;应用案例 - 使用正则表达式验证输入的字符串&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day13---进程和线程" class="anchor" aria-hidden="true" href="#day13---进程和线程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day13 - &lt;a href="./Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B.md"&gt;进程和线程&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;进程和线程的概念 - 什么是进程 / 什么是线程 / 多线程的应用场景&lt;/li&gt;
&lt;li&gt;使用进程 - fork函数 / multiprocessing模块 / 进程池 / 进程间通信&lt;/li&gt;
&lt;li&gt;使用线程 - thread模块 / threading模块 / Thread类 / Lock类 / Condition类 / 线程池&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day14---网络编程入门和网络应用开发" class="anchor" aria-hidden="true" href="#day14---网络编程入门和网络应用开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day14 - &lt;a href="./Day01-15/14.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91.md"&gt;网络编程入门和网络应用开发&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;计算机网络基础 - 计算机网络发展史 / “TCP-IP”模型 / IP地址 / 端口 / 协议 / 其他相关概念&lt;/li&gt;
&lt;li&gt;网络应用模式 - “客户端-服务器”模式 / “浏览器-服务器”模式&lt;/li&gt;
&lt;li&gt;基于HTTP协议访问网络资源 - 网络API概述 / 访问URL / requests模块 / 解析JSON格式数据&lt;/li&gt;
&lt;li&gt;Python网络编程 - 套接字的概念 / socket模块 /  socket函数 / 创建TCP服务器 / 创建TCP客户端 / 创建UDP服务器 / 创建UDP客户端 / SocketServer模块&lt;/li&gt;
&lt;li&gt;电子邮件 - SMTP协议 / POP3协议 / IMAP协议 / smtplib模块 / poplib模块 / imaplib模块&lt;/li&gt;
&lt;li&gt;短信服务 - 调用短信服务网关&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day15---图像和文档处理" class="anchor" aria-hidden="true" href="#day15---图像和文档处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day15 - &lt;a href="./Day01-15/15.%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86.md"&gt;图像和文档处理&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;用Pillow处理图片 - 图片读写 / 图片合成 / 几何变换 / 色彩转换 / 滤镜效果&lt;/li&gt;
&lt;li&gt;读写Word文档 - 文本内容的处理 / 段落 / 页眉和页脚 / 样式的处理&lt;/li&gt;
&lt;li&gt;读写Excel文件 - xlrd模块 / xlwt模块&lt;/li&gt;
&lt;li&gt;生成PDF文件 - pypdf2模块 / reportlab模块&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day16day20---python语言进阶-" class="anchor" aria-hidden="true" href="#day16day20---python语言进阶-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day16~Day20 - &lt;a href="./Day16-20/16-20.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md"&gt;Python语言进阶 &lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;常用数据结构&lt;/li&gt;
&lt;li&gt;函数的高级用法 - “一等公民” / 高阶函数 / Lambda函数 / 作用域和闭包 / 装饰器&lt;/li&gt;
&lt;li&gt;面向对象高级知识 - “三大支柱” / 类与类之间的关系 / 垃圾回收 / 魔术属性和方法 / 混入 / 元类 / 面向对象设计原则 / GoF设计模式&lt;/li&gt;
&lt;li&gt;迭代器和生成器 - 相关魔术方法 / 创建生成器的两种方式 /&lt;/li&gt;
&lt;li&gt;并发和异步编程 - 多线程 / 多进程 / 异步IO / async和await&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day2130---web前端入门" class="anchor" aria-hidden="true" href="#day2130---web前端入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day21~30 - &lt;a href="./Day21-30/21-30.Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0.md"&gt;Web前端入门&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用HTML标签承载页面内容&lt;/li&gt;
&lt;li&gt;用CSS渲染页面&lt;/li&gt;
&lt;li&gt;用JavaScript处理交互式行为&lt;/li&gt;
&lt;li&gt;jQuery入门和提高&lt;/li&gt;
&lt;li&gt;Vue.js入门&lt;/li&gt;
&lt;li&gt;Element的使用&lt;/li&gt;
&lt;li&gt;Bootstrap的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3135---玩转linux操作系统" class="anchor" aria-hidden="true" href="#day3135---玩转linux操作系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day31~35 - &lt;a href="./Day31-35/31-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md"&gt;玩转Linux操作系统&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;操作系统发展史和Linux概述&lt;/li&gt;
&lt;li&gt;Linux基础命令&lt;/li&gt;
&lt;li&gt;Linux中的实用程序&lt;/li&gt;
&lt;li&gt;Linux的文件系统&lt;/li&gt;
&lt;li&gt;Vim编辑器的应用&lt;/li&gt;
&lt;li&gt;环境变量和Shell编程&lt;/li&gt;
&lt;li&gt;软件的安装和服务的配置&lt;/li&gt;
&lt;li&gt;网络访问和管理&lt;/li&gt;
&lt;li&gt;其他相关内容&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3640---数据库基础和进阶" class="anchor" aria-hidden="true" href="#day3640---数据库基础和进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day36~40 - &lt;a href="./Day36-40"&gt;数据库基础和进阶&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./Day36-40/36-38.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL.md"&gt;关系型数据库MySQL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;关系型数据库概述&lt;/li&gt;
&lt;li&gt;MySQL的安装和使用&lt;/li&gt;
&lt;li&gt;SQL的使用
&lt;ul&gt;
&lt;li&gt;DDL - 数据定义语言 - create / drop / alter&lt;/li&gt;
&lt;li&gt;DML - 数据操作语言 - insert / delete / update / select&lt;/li&gt;
&lt;li&gt;DCL - 数据控制语言 - grant / revoke&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;相关知识
&lt;ul&gt;
&lt;li&gt;范式理论 - 设计二维表的指导思想&lt;/li&gt;
&lt;li&gt;数据完整性&lt;/li&gt;
&lt;li&gt;数据一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在Python中操作MySQL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="./Day36-40/39-40.NoSQL%E5%85%A5%E9%97%A8.md"&gt;NoSQL入门&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;NoSQL概述&lt;/li&gt;
&lt;li&gt;Redis概述&lt;/li&gt;
&lt;li&gt;Mongo概述&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day4155---实战django" class="anchor" aria-hidden="true" href="#day4155---实战django"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41~55 - &lt;a href="./Day41-55"&gt;实战Django&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day41---快速上手" class="anchor" aria-hidden="true" href="#day41---快速上手"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41 - &lt;a href="./Day41-55/41.%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md"&gt;快速上手&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Web应用工作原理和HTTP协议&lt;/li&gt;
&lt;li&gt;Django框架概述&lt;/li&gt;
&lt;li&gt;5分钟快速上手&lt;/li&gt;
&lt;li&gt;使用视图模板&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day42---深入模型" class="anchor" aria-hidden="true" href="#day42---深入模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day42 - &lt;a href="./Day41-55/42.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md"&gt;深入模型&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;关系型数据库配置&lt;/li&gt;
&lt;li&gt;管理后台的使用&lt;/li&gt;
&lt;li&gt;使用ORM完成对模型的CRUD操作&lt;/li&gt;
&lt;li&gt;Django模型最佳实践&lt;/li&gt;
&lt;li&gt;模型定义参考&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day43---静态资源和ajax请求" class="anchor" aria-hidden="true" href="#day43---静态资源和ajax请求"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day43 - &lt;a href="./Day41-55/43.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md"&gt;静态资源和Ajax请求&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;加载静态资源&lt;/li&gt;
&lt;li&gt;用Ajax请求获取数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day44---表单的应用" class="anchor" aria-hidden="true" href="#day44---表单的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day44 - &lt;a href="./Day41-55/44.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;表单的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;表单和表单控件&lt;/li&gt;
&lt;li&gt;跨站请求伪造和CSRF令牌&lt;/li&gt;
&lt;li&gt;Form和ModelForm&lt;/li&gt;
&lt;li&gt;表单验证&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day45---cookie和session" class="anchor" aria-hidden="true" href="#day45---cookie和session"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day45 - &lt;a href="./Day41-55/45.Cookie%E5%92%8CSession.md"&gt;Cookie和Session&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;实现用户跟踪&lt;/li&gt;
&lt;li&gt;cookie和session的关系&lt;/li&gt;
&lt;li&gt;Django框架对session的支持&lt;/li&gt;
&lt;li&gt;视图函数中的cookie读写操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day46---报表和日志" class="anchor" aria-hidden="true" href="#day46---报表和日志"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day46 - &lt;a href="./Day41-55/46.%E6%8A%A5%E8%A1%A8%E5%92%8C%E6%97%A5%E5%BF%97.md"&gt;报表和日志&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;通过HttpResponse修改响应头&lt;/li&gt;
&lt;li&gt;使用StreamingHttpResponse处理大文件&lt;/li&gt;
&lt;li&gt;使用xlwt生成Excel报表&lt;/li&gt;
&lt;li&gt;使用reportlab生成PDF报表&lt;/li&gt;
&lt;li&gt;使用ECharts生成前端图表&lt;/li&gt;
&lt;li&gt;配置日志和Django-Debug-Toolbar&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day47---中间件的应用" class="anchor" aria-hidden="true" href="#day47---中间件的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day47 - &lt;a href="./Day41-55/47.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;中间件的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;什么是中间件&lt;/li&gt;
&lt;li&gt;Django框架内置的中间件&lt;/li&gt;
&lt;li&gt;自定义中间件及其应用场景&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day48---前后端分离开发入门" class="anchor" aria-hidden="true" href="#day48---前后端分离开发入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day48 - &lt;a href="./Day41-55/48.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md"&gt;前后端分离开发入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;返回JSON格式的数据&lt;/li&gt;
&lt;li&gt;用Vue.js渲染页面&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day49---restful架构和drf入门" class="anchor" aria-hidden="true" href="#day49---restful架构和drf入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day49 - &lt;a href="./Day41-55/49.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md"&gt;RESTful架构和DRF入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day50---restful架构和drf进阶" class="anchor" aria-hidden="true" href="#day50---restful架构和drf进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day50 - &lt;a href="./Day41-55/50.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md"&gt;RESTful架构和DRF进阶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day51---使用缓存" class="anchor" aria-hidden="true" href="#day51---使用缓存"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day51 - &lt;a href="./Day41-55/51.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md"&gt;使用缓存&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;网站优化第一定律&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在Django项目中使用Redis提供缓存服务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在视图函数中读写缓存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用装饰器实现页面缓存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为数据接口提供缓存服务&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day52---文件上传和富文本编辑" class="anchor" aria-hidden="true" href="#day52---文件上传和富文本编辑"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day52 - &lt;a href="./Day41-55/52.%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8.md"&gt;文件上传和富文本编辑&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;文件上传表单控件和图片文件预览&lt;/li&gt;
&lt;li&gt;服务器端如何处理上传的文件&lt;/li&gt;
&lt;li&gt;富文本编辑器概述&lt;/li&gt;
&lt;li&gt;wangEditor的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day53---短信和邮件" class="anchor" aria-hidden="true" href="#day53---短信和邮件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day53 - &lt;a href="./Day41-55/53.%E7%9F%AD%E4%BF%A1%E5%92%8C%E9%82%AE%E4%BB%B6.md"&gt;短信和邮件&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;常用短信网关平台介绍&lt;/li&gt;
&lt;li&gt;使用螺丝帽发送短信&lt;/li&gt;
&lt;li&gt;Django框架对邮件服务的支持&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day54---异步任务和定时任务" class="anchor" aria-hidden="true" href="#day54---异步任务和定时任务"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day54 - &lt;a href="./Day41-55/54.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md"&gt;异步任务和定时任务&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;网站优化第二定律&lt;/li&gt;
&lt;li&gt;配置消息队列服务&lt;/li&gt;
&lt;li&gt;在项目中使用celery实现任务异步化&lt;/li&gt;
&lt;li&gt;在项目中使用celery实现定时任务&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day55---单元测试和项目上线" class="anchor" aria-hidden="true" href="#day55---单元测试和项目上线"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day55 - &lt;a href="./Day41-55/55.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md"&gt;单元测试和项目上线&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python中的单元测试&lt;/li&gt;
&lt;li&gt;Django框架对单元测试的支持&lt;/li&gt;
&lt;li&gt;使用版本控制系统&lt;/li&gt;
&lt;li&gt;配置和使用uWSGI&lt;/li&gt;
&lt;li&gt;动静分离和Nginx配置&lt;/li&gt;
&lt;li&gt;配置HTTPS&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day5660---实战flask" class="anchor" aria-hidden="true" href="#day5660---实战flask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56~60 - &lt;a href="./Day56-65"&gt;实战Flask&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day56---flask入门" class="anchor" aria-hidden="true" href="#day56---flask入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56 - &lt;a href="./Day56-60/56.Flask%E5%85%A5%E9%97%A8.md"&gt;Flask入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day57---模板的使用" class="anchor" aria-hidden="true" href="#day57---模板的使用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day57 - &lt;a href="./Day56-60/57.%E6%A8%A1%E6%9D%BF%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;模板的使用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day58---表单的处理" class="anchor" aria-hidden="true" href="#day58---表单的处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day58 - &lt;a href="./Day56-60/58.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%A4%84%E7%90%86.md"&gt;表单的处理&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day59---数据库操作" class="anchor" aria-hidden="true" href="#day59---数据库操作"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day59 - &lt;a href="./Day56-60/59.%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C.md"&gt;数据库操作&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day60---项目实战" class="anchor" aria-hidden="true" href="#day60---项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day60 - &lt;a href="./Day56-60/60.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day6165---实战tornado" class="anchor" aria-hidden="true" href="#day6165---实战tornado"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61~65 - &lt;a href="./Day61-65"&gt;实战Tornado&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day61---预备知识" class="anchor" aria-hidden="true" href="#day61---预备知识"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61 - &lt;a href="./Day61-65/61.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md"&gt;预备知识&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;并发编程&lt;/li&gt;
&lt;li&gt;I/O模式和事件驱动&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day62---tornado入门" class="anchor" aria-hidden="true" href="#day62---tornado入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day62 - &lt;a href="./Day61-65/62.Tornado%E5%85%A5%E9%97%A8.md"&gt;Tornado入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tornado概述&lt;/li&gt;
&lt;li&gt;5分钟上手Tornado&lt;/li&gt;
&lt;li&gt;路由解析&lt;/li&gt;
&lt;li&gt;请求处理器&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day63---异步化" class="anchor" aria-hidden="true" href="#day63---异步化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day63 - &lt;a href="./Day61-65/63.%E5%BC%82%E6%AD%A5%E5%8C%96.md"&gt;异步化&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;aiomysql和aioredis的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day64---websocket的应用" class="anchor" aria-hidden="true" href="#day64---websocket的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day64 - &lt;a href="./Day61-65/64.WebSocket%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;WebSocket的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebSocket简介&lt;/li&gt;
&lt;li&gt;WebSocket服务器端编程&lt;/li&gt;
&lt;li&gt;WebSocket客户端编程&lt;/li&gt;
&lt;li&gt;项目：Web聊天室&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day65---项目实战" class="anchor" aria-hidden="true" href="#day65---项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day65 - &lt;a href="./Day61-65/65.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;前后端分离开发和接口文档的撰写&lt;/li&gt;
&lt;li&gt;使用Vue.js实现前端渲染&lt;/li&gt;
&lt;li&gt;使用ECharts实现报表功能&lt;/li&gt;
&lt;li&gt;使用WebSocket实现推送服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day6675---爬虫开发" class="anchor" aria-hidden="true" href="#day6675---爬虫开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66~75 - &lt;a href="./Day66-75"&gt;爬虫开发&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day66---网络爬虫和相关工具" class="anchor" aria-hidden="true" href="#day66---网络爬虫和相关工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66 - &lt;a href="./Day66-75/66.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7.md"&gt;网络爬虫和相关工具&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;网络爬虫的概念及其应用领域&lt;/li&gt;
&lt;li&gt;网络爬虫的合法性探讨&lt;/li&gt;
&lt;li&gt;开发网络爬虫的相关工具&lt;/li&gt;
&lt;li&gt;一个爬虫程序的构成&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day67---数据采集和解析" class="anchor" aria-hidden="true" href="#day67---数据采集和解析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day67 - &lt;a href="./Day66-75/67.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90.md"&gt;数据采集和解析&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;数据采集的标准和三方库&lt;/li&gt;
&lt;li&gt;页面解析的三种方式：正则表达式解析 / XPath解析 / CSS选择器解析&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day68---存储数据" class="anchor" aria-hidden="true" href="#day68---存储数据"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day68 - &lt;a href="./Day66-75/68.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE.md"&gt;存储数据&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;如何存储海量数据&lt;/li&gt;
&lt;li&gt;实现数据的缓存&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day69---并发下载" class="anchor" aria-hidden="true" href="#day69---并发下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day69 - &lt;a href="./Day66-75/69.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD.md"&gt;并发下载&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;多线程和多进程&lt;/li&gt;
&lt;li&gt;异步I/O和协程&lt;/li&gt;
&lt;li&gt;async和await关键字的使用&lt;/li&gt;
&lt;li&gt;三方库aiohttp的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day70---解析动态内容" class="anchor" aria-hidden="true" href="#day70---解析动态内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day70 - &lt;a href="./Day66-75/70.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md"&gt;解析动态内容&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JavaScript逆向工程&lt;/li&gt;
&lt;li&gt;使用Selenium获取动态内容&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day71---表单交互和验证码处理" class="anchor" aria-hidden="true" href="#day71---表单交互和验证码处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day71 - &lt;a href="./Day66-75/71.%E8%A1%A8%E5%8D%95%E4%BA%A4%E4%BA%92%E5%92%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E5%A4%84%E7%90%86.md"&gt;表单交互和验证码处理&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;自动提交表单&lt;/li&gt;
&lt;li&gt;Cookie池的应用&lt;/li&gt;
&lt;li&gt;验证码处理&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day72---scrapy入门" class="anchor" aria-hidden="true" href="#day72---scrapy入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day72 - &lt;a href="./Day66-75/72.Scrapy%E5%85%A5%E9%97%A8.md"&gt;Scrapy入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Scrapy爬虫框架概述&lt;/li&gt;
&lt;li&gt;安装和使用Scrapy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day73---scrapy高级应用" class="anchor" aria-hidden="true" href="#day73---scrapy高级应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day73 - &lt;a href="./Day66-75/73.Scrapy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md"&gt;Scrapy高级应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Spider的用法&lt;/li&gt;
&lt;li&gt;中间件的应用：下载中间件 / 蜘蛛中间件&lt;/li&gt;
&lt;li&gt;Scrapy对接Selenium抓取动态内容&lt;/li&gt;
&lt;li&gt;Scrapy部署到Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day74---scrapy分布式实现" class="anchor" aria-hidden="true" href="#day74---scrapy分布式实现"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day74 - &lt;a href="./Day66-75/74.Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0.md"&gt;Scrapy分布式实现&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;分布式爬虫的原理&lt;/li&gt;
&lt;li&gt;Scrapy分布式实现&lt;/li&gt;
&lt;li&gt;使用Scrapyd实现分布式部署&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day75---爬虫项目实战" class="anchor" aria-hidden="true" href="#day75---爬虫项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day75 - &lt;a href="./Day66-75/75.%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;爬虫项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;爬取招聘网站数据&lt;/li&gt;
&lt;li&gt;爬取房地产行业数据&lt;/li&gt;
&lt;li&gt;爬取二手车交易平台数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day7690---数据处理和机器学习" class="anchor" aria-hidden="true" href="#day7690---数据处理和机器学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76~90 - &lt;a href="./Day76-90"&gt;数据处理和机器学习&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day76---机器学习基础" class="anchor" aria-hidden="true" href="#day76---机器学习基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76 - &lt;a href="./Day76-90/76.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md"&gt;机器学习基础&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day77---pandas的应用" class="anchor" aria-hidden="true" href="#day77---pandas的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day77 - &lt;a href="./Day76-90/77.Pandas%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;Pandas的应用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day78---numpy和scipy的应用" class="anchor" aria-hidden="true" href="#day78---numpy和scipy的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day78 - &lt;a href="./Day76-90/78.NumPy%E5%92%8CSciPy%E7%9A%84%E5%BA%94%E7%94%A8"&gt;NumPy和SciPy的应用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day79---matplotlib和数据可视化" class="anchor" aria-hidden="true" href="#day79---matplotlib和数据可视化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day79 - &lt;a href="./Day76-90/79.Matplotlib%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"&gt;Matplotlib和数据可视化&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day80---k最近邻knn分类" class="anchor" aria-hidden="true" href="#day80---k最近邻knn分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day80 - &lt;a href="./Day76-90/80.k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB.md"&gt;k最近邻(KNN)分类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day81---决策树" class="anchor" aria-hidden="true" href="#day81---决策树"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day81 - &lt;a href="./Day76-90/81.%E5%86%B3%E7%AD%96%E6%A0%91.md"&gt;决策树&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day82---贝叶斯分类" class="anchor" aria-hidden="true" href="#day82---贝叶斯分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day82 - &lt;a href="./Day76-90/82.%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB.md"&gt;贝叶斯分类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day83---支持向量机svm" class="anchor" aria-hidden="true" href="#day83---支持向量机svm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day83 - &lt;a href="./Day76-90/83.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md"&gt;支持向量机(SVM)&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day84---k-均值聚类" class="anchor" aria-hidden="true" href="#day84---k-均值聚类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day84 - &lt;a href="./Day76-90/84.K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB.md"&gt;K-均值聚类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day85---回归分析" class="anchor" aria-hidden="true" href="#day85---回归分析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day85 - &lt;a href="./Day76-90/85.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90.md"&gt;回归分析&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day86---大数据分析入门" class="anchor" aria-hidden="true" href="#day86---大数据分析入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day86 - &lt;a href="./Day76-90/86.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8.md"&gt;大数据分析入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day87---大数据分析进阶" class="anchor" aria-hidden="true" href="#day87---大数据分析进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day87 - &lt;a href="./Day76-90/87.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BF%9B%E9%98%B6.md"&gt;大数据分析进阶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day88---tensorflow入门" class="anchor" aria-hidden="true" href="#day88---tensorflow入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day88 - &lt;a href="./Day76-90/88.Tensorflow%E5%85%A5%E9%97%A8.md"&gt;Tensorflow入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day89---tensorflow实战" class="anchor" aria-hidden="true" href="#day89---tensorflow实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day89 - &lt;a href="./Day76-90/89.Tensorflow%E5%AE%9E%E6%88%98.md"&gt;Tensorflow实战&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day90---推荐系统" class="anchor" aria-hidden="true" href="#day90---推荐系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day90 - &lt;a href="./Day76-90/90.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.md"&gt;推荐系统&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day91100---团队项目开发" class="anchor" aria-hidden="true" href="#day91100---团队项目开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day91~100 - &lt;a href="./Day91-100"&gt;团队项目开发&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第91天团队项目开发的问题和解决方案" class="anchor" aria-hidden="true" href="#第91天团队项目开发的问题和解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第91天：&lt;a href="./Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;团队项目开发的问题和解决方案&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;软件过程模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;经典过程模型（瀑布模型）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可行性分析（研究做还是不做），输出《可行性分析报告》。&lt;/li&gt;
&lt;li&gt;需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。&lt;/li&gt;
&lt;li&gt;概要设计和详细设计，输出概念模型图、物理模型图、类图、时序图等。&lt;/li&gt;
&lt;li&gt;编码 / 测试。&lt;/li&gt;
&lt;li&gt;上线 / 维护。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;产品的Backlog（用户故事、产品原型）。&lt;/li&gt;
&lt;li&gt;计划会议（评估和预算）。&lt;/li&gt;
&lt;li&gt;日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。&lt;/li&gt;
&lt;li&gt;修复bug（问题描述、重现步骤、测试人员、被指派人）。&lt;/li&gt;
&lt;li&gt;评审会议（Showcase）。&lt;/li&gt;
&lt;li&gt;回顾会议（当前周期做得好和不好的地方）。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;补充：敏捷软件开发宣言&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;个体和互动&lt;/strong&gt; 高于 流程和工具&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作的软件&lt;/strong&gt; 高于 详尽的文档&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;客户合作&lt;/strong&gt; 高于 合同谈判&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;响应变化&lt;/strong&gt; 高于 遵循计划&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/agile-scrum-sprint-cycle.png"&gt;&lt;img src="./res/agile-scrum-sprint-cycle.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;敏捷团队通常人数为8-10人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在白板上面，白板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;项目团队组建&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;团队的构成和角色&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;说明：谢谢付祥英女士绘制了下面这张精美的公司组织架构图。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/company_architecture.png"&gt;&lt;img src="./res/company_architecture.png" alt="company_architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编程规范和代码审查（flake8、pylint）&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/pylint.png"&gt;&lt;img src="./res/pylint.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python中的一些“惯例”（请参考&lt;a href="Python%E6%83%AF%E4%BE%8B.md"&gt;《Python惯例-如何编写Pythonic的代码》&lt;/a&gt;）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;影响代码可读性的原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码注释太少或者没有注释&lt;/li&gt;
&lt;li&gt;代码破坏了语言的最佳实践&lt;/li&gt;
&lt;li&gt;反模式编程（意大利面代码、复制-黏贴编程、自负编程、……）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;团队开发工具介绍&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;版本控制：Git、Mercury&lt;/li&gt;
&lt;li&gt;缺陷管理：&lt;a href="https://about.gitlab.com/" rel="nofollow"&gt;Gitlab&lt;/a&gt;、&lt;a href="http://www.redmine.org.cn/" rel="nofollow"&gt;Redmine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;敏捷闭环工具：&lt;a href="https://www.zentao.net/" rel="nofollow"&gt;禅道&lt;/a&gt;、&lt;a href="https://www.atlassian.com/software/jira/features" rel="nofollow"&gt;JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;持续集成：&lt;a href="https://jenkins.io/" rel="nofollow"&gt;Jenkins&lt;/a&gt;、&lt;a href="https://travis-ci.org/" rel="nofollow"&gt;Travis-CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;请参考&lt;a href="Day91-100/%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91.md"&gt;《团队项目开发》&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目选题和理解业务" class="anchor" aria-hidden="true" href="#项目选题和理解业务"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目选题和理解业务&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;选题范围设定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CMS（用户端）：新闻聚合网站、问答/分享社区、影评/书评网站等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他类型：自身行业背景和工作经验、业务容易理解和把控。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需求理解、模块划分和任务分配&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需求理解：头脑风暴和竞品分析。&lt;/li&gt;
&lt;li&gt;模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。&lt;/li&gt;
&lt;li&gt;任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/requirements_by_xmind.png"&gt;&lt;img src="./res/requirements_by_xmind.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;制定项目进度表（每日更新）&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;th&gt;人员&lt;/th&gt;
&lt;th&gt;状态&lt;/th&gt;
&lt;th&gt;完成&lt;/th&gt;
&lt;th&gt;工时&lt;/th&gt;
&lt;th&gt;计划开始&lt;/th&gt;
&lt;th&gt;实际开始&lt;/th&gt;
&lt;th&gt;计划结束&lt;/th&gt;
&lt;th&gt;实际结束&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;评论&lt;/td&gt;
&lt;td&gt;添加评论&lt;/td&gt;
&lt;td&gt;王大锤&lt;/td&gt;
&lt;td&gt;正在进行&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;删除评论&lt;/td&gt;
&lt;td&gt;王大锤&lt;/td&gt;
&lt;td&gt;等待&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;查看评论&lt;/td&gt;
&lt;td&gt;白元芳&lt;/td&gt;
&lt;td&gt;正在进行&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;需要进行代码审查&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;评论投票&lt;/td&gt;
&lt;td&gt;白元芳&lt;/td&gt;
&lt;td&gt;等待&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OOAD和数据库设计&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;UML（统一建模语言）的类图&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/uml-class-diagram.png"&gt;&lt;img src="./res/uml-class-diagram.png" alt="uml" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过模型创建表（正向工程）&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py makemigrations app
python manage.py migrate&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用PowerDesigner绘制物理模型图。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/power-designer-pdm.png"&gt;&lt;img src="./res/power-designer-pdm.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过数据表创建模型（反向工程）&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py inspectdb &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; app/models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-第92天使用docker部署应用" class="anchor" aria-hidden="true" href="#第92天使用docker部署应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第92天：&lt;a href="./Day91-100/92.%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8.md"&gt;使用Docker部署应用&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Docker简介&lt;/li&gt;
&lt;li&gt;安装Docker&lt;/li&gt;
&lt;li&gt;使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins）&lt;/li&gt;
&lt;li&gt;构建Docker镜像（Dockerfile的编写和相关指令）&lt;/li&gt;
&lt;li&gt;容器编排（Docker-compose）&lt;/li&gt;
&lt;li&gt;集群管理&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第93天mysql性能优化" class="anchor" aria-hidden="true" href="#第93天mysql性能优化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第93天：&lt;a href="./Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md"&gt;MySQL性能优化&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第94天网络api接口设计" class="anchor" aria-hidden="true" href="#第94天网络api接口设计"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第94天：&lt;a href="./Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md"&gt;网络API接口设计&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第95天使用django开发商业项目day91-10095使用django开发商业项目md" class="anchor" aria-hidden="true" href="#第95天使用django开发商业项目day91-10095使用django开发商业项目md"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项	目.md)&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-项目开发中的公共问题" class="anchor" aria-hidden="true" href="#项目开发中的公共问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目开发中的公共问题&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;数据库的配置（多数据库、主从复制、数据库路由）&lt;/li&gt;
&lt;li&gt;缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵））&lt;/li&gt;
&lt;li&gt;日志的配置&lt;/li&gt;
&lt;li&gt;分析和调试（Django-Debug-ToolBar）&lt;/li&gt;
&lt;li&gt;好用的Python模块（日期计算、图像处理、数据加密、三方API）&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-rest-api设计" class="anchor" aria-hidden="true" href="#rest-api设计"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;REST API设计&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;RESTful架构
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2011/09/restful.html" rel="nofollow"&gt;理解RESTful架构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html" rel="nofollow"&gt;RESTful API设计指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html" rel="nofollow"&gt;RESTful API最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;API接口文档的撰写
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rap2.taobao.org/" rel="nofollow"&gt;RAP2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yapi.demo.qunar.com/" rel="nofollow"&gt;YAPI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.django-rest-framework.org/" rel="nofollow"&gt;django-REST-framework&lt;/a&gt;的应用&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目中的重点难点剖析" class="anchor" aria-hidden="true" href="#项目中的重点难点剖析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目中的重点难点剖析&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;使用缓存缓解数据库压力 - Redis&lt;/li&gt;
&lt;li&gt;使用消息队列做解耦合和削峰 - Celery + RabbitMQ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第96天软件测试和自动化测试" class="anchor" aria-hidden="true" href="#第96天软件测试和自动化测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第96天：&lt;a href="Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md"&gt;软件测试和自动化测试&lt;/a&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-单元测试" class="anchor" aria-hidden="true" href="#单元测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;单元测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;测试的种类&lt;/li&gt;
&lt;li&gt;编写单元测试（unittest、pytest、nose2、tox、ddt、……）&lt;/li&gt;
&lt;li&gt;测试覆盖率（coverage）&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目部署" class="anchor" aria-hidden="true" href="#项目部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目部署&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;部署前的准备工作
&lt;ul&gt;
&lt;li&gt;关键设置（SECRET_KEY / DEBUG / ALLOWED_HOSTS / 缓存 / 数据库）&lt;/li&gt;
&lt;li&gt;HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE&lt;/li&gt;
&lt;li&gt;日志相关配置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linux常用命令回顾&lt;/li&gt;
&lt;li&gt;Linux常用服务的安装和配置&lt;/li&gt;
&lt;li&gt;uWSGI/Gunicorn和Nginx的使用
&lt;ul&gt;
&lt;li&gt;Gunicorn和uWSGI的比较
&lt;ul&gt;
&lt;li&gt;对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。&lt;/li&gt;
&lt;li&gt;uWSGI支持异构部署。&lt;/li&gt;
&lt;li&gt;由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。&lt;/li&gt;
&lt;li&gt;在性能上，Gunicorn和uWSGI其实表现相当。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用虚拟化技术（Docker）部署测试环境和生产环境&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-性能测试" class="anchor" aria-hidden="true" href="#性能测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;性能测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;AB的使用&lt;/li&gt;
&lt;li&gt;SQLslap的使用&lt;/li&gt;
&lt;li&gt;sysbench的使用&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-自动化测试" class="anchor" aria-hidden="true" href="#自动化测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自动化测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;使用Shell和Python进行自动化测试&lt;/li&gt;
&lt;li&gt;使用Selenium实现自动化测试
&lt;ul&gt;
&lt;li&gt;Selenium IDE&lt;/li&gt;
&lt;li&gt;Selenium WebDriver&lt;/li&gt;
&lt;li&gt;Selenium Remote Control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;测试工具Robot Framework介绍&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第97天电商网站技术要点剖析" class="anchor" aria-hidden="true" href="#第97天电商网站技术要点剖析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第97天：&lt;a href="./Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md"&gt;电商网站技术要点剖析&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第98天项目部署上线和性能调优" class="anchor" aria-hidden="true" href="#第98天项目部署上线和性能调优"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第98天：&lt;a href="./Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md"&gt;项目部署上线和性能调优&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;MySQL数据库调优&lt;/li&gt;
&lt;li&gt;Web服务器性能优化
&lt;ul&gt;
&lt;li&gt;Nginx负载均衡配置&lt;/li&gt;
&lt;li&gt;Keepalived实现高可用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;代码性能调优
&lt;ul&gt;
&lt;li&gt;多线程&lt;/li&gt;
&lt;li&gt;异步化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;静态资源访问优化
&lt;ul&gt;
&lt;li&gt;云存储&lt;/li&gt;
&lt;li&gt;CDN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第99天面试中的公共问题" class="anchor" aria-hidden="true" href="#第99天面试中的公共问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第99天：&lt;a href="./Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md"&gt;面试中的公共问题&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第100天python面试题集" class="anchor" aria-hidden="true" href="#第100天python面试题集"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第100天：&lt;a href="./Day91-100/100.Python%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86.md"&gt;Python面试题集&lt;/a&gt;&lt;/h4&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jackfrued</author><guid isPermaLink="false">https://github.com/jackfrued/Python-100-Days</guid><pubDate>Fri, 08 Nov 2019 00:04:00 GMT</pubDate></item><item><title>ShusenTang/Dive-into-DL-PyTorch #5 in Jupyter Notebook, Today</title><link>https://github.com/ShusenTang/Dive-into-DL-PyTorch</link><description>&lt;p&gt;&lt;i&gt;本项目将《动手学深度学习》(Dive into Deep Learning)原书中的MXNet实现改为PyTorch实现。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="docs/README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="img/cover.png"&gt;&lt;img width="500" src="img/cover.png" alt="封面" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch" rel="nofollow"&gt;本项目&lt;/a&gt;将&lt;a href="http://zh.d2l.ai/" rel="nofollow"&gt;《动手学深度学习》&lt;/a&gt; 原书中MXNet代码实现改为PyTorch实现。原书作者：阿斯顿·张、李沐、扎卡里 C. 立顿、亚历山大 J. 斯莫拉以及其他社区贡献者，GitHub地址：&lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;此书的&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;中&lt;/a&gt;&lt;a href="https://d2l.ai/" rel="nofollow"&gt;英&lt;/a&gt;版本存在一些不同，针对此书英文版的PyTorch重构可参考&lt;a href="https://github.com/dsgiitr/d2l-pytorch"&gt;这个项目&lt;/a&gt;。
There are some differences between the &lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;Chinese&lt;/a&gt; and &lt;a href="https://d2l.ai/" rel="nofollow"&gt;English&lt;/a&gt; versions of this book. For the PyTorch modifying of the English version, you can refer to &lt;a href="https://github.com/dsgiitr/d2l-pytorch"&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;本仓库主要包含code和docs两个文件夹（外加一些数据存放在data中）。其中code文件夹就是每章相关jupyter notebook代码（基于PyTorch）；docs文件夹就是markdown格式的《动手学深度学习》书中的相关内容，然后利用&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;将网页文档部署到GitHub Pages上，由于原书使用的是MXNet框架，所以docs内容可能与原书略有不同，但是整体内容是一样的。欢迎对本项目做出贡献或提出issue。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-面向人群" class="anchor" aria-hidden="true" href="#面向人群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;面向人群&lt;/h2&gt;
&lt;p&gt;本项目面向对深度学习感兴趣，尤其是想使用PyTorch进行深度学习的童鞋。本项目并不要求你有任何深度学习或者机器学习的背景知识，你只需了解基础的数学和编程，如基础的线性代数、微分和概率，以及基础的Python编程。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-食用方法" class="anchor" aria-hidden="true" href="#食用方法"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;食用方法&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-方法一" class="anchor" aria-hidden="true" href="#方法一"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;方法一&lt;/h3&gt;
&lt;p&gt;本仓库包含一些latex公式，但github的markdown原生是不支持公式显示的，而docs文件夹已经利用&lt;a href="https://docsify.js.org/#/zh-cn/" rel="nofollow"&gt;docsify&lt;/a&gt;被部署到了GitHub Pages上，所以查看文档最简便的方法就是直接访问&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch" rel="nofollow"&gt;本项目网页版&lt;/a&gt;。当然如果你还想跑一下运行相关代码的话还是得把本项目clone下来，然后运行code文件夹下相关代码。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-方法二" class="anchor" aria-hidden="true" href="#方法二"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;方法二&lt;/h3&gt;
&lt;p&gt;你还可以在本地访问文档，先安装&lt;code&gt;docsify-cli&lt;/code&gt;工具:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;npm i docsify-cli -g&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后将本项目clone到本地:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; Dive-into-DL-PyTorch&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后运行一个本地服务器，这样就可以很方便的在&lt;code&gt;http://localhost:3000&lt;/code&gt;实时访问文档网页渲染效果。&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;docsify serve docs&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="read_guide.md"&gt;阅读指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter01_DL-intro/deep-learning-intro.md"&gt;1. 深度学习简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2. 预备知识
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.1_install.md"&gt;2.1 环境配置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.2_tensor.md"&gt;2.2 数据操作&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter02_prerequisite/2.3_autograd.md"&gt;2.3 自动求梯度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3. 深度学习基础
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.1_linear-regression.md"&gt;3.1 线性回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.2_linear-regression-scratch.md"&gt;3.2 线性回归的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.3_linear-regression-pytorch.md"&gt;3.3 线性回归的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.4_softmax-regression.md"&gt;3.4 softmax回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.5_fashion-mnist.md"&gt;3.5 图像分类数据集（Fashion-MNIST）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.6_softmax-regression-scratch.md"&gt;3.6 softmax回归的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.7_softmax-regression-pytorch.md"&gt;3.7 softmax回归的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.8_mlp.md"&gt;3.8 多层感知机&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.9_mlp-scratch.md"&gt;3.9 多层感知机的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.10_mlp-pytorch.md"&gt;3.10 多层感知机的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.11_underfit-overfit.md"&gt;3.11 模型选择、欠拟合和过拟合&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.12_weight-decay.md"&gt;3.12 权重衰减&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.13_dropout.md"&gt;3.13 丢弃法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.14_backprop.md"&gt;3.14 正向传播、反向传播和计算图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.15_numerical-stability-and-init.md"&gt;3.15 数值稳定性和模型初始化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter03_DL-basics/3.16_kaggle-house-price.md"&gt;3.16 实战Kaggle比赛：房价预测&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4. 深度学习计算
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.1_model-construction.md"&gt;4.1 模型构造&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.2_parameters.md"&gt;4.2 模型参数的访问、初始化和共享&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.3_deferred-init.md"&gt;4.3 模型参数的延后初始化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.4_custom-layer.md"&gt;4.4 自定义层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.5_read-write.md"&gt;4.5 读取和存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter04_DL_computation/4.6_use-gpu.md"&gt;4.6 GPU计算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;5. 卷积神经网络
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.1_conv-layer.md"&gt;5.1 二维卷积层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.2_padding-and-strides.md"&gt;5.2 填充和步幅&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.3_channels.md"&gt;5.3 多输入通道和多输出通道&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.4_pooling.md"&gt;5.4 池化层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.5_lenet.md"&gt;5.5 卷积神经网络（LeNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.6_alexnet.md"&gt;5.6 深度卷积神经网络（AlexNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.7_vgg.md"&gt;5.7 使用重复元素的网络（VGG）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.8_nin.md"&gt;5.8 网络中的网络（NiN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.9_googlenet.md"&gt;5.9 含并行连结的网络（GoogLeNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.10_batch-norm.md"&gt;5.10 批量归一化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.11_resnet.md"&gt;5.11 残差网络（ResNet）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter05_CNN/5.12_densenet.md"&gt;5.12 稠密连接网络（DenseNet）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;6. 循环神经网络
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.1_lang-model.md"&gt;6.1 语言模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.2_rnn.md"&gt;6.2 循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.3_lang-model-dataset.md"&gt;6.3 语言模型数据集（周杰伦专辑歌词）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.4_rnn-scratch.md"&gt;6.4 循环神经网络的从零开始实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.5_rnn-pytorch.md"&gt;6.5 循环神经网络的简洁实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.6_bptt.md"&gt;6.6 通过时间反向传播&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.7_gru.md"&gt;6.7 门控循环单元（GRU）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.8_lstm.md"&gt;6.8 长短期记忆（LSTM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.9_deep-rnn.md"&gt;6.9 深度循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter06_RNN/6.10_bi-rnn.md"&gt;6.10 双向循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;7. 优化算法
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.1_optimization-intro.md"&gt;7.1 优化与深度学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.2_gd-sgd.md"&gt;7.2 梯度下降和随机梯度下降&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.3_minibatch-sgd.md"&gt;7.3 小批量随机梯度下降&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.4_momentum.md"&gt;7.4 动量法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.5_adagrad.md"&gt;7.5 AdaGrad算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.6_rmsprop.md"&gt;7.6 RMSProp算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.7_adadelta.md"&gt;7.7 AdaDelta算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter07_optimization/7.8_adam.md"&gt;7.8 Adam算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;8. 计算性能
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.1_hybridize.md"&gt;8.1 命令式和符号式混合编程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.2_async-computation.md"&gt;8.2 异步计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.3_auto-parallelism.md"&gt;8.3 自动并行计算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter08_computational-performance/8.4_multiple-gpus.md"&gt;8.4 多GPU计算&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9. 计算机视觉
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.1_image-augmentation.md"&gt;9.1 图像增广&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.2_fine-tuning.md"&gt;9.2 微调&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.3_bounding-box.md"&gt;9.3 目标检测和边界框&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.4_anchor.md"&gt;9.4 锚框&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.5_multiscale-object-detection.md"&gt;9.5 多尺度目标检测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter09_computer-vision/9.6_object-detection-dataset.md"&gt;9.6 目标检测数据集（皮卡丘）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;待更新...&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;10. 自然语言处理
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.1_word2vec.md"&gt;10.1 词嵌入（word2vec）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.2_approx-training.md"&gt;10.2 近似训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.3_word2vec-pytorch.md"&gt;10.3 word2vec的实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.4_fasttext.md"&gt;10.4 子词嵌入（fastText）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.5_glove.md"&gt;10.5 全局向量的词嵌入（GloVe）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.6_similarity-analogy.md"&gt;10.6 求近义词和类比词&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md"&gt;10.7 文本情感分类：使用循环神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md"&gt;10.8 文本情感分类：使用卷积神经网络（textCNN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.9_seq2seq.md"&gt;10.9 编码器—解码器（seq2seq）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.10_beam-search.md"&gt;10.10 束搜索&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.11_attention.md"&gt;10.11 注意力机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter10_natural-language-processing/10.12_machine-translation.md"&gt;10.12 机器翻译&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;持续更新中......&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-原书地址" class="anchor" aria-hidden="true" href="#原书地址"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;原书地址&lt;/h2&gt;
&lt;p&gt;中文版：&lt;a href="https://zh.d2l.ai/" rel="nofollow"&gt;动手学深度学习&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-zh"&gt;Github仓库&lt;/a&gt;&lt;br&gt;
English Version: &lt;a href="https://d2l.ai/" rel="nofollow"&gt;Dive into Deep Learning&lt;/a&gt; | &lt;a href="https://github.com/d2l-ai/d2l-en"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-引用" class="anchor" aria-hidden="true" href="#引用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;引用&lt;/h2&gt;
&lt;p&gt;如果您在研究中使用了这个项目请引用原书:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ShusenTang</author><guid isPermaLink="false">https://github.com/ShusenTang/Dive-into-DL-PyTorch</guid><pubDate>Fri, 08 Nov 2019 00:05:00 GMT</pubDate></item><item><title>nianticlabs/monodepth2 #6 in Jupyter Notebook, Today</title><link>https://github.com/nianticlabs/monodepth2</link><description>&lt;p&gt;&lt;i&gt;Monocular depth estimation from a single image&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-monodepth2" class="anchor" aria-hidden="true" href="#monodepth2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Monodepth2&lt;/h1&gt;
&lt;p&gt;This is the reference PyTorch implementation for training and testing depth estimation models using the method described in&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Digging into Self-Supervised Monocular Depth Prediction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www0.cs.ucl.ac.uk/staff/C.Godard/" rel="nofollow"&gt;Clément Godard&lt;/a&gt;, &lt;a href="http://vision.caltech.edu/~macaodha/" rel="nofollow"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://www.michaelfirman.co.uk" rel="nofollow"&gt;Michael Firman&lt;/a&gt; and &lt;a href="http://www0.cs.ucl.ac.uk/staff/g.brostow/" rel="nofollow"&gt;Gabriel J. Brostow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1806.01260" rel="nofollow"&gt;ICCV 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="assets/teaser.gif"&gt;&lt;img src="assets/teaser.gif" alt="example input output gif" width="600" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;This code is for non-commercial use; please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;p&gt;If you find our work useful in your research please consider citing our paper:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{monodepth2,
  title     = {Digging into Self-Supervised Monocular Depth Prediction},
  author    = {Cl{\'{e}}ment Godard and
               Oisin {Mac Aodha} and
               Michael Firman and
               Gabriel J. Brostow},
  booktitle = {The International Conference on Computer Vision (ICCV)},
  month = {October},
year = {2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-️-setup" class="anchor" aria-hidden="true" href="#️-setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png"&gt;⚙️&lt;/g-emoji&gt; Setup&lt;/h2&gt;
&lt;p&gt;Assuming a fresh &lt;a href="https://www.anaconda.com/download/" rel="nofollow"&gt;Anaconda&lt;/a&gt; distribution, you can install the dependencies with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch
pip install tensorboardX==1.4
conda install opencv=3.3.1   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; just needed for evaluation&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We ran our experiments with PyTorch 0.4.1, CUDA 9.1, Python 3.6 and Ubuntu 18.04.
We have also successfully trained models with PyTorch 1.0, and our code is compatible with Python 2.7.&lt;/p&gt;

&lt;h2&gt;&lt;a id="user-content-️-prediction-for-a-single-image" class="anchor" aria-hidden="true" href="#️-prediction-for-a-single-image"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="framed_picture" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5bc.png"&gt;🖼️&lt;/g-emoji&gt; Prediction for a single image&lt;/h2&gt;
&lt;p&gt;You can predict depth for a single image with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python test_simple.py --image_path assets/test_image.jpg --model_name mono+stereo_640x192&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On its first run this will download the &lt;code&gt;mono+stereo_640x192&lt;/code&gt; pretrained model (99MB) into the &lt;code&gt;models/&lt;/code&gt; folder.
We provide the following  options for &lt;code&gt;--model_name&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--model_name&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Imagenet pretrained?&lt;/th&gt;
&lt;th&gt;Model resolution&lt;/th&gt;
&lt;th&gt;KITTI abs. rel. error&lt;/th&gt;
&lt;th&gt;delta &amp;lt; 1.25&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.877&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.109&lt;/td&gt;
&lt;td&gt;0.864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.879&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.107&lt;/td&gt;
&lt;td&gt;0.874&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_1024x320&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;0.106&lt;/td&gt;
&lt;td&gt;0.876&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.132&lt;/td&gt;
&lt;td&gt;0.845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.130&lt;/td&gt;
&lt;td&gt;0.831&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip" rel="nofollow"&gt;&lt;code&gt;mono+stereo_no_pt_640x192&lt;/code&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;0.127&lt;/td&gt;
&lt;td&gt;0.836&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can also download models trained on the odometry split with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_odom_640x192.zip" rel="nofollow"&gt;monocular&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_odom_640x192.zip" rel="nofollow"&gt;mono+stereo&lt;/a&gt; training modalities.
Finally, we provide resnet 50 depth estimation models trained with &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_640x192.zip" rel="nofollow"&gt;ImageNet pretrained weights&lt;/a&gt; and &lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_no_pt_640x192.zip" rel="nofollow"&gt;trained from scratch&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-training-data" class="anchor" aria-hidden="true" href="#-kitti-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png"&gt;💾&lt;/g-emoji&gt; KITTI training data&lt;/h2&gt;
&lt;p&gt;You can download the entire &lt;a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" rel="nofollow"&gt;raw KITTI dataset&lt;/a&gt; by running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;wget -i splits/kitti_archives_to_download.txt -P kitti_data/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then unzip with&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; kitti_data
unzip &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;*.zip&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; ..&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; it weighs about &lt;strong&gt;175GB&lt;/strong&gt;, so make sure you have enough space to unzip too!&lt;/p&gt;
&lt;p&gt;Our default settings expect that you have converted the png images to jpeg with this command, &lt;strong&gt;which also deletes the raw KITTI &lt;code&gt;.png&lt;/code&gt; files&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;find kitti_data/ -name &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;*.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;|&lt;/span&gt; parallel &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;convert -quality 92 -sampling-factor 2x2,1x1,1x1 {.}.png {.}.jpg &amp;amp;&amp;amp; rm {}&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;or&lt;/strong&gt; you can skip this conversion step and train from raw png files by adding the flag &lt;code&gt;--png&lt;/code&gt; when training, at the expense of slower load times.&lt;/p&gt;
&lt;p&gt;The above conversion command creates images which match our experiments, where KITTI &lt;code&gt;.png&lt;/code&gt; images were converted to &lt;code&gt;.jpg&lt;/code&gt; on Ubuntu 16.04 with default chroma subsampling &lt;code&gt;2x2,1x1,1x1&lt;/code&gt;.
We found that Ubuntu 18.04 defaults to &lt;code&gt;2x2,2x2,2x2&lt;/code&gt;, which gives different results, hence the explicit parameter in the conversion command.&lt;/p&gt;
&lt;p&gt;You can also place the KITTI dataset wherever you like and point towards it with the &lt;code&gt;--data_path&lt;/code&gt; flag during training and evaluation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Splits&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The train/test/validation splits are defined in the &lt;code&gt;splits/&lt;/code&gt; folder.
By default, the code will train a depth model using &lt;a href="https://github.com/tinghuiz/SfMLearner"&gt;Zhou's subset&lt;/a&gt; of the standard Eigen split of KITTI, which is designed for monocular training.
You can also train a model using the new &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;benchmark split&lt;/a&gt; or the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;odometry split&lt;/a&gt; by setting the &lt;code&gt;--split&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can train on a custom monocular or stereo dataset by writing a new dataloader class which inherits from &lt;code&gt;MonoDataset&lt;/code&gt; – see the &lt;code&gt;KITTIDataset&lt;/code&gt; class in &lt;code&gt;datasets/kitti_dataset.py&lt;/code&gt; for an example.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--training" class="anchor" aria-hidden="true" href="#-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="hourglass_flowing_sand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png"&gt;⏳&lt;/g-emoji&gt; Training&lt;/h2&gt;
&lt;p&gt;By default models and tensorboard event files are saved to &lt;code&gt;~/tmp/&amp;lt;model_name&amp;gt;&lt;/code&gt;.
This can be changed with the &lt;code&gt;--log_dir&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monocular training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our code defaults to using Zhou's subsampled Eigen training data. For stereo-only training we have to specify that we want to use the full Eigen training set – see paper for details.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name stereo_model \
  --frame_ids 0 --use_stereo --split eigen_full&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Monocular + stereo training:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name mono+stereo_model \
  --frame_ids 0 -1 1 --use_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-gpus" class="anchor" aria-hidden="true" href="#gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPUs&lt;/h3&gt;
&lt;p&gt;The code can only be run on a single GPU.
You can specify which GPU to use with the &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; environment variable:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;CUDA_VISIBLE_DEVICES=2 python train.py --model_name mono_model&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All our experiments were performed on a single NVIDIA Titan Xp.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Approximate GPU memory&lt;/th&gt;
&lt;th&gt;Approximate training time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;9GB&lt;/td&gt;
&lt;td&gt;12 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;6GB&lt;/td&gt;
&lt;td&gt;8 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;11GB&lt;/td&gt;
&lt;td&gt;15 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content--finetuning-a-pretrained-model" class="anchor" aria-hidden="true" href="#-finetuning-a-pretrained-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="minidisc" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bd.png"&gt;💽&lt;/g-emoji&gt; Finetuning a pretrained model&lt;/h3&gt;
&lt;p&gt;Add the following to the training command to load an existing model for finetuning:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --model_name finetuned_mono --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content--other-training-options" class="anchor" aria-hidden="true" href="#-other-training-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="wrench" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png"&gt;🔧&lt;/g-emoji&gt; Other training options&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;python train.py -h&lt;/code&gt; (or look at &lt;code&gt;options.py&lt;/code&gt;) to see the range of other training options, such as learning rates and ablation settings.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content--kitti-evaluation" class="anchor" aria-hidden="true" href="#-kitti-evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png"&gt;📊&lt;/g-emoji&gt; KITTI evaluation&lt;/h2&gt;
&lt;p&gt;To prepare the ground truth depth maps run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python export_gt_depth.py --data_path kitti_data --split eigen
python export_gt_depth.py --data_path kitti_data --split eigen_benchmark&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;...assuming that you have placed the KITTI dataset in the default location of &lt;code&gt;./kitti_data/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The following example command evaluates the epoch 19 weights of a model named &lt;code&gt;mono_model&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/mono_model/models/weights_19/ --eval_mono&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For stereo models, you must use the &lt;code&gt;--eval_stereo&lt;/code&gt; flag (see note below):&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --load_weights_folder &lt;span class="pl-k"&gt;~&lt;/span&gt;/tmp/stereo_model/models/weights_19/ --eval_stereo&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you train your own model with our code you are likely to see slight differences to the publication results due to randomization in the weights initialization and data loading.&lt;/p&gt;
&lt;p&gt;An additional parameter &lt;code&gt;--eval_split&lt;/code&gt; can be set.
The three different values possible for &lt;code&gt;eval_split&lt;/code&gt; are explained here:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;code&gt;--eval_split&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;Test set size&lt;/th&gt;
&lt;th&gt;For models trained with...&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;697&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The standard Eigen test files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;eigen_benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;652&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split eigen_zhou&lt;/code&gt; (default) or &lt;code&gt;--split eigen_full&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Evaluate with the improved ground truth from the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;&lt;code&gt;--split benchmark&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction" rel="nofollow"&gt;new KITTI depth benchmark&lt;/a&gt; test files.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because no ground truth is available for the new KITTI depth benchmark, no scores will be reported  when &lt;code&gt;--eval_split benchmark&lt;/code&gt; is set.
Instead, a set of &lt;code&gt;.png&lt;/code&gt; images will be saved to disk ready for upload to the evaluation server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;External disparities evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally you can also use &lt;code&gt;evaluate_depth.py&lt;/code&gt; to evaluate raw disparities (or inverse depth) from other methods by using the &lt;code&gt;--ext_disp_to_eval&lt;/code&gt; flag:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_depth.py --ext_disp_to_eval &lt;span class="pl-k"&gt;~&lt;/span&gt;/other_method_disp.npy&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f7.png"&gt;📷&lt;/g-emoji&gt; Note on stereo evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our stereo models are trained with an effective baseline of &lt;code&gt;0.1&lt;/code&gt; units, while the actual KITTI stereo rig has a baseline of &lt;code&gt;0.54m&lt;/code&gt;. This means a scaling of &lt;code&gt;5.4&lt;/code&gt; must be applied for evaluation.
In addition, for models trained with stereo supervision we disable median scaling.
Setting the &lt;code&gt;--eval_stereo&lt;/code&gt; flag when evaluating will automatically disable median scaling and scale predicted depths by &lt;code&gt;5.4&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2934.png"&gt;⤴️&lt;/g-emoji&gt;&lt;g-emoji class="g-emoji" alias="arrow_heading_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2935.png"&gt;⤵️&lt;/g-emoji&gt; Odometry evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We include code for evaluating poses predicted by models trained with &lt;code&gt;--split odom --dataset kitti_odom --data_path /path/to/kitti/odometry/dataset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this evaluation, the &lt;a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" rel="nofollow"&gt;KITTI odometry dataset&lt;/a&gt; &lt;strong&gt;(color, 65GB)&lt;/strong&gt; and &lt;strong&gt;ground truth poses&lt;/strong&gt; zip files must be downloaded.
As above, we assume that the pngs have been converted to jpgs.&lt;/p&gt;
&lt;p&gt;If this data has been unzipped to folder &lt;code&gt;kitti_odom&lt;/code&gt;, a model can be evaluated with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python evaluate_pose.py --eval_split odom_9 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/
python evaluate_pose.py --eval_split odom_10 --load_weights_folder ./odom_split.M/models/weights_29 --data_path kitti_odom/&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content--precomputed-results" class="anchor" aria-hidden="true" href="#-precomputed-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="package" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png"&gt;📦&lt;/g-emoji&gt; Precomputed results&lt;/h2&gt;
&lt;p&gt;You can download our precomputed disparity predictions from the following links:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Training modality&lt;/th&gt;
&lt;th&gt;Input size&lt;/th&gt;
&lt;th&gt;&lt;code&gt;.npy&lt;/code&gt; filesize&lt;/th&gt;
&lt;th&gt;Eigen disparities&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;640 x 192&lt;/td&gt;
&lt;td&gt;343 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mono + Stereo&lt;/td&gt;
&lt;td&gt;1024 x 320&lt;/td&gt;
&lt;td&gt;914 MB&lt;/td&gt;
&lt;td&gt;&lt;a href="https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320_eigen.npy" rel="nofollow"&gt;Download &lt;g-emoji class="g-emoji" alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png"&gt;🔗&lt;/g-emoji&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-️-license" class="anchor" aria-hidden="true" href="#️-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="woman_judge" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-2696.png"&gt;👩‍⚖️&lt;/g-emoji&gt; License&lt;/h2&gt;
&lt;p&gt;Copyright © Niantic, Inc. 2019. Patent Pending.
All rights reserved.
Please see the &lt;a href="LICENSE"&gt;license file&lt;/a&gt; for terms.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nianticlabs</author><guid isPermaLink="false">https://github.com/nianticlabs/monodepth2</guid><pubDate>Fri, 08 Nov 2019 00:06:00 GMT</pubDate></item><item><title>WillKoehrsen/feature-selector #7 in Jupyter Notebook, Today</title><link>https://github.com/WillKoehrsen/feature-selector</link><description>&lt;p&gt;&lt;i&gt;Feature selector is a tool for dimensionality reduction of machine learning datasets&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-feature-selector-simple-feature-selection-in-python" class="anchor" aria-hidden="true" href="#feature-selector-simple-feature-selection-in-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feature Selector: Simple Feature Selection in Python&lt;/h1&gt;
&lt;p&gt;Feature selector is a tool for dimensionality reduction of machine learning datasets.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-methods" class="anchor" aria-hidden="true" href="#methods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Methods&lt;/h1&gt;
&lt;p&gt;There are five methods used to identify features to remove:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Missing Values&lt;/li&gt;
&lt;li&gt;Single Unique Values&lt;/li&gt;
&lt;li&gt;Collinear Features&lt;/li&gt;
&lt;li&gt;Zero Importance Features&lt;/li&gt;
&lt;li&gt;Low Importance Features&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;Refer to the &lt;a href="https://github.com/WillKoehrsen/feature-selector/blob/master/Feature%20Selector%20Usage.ipynb"&gt;Feature Selector Usage notebook&lt;/a&gt; for how to use&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-visualizations" class="anchor" aria-hidden="true" href="#visualizations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visualizations&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FeatureSelector&lt;/code&gt; also includes a number of visualization methods to inspect
characteristics of a dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correlation Heatmap&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/example_collinear_heatmap.png"&gt;&lt;img src="images/example_collinear_heatmap.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Most Important Features&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="images/example_top_feature_importances.png"&gt;&lt;img src="images/example_top_feature_importances.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Requires:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python==3.6+
lightgbm==2.1.1
matplotlib==2.1.2
seaborn==0.8.1
numpy==1.14.5
pandas==0.23.1
scikit-learn==0.19.1

&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;Any questions can be directed to &lt;a href="mailto:wjk68@case.edu"&gt;wjk68@case.edu&lt;/a&gt;!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>WillKoehrsen</author><guid isPermaLink="false">https://github.com/WillKoehrsen/feature-selector</guid><pubDate>Fri, 08 Nov 2019 00:07:00 GMT</pubDate></item><item><title>AtsushiSakai/PythonRobotics #8 in Jupyter Notebook, Today</title><link>https://github.com/AtsushiSakai/PythonRobotics</link><description>&lt;p&gt;&lt;i&gt;Python sample codes for robotics algorithms.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true" align="right" width="300" alt="header pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pythonrobotics" class="anchor" aria-hidden="true" href="#pythonrobotics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PythonRobotics&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/AtsushiSakai/PythonRobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/58f87d5d3604646322c28abd8c5a9b2faa05fa51/68747470733a2f2f7472617669732d63692e6f72672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/AtsushiSakai/PythonRobotics.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pythonrobotics.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a60f894ef011c8a7e648348c16aabfdfb603613a/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f707974686f6e726f626f746963732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/pythonrobotics/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2e66a00c9dcf7ecc1f24189c6055aa7e6da233dc/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f73623237396b787576316265333931673f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/AtsushiSakai/PythonRobotics?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c26144817eba34b4ee9f9a6aee913e6b466218b/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4174737573686953616b61692f507974686f6e526f626f746963732f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/AtsushiSakai/PythonRobotics/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://lgtm.com/projects/g/AtsushiSakai/PythonRobotics/context:python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4c3af4cd47bb2ea2c71cac274f1f7dd392eea893/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f4174737573686953616b61692f507974686f6e526f626f746963732e7376673f6c6f676f3d6c67746d266c6f676f57696474683d3138" alt="Language grade: Python" data-canonical-src="https://img.shields.io/lgtm/grade/python/g/AtsushiSakai/PythonRobotics.svg?logo=lgtm&amp;amp;logoWidth=18" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/overview/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c3cd55e61ef2e22ff00427b50b9e7f1c3547de91/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6174737573686973616b61692f707974686f6e726f626f746963732f62616467652f6d6173746572" alt="CodeFactor" data-canonical-src="https://www.codefactor.io/repository/github/atsushisakai/pythonrobotics/badge/master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/AtsushiSakai/PythonRobotics"&gt;&lt;img src="https://camo.githubusercontent.com/230f0a1eaa529fa727cad2c9d3c1ace4738bd25d/68747470733a2f2f746f6b65692e72732f62312f6769746875622f4174737573686953616b61692f507974686f6e526f626f74696373" alt="tokei" data-canonical-src="https://tokei.rs/b1/github/AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Python codes for robotics algorithm.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-this"&gt;What is this?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#localization"&gt;Localization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#extended-kalman-filter-localization"&gt;Extended Kalman Filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#particle-filter-localization"&gt;Particle filter localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#histogram-filter-localization"&gt;Histogram filter localization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#mapping"&gt;Mapping&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#gaussian-grid-map"&gt;Gaussian grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ray-casting-grid-map"&gt;Ray casting grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lidar-to-grid-map"&gt;Lidar to grid map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#k-means-object-clustering"&gt;k-means object clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rectangle-fitting"&gt;Rectangle fitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#slam"&gt;SLAM&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#iterative-closest-point-icp-matching"&gt;Iterative Closest Point (ICP) Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#fastslam-10"&gt;FastSLAM 1.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pose-optimization-slam-2d"&gt;Pose Optimization SLAM 2D&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pose-optimization-slam-3d"&gt;Pose Optimization SLAM 3D&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-planning"&gt;Path Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dynamic-window-approach"&gt;Dynamic Window Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-search"&gt;Grid based search&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#dijkstra-algorithm"&gt;Dijkstra algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#a-algorithm"&gt;A* algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#potential-field-algorithm"&gt;Potential Field algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#grid-based-coverage-path-planning"&gt;Grid based coverage path planning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#state-lattice-planning"&gt;State Lattice Planning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#biased-polar-sampling"&gt;Biased polar sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lane-sampling"&gt;Lane sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilistic-road-map-prm-planning"&gt;Probabilistic Road-Map (PRM) planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rapidly-exploring-random-trees-rrt"&gt;Rapidly-Exploring Random Trees (RRT)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#rrt"&gt;RRT*&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rrt-with-reeds-shepp-path"&gt;RRT* with reeds-shepp path&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-rrt"&gt;LQR-RRT*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#quintic-polynomials-planning"&gt;Quintic polynomials planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reeds-shepp-planning"&gt;Reeds Shepp planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lqr-based-path-planning"&gt;LQR based path planning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimal-trajectory-in-a-frenet-frame"&gt;Optimal Trajectory in a Frenet Frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#path-tracking"&gt;Path Tracking&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#move-to-a-pose-control"&gt;move to a pose control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stanley-control"&gt;Stanley control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rear-wheel-feedback-control"&gt;Rear wheel feedback control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model-predictive-speed-and-steering-control"&gt;Model predictive speed and steering control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#nonlinear-model-predictive-control-with-c-gmres"&gt;Nonlinear Model predictive control with C-GMRES&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation"&gt;Arm Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#n-joint-arm-to-point-control"&gt;N joint arm to point control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#arm-navigation-with-obstacle-avoidance"&gt;Arm navigation with obstacle avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#aerial-navigation"&gt;Aerial Navigation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#drone-3d-trajectory-following"&gt;drone 3d trajectory following&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#rocket-powered-landing"&gt;rocket powered landing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#bipedal"&gt;Bipedal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#bipedal-planner-with-inverted-pendulum"&gt;bipedal planner with inverted pendulum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#license"&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-case"&gt;Use-case&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#support"&gt;Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-what-is-this" class="anchor" aria-hidden="true" href="#what-is-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is this?&lt;/h1&gt;
&lt;p&gt;This is a Python code collection of robotics algorithms, especially for autonomous navigation.&lt;/p&gt;
&lt;p&gt;Features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Easy to read for understanding each algorithm's basic idea.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Widely used and practical algorithms are selected.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimum dependency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See this paper for more details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;[1808.10703] PythonRobotics: a Python code collection of robotics algorithms&lt;/a&gt; (&lt;a href="https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib"&gt;BibTeX&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Python 3.7.x (2.7 is not supported)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;numpy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scipy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;matplotlib&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pandas&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.cvxpy.org/index.html" rel="nofollow"&gt;cvxpy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h1&gt;
&lt;p&gt;This README only shows some examples of this project.&lt;/p&gt;
&lt;p&gt;If you are interested in other examples or mathematical backgrounds of each algorithm,&lt;/p&gt;
&lt;p&gt;You can check the full documentation online: &lt;a href="https://pythonrobotics.readthedocs.io/" rel="nofollow"&gt;https://pythonrobotics.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All animation gifs are stored here: &lt;a href="https://github.com/AtsushiSakai/PythonRoboticsGifs"&gt;AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Clone this repo.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone &lt;a href="https://github.com/AtsushiSakai/PythonRobotics.git"&gt;https://github.com/AtsushiSakai/PythonRobotics.git&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;cd PythonRobotics/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Install the required libraries. You can use environment.yml with conda command.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;conda env create -f environment.yml&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;p&gt;Execute python script in each directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add star to this repo if you like it &lt;g-emoji class="g-emoji" alias="smiley" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png"&gt;😃&lt;/g-emoji&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;a id="user-content-localization" class="anchor" aria-hidden="true" href="#localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Localization&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-extended-kalman-filter-localization" class="anchor" aria-hidden="true" href="#extended-kalman-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Extended Kalman Filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif" width="640" alt="EKF pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/Localization/extended_kalman_filter/extended_kalman_filter_localization.ipynb"&gt;Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-particle-filter-localization" class="anchor" aria-hidden="true" href="#particle-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Particle filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a sensor fusion localization with Particle Filter(PF).&lt;/p&gt;
&lt;p&gt;The blue line is true trajectory, the black line is dead reckoning trajectory,&lt;/p&gt;
&lt;p&gt;and the red line is estimated trajectory with PF.&lt;/p&gt;
&lt;p&gt;It is assumed that the robot can measure a distance from landmarks (RFID).&lt;/p&gt;
&lt;p&gt;This measurements are used for PF localization.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-histogram-filter-localization" class="anchor" aria-hidden="true" href="#histogram-filter-localization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Histogram filter localization&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a 2D localization example with Histogram filter.&lt;/p&gt;
&lt;p&gt;The red cross is true position, black points are RFID positions.&lt;/p&gt;
&lt;p&gt;The blue grid shows a position probability of histogram filter.&lt;/p&gt;
&lt;p&gt;In this simulation, x,y are unknown, yaw is known.&lt;/p&gt;
&lt;p&gt;The filter integrates speed input and range observations from RFID for localization.&lt;/p&gt;
&lt;p&gt;Initial position is not needed.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-mapping" class="anchor" aria-hidden="true" href="#mapping"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Mapping&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-grid-map" class="anchor" aria-hidden="true" href="#gaussian-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D Gaussian grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ray-casting-grid-map" class="anchor" aria-hidden="true" href="#ray-casting-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ray casting grid map&lt;/h2&gt;
&lt;p&gt;This is a 2D ray casting grid mapping example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lidar-to-grid-map" class="anchor" aria-hidden="true" href="#lidar-to-grid-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lidar to grid map&lt;/h2&gt;
&lt;p&gt;This example shows how to convert a 2D range measurement to a grid map.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Mapping/lidar_to_grid_map/animation.gif"&gt;&lt;img src="Mapping/lidar_to_grid_map/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k-means-object-clustering" class="anchor" aria-hidden="true" href="#k-means-object-clustering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;k-means object clustering&lt;/h2&gt;
&lt;p&gt;This is a 2D object clustering with k-means algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rectangle-fitting" class="anchor" aria-hidden="true" href="#rectangle-fitting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rectangle fitting&lt;/h2&gt;
&lt;p&gt;This is a 2D rectangle fitting for vehicle detection.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-slam" class="anchor" aria-hidden="true" href="#slam"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SLAM&lt;/h1&gt;
&lt;p&gt;Simultaneous Localization and Mapping(SLAM) examples&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-iterative-closest-point-icp-matching" class="anchor" aria-hidden="true" href="#iterative-closest-point-icp-matching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Iterative Closest Point (ICP) Matching&lt;/h2&gt;
&lt;p&gt;This is a 2D ICP matching example with singular value decomposition.&lt;/p&gt;
&lt;p&gt;It can calculate a rotation matrix and a translation vector between points to points.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf" rel="nofollow"&gt;Introduction to Mobile Robotics: Iterative Closest Point Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fastslam-10" class="anchor" aria-hidden="true" href="#fastslam-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FastSLAM 1.0&lt;/h2&gt;
&lt;p&gt;This is a feature based SLAM example using FastSLAM 1.0.&lt;/p&gt;
&lt;p&gt;The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.&lt;/p&gt;
&lt;p&gt;The red points are particles of FastSLAM.&lt;/p&gt;
&lt;p&gt;Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.probabilistic-robotics.org/" rel="nofollow"&gt;PROBABILISTIC ROBOTICS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm" rel="nofollow"&gt;SLAM simulations by Tim Bailey&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pose-optimization-slam-2d" class="anchor" aria-hidden="true" href="#pose-optimization-slam-2d"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pose Optimization SLAM 2D&lt;/h2&gt;
&lt;p&gt;This is a graph based 2D pose optimization SLAM example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/PoseOptimizationSLAM2D/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/PoseOptimizationSLAM2D/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pose-optimization-slam-3d" class="anchor" aria-hidden="true" href="#pose-optimization-slam-3d"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pose Optimization SLAM 3D&lt;/h2&gt;
&lt;p&gt;This is a graph based 3D pose optimization SLAM example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/PoseOptimizationSLAM3D/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/PoseOptimizationSLAM3D/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-path-planning" class="anchor" aria-hidden="true" href="#path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Planning&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-dynamic-window-approach" class="anchor" aria-hidden="true" href="#dynamic-window-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamic Window Approach&lt;/h2&gt;
&lt;p&gt;This is a 2D navigation sample code with Dynamic Window Approach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf" rel="nofollow"&gt;The Dynamic Window Approach to Collision Avoidance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-grid-based-search" class="anchor" aria-hidden="true" href="#grid-based-search"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based search&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-dijkstra-algorithm" class="anchor" aria-hidden="true" href="#dijkstra-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dijkstra algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with Dijkstra's algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-a-algorithm" class="anchor" aria-hidden="true" href="#a-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A* algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based shortest path planning with A star algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, cyan points are searched nodes.&lt;/p&gt;
&lt;p&gt;Its heuristic is 2D Euclid distance.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-potential-field-algorithm" class="anchor" aria-hidden="true" href="#potential-field-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Potential Field algorithm&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based path planning with Potential Field algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the animation, the blue heat map shows potential value on each grid.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf" rel="nofollow"&gt;Robotic Motion Planning:Potential Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-grid-based-coverage-path-planning" class="anchor" aria-hidden="true" href="#grid-based-coverage-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grid based coverage path planning&lt;/h3&gt;
&lt;p&gt;This is a 2D grid based coverage path planning simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif" alt="PotentialField" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-state-lattice-planning" class="anchor" aria-hidden="true" href="#state-lattice-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;State Lattice Planning&lt;/h2&gt;
&lt;p&gt;This script is a path planning code with state lattice planning.&lt;/p&gt;
&lt;p&gt;This code uses the model predictive trajectory generator to solve boundary problem.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://journals.sagepub.com/doi/pdf/10.1177/0278364906075328" rel="nofollow"&gt;Optimal rough terrain trajectory generation for wheeled mobile robots&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf" rel="nofollow"&gt;State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-biased-polar-sampling" class="anchor" aria-hidden="true" href="#biased-polar-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Biased polar sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lane-sampling" class="anchor" aria-hidden="true" href="#lane-sampling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lane sampling&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-probabilistic-road-map-prm-planning" class="anchor" aria-hidden="true" href="#probabilistic-road-map-prm-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Probabilistic Road-Map (PRM) planning&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif" alt="PRM" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This PRM planner uses Dijkstra method for graph search.&lt;/p&gt;
&lt;p&gt;In the animation, blue points are sampled points,&lt;/p&gt;
&lt;p&gt;Cyan crosses means searched points with Dijkstra method,&lt;/p&gt;
&lt;p&gt;The red line is the final path of PRM.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Probabilistic_roadmap" rel="nofollow"&gt;Probabilistic roadmap - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rapidly-exploring-random-trees-rrt" class="anchor" aria-hidden="true" href="#rapidly-exploring-random-trees-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rapidly-Exploring Random Trees (RRT)&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-rrt" class="anchor" aria-hidden="true" href="#rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT*&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a path planning code with RRT*&lt;/p&gt;
&lt;p&gt;Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1005.0416" rel="nofollow"&gt;Incremental Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.419.5503&amp;amp;rep=rep1&amp;amp;type=pdf" rel="nofollow"&gt;Sampling-based Algorithms for Optimal Motion Planning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-rrt-with-reeds-shepp-path" class="anchor" aria-hidden="true" href="#rrt-with-reeds-shepp-path"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RRT* with reeds-shepp path&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif" alt="Robotics/animation.gif at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Path planning for a car robot with RRT* and reeds shepp path planner.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-lqr-rrt" class="anchor" aria-hidden="true" href="#lqr-rrt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR-RRT*&lt;/h3&gt;
&lt;p&gt;This is a path planning simulation with LQR-RRT*.&lt;/p&gt;
&lt;p&gt;A double integrator motion model is used for LQR local planner.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif" alt="LQRRRT" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://lis.csail.mit.edu/pubs/perez-icra12.pdf" rel="nofollow"&gt;LQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/MahanFathi/LQR-RRTstar"&gt;MahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-quintic-polynomials-planning" class="anchor" aria-hidden="true" href="#quintic-polynomials-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quintic polynomials planning&lt;/h2&gt;
&lt;p&gt;Motion planning with quintic polynomials.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It can calculate 2D path, velocity, and acceleration profile based on quintic polynomials.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/637936/" rel="nofollow"&gt;Local Path Planning And Motion Control For Agv In Positioning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-reeds-shepp-planning" class="anchor" aria-hidden="true" href="#reeds-shepp-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reeds Shepp planning&lt;/h2&gt;
&lt;p&gt;A sample code with Reeds Shepp path planning.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://planning.cs.uiuc.edu/node822.html" rel="nofollow"&gt;15.3.2 Reeds-Shepp Curves&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf" rel="nofollow"&gt;optimal paths for a car that goes both forwards and backwards&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ghliu/pyReedsShepp"&gt;ghliu/pyReedsShepp: Implementation of Reeds Shepp curve.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lqr-based-path-planning" class="anchor" aria-hidden="true" href="#lqr-based-path-planning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LQR based path planning&lt;/h2&gt;
&lt;p&gt;A sample code using LQR based path planning for double integrator model.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true" alt="RSPlanning" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-optimal-trajectory-in-a-frenet-frame" class="anchor" aria-hidden="true" href="#optimal-trajectory-in-a-frenet-frame"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimal Trajectory in a Frenet Frame&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is optimal trajectory generation in a Frenet Frame.&lt;/p&gt;
&lt;p&gt;The cyan line is the target course and black crosses are obstacles.&lt;/p&gt;
&lt;p&gt;The red line is predicted path.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf" rel="nofollow"&gt;Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Cj6tAQe7UCY" rel="nofollow"&gt;Optimal trajectory generation for dynamic street scenarios in a Frenet Frame&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-path-tracking" class="anchor" aria-hidden="true" href="#path-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Path Tracking&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-move-to-a-pose-control" class="anchor" aria-hidden="true" href="#move-to-a-pose-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;move to a pose control&lt;/h2&gt;
&lt;p&gt;This is a simulation of moving to a pose control&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/move_to_pose/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://link.springer.com/book/10.1007/978-3-642-20144-8" rel="nofollow"&gt;P. I. Corke, "Robotics, Vision and Control" | SpringerLink p102&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stanley-control" class="anchor" aria-hidden="true" href="#stanley-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stanley control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with Stanley steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif" alt="2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://robots.stanford.edu/papers/thrun.stanley05.pdf" rel="nofollow"&gt;Stanley: The robot that won the DARPA grand challenge&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf" rel="nofollow"&gt;Automatic Steering Methods for Autonomous Automobile Path Tracking&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-rear-wheel-feedback-control" class="anchor" aria-hidden="true" href="#rear-wheel-feedback-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Rear wheel feedback control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with rear wheel feedback steering control and PID speed control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif" alt="PythonRobotics/figure_1.png at master · AtsushiSakai/PythonRobotics" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1604.07446" rel="nofollow"&gt;A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-linearquadratic-regulator-lqr-speed-and-steering-control" class="anchor" aria-hidden="true" href="#linearquadratic-regulator-lqr-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linear–quadratic regulator (LQR) speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with LQR speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ieeexplore.ieee.org/document/5940562/" rel="nofollow"&gt;Towards fully autonomous driving: Systems and algorithms - IEEE Conference Publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-model-predictive-speed-and-steering-control" class="anchor" aria-hidden="true" href="#model-predictive-speed-and-steering-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model predictive speed and steering control&lt;/h2&gt;
&lt;p&gt;Path tracking simulation with iterative linear model predictive speed and steering control.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif" width="640" alt="MPC pic" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/model_predictive_speed_and_steer_control/Model_predictive_speed_and_steering_control.ipynb"&gt;notebook&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://grauonline.de/wordpress/?page_id=3244" rel="nofollow"&gt;Real-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nonlinear-model-predictive-control-with-c-gmres" class="anchor" aria-hidden="true" href="#nonlinear-model-predictive-control-with-c-gmres"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Nonlinear Model predictive control with C-GMRES&lt;/h2&gt;
&lt;p&gt;A motion planning and path tracking simulation with NMPC of C-GMRES&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/cgmres_nmpc/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/PathTracking/cgmres_nmpc/cgmres_nmpc.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-arm-navigation" class="anchor" aria-hidden="true" href="#arm-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-n-joint-arm-to-point-control" class="anchor" aria-hidden="true" href="#n-joint-arm-to-point-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;N joint arm to point control&lt;/h2&gt;
&lt;p&gt;N joint arm to a point control simulation.&lt;/p&gt;
&lt;p&gt;This is a interactive simulation.&lt;/p&gt;
&lt;p&gt;You can set the goal position of the end effector with left-click on the ploting area.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/n_joint_arm_to_point_control/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this simulation N = 10, however, you can change it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-arm-navigation-with-obstacle-avoidance" class="anchor" aria-hidden="true" href="#arm-navigation-with-obstacle-avoidance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Arm navigation with obstacle avoidance&lt;/h2&gt;
&lt;p&gt;Arm navigation with obstacle avoidance simulation.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/ArmNavigation/arm_obstacle_navigation/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-aerial-navigation" class="anchor" aria-hidden="true" href="#aerial-navigation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial Navigation&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-drone-3d-trajectory-following" class="anchor" aria-hidden="true" href="#drone-3d-trajectory-following"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;drone 3d trajectory following&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory following simulation for a quadrotor.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/drone_3d_trajectory_following/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-rocket-powered-landing" class="anchor" aria-hidden="true" href="#rocket-powered-landing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;rocket powered landing&lt;/h2&gt;
&lt;p&gt;This is a 3d trajectory generation simulation for a rocket powered landing.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/AerialNavigation/rocket_powered_landing/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/AerialNavigation/rocket_powered_landing/rocket_powered_landing.ipynb"&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-bipedal" class="anchor" aria-hidden="true" href="#bipedal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bipedal&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-bipedal-planner-with-inverted-pendulum" class="anchor" aria-hidden="true" href="#bipedal-planner-with-inverted-pendulum"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;bipedal planner with inverted pendulum&lt;/h2&gt;
&lt;p&gt;This is a bipedal planner for modifying footsteps with inverted pendulum.&lt;/p&gt;
&lt;p&gt;You can set the footsteps and the planner will modify those automatically.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif"&gt;&lt;img src="https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Bipedal/bipedal_planner/animation.gif" alt="3" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h1&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-use-case" class="anchor" aria-hidden="true" href="#use-case"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use-case&lt;/h1&gt;
&lt;p&gt;If this project helps your robotics project, please let me know with &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Your robot's video, which is using PythonRobotics, is very welcome!!&lt;/p&gt;
&lt;p&gt;This is a list of other user's comment and references:&lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;users_comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contribution" class="anchor" aria-hidden="true" href="#contribution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contribution&lt;/h1&gt;
&lt;p&gt;A small PR like bug fix is welcome.&lt;/p&gt;
&lt;p&gt;If your PR is merged multiple times, I will add your account to the author list.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;If you use this project's code for your academic work, we encourage you to cite &lt;a href="https://arxiv.org/abs/1808.10703" rel="nofollow"&gt;our papers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h1&gt;
&lt;p&gt;If you or your company would like to support this project, please consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.patreon.com/myenigma" rel="nofollow"&gt;Become a backer or sponsor on Patreon&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.paypal.me/myenigmapay/" rel="nofollow"&gt;One-time donation via PayPal&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can add your name or your company logo in README if you are a patron.&lt;/p&gt;
&lt;p&gt;E-mail consultant is also available.&lt;/p&gt;
&lt;p&gt;　&lt;/p&gt;
&lt;p&gt;Your comment using &lt;a href="https://saythanks.io/to/AtsushiSakai" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c9f6dc1c6a604b58d3c56bc5d7624e44f7eee2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5361792532305468616e6b732d212d3145414544422e737667" alt="Say Thanks!" data-canonical-src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" style="max-width:100%;"&gt;&lt;/a&gt; is also welcome.&lt;/p&gt;
&lt;p&gt;This is a list: &lt;a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/users_comments.md"&gt;Users comments&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AtsushiSakai/"&gt;Atsushi Sakai&lt;/a&gt; (&lt;a href="https://twitter.com/Atsushi_twi" rel="nofollow"&gt;@Atsushi_twi&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/daniel-s-ingram"&gt;Daniel Ingram&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/jwdinius"&gt;Joe Dinius&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/karanchawla"&gt;Karan Chawla&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/araffin"&gt;Antonin RAFFIN&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/AlexisTM"&gt;Alexis Paques&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsasaki0109"&gt;Ryohei Sasaki&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AtsushiSakai</author><guid isPermaLink="false">https://github.com/AtsushiSakai/PythonRobotics</guid><pubDate>Fri, 08 Nov 2019 00:08:00 GMT</pubDate></item><item><title>Azure/MachineLearningNotebooks #9 in Jupyter Notebook, Today</title><link>https://github.com/Azure/MachineLearningNotebooks</link><description>&lt;p&gt;&lt;i&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning | Microsoft&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-azure-machine-learning-service-example-notebooks" class="anchor" aria-hidden="true" href="#azure-machine-learning-service-example-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Azure Machine Learning service example notebooks&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks demonstrating the &lt;a href="https://azure.microsoft.com/en-us/services/machine-learning-service/" rel="nofollow"&gt;Azure Machine Learning&lt;/a&gt; Python SDK which allows you to build, train, deploy and manage machine learning solutions using Azure.  The AML SDK allows you the choice of using local or cloud compute resources, while managing and maintaining the complete data science workflow from the cloud.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/machine-learning/service/media/concept-azure-machine-learning-architecture/workflow.png"&gt;&lt;img src="https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/machine-learning/service/media/concept-azure-machine-learning-architecture/workflow.png" alt="Azure ML Workflow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-installation" class="anchor" aria-hidden="true" href="#quick-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick installation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install azureml-sdk&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Read more detailed instructions on &lt;a href="./NBSETUP.md"&gt;how to set up your environment&lt;/a&gt; using Azure Notebook service, your own Jupyter notebook server, or Docker.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-navigate-and-use-the-example-notebooks" class="anchor" aria-hidden="true" href="#how-to-navigate-and-use-the-example-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to navigate and use the example notebooks?&lt;/h2&gt;
&lt;p&gt;If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, you should always run the &lt;a href="./configuration.ipynb"&gt;Configuration&lt;/a&gt; notebook first when setting up a notebook library on a new machine or in a new environment. It configures your notebook library to connect to an Azure Machine Learning workspace, and sets up your workspace and compute to be used by many of the other examples.
This &lt;a href=".index.md"&gt;index&lt;/a&gt; should assist in navigating the Azure Machine Learning notebook samples and encourage efficient retrieval of topics and content.&lt;/p&gt;
&lt;p&gt;If you want to...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;...try out and explore Azure ML, start with image classification tutorials: &lt;a href="./tutorials/img-classification-part1-training.ipynb"&gt;Part 1 (Training)&lt;/a&gt; and &lt;a href="./tutorials/img-classification-part2-deploy.ipynb"&gt;Part 2 (Deployment)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...learn about experimentation and tracking run history, first &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;train within Notebook&lt;/a&gt;, then try &lt;a href="./how-to-use-azureml/training/train-on-remote-vm/train-on-remote-vm.ipynb"&gt;training on remote VM&lt;/a&gt; and &lt;a href="./how-to-use-azureml/training/logging-api/logging-api.ipynb"&gt;using logging APIs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...train deep learning models at scale, first learn about &lt;a href="./how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"&gt;Machine Learning Compute&lt;/a&gt;, and then try &lt;a href="./how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-pytorch/train-hyperparameter-tune-deploy-with-pytorch.ipynb"&gt;distributed hyperparameter tuning&lt;/a&gt; and &lt;a href="./how-to-use-azureml/training-with-deep-learning/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.ipynb"&gt;distributed training&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...deploy models as a realtime scoring service, first learn the basics by &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;training within Notebook and deploying to Azure Container Instance&lt;/a&gt;, then learn how to &lt;a href="./how-to-use-azureml/deployment/register-model-create-image-deploy-service/register-model-create-image-deploy-service.ipynb"&gt;register and manage models, and create Docker images&lt;/a&gt;, and &lt;a href="./how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"&gt;production deploy models on Azure Kubernetes Cluster&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...deploy models as a batch scoring service, first &lt;a href="./how-to-use-azureml/training/train-within-notebook/train-within-notebook.ipynb"&gt;train a model within Notebook&lt;/a&gt;, learn how to &lt;a href="./how-to-use-azureml/deployment/register-model-create-image-deploy-service/register-model-create-image-deploy-service.ipynb"&gt;register and manage models&lt;/a&gt;, then &lt;a href="./how-to-use-azureml/training/train-on-amlcompute/train-on-amlcompute.ipynb"&gt;create Machine Learning Compute for scoring compute&lt;/a&gt;, and &lt;a href="https://aka.ms/pl-batch-scoring" rel="nofollow"&gt;use Machine Learning Pipelines to deploy your model&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;...monitor your deployed models, learn about using &lt;a href="./how-to-use-azureml/deployment/enable-app-insights-in-production-service/enable-app-insights-in-production-service.ipynb"&gt;App Insights&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;The &lt;a href="./tutorials"&gt;Tutorials&lt;/a&gt; folder contains notebooks for the tutorials described in the &lt;a href="https://aka.ms/aml-docs" rel="nofollow"&gt;Azure Machine Learning documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use-azure-ml" class="anchor" aria-hidden="true" href="#how-to-use-azure-ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use Azure ML&lt;/h2&gt;
&lt;p&gt;The &lt;a href="./how-to-use-azureml"&gt;How to use Azure ML&lt;/a&gt; folder contains specific examples demonstrating the features of the Azure Machine Learning SDK&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/training"&gt;Training&lt;/a&gt; - Examples of how to build models using Azure ML's logging and execution capabilities on local and remote compute targets&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/training-with-deep-learning"&gt;Training with Deep Learning&lt;/a&gt; - Examples demonstrating how to build deep learning models using estimators and parameter sweeps&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/manage-azureml-service"&gt;Manage Azure ML Service&lt;/a&gt; - Examples how to perform tasks, such as authenticate against Azure ML service in different ways.&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/automated-machine-learning"&gt;Automated Machine Learning&lt;/a&gt; - Examples using Automated Machine Learning to automatically generate optimal machine learning pipelines and models&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/machine-learning-pipelines"&gt;Machine Learning Pipelines&lt;/a&gt; - Examples showing how to create and use reusable pipelines for training and batch scoring&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/deployment"&gt;Deployment&lt;/a&gt; - Examples showing how to deploy and manage machine learning models and solutions&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/azure-databricks"&gt;Azure Databricks&lt;/a&gt; - Examples showing how to use Azure ML with Azure Databricks&lt;/li&gt;
&lt;li&gt;&lt;a href="./how-to-use-azureml/monitor-models"&gt;Monitor Models&lt;/a&gt; - Examples showing how to enable model monitoring services such as DataDrift&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Quickstarts, end-to-end tutorials, and how-tos on the &lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/" rel="nofollow"&gt;official documentation site for Azure Machine Learning service&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py" rel="nofollow"&gt;Python SDK reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure ML Data Prep SDK &lt;a href="https://aka.ms/data-prep-sdk" rel="nofollow"&gt;overview&lt;/a&gt;, &lt;a href="https://aka.ms/aml-data-prep-apiref" rel="nofollow"&gt;Python SDK reference&lt;/a&gt;, and &lt;a href="https://aka.ms/aml-data-prep-notebooks" rel="nofollow"&gt;tutorials and how-tos&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-community-repository" class="anchor" aria-hidden="true" href="#community-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Community Repository&lt;/h2&gt;
&lt;p&gt;Visit this &lt;a href="https://github.com/microsoft/MLOps/tree/master/examples"&gt;community repository&lt;/a&gt; to find useful end-to-end sample notebooks. Also, please follow these &lt;a href="https://github.com/microsoft/MLOps/blob/master/contributing.md"&gt;contribution guidelines&lt;/a&gt; when contributing to this repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects-using-azure-machine-learning" class="anchor" aria-hidden="true" href="#projects-using-azure-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects using Azure Machine Learning&lt;/h2&gt;
&lt;p&gt;Visit following repos to see projects contributed by Azure ML users:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Azure/AMLSamples"&gt;AMLSamples&lt;/a&gt; Number of end-to-end examples, including face recognition, predictive maintenance, customer churn and sentiment analysis.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/microsoft/nlp"&gt;Learn about Natural Language Processing best practices using Azure Machine Learning service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Microsoft/AzureML-BERT"&gt;Pre-Train BERT models using Azure Machine Learning service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/amynic/azureml-sdk-fashion"&gt;Fashion MNIST with Azure ML SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/katiehouse3/microsoft-azure-ml-notebooks"&gt;UMass Amherst Student Samples&lt;/a&gt; - A number of end-to-end machine learning notebooks, including machine translation, image classification, and customer churn, created by students in the 696DS course at UMass Amherst.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-datatelemetry" class="anchor" aria-hidden="true" href="#datatelemetry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data/Telemetry&lt;/h2&gt;
&lt;p&gt;This repository collects usage data and sends it to Mircosoft to help improve our products and services. Read Microsoft's &lt;a href="https://privacy.microsoft.com/en-US/privacystatement" rel="nofollow"&gt;privacy statement to learn more&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To opt out of tracking, please go to the raw markdown or .ipynb files and remove the following line of code:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/README.png)&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This URL will be slightly different depending on the file.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c46ad7e1ce8c1c4ffaf085f08619fe9c1b16eb50/68747470733a2f2f506978656c53657276657232303139303432333131343233382e617a75726577656273697465732e6e65742f6170692f696d7072657373696f6e732f4d616368696e654c6561726e696e674e6f7465626f6f6b732f524541444d452e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c46ad7e1ce8c1c4ffaf085f08619fe9c1b16eb50/68747470733a2f2f506978656c53657276657232303139303432333131343233382e617a75726577656273697465732e6e65742f6170692f696d7072657373696f6e732f4d616368696e654c6561726e696e674e6f7465626f6f6b732f524541444d452e706e67" alt="Impressions" data-canonical-src="https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/README.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Azure</author><guid isPermaLink="false">https://github.com/Azure/MachineLearningNotebooks</guid><pubDate>Fri, 08 Nov 2019 00:09:00 GMT</pubDate></item><item><title>justmarkham/scikit-learn-videos #10 in Jupyter Notebook, Today</title><link>https://github.com/justmarkham/scikit-learn-videos</link><description>&lt;p&gt;&lt;i&gt;Jupyter notebooks from the scikit-learn video series&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-introduction-to-machine-learning-with-scikit-learn" class="anchor" aria-hidden="true" href="#introduction-to-machine-learning-with-scikit-learn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to machine learning with scikit-learn&lt;/h1&gt;
&lt;p&gt;This video series will teach you how to solve machine learning problems using Python's popular scikit-learn library. It was &lt;a href="http://blog.kaggle.com/author/kevin-markham/" rel="nofollow"&gt;featured on Kaggle's blog&lt;/a&gt; in 2015.&lt;/p&gt;
&lt;p&gt;There are &lt;strong&gt;9 video tutorials&lt;/strong&gt; totaling 4 hours, each with a corresponding &lt;strong&gt;Jupyter notebook&lt;/strong&gt;. The notebook contains everything you see in the video: code, output, images, and comments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The notebooks in this repository have been updated to use Python 3.6 and scikit-learn 0.19.1. The original notebooks (shown in the video) used Python 2.7 and scikit-learn 0.16, and can be downloaded from the &lt;a href="https://github.com/justmarkham/scikit-learn-videos/tree/archive"&gt;archive branch&lt;/a&gt;. You can read about how I updated the code in this &lt;a href="https://www.dataschool.io/how-to-update-your-scikit-learn-code-for-2018/" rel="nofollow"&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can &lt;a href="https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A" rel="nofollow"&gt;watch the entire series&lt;/a&gt; on YouTube, and &lt;a href="http://nbviewer.jupyter.org/github/justmarkham/scikit-learn-videos/tree/master/" rel="nofollow"&gt;view all of the notebooks&lt;/a&gt; using nbviewer.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=elojMnjn4kk&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=1" title="Watch the first tutorial video" rel="nofollow"&gt;&lt;img src="images/youtube.png" alt="Watch the first tutorial video" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once you complete this video series, I recommend enrolling in my online course, &lt;a href="http://www.dataschool.io/learn/" rel="nofollow"&gt;Machine Learning with Text in Python&lt;/a&gt;, to gain a deeper understanding of scikit-learn and Natural Language Processing.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What is machine learning, and how does it work? (&lt;a href="https://www.youtube.com/watch?v=elojMnjn4kk&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=1" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="01_machine_learning_intro.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/04/08/new-video-series-introduction-to-machine-learning-with-scikit-learn/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is machine learning?&lt;/li&gt;
&lt;li&gt;What are the two main categories of machine learning?&lt;/li&gt;
&lt;li&gt;What are some examples of machine learning?&lt;/li&gt;
&lt;li&gt;How does machine learning "work"?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setting up Python for machine learning: scikit-learn and Jupyter Notebook (&lt;a href="https://www.youtube.com/watch?v=IsXXlYVBt1M&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=2" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="02_machine_learning_setup.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/04/15/scikit-learn-video-2-setting-up-python-for-machine-learning/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the benefits and drawbacks of scikit-learn?&lt;/li&gt;
&lt;li&gt;How do I install scikit-learn?&lt;/li&gt;
&lt;li&gt;How do I use the Jupyter Notebook?&lt;/li&gt;
&lt;li&gt;What are some good resources for learning Python?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Getting started in scikit-learn with the famous iris dataset (&lt;a href="https://www.youtube.com/watch?v=hd1W4CyPX58&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=3" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="03_getting_started_with_iris.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/04/22/scikit-learn-video-3-machine-learning-first-steps-with-the-iris-dataset/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the famous iris dataset, and how does it relate to machine learning?&lt;/li&gt;
&lt;li&gt;How do we load the iris dataset into scikit-learn?&lt;/li&gt;
&lt;li&gt;How do we describe a dataset using machine learning terminology?&lt;/li&gt;
&lt;li&gt;What are scikit-learn's four key requirements for working with data?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training a machine learning model with scikit-learn (&lt;a href="https://www.youtube.com/watch?v=RlQuVL6-qe8&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=4" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="04_model_training.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/04/30/scikit-learn-video-4-model-training-and-prediction-with-k-nearest-neighbors/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the K-nearest neighbors classification model?&lt;/li&gt;
&lt;li&gt;What are the four steps for model training and prediction in scikit-learn?&lt;/li&gt;
&lt;li&gt;How can I apply this pattern to other machine learning models?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Comparing machine learning models in scikit-learn (&lt;a href="https://www.youtube.com/watch?v=0pP4EwWJgIU&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=5" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="05_model_evaluation.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/05/14/scikit-learn-video-5-choosing-a-machine-learning-model/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do I choose which model to use for my supervised learning task?&lt;/li&gt;
&lt;li&gt;How do I choose the best tuning parameters for that model?&lt;/li&gt;
&lt;li&gt;How do I estimate the likely performance of my model on out-of-sample data?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data science pipeline: pandas, seaborn, scikit-learn (&lt;a href="https://www.youtube.com/watch?v=3ZWuPVWq7p4&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=6" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="06_linear_regression.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/05/28/scikit-learn-video-6-linear-regression-plus-pandas-seaborn/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do I use the pandas library to read data into Python?&lt;/li&gt;
&lt;li&gt;How do I use the seaborn library to visualize data?&lt;/li&gt;
&lt;li&gt;What is linear regression, and how does it work?&lt;/li&gt;
&lt;li&gt;How do I train and interpret a linear regression model in scikit-learn?&lt;/li&gt;
&lt;li&gt;What are some evaluation metrics for regression problems?&lt;/li&gt;
&lt;li&gt;How do I choose which features to include in my model?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cross-validation for parameter tuning, model selection, and feature selection (&lt;a href="https://www.youtube.com/watch?v=6dbrR-WymjI&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=7" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="07_cross_validation.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/06/29/scikit-learn-video-7-optimizing-your-model-with-cross-validation/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the drawback of using the train/test split procedure for model evaluation?&lt;/li&gt;
&lt;li&gt;How does K-fold cross-validation overcome this limitation?&lt;/li&gt;
&lt;li&gt;How can cross-validation be used for selecting tuning parameters, choosing between models, and selecting features?&lt;/li&gt;
&lt;li&gt;What are some possible improvements to cross-validation?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficiently searching for optimal tuning parameters (&lt;a href="https://www.youtube.com/watch?v=Gol_qOgRqfA&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=8" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="08_grid_search.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/07/16/scikit-learn-video-8-efficiently-searching-for-optimal-tuning-parameters/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can K-fold cross-validation be used to search for an optimal tuning parameter?&lt;/li&gt;
&lt;li&gt;How can this process be made more efficient?&lt;/li&gt;
&lt;li&gt;How do you search for multiple tuning parameters at once?&lt;/li&gt;
&lt;li&gt;What do you do with those tuning parameters before making real predictions?&lt;/li&gt;
&lt;li&gt;How can the computational expense of this process be reduced?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluating a classification model (&lt;a href="https://www.youtube.com/watch?v=85dtiMz9tSo&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=9" rel="nofollow"&gt;video&lt;/a&gt;, &lt;a href="09_classification_metrics.ipynb"&gt;notebook&lt;/a&gt;, &lt;a href="http://blog.kaggle.com/2015/10/23/scikit-learn-video-9-better-evaluation-of-classification-models/" rel="nofollow"&gt;blog post&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the purpose of model evaluation, and what are some common evaluation procedures?&lt;/li&gt;
&lt;li&gt;What is the usage of classification accuracy, and what are its limitations?&lt;/li&gt;
&lt;li&gt;How does a confusion matrix describe the performance of a classifier?&lt;/li&gt;
&lt;li&gt;What metrics can be computed from a confusion matrix?&lt;/li&gt;
&lt;li&gt;How can you adjust classifier performance by changing the classification threshold?&lt;/li&gt;
&lt;li&gt;What is the purpose of an ROC curve?&lt;/li&gt;
&lt;li&gt;How does Area Under the Curve (AUC) differ from classification accuracy?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-bonus-video" class="anchor" aria-hidden="true" href="#bonus-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Bonus Video&lt;/h2&gt;
&lt;p&gt;At the PyCon 2016 conference, I taught a &lt;strong&gt;3-hour tutorial&lt;/strong&gt; that builds upon this video series and focuses on &lt;strong&gt;text-based data&lt;/strong&gt;. You can watch the &lt;a href="https://www.youtube.com/watch?v=ZiKMIuYidY0&amp;amp;list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&amp;amp;index=10" rel="nofollow"&gt;tutorial video&lt;/a&gt; on YouTube.&lt;/p&gt;
&lt;p&gt;Here are the topics I covered:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Model building in scikit-learn (refresher)&lt;/li&gt;
&lt;li&gt;Representing text as numerical data&lt;/li&gt;
&lt;li&gt;Reading a text-based dataset into pandas&lt;/li&gt;
&lt;li&gt;Vectorizing our dataset&lt;/li&gt;
&lt;li&gt;Building and evaluating a model&lt;/li&gt;
&lt;li&gt;Comparing models&lt;/li&gt;
&lt;li&gt;Examining a model for further insight&lt;/li&gt;
&lt;li&gt;Practicing this workflow on another dataset&lt;/li&gt;
&lt;li&gt;Tuning the vectorizer (discussion)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Visit this &lt;a href="https://github.com/justmarkham/pycon-2016-tutorial"&gt;GitHub repository&lt;/a&gt; to access the tutorial notebooks and many other recommended resources.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>justmarkham</author><guid isPermaLink="false">https://github.com/justmarkham/scikit-learn-videos</guid><pubDate>Fri, 08 Nov 2019 00:10:00 GMT</pubDate></item><item><title>microsoft/recommenders #11 in Jupyter Notebook, Today</title><link>https://github.com/microsoft/recommenders</link><description>&lt;p&gt;&lt;i&gt;Best Practices on Recommendation Systems&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-recommenders" class="anchor" aria-hidden="true" href="#recommenders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommenders&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://microsoft-recommenders.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/100fcbc0137c448427b7a3b2ff5d9f1392d8bd69/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6d6963726f736f66742d7265636f6d6d656e646572732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/microsoft-recommenders/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail our learnings on five key tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="notebooks/01_prepare_data"&gt;Prepare Data&lt;/a&gt;: Preparing and loading data for each recommender algorithm&lt;/li&gt;
&lt;li&gt;&lt;a href="notebooks/02_model"&gt;Model&lt;/a&gt;: Building models using various classical and deep learning recommender algorithms such as Alternating Least Squares (&lt;a href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS" rel="nofollow"&gt;ALS&lt;/a&gt;) or eXtreme Deep Factorization Machines (&lt;a href="https://arxiv.org/abs/1803.05170" rel="nofollow"&gt;xDeepFM&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;a href="notebooks/03_evaluate"&gt;Evaluate&lt;/a&gt;: Evaluating algorithms with offline metrics&lt;/li&gt;
&lt;li&gt;&lt;a href="notebooks/04_model_select_and_optimize"&gt;Model Select and Optimize&lt;/a&gt;: Tuning and optimizing hyperparameters for recommender models&lt;/li&gt;
&lt;li&gt;&lt;a href="notebooks/05_operationalize"&gt;Operationalize&lt;/a&gt;: Operationalizing models in a production environment on Azure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Several utilities are provided in &lt;a href="reco_utils"&gt;reco_utils&lt;/a&gt; to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the &lt;a href="https://readthedocs.org/projects/microsoft-recommenders/" rel="nofollow"&gt;reco_utils documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a more detailed overview of the repository, please see the documents at the &lt;a href="https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations"&gt;wiki page&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;Please see the &lt;a href="SETUP.md"&gt;setup guide&lt;/a&gt; for more details on setting up your machine locally, on a &lt;a href="https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/" rel="nofollow"&gt;data science virtual machine (DSVM)&lt;/a&gt; or on &lt;a href="SETUP.md#setup-guide-for-azure-databricks"&gt;Azure Databricks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To setup on your local machine:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install Anaconda with Python &amp;gt;= 3.6. &lt;a href="https://conda.io/miniconda.html" rel="nofollow"&gt;Miniconda&lt;/a&gt; is a quick way to get started.&lt;/li&gt;
&lt;li&gt;Clone the repository
&lt;pre&gt;&lt;code&gt;git clone https://github.com/Microsoft/Recommenders
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Run the generate conda file script to create a conda environment:
(This is for a basic python environment, see &lt;a href="SETUP.md"&gt;SETUP.md&lt;/a&gt; for PySpark and GPU environment setup)
&lt;pre&gt;&lt;code&gt;cd Recommenders
python scripts/generate_conda_file.py
conda env create -f reco_base.yaml  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Activate the conda environment and register it with Jupyter:
&lt;pre&gt;&lt;code&gt;conda activate reco_base
python -m ipykernel install --user --name reco_base --display-name "Python (reco)"
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Start the Jupyter notebook server
&lt;pre&gt;&lt;code&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Run the &lt;a href="notebooks/00_quick_start/sar_movielens.ipynb"&gt;SAR Python CPU MovieLens&lt;/a&gt; notebook under the &lt;code&gt;00_quick_start&lt;/code&gt; folder. Make sure to change the kernel to "Python (reco)".&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; - The &lt;a href="notebooks/00_quick_start/als_movielens.ipynb"&gt;Alternating Least Squares (ALS)&lt;/a&gt; notebooks require a PySpark environment to run. Please follow the steps in the &lt;a href="SETUP.md#dependencies-setup"&gt;setup guide&lt;/a&gt; to run these notebooks in a PySpark environment. For the deep learning algorithms, it is recommended to use a GPU machine.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-algorithms" class="anchor" aria-hidden="true" href="#algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algorithms&lt;/h2&gt;
&lt;p&gt;The table below lists the recommender algorithms currently available in the repository. Notebooks are linked under the Environment column when different implementations are available.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;th&gt;Environment&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Alternating Least Squares (ALS)&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/als_movielens.ipynb"&gt;PySpark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized by Spark MLLib for scalability and distributed computing capability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cornac/Bayesian Personalized Ranking (BPR)&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/02_model/cornac_bpr_deep_dive.ipynb"&gt;Python CPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Matrix factorization algorithm for predicting item ranking with implicit feedback&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deep Knowledge-Aware Network (DKN)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/dkn_synthetic.ipynb"&gt;Python CPU / Python GPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Content-Based Filtering&lt;/td&gt;
&lt;td&gt;Deep learning algorithm incorporating a knowledge graph and article embeddings to provide powerful news or article recommendations&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Extreme Deep Factorization Machine (xDeepFM)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/xdeepfm_criteo.ipynb"&gt;Python CPU / Python GPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Hybrid&lt;/td&gt;
&lt;td&gt;Deep learning based algorithm for implicit and explicit feedback with user/item features&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FastAI Embedding Dot Bias (FAST)&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/fastai_movielens.ipynb"&gt;Python CPU / Python GPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;General purpose algorithm with embeddings and biases for users and items&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LightGBM/Gradient Boosting Tree&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/lightgbm_tinycriteo.ipynb"&gt;Python CPU&lt;/a&gt; / &lt;a href="notebooks/02_model/mmlspark_lightgbm_criteo.ipynb"&gt;PySpark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Content-Based Filtering&lt;/td&gt;
&lt;td&gt;Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neural Collaborative Filtering (NCF)&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/ncf_movielens.ipynb"&gt;Python CPU / Python GPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Deep learning algorithm with enhanced performance for implicit feedback&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Restricted Boltzmann Machines (RBM)&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/rbm_movielens.ipynb"&gt;Python CPU / Python GPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Neural network based algorithm for learning the underlying probability distribution for explicit or implicit feedback&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Riemannian Low-rank Matrix Completion (RLRMC)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/rlrmc_movielens.ipynb"&gt;Python CPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Simple Algorithm for Recommendation (SAR)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/sar_movielens.ipynb"&gt;Python CPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Similarity-based algorithm for implicit feedback dataset&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Surprise/Singular Value Decomposition (SVD)&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/02_model/surprise_svd_deep_dive.ipynb"&gt;Python CPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Matrix factorization algorithm for predicting explicit rating feedback in datasets that are not very large&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vowpal Wabbit Family (VW)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/02_model/vowpal_wabbit_deep_dive.ipynb"&gt;Python CPU (online training)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Content-Based Filtering&lt;/td&gt;
&lt;td&gt;Fast online learning algorithms, great for scenarios where user features / context are constantly changing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wide and Deep&lt;/td&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/wide_deep_movielens.ipynb"&gt;Python CPU / Python GPU&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Hybrid&lt;/td&gt;
&lt;td&gt;Deep learning algorithm that can memorize feature interactions and generalize user features&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;sup&gt;*&lt;/sup&gt; indicates algorithms invented/contributed by Microsoft.&lt;/p&gt;
&lt;p&gt;Independent or incubating algorithms and utilities are candidates for the &lt;a href="contrib"&gt;contrib&lt;/a&gt; folder. This will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;th&gt;Environment&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SARplus &lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="contrib/sarplus/README.md"&gt;PySpark&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Collaborative Filtering&lt;/td&gt;
&lt;td&gt;Optimized implementation of SAR for Spark&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary-comparison" class="anchor" aria-hidden="true" href="#preliminary-comparison"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary Comparison&lt;/h3&gt;
&lt;p&gt;We provide a &lt;a href="benchmarks/movielens.ipynb"&gt;benchmark notebook&lt;/a&gt; to illustrate how different algorithms could be evaluated and compared. In this notebook, the MovieLens dataset is split into training/test sets at a 75/25 ratio using a stratified split. A recommendation model is trained using each of the collaborative filtering algorithms below. We utilize empirical parameter values reported in literature &lt;a href="http://mymedialite.net/examples/datasets.html" rel="nofollow"&gt;here&lt;/a&gt;. For ranking metrics we use &lt;code&gt;k=10&lt;/code&gt; (top 10 recommended items). We run the comparison on a Standard NC6s_v2 &lt;a href="https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/" rel="nofollow"&gt;Azure DSVM&lt;/a&gt; (6 vCPUs, 112 GB memory and 1 P100 GPU). Spark ALS is run in local standalone mode. In this table we show the results on Movielens 100k, running the algorithms for 15 epochs.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Algo&lt;/th&gt;
&lt;th&gt;MAP&lt;/th&gt;
&lt;th&gt;nDCG@k&lt;/th&gt;
&lt;th&gt;Precision@k&lt;/th&gt;
&lt;th&gt;Recall@k&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt;
&lt;th&gt;Explained Variance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/als_movielens.ipynb"&gt;ALS&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.004732&lt;/td&gt;
&lt;td&gt;0.044239&lt;/td&gt;
&lt;td&gt;0.048462&lt;/td&gt;
&lt;td&gt;0.017796&lt;/td&gt;
&lt;td&gt;0.965038&lt;/td&gt;
&lt;td&gt;0.753001&lt;/td&gt;
&lt;td&gt;0.255647&lt;/td&gt;
&lt;td&gt;0.251648&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="notebooks/02_model/surprise_svd_deep_dive.ipynb"&gt;SVD&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.012873&lt;/td&gt;
&lt;td&gt;0.095930&lt;/td&gt;
&lt;td&gt;0.091198&lt;/td&gt;
&lt;td&gt;0.032783&lt;/td&gt;
&lt;td&gt;0.938681&lt;/td&gt;
&lt;td&gt;0.742690&lt;/td&gt;
&lt;td&gt;0.291967&lt;/td&gt;
&lt;td&gt;0.291971&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/sar_movielens.ipynb"&gt;SAR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.113028&lt;/td&gt;
&lt;td&gt;0.388321&lt;/td&gt;
&lt;td&gt;0.333828&lt;/td&gt;
&lt;td&gt;0.183179&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="notebooks/02_model/ncf_deep_dive.ipynb"&gt;NCF&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.107720&lt;/td&gt;
&lt;td&gt;0.396118&lt;/td&gt;
&lt;td&gt;0.347296&lt;/td&gt;
&lt;td&gt;0.180775&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="notebooks/02_model/cornac_bpr_deep_dive.ipynb"&gt;BPR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.105365&lt;/td&gt;
&lt;td&gt;0.389948&lt;/td&gt;
&lt;td&gt;0.349841&lt;/td&gt;
&lt;td&gt;0.181807&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="notebooks/00_quick_start/fastai_movielens.ipynb"&gt;FastAI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;0.025503&lt;/td&gt;
&lt;td&gt;0.147866&lt;/td&gt;
&lt;td&gt;0.130329&lt;/td&gt;
&lt;td&gt;0.053824&lt;/td&gt;
&lt;td&gt;0.943084&lt;/td&gt;
&lt;td&gt;0.744337&lt;/td&gt;
&lt;td&gt;0.285308&lt;/td&gt;
&lt;td&gt;0.287671&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;This project welcomes contributions and suggestions. Before contributing, please see our &lt;a href="CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-build-status" class="anchor" aria-hidden="true" href="#build-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Build Status&lt;/h2&gt;
&lt;p&gt;These tests are the nightly builds, which compute the smoke and integration tests. &lt;code&gt;master&lt;/code&gt; is our main branch and &lt;code&gt;staging&lt;/code&gt; is our development branch. We use &lt;code&gt;pytest&lt;/code&gt; for testing python utilities in &lt;a href="reco_utils"&gt;reco_utils&lt;/a&gt; and &lt;code&gt;papermill&lt;/code&gt; for the &lt;a href="notebooks"&gt;notebooks&lt;/a&gt;. For more information about the testing pipelines, please see the &lt;a href="tests/README.md"&gt;test documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-dsvm-build-status" class="anchor" aria-hidden="true" href="#dsvm-build-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DSVM Build Status&lt;/h3&gt;
&lt;p&gt;The following tests run on a Windows and Linux DSVM daily. These machines run 24/7.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Build Type&lt;/th&gt;
&lt;th&gt;Branch&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Branch&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=4792" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/1d4e25df9458f609329e03800010abf990869bd2/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c793f6272616e63684e616d653d6d6173746572" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=4594" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6e9d78c96397d2b11e3a5f7efff87a99ce28a964/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f73746167696e673f6272616e63684e616d653d73746167696e67" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_staging?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_build/latest?definitionId=4997" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a205e994a3544dcf1b0c31a7f7614ead4a0fdc9b/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f6770753f6272616e63684e616d653d6d6173746572" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_gpu?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_build/latest?definitionId=4998" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/99c66c537b1f64a8c69d539093022e6186fe76e2/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f6770755f73746167696e673f6272616e63684e616d653d73746167696e67" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_gpu_staging?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linux Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=4804" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a6e38a6d0b32515cbad3079f6e65c26de36ec955/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f737061726b3f6272616e63684e616d653d6d6173746572" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_spark?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=5186" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/754ee8b7e3666df639a9ad2d0ea69d5396b62dc4/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f5265636f6d6d656e646572732f6e696768746c795f737061726b5f73746167696e67" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/Recommenders/nightly_spark_staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Windows CPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=6743" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6d2c52bc7ebae7ee452f75d72dc830286e41afda/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f77696e3f6272616e63684e616d653d6d6173746572" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_win?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=6752" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9ae3c63985b356e442c44a9e34fa5412d8a1a325/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f73746167696e675f77696e3f6272616e63684e616d653d73746167696e67" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_staging_win?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Windows GPU&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=6756" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/67e609cc9f1edf193f32412fcb93432474df2571/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f6770755f77696e3f6272616e63684e616d653d6d6173746572" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_gpu_win?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=6761" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bb8be1db3e2ed82963331ae663216f4e5bd3fc3c/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f6770755f73746167696e675f77696e3f6272616e63684e616d653d73746167696e67" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_gpu_staging_win?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Windows Spark&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=6757" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52dd66d61d3844e6a05d354292e7dad5120a5f27/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f737061726b5f77696e3f6272616e63684e616d653d6d6173746572" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_spark_win?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_build/latest?definitionId=6754" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9040554bad197e6fc743384c66194c1d81db414b/68747470733a2f2f6d73646174612e76697375616c73747564696f2e636f6d2f416c676f726974686d73416e6444617461536369656e63652f5f617069732f6275696c642f7374617475732f6e696768746c795f737061726b5f73746167696e675f77696e3f6272616e63684e616d653d73746167696e67" alt="Status" data-canonical-src="https://msdata.visualstudio.com/AlgorithmsAndDataScience/_apis/build/status/nightly_spark_staging_win?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-azureml-build-status" class="anchor" aria-hidden="true" href="#azureml-build-status"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AzureML Build Status&lt;/h3&gt;
&lt;p&gt;The following tests run on an AzureML &lt;a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-compute-target" rel="nofollow"&gt;compute target&lt;/a&gt;. AzureML allows to programmatically start a virtual machine, execute the tests, gather the results in &lt;a href="https://azure.microsoft.com/en-gb/services/devops/" rel="nofollow"&gt;Azure DevOps&lt;/a&gt; and shut down the machine.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Build Type&lt;/th&gt;
&lt;th&gt;Branch&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Branch&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;nightly_cpu_tests&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/recommenders/_build/latest?definitionId=25&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6f458b221e43aebf9aaa0441315886e37ba34662/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f7265636f6d6d656e646572732f5f617069732f6275696c642f7374617475732f6e696768746c795f6370755f74657374733f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/recommenders/_apis/build/status/nightly_cpu_tests?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/recommenders/_build/latest?definitionId=25&amp;amp;branchName=staging" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ef23ac0b8ee5a86afb75543c4151fd5518976796/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f7265636f6d6d656e646572732f5f617069732f6275696c642f7374617475732f6e696768746c795f6370755f74657374733f6272616e63684e616d653d73746167696e67" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/recommenders/_apis/build/status/nightly_cpu_tests?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;nightly_gpu_tests&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/recommenders/_build/latest?definitionId=5&amp;amp;branchName=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4c5076ea30f54411d5d0683e1d71366d8cc98ce2/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f7265636f6d6d656e646572732f5f617069732f6275696c642f7374617475732f62702d6e696768746c795f6770755f74657374733f6272616e63684e616d653d6d6173746572" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/recommenders/_apis/build/status/bp-nightly_gpu_tests?branchName=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Staging&lt;/td&gt;
&lt;td&gt;&lt;a href="https://dev.azure.com/best-practices/recommenders/_build/latest?definitionId=5&amp;amp;branchName=staging" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/7daa16def4201b749bbdd2b99d179b9b535272ec/68747470733a2f2f6465762e617a7572652e636f6d2f626573742d7072616374696365732f7265636f6d6d656e646572732f5f617069732f6275696c642f7374617475732f62702d6e696768746c795f6770755f74657374733f6272616e63684e616d653d73746167696e67" alt="Build Status" data-canonical-src="https://dev.azure.com/best-practices/recommenders/_apis/build/status/bp-nightly_gpu_tests?branchName=staging" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>microsoft</author><guid isPermaLink="false">https://github.com/microsoft/recommenders</guid><pubDate>Fri, 08 Nov 2019 00:11:00 GMT</pubDate></item><item><title>awslabs/amazon-sagemaker-examples #12 in Jupyter Notebook, Today</title><link>https://github.com/awslabs/amazon-sagemaker-examples</link><description>&lt;p&gt;&lt;i&gt;Example notebooks that show how to apply machine learning, deep learning and reinforcement learning in Amazon SageMaker&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-amazon-sagemaker-examples" class="anchor" aria-hidden="true" href="#amazon-sagemaker-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Examples&lt;/h1&gt;
&lt;p&gt;This repository contains example notebooks that show how to apply machine learning and deep learning in &lt;a href="https://aws.amazon.com/sagemaker" rel="nofollow"&gt;Amazon SageMaker&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples" class="anchor" aria-hidden="true" href="#examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-ground-truth-labeling-jobs" class="anchor" aria-hidden="true" href="#introduction-to-ground-truth-labeling-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Ground Truth Labeling Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with the labeling job workflow for Amazon SageMaker Ground Truth.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/from_unlabeled_data_to_deployed_machine_learning_model_ground_truth_demo_image_classification"&gt;From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification&lt;/a&gt; is an end-to-end example that starts with an unlabeled dataset, labels it using the Ground Truth API, analyzes the results, trains an image classification neural net using the annotated dataset, and finally uses the trained model to perform batch and online inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/ground_truth_object_detection_tutorial"&gt;Ground Truth Object Detection Tutorial&lt;/a&gt; is a similar end-to-end example but for an object detection task.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/data_analysis_of_ground_truth_image_classification_output"&gt;Basic Data Analysis of an Image Classification Output Manifest&lt;/a&gt; presents charts to visualize the number of annotations for each class, differentiating between human annotations and automatic labels (if your job used auto-labeling). It also displays sample images in each class, and creates a pdf which concisely displays the full results.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/object_detection_augmented_manifest_training"&gt;Training a Machine Learning Model Using an Output Manifest&lt;/a&gt; introduces the concept of an "augmented manifest" and demonstrates that the output file of a labeling job can be immediately used as the input file to train a SageMaker machine learning model.&lt;/li&gt;
&lt;li&gt;&lt;a href="ground_truth_labeling_jobs/annotation_consolidation"&gt;Annotation Consolidation&lt;/a&gt; demonstrates Amazon SageMaker Ground Truth annotation consolidation techniques for image classification for a completed labeling job.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-applying-machine-learning" class="anchor" aria-hidden="true" href="#introduction-to-applying-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Applying Machine Learning&lt;/h3&gt;
&lt;p&gt;These examples provide a gentle introduction to machine learning concepts as they are applied in practical use cases across a variety of sectors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; predicts potential customers that are most likely to convert based on customer and aggregate level metrics, using Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; uses customer interaction and service usage data to find those most likely to churn, and then walks through the cost/benefit trade-offs of providing retention incentives.  This uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to create a highly predictive model.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/linear_time_series_forecast"&gt;Time-series Forecasting&lt;/a&gt; generates a forecast for topline product demand using Amazon SageMaker's Linear Learner algorithm.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/breast_cancer_prediction"&gt;Cancer Prediction&lt;/a&gt; predicts Breast Cancer based on features derived from images, using SageMaker's Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/ensemble_modeling"&gt;Ensembling&lt;/a&gt; predicts income using two Amazon SageMaker models to show the advantages in ensembling.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/video_game_sales"&gt;Video Game Sales&lt;/a&gt; develops a binary prediction model for the success of video games based on review scores.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/gluon_recommender_system"&gt;MXNet Gluon Recommender System&lt;/a&gt; uses neural network embeddings for non-linear matrix factorization to predict user movie ratings on Amazon digital reviews.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/fair_linear_learner"&gt;Fair Linear Learner&lt;/a&gt; is an example of an effective way to create fair linear models with respect to sensitive features.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans"&gt;Population Segmentation of US Census Data using PCA and Kmeans&lt;/a&gt; analyzes US census data and reduces dimensionality using PCA then clusters US counties using KMeans to identify segments of similar counties.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_applying_machine_learning/object2vec_document_embedding"&gt;Document Embedding using Object2Vec&lt;/a&gt; is an example to embed a large collection of documents in a common low-dimensional space, so that the semantic distances between these documents are preserved.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-sagemaker-automatic-model-tuning" class="anchor" aria-hidden="true" href="#sagemaker-automatic-model-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SageMaker Automatic Model Tuning&lt;/h3&gt;
&lt;p&gt;These examples introduce SageMaker's hyperparameter tuning functionality which helps deliver the best possible predictions by running a large number of training jobs to determine which hyperparameter values are the most impactful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/xgboost_direct_marketing"&gt;XGBoost Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning to improve your model fits for the &lt;a href="introduction_to_applying_machine_learning/xgboost_direct_marketing"&gt;Targeted Direct Marketing&lt;/a&gt; task.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/tensorflow_mnist"&gt;TensorFlow Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built TensorFlow container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/mxnet_mnist"&gt;MXNet Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the pre-built MXNet container and MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/keras_bring_your_own"&gt;Keras BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with a custom container running a Keras convolutional network on CIFAR-10 data.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/r_bring_your_own"&gt;R BYO Tuning&lt;/a&gt; shows how to use SageMaker hyperparameter tuning with the custom container from the &lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; example.&lt;/li&gt;
&lt;li&gt;&lt;a href="hyperparameter_tuning/analyze_results"&gt;Analyzing Results&lt;/a&gt; is a shared notebook that can be used after each of the above notebooks to provide analysis on how training jobs with different hyperparameters performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-amazon-algorithms" class="anchor" aria-hidden="true" href="#introduction-to-amazon-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Amazon Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide quick walkthroughs to get you up and running with Amazon SageMaker's custom developed algorithms.  Most of these algorithms can train on distributed hardware, scale incredibly well, and are faster and cheaper than popular alternatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/1P_kmeans_highlevel"&gt;k-means&lt;/a&gt; is our introductory example for Amazon SageMaker.  It walks through the process of clustering MNIST images of handwritten digits using Amazon SageMaker k-means.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/factorization_machines_mnist"&gt;Factorization Machines&lt;/a&gt; showcases Amazon SageMaker's implementation of the algorithm to predict whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; introduces topic modeling using Amazon SageMaker Latent Dirichlet Allocation (LDA) on a synthetic dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/linear_learner_mnist"&gt;Linear Learner&lt;/a&gt; predicts whether a handwritten digit from the MNIST dataset is a 0 or not using a binary classifier from Amazon SageMaker Linear Learner.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ntm_synthetic"&gt;Neural Topic Model (NTM)&lt;/a&gt; uses Amazon SageMaker Neural Topic Model (NTM) to uncover topics in documents from a synthetic data source, where topic distributions are known.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/pca_mnist"&gt;Principal Components Analysis (PCA)&lt;/a&gt; uses Amazon SageMaker PCA to calculate eigendigits from MNIST.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/seq2seq_translation_en-de"&gt;Seq2Seq&lt;/a&gt; uses the Amazon SageMaker Seq2Seq algorithm that's built on top of &lt;a href="https://github.com/awslabs/sockeye"&gt;Sockeye&lt;/a&gt;, which is a sequence-to-sequence framework for Neural Machine Translation based on MXNet.  Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation.  This notebook shows translation from English to German text.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; includes full training and transfer learning examples of Amazon SageMaker's Image Classification algorithm.  This uses a ResNet deep convolutional neural network to classify images from the caltech dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_abalone"&gt;XGBoost for regression&lt;/a&gt; predicts the age of abalone (&lt;a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html" rel="nofollow"&gt;Abalone dataset&lt;/a&gt;) using regression from Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/xgboost_mnist"&gt;XGBoost for multi-class classification&lt;/a&gt; uses Amazon SageMaker's implementation of &lt;a href="https://github.com/dmlc/xgboost"&gt;XGBoost&lt;/a&gt; to classify handwritten digits from the MNIST dataset as one of the ten digits using a multi-class classifier. Both single machine and distributed use-cases are presented.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/deepar_synthetic"&gt;DeepAR for time series forecasting&lt;/a&gt; illustrates how to use the Amazon SageMaker DeepAR algorithm for time series forecasting on a synthetically generated data set.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/blazingtext_word2vec_text8"&gt;BlazingText Word2Vec&lt;/a&gt; generates Word2Vec embeddings from a cleaned text dump of Wikipedia articles using SageMaker's fast and scalable BlazingText implementation.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_pascalvoc_coco"&gt;Object Detection&lt;/a&gt; illustrates how to train an object detector using the Amazon SageMaker Object Detection algorithm with different input formats (RecordIO and image).  It uses the Pascal VOC dataset. A third notebook is provided to demonstrate the use of incremental training.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object_detection_birds"&gt;Object detection for bird images&lt;/a&gt; demonstrates how to use the Amazon SageMaker Object Detection algorithm with a public dataset of Bird images.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_movie_recommendation"&gt;Object2Vec for movie recommendation&lt;/a&gt; demonstrates how Object2Vec can be used to model data consisting of pairs of singleton tokens using movie recommendation as a running example.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_multilabel_genre_classification"&gt;Object2Vec for multi-label classification&lt;/a&gt; shows how ObjectToVec algorithm can train on data consisting of pairs of sequences and singleton tokens using the setting of genre prediction of movies based on their plot descriptions.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/object2vec_sentence_similarity"&gt;Object2Vec for sentence similarity&lt;/a&gt; explains how to train Object2Vec using sequence pairs as input using sentence similarity analysis as the application.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/ipinsights_login"&gt;IP Insights for suspicious logins&lt;/a&gt; shows how to train IP Insights on a login events for a web server to identify suspicious login attempts.&lt;/li&gt;
&lt;li&gt;&lt;a href="introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc"&gt;Semantic Segmentation&lt;/a&gt; shows how to train a semantic segmentation algorithm using the Amazon SageMaker Semantic Segmentation algorithm. It also demonstrates how to host the model and produce segmentaion masks and probability of segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-rl" class="anchor" aria-hidden="true" href="#amazon-sagemaker-rl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker RL&lt;/h3&gt;
&lt;p&gt;The following provide examples demonstrating different capabilities of Amazon SageMaker RL.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_cartpole_coach"&gt;Cartpole using Coach&lt;/a&gt; demonstrates the simplest usecase of Amazon SageMaker RL using Intel's RL Coach.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_deepracer_robomaker_coach_gazebo"&gt;AWS DeepRacer&lt;/a&gt; demonstrates AWS DeepRacer trainig using RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_hvac_coach_energyplus"&gt;HVAC using EnergyPlus&lt;/a&gt; demonstrates the training of HVAC systems using the EnergyPlus environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_knapsack_coach_custom"&gt;Knapsack Problem&lt;/a&gt; demonstrates how to solve the knapsack problem using a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_mountain_car_coach_gymEnv"&gt;Mountain Car&lt;/a&gt; Mountain car is a classic RL problem. This notebook explains how to solve this using the OpenAI Gym environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_network_compression_ray_custom"&gt;Distributed Neural Network Compression&lt;/a&gt; This notebook explains how to compress ResNets using RL, using a custom environment and the RLLib toolkit.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_objecttracker_robomaker_coach_gazebo"&gt;Turtlebot Tracker&lt;/a&gt; This notebook demonstrates object tracking using AWS Robomaker and RL Coach in the Gazebo environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_portfolio_management_coach_customEnv"&gt;Portfolio Management&lt;/a&gt; This notebook uses a custom Gym environment to manage multiple financial investments.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_predictive_autoscaling_coach_customEnv"&gt;Autoscaling&lt;/a&gt; demonstrates how to adjust load depending on demand. This uses RL Coach and a custom environment.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_ray"&gt;Roboschool&lt;/a&gt; is an open source physics simulator that is commonly used to train RL policies for robotic systems. This notebook demonstrates training a few agents using it.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_roboschool_stable_baselines"&gt;Stable Baselines&lt;/a&gt; In this notebook example, we will make the HalfCheetah agent learn to walk using the stable-baselines, which are a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_traveling_salesman_vehicle_routing_coach"&gt;Travelling Salesman&lt;/a&gt; is a classic NP hard problem, which this notebook solves with AWS SageMaker RL.&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning/rl_tic_tac_toe_coach_customEnv"&gt;Tic-tac-toe&lt;/a&gt; is a simple implementation of a custom Gym environment to train and deploy an RL agent in Coach that then plays tic-tac-toe interactively in a Jupyter Notebook.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-scientific-details-of-algorithms" class="anchor" aria-hidden="true" href="#scientific-details-of-algorithms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Scientific Details of Algorithms&lt;/h3&gt;
&lt;p&gt;These examples provide more thorough mathematical treatment on a select group of algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/streaming_median"&gt;Streaming Median&lt;/a&gt; sequentially introduces concepts used in streaming algorithms, which many SageMaker algorithms rely on to deliver speed and scalability.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/lda_topic_modeling"&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; dives into Amazon SageMaker's spectral decomposition approach to LDA.&lt;/li&gt;
&lt;li&gt;&lt;a href="scientific_details_of_algorithms/linear_learner_class_weights_loss_functions"&gt;Linear Learner features&lt;/a&gt; shows how to use the class weights and loss functions features of the SageMaker Linear Learner algorithm to improve performance on a credit card fraud prediction task&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-advanced-amazon-sagemaker-functionality" class="anchor" aria-hidden="true" href="#advanced-amazon-sagemaker-functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Advanced Amazon SageMaker Functionality&lt;/h3&gt;
&lt;p&gt;These examples that showcase unique functionality available in Amazon SageMaker.  They cover a broad range of topics and will utilize a variety of methods, but aim to provide the user with sufficient insight or inspiration to develop within Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="advanced_functionality/data_distribution_types"&gt;Data Distribution Types&lt;/a&gt; showcases the difference between two methods for sending data from S3 to Amazon SageMaker Training instances.  This has particular implication for scalability and accuracy of distributed training.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/handling_kms_encrypted_data"&gt;Encrypting Your Data&lt;/a&gt; shows how to use Server Side KMS encrypted data with Amazon SageMaker training. The IAM role used for S3 access needs to have permissions to encrypt and decrypt data with the KMS key.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/parquet_to_recordio_protobuf"&gt;Using Parquet Data&lt;/a&gt; shows how to bring &lt;a href="https://parquet.apache.org/" rel="nofollow"&gt;Parquet&lt;/a&gt; data sitting in S3 into an Amazon SageMaker Notebook and convert it into the recordIO-protobuf format that many SageMaker algorithms consume.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/working_with_redshift_data"&gt;Connecting to Redshift&lt;/a&gt; demonstrates how to copy data from Redshift to S3 and vice-versa without leaving Amazon SageMaker Notebooks.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/xgboost_bring_your_own_model"&gt;Bring Your Own XGBoost Model&lt;/a&gt; shows how to use Amazon SageMaker Algorithms containers to bring a pre-trained model to a realtime hosted endpoint without ever needing to think about REST APIs.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/kmeans_bring_your_own_model"&gt;Bring Your Own k-means Model&lt;/a&gt; shows how to take a model that's been fit elsewhere and use Amazon SageMaker Algorithms containers to host it.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/r_bring_your_own"&gt;Bring Your Own R Algorithm&lt;/a&gt; shows how to bring your own algorithm container to Amazon SageMaker using the R language.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/install_r_kernel"&gt;Installing the R Kernel&lt;/a&gt; shows how to install the R kernel into an Amazon SageMaker Notebook Instance.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/scikit_bring_your_own"&gt;Bring Your Own scikit Algorithm&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm for training and production-ready hosting.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/mxnet_mnist_byom"&gt;Bring Your Own MXNet Model&lt;/a&gt; shows how to bring a model trained anywhere using MXNet into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/tensorflow_iris_byom"&gt;Bring Your Own TensorFlow Model&lt;/a&gt; shows how to bring a model trained anywhere using TensorFlow into Amazon SageMaker.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_xgboost_abalone"&gt;Inference Pipeline with SparkML and XGBoost&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and XGBoost for training on the Abalone dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/inference_pipeline_sparkml_blazingtext_dbpedia"&gt;Inference Pipeline with SparkML and BlazingText&lt;/a&gt; shows how to deploy an Inference Pipeline with SparkML for data pre-processing and BlazingText for training on the DBPedia dataset. The pre-processing code is written once and used between training and inference.&lt;/li&gt;
&lt;li&gt;&lt;a href="advanced_functionality/search"&gt;Experiment Management Capabilities with Search&lt;/a&gt; shows how to organize Training Jobs into projects, and track relationships between Models, Endpoints, and Training Jobs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-neo-compilation-jobs" class="anchor" aria-hidden="true" href="#amazon-sagemaker-neo-compilation-jobs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Neo Compilation Jobs&lt;/h3&gt;
&lt;p&gt;These examples provide you an introduction to how to use Neo to optimizes deep learning model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/imageclassification_caltech"&gt;Image Classification&lt;/a&gt; Adapts form &lt;a href="introduction_to_amazon_algorithms/imageclassification_caltech"&gt;image classification&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;mxnet mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/pytorch_torchvision"&gt;Deploying pre-trained PyTorch vision models&lt;/a&gt; shows how to use Amazon SageMaker Neo to compile and optimize pre-trained PyTorch models from TorchVision.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; Adapts form &lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;tensorflow mnist&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker_neo_compilation_jobs/xgboost_customer_churn"&gt;Predicting Customer Churn&lt;/a&gt; Adapts form &lt;a href="introduction_to_applying_machine_learning/xgboost_customer_churn"&gt;xgboost customer churn&lt;/a&gt; including Neo API and comparsion between the baseline&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk" class="anchor" aria-hidden="true" href="#amazon-sagemaker-pre-built-framework-containers-and-the-python-sdk"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Amazon SageMaker Pre-Built Framework Containers and the Python SDK&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-deep-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-deep-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Deep Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you to write idiomatic TensorFlow or MXNet and then train or host in pre-built containers using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_cifar10"&gt;Chainer CIFAR-10&lt;/a&gt; trains a VGG image classification network on CIFAR-10 using Chainer (both single machine and multi-machine versions are included)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_mnist"&gt;Chainer MNIST&lt;/a&gt; trains a basic neural network on MNIST using Chainer (shows how to use local mode)&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/chainer_sentiment_analysis"&gt;Chainer sentiment analysis&lt;/a&gt; trains a LSTM network with embeddings to predict text sentiment using Chainer&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_iris"&gt;IRIS with Scikit-learn&lt;/a&gt; trains a Scikit-learn classifier on IRIS data&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_cifar10"&gt;CIFAR-10 with MXNet Gluon&lt;/a&gt; trains a ResNet-34  image classification model using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_mnist"&gt;MNIST with MXNet Gluon&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit dataset using MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_mnist"&gt;MNIST with MXNet&lt;/a&gt; trains a basic neural network on the MNIST handwritten digit data using MXNet's symbolic syntax&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/mxnet_gluon_sentiment"&gt;Sentiment Analysis with MXNet Gluon&lt;/a&gt; trains a text classifier using embeddings with MXNet Gluon&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_layers"&gt;TensorFlow Neural Networks with Layers&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow layers&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_abalone_age_predictor_using_keras"&gt;TensorFlow Networks with Keras&lt;/a&gt; trains a basic neural network on the abalone dataset using TensorFlow and Keras&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_iris_dnn_classifier_using_estimators"&gt;Introduction to Estimators in TensorFlow&lt;/a&gt; trains a DNN classifier estimator on the Iris dataset using TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_resnet_cifar10_with_tensorboard"&gt;TensorFlow and TensorBoard&lt;/a&gt; trains a ResNet image classification model on CIFAR-10 using TensorFlow and showcases how to track results using TensorBoard&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/tensorflow_distributed_mnist"&gt;Distributed TensorFlow&lt;/a&gt; trains a simple convolutional neural network on MNIST using TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-pre-built-machine-learning-framework-containers" class="anchor" aria-hidden="true" href="#pre-built-machine-learning-framework-containers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-Built Machine Learning Framework Containers&lt;/h4&gt;
&lt;p&gt;These examples show you how to build Machine Learning models with frameworks like Apache Spark or Scikit-learn using SageMaker Python SDK.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/sparkml_serving_emr_mleap_abalone"&gt;Inference with SparkML Serving&lt;/a&gt; shows how to build an ML model with Apache Spark using Amazon EMR on Abalone dataset and deploy in SageMaker with SageMaker SparkML Serving.&lt;/li&gt;
&lt;li&gt;&lt;a href="sagemaker-python-sdk/scikit_learn_inference_pipeline"&gt;Pipeline Inference with Scikit-learn and LinearLearner&lt;/a&gt; builds a ML pipeline using Scikit-learn preprocessing and LinearLearner algorithm in single endpoint&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-using-amazon-sagemaker-with-apache-spark" class="anchor" aria-hidden="true" href="#using-amazon-sagemaker-with-apache-spark"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Amazon SageMaker with Apache Spark&lt;/h3&gt;
&lt;p&gt;These examples show how to use Amazon SageMaker for model training, hosting, and inference through Apache Spark using &lt;a href="https://github.com/aws/sagemaker-spark"&gt;SageMaker Spark&lt;/a&gt;. SageMaker Spark allows you to interleave Spark Pipeline stages with Pipeline stages that interact with Amazon SageMaker.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="sagemaker-spark/pyspark_mnist"&gt;MNIST with SageMaker PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aws-marketplace" class="anchor" aria-hidden="true" href="#aws-marketplace"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS Marketplace&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#create-algorithmsmodel-packages-for-listing-in-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Create algorithms/model packages for listing in AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;This example shows you how to package a model-package/algorithm for listing in AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/creating_marketplace_products"&gt;Creating Algorithm and Model Package - Listing on AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to package a scikit learn algorithm to create SageMaker Algorithm and SageMaker Model Package entities that can be used with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs and listed on AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning" class="anchor" aria-hidden="true" href="#use-algorithms-and-model-packages-from-aws-marketplace-for-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Use algorithms and model packages from AWS Marketplace for machine learning.&lt;/h4&gt;
&lt;p&gt;These examples show you how to use model-packages and algorithms from AWS Marketplace for machine learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_algorithms"&gt;Using Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/amazon_demo_product"&gt;Using Algorithm From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Algorithm with the enhanced SageMaker Train/Transform/Hosting/Tuning APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_algorithms/automl"&gt;Using AutoML algorithm&lt;/a&gt; provides a detailed walkthrough on how to use AutoML algorithm from AWS Marketplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="aws_marketplace/using_model_packages"&gt;Using Model Packages&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/amazon_demo_product"&gt;Using Model Packages From AWS Marketplace&lt;/a&gt; provides a detailed walkthrough on how to use Model Package entities with the enhanced SageMaker Transform/Hosting APIs by choosing a canonical product listed on AWS Marketplace.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/auto_insurance"&gt;Using models for extracting vehicle metadata&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of auto-insurance claim processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="aws_marketplace/using_model_packages/improving_industrial_workplace_safety"&gt;Using models for identifying non-compliance at a workplace&lt;/a&gt; provides a detailed walkthrough on how to use pre-trained models from AWS Marketplace for extracting metadata for a sample use-case of generating summary reports for identifying non-compliance at a construction/industrial workplace.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-under-development" class="anchor" aria-hidden="true" href="#under-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Under Development&lt;/h3&gt;
&lt;p&gt;These Amazon SageMaker examples fully illustrate a concept, but may require some additional configuration on the users part to complete.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What do I need in order to get started?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The quickest setup to run example notebooks includes:
&lt;ul&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html" rel="nofollow"&gt;AWS account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Proper &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html" rel="nofollow"&gt;IAM User and Role&lt;/a&gt; setup&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html" rel="nofollow"&gt;Amazon SageMaker Notebook Instance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An &lt;a href="http://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html" rel="nofollow"&gt;S3 bucket&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Will these examples work outside of Amazon SageMaker Notebook Instances?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although most examples utilize key Amazon SageMaker functionality like distributed, managed training or real-time hosted endpoints, these notebooks can be run outside of Amazon SageMaker Notebook Instances with minimal modification (updating IAM role definition and installing the necessary libraries).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;How do I contribute my own example notebook?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although we're extremely excited to receive contributions from the community, we're still working on the best mechanism to take in examples from external sources.  Please bear with us in the short-term if pull requests take longer than expected or are closed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>awslabs</author><guid isPermaLink="false">https://github.com/awslabs/amazon-sagemaker-examples</guid><pubDate>Fri, 08 Nov 2019 00:12:00 GMT</pubDate></item><item><title>chenyuntc/simple-faster-rcnn-pytorch #13 in Jupyter Notebook, Today</title><link>https://github.com/chenyuntc/simple-faster-rcnn-pytorch</link><description>&lt;p&gt;&lt;i&gt;A simplified implemention of Faster R-CNN that replicate performance from origin paper&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body MD" data-path="README.MD"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-a-simple-and-fast-implementation-of-faster-r-cnn" class="anchor" aria-hidden="true" href="#a-simple-and-fast-implementation-of-faster-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Simple and Fast Implementation of Faster R-CNN&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-1-introduction" class="anchor" aria-hidden="true" href="#1-introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;I've update the code to support both Python2 and Python3, PyTorch 0.4. If you want the old version code please checkout branch &lt;a href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch/tree/0.3"&gt;v0.3&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This project is a &lt;strong&gt;Simplified&lt;/strong&gt; Faster R-CNN implementation based on &lt;a href="https://github.com/chainer/chainercv"&gt;chainercv&lt;/a&gt; and other &lt;a href="#acknowledgement"&gt;projects&lt;/a&gt; . It aims to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplify the code (&lt;em&gt;Simple is better than complex&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Make the code more straightforward (&lt;em&gt;Flat is better than nested&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Match the performance reported in &lt;a href="https://arxiv.org/abs/1506.01497" rel="nofollow"&gt;origin paper&lt;/a&gt; (&lt;em&gt;Speed Counts and mAP Matters&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And it has the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It can be run as pure Python code, no more build affair. (cuda code moves to cupy, Cython acceleration are optional)&lt;/li&gt;
&lt;li&gt;It's a minimal implemention in around 2000 lines valid code with a lot of comment and instruction.(thanks to chainercv's excellent documentation)&lt;/li&gt;
&lt;li&gt;It achieves higher mAP than the origin implementation (0.712 VS 0.699)&lt;/li&gt;
&lt;li&gt;It achieve speed compariable with other implementation (6fps and 14fps for train and test in TITAN XP with cython)&lt;/li&gt;
&lt;li&gt;It's memory-efficient (about 3GB for vgg16)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/cb6764fe9c5f596be1b9d7d3bdf3d1368b50eb47/687474703a2f2f377a683433722e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f6661737465722d73706565642e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/cb6764fe9c5f596be1b9d7d3bdf3d1368b50eb47/687474703a2f2f377a683433722e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f6661737465722d73706565642e6a7067" alt="img" data-canonical-src="http://7zh43r.com1.z0.glb.clouddn.com/del/faster-speed.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-2-performance" class="anchor" aria-hidden="true" href="#2-performance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Performance&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-21-map" class="anchor" aria-hidden="true" href="#21-map"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.1 mAP&lt;/h3&gt;
&lt;p&gt;VGG16 train on &lt;code&gt;trainval&lt;/code&gt; and test on &lt;code&gt;test&lt;/code&gt; split.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the training shows great randomness, you may need a bit of luck and more epoches of training to reach the highest mAP. However, it should be easy to surpass the lower bound.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Implementation&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://arxiv.org/abs/1506.01497" rel="nofollow"&gt;origin paper&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;0.699&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;train with caffe pretrained model&lt;/td&gt;
&lt;td align="center"&gt;0.700-0.712&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;train with torchvision pretrained model&lt;/td&gt;
&lt;td align="center"&gt;0.685-0.701&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;model converted from &lt;a href="https://github.com/chainer/chainercv/tree/master/examples/faster_rcnn"&gt;chainercv&lt;/a&gt; (reported 0.706)&lt;/td&gt;
&lt;td align="center"&gt;0.7053&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-22-speed" class="anchor" aria-hidden="true" href="#22-speed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.2 Speed&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Implementation&lt;/th&gt;
&lt;th align="center"&gt;GPU&lt;/th&gt;
&lt;th align="center"&gt;Inference&lt;/th&gt;
&lt;th align="center"&gt;Trainining&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://arxiv.org/abs/1506.01497" rel="nofollow"&gt;origin paper&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;K40&lt;/td&gt;
&lt;td align="center"&gt;5 fps&lt;/td&gt;
&lt;td align="center"&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;This[1]&lt;/td&gt;
&lt;td align="center"&gt;TITAN Xp&lt;/td&gt;
&lt;td align="center"&gt;14-15 fps&lt;/td&gt;
&lt;td align="center"&gt;6 fps&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;a href="https://github.com/ruotianluo/pytorch-faster-rcnn"&gt;pytorch-faster-rcnn&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;TITAN Xp&lt;/td&gt;
&lt;td align="center"&gt;15-17fps&lt;/td&gt;
&lt;td align="center"&gt;6fps&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;[1]: make sure you install cupy correctly and only one program run on the GPU. The training speed is sensitive to your gpu status. see &lt;a href="troubleshooting"&gt;troubleshooting&lt;/a&gt; for more info. Morever it's slow in the start of the program -- it need time to warm up.&lt;/p&gt;
&lt;p&gt;It could be faster by removing visualization, logging, averaging loss etc.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-3-install-dependencies" class="anchor" aria-hidden="true" href="#3-install-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3. Install dependencies&lt;/h2&gt;
&lt;p&gt;requires PyTorch &amp;gt;=0.4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;install PyTorch &amp;gt;=0.4 with GPU (code are GPU-only), refer to &lt;a href="http://pytorch.org" rel="nofollow"&gt;official website&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;install cupy, you can install via &lt;code&gt;pip install cupy-cuda80&lt;/code&gt; or(cupy-cuda90,cupy-cuda91, etc).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;install other dependencies:  &lt;code&gt;pip install -r requirements.txt &lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optional, but strongly recommended: build cython code &lt;code&gt;nms_gpu_post&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd&lt;/span&gt; model/utils/nms/
python build.py build_ext --inplace
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; -&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;start visdom for visualization&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;nohup python -m visdom.server &lt;span class="pl-k"&gt;&amp;amp;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-4-demo" class="anchor" aria-hidden="true" href="#4-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4. Demo&lt;/h2&gt;
&lt;p&gt;Download pretrained model from &lt;a href="https://drive.google.com/open?id=1cQ27LIn-Rig4-Uayzy_gH5-cW-NRGVzY" rel="nofollow"&gt;Google Drive&lt;/a&gt; or &lt;a href="https://pan.baidu.com/s/1o87RuXW" rel="nofollow"&gt;Baidu Netdisk( passwd: scxn)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch/blob/master/demo.ipynb"&gt;demo.ipynb&lt;/a&gt; for more detail.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-5-train" class="anchor" aria-hidden="true" href="#5-train"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5. Train&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-51-prepare-data" class="anchor" aria-hidden="true" href="#51-prepare-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.1 Prepare data&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-pascal-voc2007" class="anchor" aria-hidden="true" href="#pascal-voc2007"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pascal VOC2007&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download the training, validation, test data and VOCdevkit&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extract all of these tars into one directory named &lt;code&gt;VOCdevkit&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;tar xvf VOCtrainval_06-Nov-2007.tar
tar xvf VOCtest_06-Nov-2007.tar
tar xvf VOCdevkit_08-Jun-2007.tar&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It should have this basic structure&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-smi"&gt;$VOCdevkit&lt;/span&gt;/                           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; development kit&lt;/span&gt;
&lt;span class="pl-smi"&gt;$VOCdevkit&lt;/span&gt;/VOCcode/                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; VOC utility code&lt;/span&gt;
&lt;span class="pl-smi"&gt;$VOCdevkit&lt;/span&gt;/VOC2007                    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; image sets, annotations, etc.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; ... and several other directories ...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;modify &lt;code&gt;voc_data_dir&lt;/code&gt; cfg item in &lt;code&gt;utils/config.py&lt;/code&gt;, or pass it to program using argument like &lt;code&gt;--voc-data-dir=/path/to/VOCdevkit/VOC2007/&lt;/code&gt; .&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-52-prepare-caffe-pretrained-vgg16" class="anchor" aria-hidden="true" href="#52-prepare-caffe-pretrained-vgg16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.2 Prepare caffe-pretrained vgg16&lt;/h3&gt;
&lt;p&gt;If you want to use caffe-pretrain model as initial weight, you can run below to get vgg16 weights converted from caffe, which is the same as the origin paper use.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python misc/convert_caffe_pretrain.py&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This scripts would download pretrained model and converted it to the format compatible with torchvision. If you are in China and can not download the pretrain model, you may refer to &lt;a href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch/issues/63"&gt;this issue&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Then you could specify where caffe-pretraind model &lt;code&gt;vgg16_caffe.pth&lt;/code&gt; stored in &lt;code&gt;utils/config.py&lt;/code&gt; by setting &lt;code&gt;caffe_pretrain_path&lt;/code&gt;. The default path is ok.&lt;/p&gt;
&lt;p&gt;If you want to use pretrained model from torchvision, you may skip this step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;, caffe pretrained model has shown slight better performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: caffe model require images in BGR 0-255, while torchvision model requires images in RGB and 0-1. See &lt;code&gt;data/dataset.py&lt;/code&gt;for more detail.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-53-begin-training" class="anchor" aria-hidden="true" href="#53-begin-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.3 begin training&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;mkdir checkpoints/ &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; folder for snapshots&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py train --env=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;fasterrcnn-caffe&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; --plot-every=100 --caffe-pretrain&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;you may refer to &lt;code&gt;utils/config.py&lt;/code&gt; for more argument.&lt;/p&gt;
&lt;p&gt;Some Key arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--caffe-pretrain=False&lt;/code&gt;: use pretrain model from caffe or torchvision (Default: torchvison)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--plot-every=n&lt;/code&gt;: visualize prediction, loss etc every &lt;code&gt;n&lt;/code&gt; batches.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--env&lt;/code&gt;: visdom env for visualization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--voc_data_dir&lt;/code&gt;: where the VOC data stored&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--use-drop&lt;/code&gt;: use dropout in RoI head, default False&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--use-Adam&lt;/code&gt;: use Adam instead of SGD, default SGD. (You need set a very low &lt;code&gt;lr&lt;/code&gt; for Adam)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--load-path&lt;/code&gt;: pretrained model path, default &lt;code&gt;None&lt;/code&gt;, if it's specified, it would be loaded.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;you may open browser, visit &lt;code&gt;http://&amp;lt;ip&amp;gt;:8097&lt;/code&gt; and see the visualization of training procedure as below:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/02642af25c30f1c0132c37a11ce36e5ea0f7e49e/687474703a2f2f377a683433722e636f6d322e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f766973646f6d2d66617374657272636e6e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/02642af25c30f1c0132c37a11ce36e5ea0f7e49e/687474703a2f2f377a683433722e636f6d322e7a302e676c622e636c6f7564646e2e636f6d2f64656c2f766973646f6d2d66617374657272636e6e2e706e67" alt="visdom" data-canonical-src="http://7zh43r.com2.z0.glb.clouddn.com/del/visdom-fasterrcnn.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;dataloader: &lt;code&gt;received 0 items of ancdata&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;see &lt;a href="https://github.com/pytorch/pytorch/issues/973#issuecomment-346405667"&gt;discussion&lt;/a&gt;, It's alreadly fixed in &lt;a href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch/blob/master/train.py#L17-L22"&gt;train.py&lt;/a&gt;. So I think you are free from this problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Windows support&lt;/p&gt;
&lt;p&gt;I don't have windows machine with GPU to debug and test it. It's welcome if anyone could make a pull request and test it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-more" class="anchor" aria-hidden="true" href="#more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; training on coco&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; resnet&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Maybe；replace cupy with THTensor+cffi?&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Maybe：Convert all numpy code to tensor?&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; python2-compatibility&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work builds on many excellent works, which include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chainer/chainercv"&gt;Yusuke Niitani's ChainerCV&lt;/a&gt; (mainly)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ruotianluo/pytorch-faster-rcnn"&gt;Ruotian Luo's pytorch-faster-rcnn&lt;/a&gt; which based on &lt;a href="https://github.com/endernewton/tf-faster-rcnn"&gt;Xinlei Chen's tf-faster-rcnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jwyang/faster-rcnn.pytorch"&gt;faster-rcnn.pytorch by Jianwei Yang and Jiasen Lu&lt;/a&gt;.It mainly refer to &lt;a href="https://github.com/longcw/faster_rcnn_pytorch"&gt;longcw's faster_rcnn_pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;All the above Repositories have referred to &lt;a href="https://github.com/rbgirshick/py-faster-rcnn"&gt;py-faster-rcnn by Ross Girshick and Sean Bell&lt;/a&gt;  either directly or indirectly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-_" class="anchor" aria-hidden="true" href="#_"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;^_^&lt;/h2&gt;
&lt;p&gt;Licensed under MIT, see the LICENSE for more detail.&lt;/p&gt;
&lt;p&gt;Contribution Welcome.&lt;/p&gt;
&lt;p&gt;If you encounter any problem, feel free to open an issue, but too busy lately.&lt;/p&gt;
&lt;p&gt;Correct me if anything is wrong or unclear.&lt;/p&gt;
&lt;p&gt;model structure
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/chenyuntc/cloud/master/faster-rcnn%E7%9A%84%E5%89%AF%E6%9C%AC%E7%9A%84%E5%89%AF%E6%9C%AC.png"&gt;&lt;img src="https://raw.githubusercontent.com/chenyuntc/cloud/master/faster-rcnn%E7%9A%84%E5%89%AF%E6%9C%AC%E7%9A%84%E5%89%AF%E6%9C%AC.png" alt="img" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>chenyuntc</author><guid isPermaLink="false">https://github.com/chenyuntc/simple-faster-rcnn-pytorch</guid><pubDate>Fri, 08 Nov 2019 00:13:00 GMT</pubDate></item><item><title>MachineLP/MachineLP-CodeFun #14 in Jupyter Notebook, Today</title><link>https://github.com/MachineLP/MachineLP-CodeFun</link><description>&lt;p&gt;&lt;i&gt;MachineLP CodeFun&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;MachineLP：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其实事物发展有自己的潮流和规律，当你身处潮流之中的时候，要紧紧抓住潮流的机会，想办法脱颖而出，即使没有成功，也会更加洞悉时代的脉搏，收获珍贵的知识和经验。而如果潮流已经退去，这个时候再去往这个方向上努力，只会收获迷茫与压抑，对时代、对自己都没有什么帮助。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;但是时代的浪潮犹如海滩上的浪花，总是一浪接着一浪，只要你站在海边，身处这个行业之中，下一个浪潮很快又会到来。你需要敏感而又深刻地去观察，略去那些浮躁的泡沫，抓住真正潮流的机会，奋力一搏，不管成败，都不会遗憾。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;切记：求精不求多，有舍才有得；不做旁观者，不拒绝身边的任何小事。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;欢迎加微信：lp9628。  因为相信所以遇见，有时候你我相遇不一定是巧合。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-01---programming_language" class="anchor" aria-hidden="true" href="#01---programming_language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./01-programming_language/"&gt;programming_language&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-01---python" class="anchor" aria-hidden="true" href="#01---python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./01-programming_language/01-python/"&gt;python&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-02---scala" class="anchor" aria-hidden="true" href="#02---scala"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./01-programming_language/02-scala/"&gt;scala&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-03---c" class="anchor" aria-hidden="true" href="#03---c"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;03 - &lt;a href="./01-programming_language/03-c++/"&gt;c++&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;该部分包含基础的语法和代码，可以快读上手，并且可根据自己的需求自行查看需要了解的基础知识。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-02---data_structure" class="anchor" aria-hidden="true" href="#02---data_structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./02-data_structure/"&gt;data_structure&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-01---sword_offer" class="anchor" aria-hidden="true" href="#01---sword_offer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./02-data_structure/01-Sword_Offer/"&gt;Sword_Offer&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-02---leetcode" class="anchor" aria-hidden="true" href="#02---leetcode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./02-data_structure/02-leetCode/"&gt;leetCode&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-03---03-" class="anchor" aria-hidden="true" href="#03---03-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;03 - &lt;a href="./02-data_structure/03-/"&gt;03-&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;随着自己的知识边界越来越...，越会发现数据结构和算法的重要性，很多框架底层都是使用像数组、链表等数据结构，后续的各种数据结构和算法也是由此构建而来。（通过查看很多框架源码也会发现这个问题）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-03---deep_learning" class="anchor" aria-hidden="true" href="#03---deep_learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;03 - &lt;a href="./03-deep_learning/"&gt;deep_learning&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-01---tensorflow_examples" class="anchor" aria-hidden="true" href="#01---tensorflow_examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./03-deep_learning/01-tensorflow_examples/"&gt;tensorflow_examples&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5&gt;&lt;a id="user-content-01---tensorflow10" class="anchor" aria-hidden="true" href="#01---tensorflow10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./03-deep_learning/01-tensorflow_examples/01-tensorflow1.0+/"&gt;tensorflow1.0+&lt;/a&gt;&lt;/h5&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5&gt;&lt;a id="user-content-02---tensorflow20" class="anchor" aria-hidden="true" href="#02---tensorflow20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./03-deep_learning/01-tensorflow_examples/02-tensorflow2.0/"&gt;tensorflow2.0&lt;/a&gt;&lt;/h5&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5&gt;&lt;a id="user-content-03---recommendation" class="anchor" aria-hidden="true" href="#03---recommendation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;03 - &lt;a href="./03-deep_learning/01-tensorflow_examples/03-recommendation/"&gt;recommendation&lt;/a&gt;&lt;/h5&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5&gt;&lt;a id="user-content-04---cv" class="anchor" aria-hidden="true" href="#04---cv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;04 - &lt;a href="./03-deep_learning/01-tensorflow_examples/04-CV/"&gt;CV&lt;/a&gt;&lt;/h5&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5&gt;&lt;a id="user-content-05---nlp" class="anchor" aria-hidden="true" href="#05---nlp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;05 - &lt;a href="./03-deep_learning/01-tensorflow_examples/05-NLP/"&gt;NLP&lt;/a&gt;&lt;/h5&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5&gt;&lt;a id="user-content-06---gan" class="anchor" aria-hidden="true" href="#06---gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;06 - &lt;a href="./03-deep_learning/01-tensorflow_examples/06-GAN/"&gt;GAN&lt;/a&gt;&lt;/h5&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h5&gt;&lt;a id="user-content-07---rl" class="anchor" aria-hidden="true" href="#07---rl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;07 - &lt;a href="./03-deep_learning/01-tensorflow_examples/07-RL/"&gt;RL&lt;/a&gt;&lt;/h5&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-02---keras_examples" class="anchor" aria-hidden="true" href="#02---keras_examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./03-deep_learning/02-keras_examples/"&gt;keras_examples&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-03---pytorch_examples" class="anchor" aria-hidden="true" href="#03---pytorch_examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;03 - &lt;a href="./03-deep_learning/03-pytorch_examples/"&gt;pytorch_examples&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;作为近两年热点的深度学习当然也是重点的内容之一，本部分会从tensorflow、keras、pytorch框架的使用进行介绍，主要涉及推荐、计算机视觉、自然语言处理、GAN、RL几个方面。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-04---machine_learning" class="anchor" aria-hidden="true" href="#04---machine_learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;04 - &lt;a href="./04-machine_learning/"&gt;machine_learning&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-01---sklearn_examples" class="anchor" aria-hidden="true" href="#01---sklearn_examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./04-machine_learning/01-sklearn_examples/"&gt;sklearn_examples&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-02---sparkml_examples" class="anchor" aria-hidden="true" href="#02---sparkml_examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./04-machine_learning/02-sparkml_examples/"&gt;sparkml_examples&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-03---feature_engineering_examples" class="anchor" aria-hidden="true" href="#03---feature_engineering_examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;03 - &lt;a href="./04-machine_learning/03-feature_engineering/"&gt;feature_engineering_examples&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;作为长久不衰的机器学习，由于其可结实性强、硬件要求不算高、高效的特点，在各行各业依然起的非常重要的作用。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-05---auto_ml_dl" class="anchor" aria-hidden="true" href="#05---auto_ml_dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;05 - &lt;a href="./05-auto_ml_dl/"&gt;auto_ml_dl&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-01---auto_ml" class="anchor" aria-hidden="true" href="#01---auto_ml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./05-auto_ml_dl/01-auto_ml/"&gt;auto_ml&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-02---auto_dl" class="anchor" aria-hidden="true" href="#02---auto_dl"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./05-auto_ml_dl/02-auto_dl/"&gt;auto_dl&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;AutoML/DL作为近期的热点，各厂都有呈现自己的paper、框架，都在想从这个热潮中脱颖而出。该部分直接影响到算法工程师今后的工作方向，值得关注。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-06---model_server" class="anchor" aria-hidden="true" href="#06---model_server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;06 - &lt;a href="./06-model_server/"&gt;model_server&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-01---py_backend" class="anchor" aria-hidden="true" href="#01---py_backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;01 - &lt;a href="./06-model_server/py_backend/"&gt;py_backend&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-02---c_backend" class="anchor" aria-hidden="true" href="#02---c_backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;02 - &lt;a href="./06-model_server/c++_backend/"&gt;c++_backend&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a id="user-content-03---java_backend" class="anchor" aria-hidden="true" href="#03---java_backend"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;03 - &lt;a href="./06-model_server/java_backend/"&gt;java_backend&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;模型服务模块可以说是必不可少的一个环境，当训练好的模块要使用的时候，可以通过服务端、sdk等方式，本部分会介绍通过服务的方式让模块跑起来。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content-07---sql" class="anchor" aria-hidden="true" href="#07---sql"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;07 - &lt;a href="./07-sql/"&gt;sql&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;作为和数据打交道的工程师，sql是必备技能之一，直接影响到你取数的成败、效率、资源占用，也要做好也是要下一份苦功夫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-参考" class="anchor" aria-hidden="true" href="#参考"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;参考：&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h3&gt;&lt;a id="user-content---reference" class="anchor" aria-hidden="true" href="#--reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;- &lt;a href="./REFERENCE.md"&gt;REFERENCE&lt;/a&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>MachineLP</author><guid isPermaLink="false">https://github.com/MachineLP/MachineLP-CodeFun</guid><pubDate>Fri, 08 Nov 2019 00:14:00 GMT</pubDate></item><item><title>priya-dwivedi/Deep-Learning #15 in Jupyter Notebook, Today</title><link>https://github.com/priya-dwivedi/Deep-Learning</link><description>&lt;p&gt;&lt;i&gt;[No description found.]&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h3&gt;&lt;a id="user-content-deep-learning" class="anchor" aria-hidden="true" href="#deep-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep-Learning&lt;/h3&gt;
&lt;p&gt;This repository contains deep learning related projects I have done over time. I am very passionate about deep learning and explore interesting ideas and tools. Each project is contained in its own folder.&lt;/p&gt;
&lt;p&gt;I have blogged about a lot of these projects on Medium - &lt;a href="https://medium.com/@priya.dwivedi" rel="nofollow"&gt;https://medium.com/@priya.dwivedi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I also run a deep learning consultancy - &lt;a href="https://deeplearninganalytics.org/" rel="nofollow"&gt;https://deeplearninganalytics.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you want to collaborate on a project please reach out through my website.&lt;/p&gt;
&lt;p&gt;Hope you enjoy cloning this repo and trying out things yourself&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>priya-dwivedi</author><guid isPermaLink="false">https://github.com/priya-dwivedi/Deep-Learning</guid><pubDate>Fri, 08 Nov 2019 00:15:00 GMT</pubDate></item><item><title>yandexdataschool/Practical_RL #16 in Jupyter Notebook, Today</title><link>https://github.com/yandexdataschool/Practical_RL</link><description>&lt;p&gt;&lt;i&gt;A course in reinforcement learning in the wild&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-practical_rl-" class="anchor" aria-hidden="true" href="#practical_rl-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Practical_RL &lt;a href="https://mybinder.org/v2/gh/yandexdataschool/practical_rl/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;An open course on reinforcement learning in the wild.
Taught on-campus at &lt;a href="https://cs.hse.ru" rel="nofollow"&gt;HSE&lt;/a&gt; and &lt;a href="https://yandexdataschool.com/" rel="nofollow"&gt;YSDA&lt;/a&gt;  and maintained to be friendly to online students (both english and russian).&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-manifesto" class="anchor" aria-hidden="true" href="#manifesto"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Manifesto:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Optimize for the curious.&lt;/strong&gt; For all the materials that aren’t covered in detail there are links to more information and related materials (D.Silver/Sutton/blogs/whatever). Assignments will have bonus sections if you want to dig deeper.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Practicality first.&lt;/strong&gt; Everything essential to solving reinforcement learning problems is worth mentioning. We won't shun away from covering tricks and heuristics. For every major idea there should be a lab that makes you to “feel” it on a practical problem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Git-course.&lt;/strong&gt; Know a way to make the course better? Noticed a typo in a formula? Found a useful link? Made the code more readable? Made a version for alternative framework? You're awesome! &lt;a href="https://help.github.com/articles/about-pull-requests/"&gt;Pull-request&lt;/a&gt; it!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/yandexdataschool/Practical_RL/graphs/contributors"&gt;&lt;img src="https://camo.githubusercontent.com/d9f36b0731eae6cd289f82e27b6c23ec07e1059f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f79616e646578646174617363686f6f6c2f50726163746963616c5f524c2e7376673f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" alt="Github contributors" data-canonical-src="https://img.shields.io/github/contributors/yandexdataschool/Practical_RL.svg?logo=github&amp;amp;logoColor=white" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-course-info" class="anchor" aria-hidden="true" href="#course-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course info&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Chat room&lt;/strong&gt; for YSDA &amp;amp; HSE students is &lt;a href="https://t.me/joinchat/CDFcMVcoAQvEiI9WAo1pEQ" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Grading&lt;/strong&gt; rules for YSDA &amp;amp; HSE students is &lt;a href="https://github.com/yandexdataschool/Practical_RL/wiki/Homeworks-and-grading"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FAQ:&lt;/strong&gt; &lt;a href="https://github.com/yandexdataschool/Practical_RL/wiki/Practical-RL"&gt;About the course&lt;/a&gt;, &lt;a href="https://github.com/yandexdataschool/Practical_RL/issues/1"&gt;Technical issues thread&lt;/a&gt;, &lt;a href="https://yadi.sk/d/loPpY45J3EAYfU" rel="nofollow"&gt;Lecture Slides&lt;/a&gt;, &lt;a href="https://github.com/yandexdataschool/Practical_RL/wiki/Online-student's-survival-guide"&gt;Online Student Survival Guide&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anonymous &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSdurWw97Sm9xCyYwC8g3iB5EibITnoPJW2IkOVQYE_kcXPh6Q/viewform" rel="nofollow"&gt;feedback form&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Virtual course environment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/yandexdataschool/Practical_RL/issues/1"&gt;Installing dependencies&lt;/a&gt; on your local machine (recommended).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://colab.research.google.com/" rel="nofollow"&gt;&lt;strong&gt;google colab&lt;/strong&gt;&lt;/a&gt; - set open -&amp;gt; github -&amp;gt; yandexdataschool/pracical_rl -&amp;gt; {branch name} and select any notebook you want.&lt;/li&gt;
&lt;li&gt;Alternatives: &lt;a href="https://mybinder.org/v2/gh/yandexdataschool/practical_rl/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt; and &lt;a href="https://notebooks.azure.com/" rel="nofollow"&gt;Azure Notebooks&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-additional-materials" class="anchor" aria-hidden="true" href="#additional-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional materials&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/yandexdataschool/Practical_RL/wiki/RL-reading-group"&gt;RL reading group&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus&lt;/h1&gt;
&lt;p&gt;The syllabus is approximate: the lectures may occur in a slightly different order and some topics may end up taking two weeks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week01_intro"&gt;&lt;strong&gt;week01_intro&lt;/strong&gt;&lt;/a&gt; Introduction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: RL problems around us. Decision processes. Stochastic optimization, Crossentropy method. Parameter space search vs action space search.&lt;/li&gt;
&lt;li&gt;Seminar: Welcome into openai gym. Tabular CEM for Taxi-v0, deep CEM for box2d environments.&lt;/li&gt;
&lt;li&gt;Homework description - see week1/README.md.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week02_value_based"&gt;&lt;strong&gt;week02_value_based&lt;/strong&gt;&lt;/a&gt; Value-based methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Discounted reward MDP. Value-based approach. Value iteration. Policy iteration. Discounted reward fails.&lt;/li&gt;
&lt;li&gt;Seminar: Value iteration.&lt;/li&gt;
&lt;li&gt;Homework description - see week2/README.md.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week03_model_free"&gt;&lt;strong&gt;week03_model_free&lt;/strong&gt;&lt;/a&gt; Model-free reinforcement learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Q-learning. SARSA. Off-policy Vs on-policy algorithms. N-step algorithms. TD(Lambda).&lt;/li&gt;
&lt;li&gt;Seminar: Qlearning Vs SARSA Vs Expected Value SARSA&lt;/li&gt;
&lt;li&gt;Homework description - see week3/README.md.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week04_%5Brecap%5D_deep_learning"&gt;&lt;strong&gt;recap_deep_learning&lt;/strong&gt;&lt;/a&gt; - deep learning recap&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Deep learning 101&lt;/li&gt;
&lt;li&gt;Seminar: Intro to pytorch/tensorflow, simple image classification with convnets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week04_approx_rl"&gt;&lt;strong&gt;week04_approx_rl&lt;/strong&gt;&lt;/a&gt; Approximate (deep) RL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Infinite/continuous state space. Value function approximation. Convergence conditions. Multiple agents trick; experience replay, target networks, double/dueling/bootstrap DQN, etc.&lt;/li&gt;
&lt;li&gt;Seminar:  Approximate Q-learning with experience replay. (CartPole, Atari)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week05_explore"&gt;&lt;strong&gt;week05_explore&lt;/strong&gt;&lt;/a&gt; Exploration&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Contextual bandits. Thompson Sampling, UCB, bayesian UCB. Exploration in model-based RL, MCTS. "Deep" heuristics for exploration.&lt;/li&gt;
&lt;li&gt;Seminar: bayesian exploration for contextual bandits. UCB for MCTS.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week06_policy_based"&gt;&lt;strong&gt;week06_policy_based&lt;/strong&gt;&lt;/a&gt; Policy Gradient methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Motivation for policy-based, policy gradient, logderivative trick, REINFORCE/crossentropy method, variance reduction(baseline), advantage actor-critic (incl. GAE)&lt;/li&gt;
&lt;li&gt;Seminar: REINFORCE, advantage actor-critic&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week07_seq2seq"&gt;&lt;strong&gt;week07_seq2seq&lt;/strong&gt;&lt;/a&gt; Reinforcement Learning for Sequence Models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Problems with sequential data. Recurrent neural netowks. Backprop through time. Vanishing &amp;amp; exploding gradients. LSTM, GRU. Gradient clipping&lt;/li&gt;
&lt;li&gt;Seminar: character-level RNN language model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week08_pomdp"&gt;&lt;strong&gt;week08_pomdp&lt;/strong&gt;&lt;/a&gt; Partially Observed MDP&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: POMDP intro. POMDP learning (agents with memory). POMDP planning (POMCP, etc)&lt;/li&gt;
&lt;li&gt;Seminar: Deep kung-fu &amp;amp; doom with recurrent A3C and DRQN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week09_policy_II"&gt;&lt;strong&gt;week09_policy_II&lt;/strong&gt;&lt;/a&gt; Advanced policy-based methods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Trust region policy optimization. NPO/PPO. Deterministic policy gradient. DDPG&lt;/li&gt;
&lt;li&gt;Seminar: Approximate TRPO for simple robot control.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week10_planning"&gt;&lt;strong&gt;week10_planning&lt;/strong&gt;&lt;/a&gt; Model-based RL &amp;amp; Co&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Model-Based RL, Planning in General, Imitation Learning and Inverse Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Seminar: MCTS for toy tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./yet_another_week"&gt;&lt;strong&gt;yet_another_week&lt;/strong&gt;&lt;/a&gt; Inverse RL and Imitation Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All that cool RL stuff that you won't learn from this course :)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-course-staff" class="anchor" aria-hidden="true" href="#course-staff"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course staff&lt;/h1&gt;
&lt;p&gt;Course materials and teaching by: &lt;em&gt;[unordered]&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/bestxolodec"&gt;Pavel Shvechikov&lt;/a&gt; - lectures, seminars, hw checkups, reading group&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/qwasser"&gt;Nikita Putintsev&lt;/a&gt; - seminars, hw checkups, organizing our hot mess&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Fritz449"&gt;Alexander Fritsler&lt;/a&gt; - lectures, seminars, hw checkups&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Omrigan"&gt;Oleg Vasilev&lt;/a&gt; - seminars, hw checkups, technical support&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pastafarianist"&gt;Dmitry Nikulin&lt;/a&gt; - tons of fixes, far and wide&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MichaelKonobeev"&gt;Mikhail Konobeev&lt;/a&gt; - seminars, hw checkups&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/neer201"&gt;Ivan Kharitonov&lt;/a&gt; - seminars, hw checkups&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zshrav"&gt;Ravil Khisamov&lt;/a&gt; - seminars, hw checkups&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/justheuristic"&gt;Fedor Ratnikov&lt;/a&gt; - admin stuff&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-contributions" class="anchor" aria-hidden="true" href="#contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Using pictures from &lt;a href="http://ai.berkeley.edu/home.html" rel="nofollow"&gt;Berkeley AI course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Massively refering to &lt;a href="http://rll.berkeley.edu/deeprlcourse/" rel="nofollow"&gt;CS294&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Several tensorflow assignments by &lt;a href="https://github.com/Scitator"&gt;Scitator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A lot of fixes from &lt;a href="https://github.com/arogozhnikov"&gt;arogozhnikov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Other awesome people: see github &lt;a href="https://github.com/yandexdataschool/Practical_RL/graphs/contributors"&gt;contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/alexeyum"&gt;Alexey Umnov&lt;/a&gt; helped us a lot during spring2018&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yandexdataschool</author><guid isPermaLink="false">https://github.com/yandexdataschool/Practical_RL</guid><pubDate>Fri, 08 Nov 2019 00:16:00 GMT</pubDate></item><item><title>fengdu78/lihang-code #17 in Jupyter Notebook, Today</title><link>https://github.com/fengdu78/lihang-code</link><description>&lt;p&gt;&lt;i&gt;《统计学习方法》的代码实现&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;代码目录&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第1章 统计学习方法概论&lt;/p&gt;
&lt;p&gt;第2章 感知机&lt;/p&gt;
&lt;p&gt;第3章 k近邻法&lt;/p&gt;
&lt;p&gt;第4章 朴素贝叶斯&lt;/p&gt;
&lt;p&gt;第5章 决策树&lt;/p&gt;
&lt;p&gt;第6章 逻辑斯谛回归&lt;/p&gt;
&lt;p&gt;第7章 支持向量机&lt;/p&gt;
&lt;p&gt;第8章 提升方法&lt;/p&gt;
&lt;p&gt;第9章 EM算法及其推广&lt;/p&gt;
&lt;p&gt;第10章 隐马尔可夫模型&lt;/p&gt;
&lt;p&gt;第11章 条件随机场&lt;/p&gt;
&lt;p&gt;第12章 监督学习方法总结&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;参考：
&lt;a href="https://github.com/wzyonggege/statistical-learning-method"&gt;https://github.com/wzyonggege/statistical-learning-method&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/WenDesi/lihang_book_algorithm"&gt;https://github.com/WenDesi/lihang_book_algorithm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/tudaodiaozhale" rel="nofollow"&gt;https://blog.csdn.net/tudaodiaozhale&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码整理和修改：机器学习初学者&lt;/p&gt;
&lt;p&gt;微信公众号：机器学习初学者 &lt;a target="_blank" rel="noopener noreferrer" href="images/gongzhong.jpg"&gt;&lt;img src="images/gongzhong.jpg" alt="gongzhong" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;知识星球：黄博的机器学习圈子&lt;a target="_blank" rel="noopener noreferrer" href="images/zhishixingqiu1.jpg"&gt;&lt;img src="images/zhishixingqiu1.jpg" alt="xingqiu" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/people/fengdu78" rel="nofollow"&gt;知乎&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fengdu78</author><guid isPermaLink="false">https://github.com/fengdu78/lihang-code</guid><pubDate>Fri, 08 Nov 2019 00:17:00 GMT</pubDate></item><item><title>rlabbe/Kalman-and-Bayesian-Filters-in-Python #18 in Jupyter Notebook, Today</title><link>https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</link><description>&lt;p&gt;&lt;i&gt;Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs.  Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-kalman-and-bayesian-filters-in-python" class="anchor" aria-hidden="true" href="#kalman-and-bayesian-filters-in-python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"&gt;Kalman and Bayesian Filters in Python&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Introductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Juptyer Notebook so that you can run and modify the code in your browser. What better way to learn?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"Kalman and Bayesian Filters in Python" looks amazing! ... your book is just what I needed&lt;/strong&gt; - Allen Downey, Professor and O'Reilly author.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.&lt;/strong&gt; - Sam Rodkey, SpaceX&lt;/p&gt;
&lt;p&gt;Start reading online now by clicking the binder or Azure badge below:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://notebooks.azure.com/import/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c33d8af3d101ffcd6ea73a8d02290b8d829ac52/68747470733a2f2f6e6f7465626f6f6b732e617a7572652e636f6d2f6c61756e63682e706e67" data-canonical-src="https://notebooks.azure.com/launch.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif"&gt;&lt;img src="https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif" alt="alt tag" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-are-kalman-and-bayesian-filters" class="anchor" aria-hidden="true" href="#what-are-kalman-and-bayesian-filters"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What are Kalman and Bayesian Filters?&lt;/h2&gt;
&lt;p&gt;Sensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.&lt;/p&gt;
&lt;p&gt;In simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, and the computer vision algorithms create very noisy and unreliable results.&lt;/p&gt;
&lt;p&gt;This book teaches you how to solve these sorts of filtering problems. I use many different algorithms, but they are all based on Bayesian probability. In simple terms Bayesian probability determines what is likely to be true based on past information.&lt;/p&gt;
&lt;p&gt;If I asked you the heading of my car at this moment you would have no idea. You'd proffer a number between 1∘∘ and 360∘∘ degrees, and have a 1 in 360 chance of being right. Now suppose I told you that 2 seconds ago its heading was 243∘∘. In 2 seconds my car could not turn very far so you could make a far more accurate prediction. You are using past information to more accurately infer information about the present or future.&lt;/p&gt;
&lt;p&gt;The world is also noisy. That prediction helps you make a better estimate, but it also subject to noise. I may have just braked for a dog or swerved around a pothole. Strong winds and ice on the road are external influences on the path of my car. In control literature we call this noise though you may not think of it that way.&lt;/p&gt;
&lt;p&gt;There is more to Bayesian probability, but you have the main idea. Knowledge is uncertain, and we alter our beliefs based on the strength of the evidence. Kalman and Bayesian filters blend our noisy and limited knowledge of how a system behaves with the noisy and limited sensor readings to produce the best possible estimate of the state of the system. Our principle is to never discard information.&lt;/p&gt;
&lt;p&gt;Say we are tracking an object and a sensor reports that it suddenly changed direction. Did it really turn, or is the data noisy? It depends. If this is a jet fighter we'd be very inclined to believe the report of a sudden maneuver. If it is a freight train on a straight track we would discount it. We'd further modify our belief depending on how accurate the sensor is. Our beliefs depend on the past and on our knowledge of the system we are tracking and on the characteristics of the sensors.&lt;/p&gt;
&lt;p&gt;The Kalman filter was invented by Rudolf Emil Kálmán to solve this sort of problem in a mathematically optimal way. Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains. There are Kalman filters in aircraft, on submarines, and on cruise missiles. Wall street uses them to track the market. They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments. Chemical plants use them to control and monitor reactions. They are used to perform medical imaging and to remove noise from cardiac signals. If it involves a sensor and/or time-series data, a Kalman filter or a close relative to the Kalman filter is usually involved.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-motivation" class="anchor" aria-hidden="true" href="#motivation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Motivation&lt;/h2&gt;
&lt;p&gt;The motivation for this book came out of my desire for a gentle introduction to Kalman filtering. I'm a software engineer that spent almost two decades in the avionics field, and so I have always been 'bumping elbows' with the Kalman filter, but never implemented one myself. As I moved into solving tracking problems with computer vision the need became urgent. There are classic textbooks in the field, such as Grewal and Andrew's excellent &lt;em&gt;Kalman Filtering&lt;/em&gt;. But sitting down and trying to read many of these books is a dismal experience if you do not have the required background. Typically the first few chapters fly through several years of undergraduate math, blithely referring you to textbooks on topics such as Itō calculus, and present an entire semester's worth of statistics in a few brief paragraphs. They are good texts for an upper undergraduate course, and an invaluable reference to researchers and professionals, but the going is truly difficult for the more casual reader. Symbology is introduced without explanation, different texts use different terms and variables for the same concept, and the books are almost devoid of examples or worked problems. I often found myself able to parse the words and comprehend the mathematics of a definition, but had no idea as to what real world phenomena they describe. "But what does that &lt;em&gt;mean?&lt;/em&gt;" was my repeated thought.&lt;/p&gt;
&lt;p&gt;However, as I began to finally understand the Kalman filter I realized the underlying concepts are quite straightforward. A few simple probability rules, some intuition about how we integrate disparate knowledge to explain events in our everyday life and the core concepts of the Kalman filter are accessible. Kalman filters have a reputation for difficulty, but shorn of much of the formal terminology the beauty of the subject and of their math became clear to me, and I fell in love with the topic.&lt;/p&gt;
&lt;p&gt;As I began to understand the math and theory more difficulties present themselves. A book or paper's author makes some statement of fact and presents a graph as proof.  Unfortunately, why the statement is true is not clear to me, nor is the method for making that plot obvious. Or maybe I wonder "is this true if R=0?"  Or the author provides pseudocode at such a high level that the implementation is not obvious. Some books offer Matlab code, but I do not have a license to that expensive package. Finally, many books end each chapter with many useful exercises. Exercises which you need to understand if you want to implement Kalman filters for yourself, but exercises with no answers. If you are using the book in a classroom, perhaps this is okay, but it is terrible for the independent reader. I loathe that an author withholds information from me, presumably to avoid 'cheating' by the student in the classroom.&lt;/p&gt;
&lt;p&gt;From my point of view none of this necessary. Certainly if you are designing a Kalman filter for a aircraft or missile you must thoroughly master of all of the mathematics and topics in a typical Kalman filter textbook. I just want to track an image on a screen, or write some code for an Arduino project. I want to know how the plots in the book are made, and chose different parameters than the author chose. I want to run simulations. I want to inject more noise in the signal and see how a filter performs. There are thousands of opportunities for using Kalman filters in everyday code, and yet this fairly straightforward topic is the provenance of rocket scientists and academics.&lt;/p&gt;
&lt;p&gt;I wrote this book to address all of those needs. This is not the book for you if you program navigation computers for Boeing or design radars for Raytheon. Go get an advanced degree at Georgia Tech, UW, or the like, because you'll need it. This book is for the hobbiest, the curious, and the working engineer that needs to filter or smooth data.&lt;/p&gt;
&lt;p&gt;This book is interactive. While you can read it online as static content, I urge you to use it as intended. It is written using Jupyter Notebook, which allows me to combine text, math, Python, and Python output in one place. Every plot, every piece of data in this book is generated from Python that is available to you right inside the notebook. Want to double the value of a parameter? Click on the Python cell, change the parameter's value, and click 'Run'. A new plot or printed output will appear in the book.&lt;/p&gt;
&lt;p&gt;This book has exercises, but it also has the answers. I trust you. If you just need an answer, go ahead and read the answer. If you want to internalize this knowledge, try to implement the exercise before you read the answer.&lt;/p&gt;
&lt;p&gt;This book has supporting libraries for computing statistics, plotting various things related to filters, and for the various filters that we cover. This does require a strong caveat; most of the code is written for didactic purposes. It is rare that I chose the most efficient solution (which often obscures the intent of the code), and in the first parts of the book I did not concern myself with numerical stability. This is important to understand - Kalman filters in aircraft are carefully designed and implemented to be numerically stable; the naive implementation is not stable in many cases. If you are serious about Kalman filters this book will not be the last book you need. My intention is to introduce you to the concepts and mathematics, and to get you to the point where the textbooks are approachable.&lt;/p&gt;
&lt;p&gt;Finally, this book is free. The cost for the books required to learn Kalman filtering is somewhat prohibitive even for a Silicon Valley engineer like myself; I cannot believe they are within the reach of someone in a depressed economy, or a financially struggling student. I have gained so much from free software like Python, and free books like those from Allen B. Downey &lt;a href="http://www.greenteapress.com/" rel="nofollow"&gt;here&lt;/a&gt;. It's time to repay that. So, the book is free, it is hosted on free servers, and it uses only free and open software such as IPython and mathjax to create the book.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reading-online" class="anchor" aria-hidden="true" href="#reading-online"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reading Online&lt;/h2&gt;
&lt;p&gt;The book is written as a collection of Jupyter Notebooks, an interactive, browser based system that allows you to combine text, Python, and math into your browser. There are multiple ways to read these online, listed below.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-binder" class="anchor" aria-hidden="true" href="#binder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;binder&lt;/h3&gt;
&lt;p&gt;binder serves interactive notebooks online, so you can run the code and change the code within your browser without downloading the book or installing Jupyter.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/70c5b4d050d4019f4f20b170d75679a9316ac5e5/687474703a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="http://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nbviewer" class="anchor" aria-hidden="true" href="#nbviewer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;nbviewer&lt;/h3&gt;
&lt;p&gt;The website &lt;a href="http://nbviewer.org" rel="nofollow"&gt;http://nbviewer.org&lt;/a&gt; provides an Jupyter Notebook server that renders notebooks stored at github (or elsewhere). The rendering is done in real time when you load the book. You may use &lt;a href="http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb" rel="nofollow"&gt;&lt;em&gt;this nbviewer link&lt;/em&gt;&lt;/a&gt; to access my book via nbviewer. If you read my book today, and then I make a change tomorrow, when you go back tomorrow you will see that change. Notebooks are rendered statically - you can read them, but not modify or run the code.&lt;/p&gt;
&lt;p&gt;nbviewer seems to lag the checked in version by a few days, so you might not be reading the most recent content.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-github" class="anchor" aria-hidden="true" href="#github"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GitHub&lt;/h3&gt;
&lt;p&gt;GitHub is able to render the notebooks directly. The quickest way to view a notebook is to just click on them above. However, it renders the math incorrectly, and I cannot recommend using it if you are doing more than just dipping into the book.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pdf-version" class="anchor" aria-hidden="true" href="#pdf-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PDF Version&lt;/h2&gt;
&lt;p&gt;A PDF version of the book is available &lt;a href="https://drive.google.com/open?id=0By_SW19c1BfhSVFzNHc0SjduNzg" rel="nofollow"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The PDF will usually lag behind what is in github as I don't update it for every minor check in.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-downloading-and-running-the-book" class="anchor" aria-hidden="true" href="#downloading-and-running-the-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Downloading and Running the Book&lt;/h2&gt;
&lt;p&gt;However, this book is intended to be interactive and I recommend using it in that form. It's a little more effort to set up, but worth it. If you install IPython and some supporting libraries on your computer and then clone this book you will be able to run all of the code in the book yourself. You can perform experiments, see how filters react to different data, see how different filters react to the same data, and so on. I find this sort of immediate feedback both vital and invigorating. You do not have to wonder "what happens if". Try it and see!&lt;/p&gt;
&lt;p&gt;The book and supporting software can be downloaded from GitHub by running this command on  the command line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python.git
pip install filterpy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instructions for installation of the IPython ecosystem can be found in the Installation appendix, found &lt;a href="http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/Appendix-A-Installation.ipynb" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once the software is installed you can navigate to the installation directory and run Juptyer notebook with the command line instruction&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open a browser window showing the contents of the base directory. The book is organized into chapters. To read Chapter 2, click on the link for chapter 2. This will cause the browser to open that subdirectory. In each subdirectory there will be one or more IPython Notebooks (all notebooks have a .ipynb file extension). The chapter contents are in the notebook with the same name as the chapter name. There are sometimes supporting notebooks for doing things like generating animations that are displayed in the chapter. These are not intended to be read by the end user, but of course if you are curious as to how an animation is made go ahead and take a look.&lt;/p&gt;
&lt;p&gt;This is admittedly a somewhat cumbersome interface to a book; I am following in the footsteps of several other projects that are somewhat repurposing Jupyter Notebook to generate entire books. I feel the slight annoyances have a huge payoff - instead of having to download a separate code base and run it in an IDE while you try to read a book, all of the code and text is in one place. If you want to alter the code, you may do so and immediately see the effects of your change. If you find a bug, you can make a fix, and push it back to my repository so that everyone in the world benefits. And, of course, you will never encounter a problem I face all the time with traditional books - the book and the code are out of sync with each other, and you are left scratching your head as to which source to trust.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-companion-software" class="anchor" aria-hidden="true" href="#companion-software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Companion Software&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://pypi.python.org/pypi/filterpy" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2514efa585c6a295744d54c87effb293c6ffee51/687474703a2f2f696d672e736869656c64732e696f2f707970692f762f66696c74657270792e737667" alt="Latest Version" data-canonical-src="http://img.shields.io/pypi/v/filterpy.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I wrote an open source Bayesian filtering Python library called &lt;strong&gt;FilterPy&lt;/strong&gt;. I have made the project available on PyPi, the Python Package Index.  To install from PyPi, at the command line issue the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install filterpy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not have pip, you may follow the instructions here: &lt;a href="https://pip.pypa.io/en/latest/installing.html" rel="nofollow"&gt;https://pip.pypa.io/en/latest/installing.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All of the filters used in this book as well as others not in this book are implemented in my Python library FilterPy, available &lt;a href="https://github.com/rlabbe/filterpy"&gt;here&lt;/a&gt;. You do not need to download or install this to read the book, but you will likely want to use this library to write your own filters. It includes Kalman filters, Fading Memory filters, H infinity filters, Extended and Unscented filters, least square filters, and many more.  It also includes helper routines that simplify the designing the matrices used by some of the filters, and other code such as Kalman based smoothers.&lt;/p&gt;
&lt;p&gt;FilterPy is hosted github at (&lt;a href="https://github.com/rlabbe/filterpy"&gt;https://github.com/rlabbe/filterpy&lt;/a&gt;).  If you want the bleading edge release you will want to grab a copy from github, and follow your Python installation's instructions for adding it to the Python search path. This might expose you to some instability since you might not get a tested release, but as a benefit you will also get all of the test scripts used to test the library. You can examine these scripts to see many examples of writing and running filters while not in the Jupyter Notebook environment.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-alternative-way-of-running-the-book-in-conda-environment" class="anchor" aria-hidden="true" href="#alternative-way-of-running-the-book-in-conda-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternative Way of Running the Book in Conda environment&lt;/h2&gt;
&lt;p&gt;If you have conda or miniconda installed, you can create environment by&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda env update -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source activate kf_bf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source deactivate kf_bf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to activate and deactivate the environment.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-issues-or-questions" class="anchor" aria-hidden="true" href="#issues-or-questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Issues or Questions&lt;/h2&gt;
&lt;p&gt;If you have comments, you can write an issue at GitHub so that everyone can read it along with my response. Please don't view it as a way to report bugs only. Alternatively I've created a gitter room for more informal discussion. &lt;a href="https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;&lt;img alt="Creative Commons License" src="https://camo.githubusercontent.com/005cfe27b7c4520ac0d6b607d6a7e33f5ad4eb6e/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792f342e302f38387833312e706e67" data-canonical-src="https://i.creativecommons.org/l/by/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;span&gt;Kalman and Bayesian Filters in Python&lt;/span&gt; by &lt;a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"&gt;Roger R. Labbe&lt;/a&gt; is licensed under a &lt;a href="http://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All software in this book, software that supports this book (such as in the the code directory) or used in the generation of the book (in the pdf directory) that is contained in this repository is licensed under the following MIT license:&lt;/p&gt;
&lt;p&gt;The MIT License (MIT)&lt;/p&gt;
&lt;p&gt;Copyright (c) 2015 Roger R. Labbe Jr&lt;/p&gt;
&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt;
&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt;
&lt;p&gt;THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.TION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;rlabbejr at gmail.com&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rlabbe</author><guid isPermaLink="false">https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python</guid><pubDate>Fri, 08 Nov 2019 00:18:00 GMT</pubDate></item><item><title>tensorflow/docs #19 in Jupyter Notebook, Today</title><link>https://github.com/tensorflow/docs</link><description>&lt;p&gt;&lt;i&gt;TensorFlow documentation&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-documentation" class="anchor" aria-hidden="true" href="#tensorflow-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Documentation&lt;/h1&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/37a83e8eca1db15cf70475cc6bdd4880b1f7b04d/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f686f72697a6f6e74616c2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/37a83e8eca1db15cf70475cc6bdd4880b1f7b04d/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f686f72697a6f6e74616c2e706e67" data-canonical-src="https://www.tensorflow.org/images/tf_logo_horizontal.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;
&lt;/div&gt;
&lt;p&gt;This is the TensorFlow documentation for &lt;a href="https://www.tensorflow.org" rel="nofollow"&gt;tensorflow.org&lt;/a&gt;.
To contribute, see &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; and the
&lt;a href="https://www.tensorflow.org/community/contribute/docs" rel="nofollow"&gt;docs contributor guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To file a docs issue, use the tracker in the
&lt;a href="https://github.com/tensorflow/tensorflow/issues/new?template=20-documentation-issue.md"&gt;tensorflow/tensorflow&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;And join the TensorFlow documentation contributors on the
&lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!forum/docs" rel="nofollow"&gt;docs@tensorflow.org mailing list&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>tensorflow</author><guid isPermaLink="false">https://github.com/tensorflow/docs</guid><pubDate>Fri, 08 Nov 2019 00:19:00 GMT</pubDate></item><item><title>NVIDIA-AI-IOT/jetbot #20 in Jupyter Notebook, Today</title><link>https://github.com/NVIDIA-AI-IOT/jetbot</link><description>&lt;p&gt;&lt;i&gt;An educational AI robot based on NVIDIA Jetson Nano.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-jetbot" class="anchor" aria-hidden="true" href="#jetbot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;JetBot&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Interested in making projects like this with Jetson Nano?  Check out the &lt;a href="https://info.nvidia.com/ai-for-makers-learn-with-jetbot-reg-page.html?nvid=nv-int-84114" rel="nofollow"&gt;webinar&lt;/a&gt; we released on 5/16/2019!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="../..//wiki/images/jetson-jetbot-illustration_1600x1260.png"&gt;&lt;img src="../..//wiki/images/jetson-jetbot-illustration_1600x1260.png" height="256" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;JetBot is an open-source robot based on NVIDIA Jetson Nano that is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Affordable&lt;/strong&gt; - Less than $150 add-on to Jetson Nano&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Educational&lt;/strong&gt; - Tutorials from basic motion to AI based collision avoidance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fun!&lt;/strong&gt; - Interactively programmed from your web browser&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Building and using JetBot gives the hands on experience needed to create entirely new AI projects.&lt;/p&gt;
&lt;p&gt;To get started, read the &lt;a href="https://github.com/NVIDIA-AI-IOT/jetbot/wiki"&gt;JetBot Wiki&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/igrigorik/ga-beacon"&gt;&lt;img src="https://camo.githubusercontent.com/48228270fe5e7df2a81064448b94b395450e67a2/68747470733a2f2f67612d626561636f6e2e61707073706f742e636f6d2f55412d3133353931393531302d312f6a6574626f742f524541444d453f706978656c" alt="Analytics" data-canonical-src="https://ga-beacon.appspot.com/UA-135919510-1/jetbot/README?pixel" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA-AI-IOT</author><guid isPermaLink="false">https://github.com/NVIDIA-AI-IOT/jetbot</guid><pubDate>Fri, 08 Nov 2019 00:20:00 GMT</pubDate></item><item><title>fivethirtyeight/data #21 in Jupyter Notebook, Today</title><link>https://github.com/fivethirtyeight/data</link><description>&lt;p&gt;&lt;i&gt;Data and code behind the articles and graphics at FiveThirtyEight&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;See &lt;a href="https://data.fivethirtyeight.com/" rel="nofollow"&gt;https://data.fivethirtyeight.com/&lt;/a&gt; for a list of the data and code we've published.&lt;/p&gt;
&lt;p&gt;Unless otherwise noted, our data sets are available under the &lt;a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;, and the code is available under the &lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;MIT License&lt;/a&gt;. If you find this information useful, please &lt;a href="mailto:data@fivethirtyeight.com"&gt;let us know&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fivethirtyeight</author><guid isPermaLink="false">https://github.com/fivethirtyeight/data</guid><pubDate>Fri, 08 Nov 2019 00:21:00 GMT</pubDate></item><item><title>yandexdataschool/nlp_course #22 in Jupyter Notebook, Today</title><link>https://github.com/yandexdataschool/nlp_course</link><description>&lt;p&gt;&lt;i&gt;YSDA course in Natural Language Processing&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ysda-natural-language-processing-course-" class="anchor" aria-hidden="true" href="#ysda-natural-language-processing-course-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;YSDA Natural Language Processing course &lt;a href="https://mybinder.org/v2/gh/yandexdataschool/nlp_course/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/483bae47a175c24dfbfc57390edd8b6982ac5fb3/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge_logo.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;This is the 2019 version. For previous year' course materials, go to &lt;a href="https://github.com/yandexdataschool/nlp_course/tree/master"&gt;this branch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lecture and seminar materials for each week are in ./week* folders&lt;/li&gt;
&lt;li&gt;YSDA homework deadlines will be listed in Anytask (&lt;a href="https://github.com/yandexdataschool/nlp_course/wiki/Homeworks-and-grading"&gt;read more&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Any technical issues, ideas, bugs in course materials, contribution ideas - add an &lt;a href="https://github.com/yandexdataschool/nlp_course/issues"&gt;issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Installing libraries and troubleshooting: &lt;a href="https://github.com/yandexdataschool/nlp_course/issues/1"&gt;this thread&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week01_embeddings"&gt;&lt;strong&gt;week01&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Embeddings&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Word embeddings. Distributional semantics, LSA, Word2Vec, GloVe. Why and when we need them.&lt;/li&gt;
&lt;li&gt;Seminar: Playing with word and sentence embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week02_classification"&gt;&lt;strong&gt;week02&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Text classification&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Text classification. Classical approaches for text representation: BOW, TF-IDF. Neural approaches: embeddings, convolutions, RNNs&lt;/li&gt;
&lt;li&gt;Seminar: Salary prediction with convolutional neural networks; explaining network predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week03_lm"&gt;&lt;strong&gt;week03&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Language models: N-gram and neural approaches; visualizing trained models&lt;/li&gt;
&lt;li&gt;Seminar: Generating ArXiv papers with language models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week04_seq2seq"&gt;&lt;strong&gt;week04&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Seq2seq/Attention&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Seq2seq: encoder-decoder framework. Attention: Bahdanau model. Self-attention, Transformer. Analysis of attention heads in Transformer.&lt;/li&gt;
&lt;li&gt;Seminar: Machine translation of hotel and hostel descriptions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week05_em"&gt;&lt;strong&gt;week05&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Expectation-Maximization&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Expectation-Maximization and Hidden Markov Models&lt;/li&gt;
&lt;li&gt;Seminar: Implementing expectation maximization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="./week06_mt"&gt;&lt;strong&gt;week06&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Machine Translation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lecture: Word Alignment Models, Noisy Channel, Machine Translation.&lt;/li&gt;
&lt;li&gt;Seminar: Introduction to word alignment assignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-contributors--course-staff" class="anchor" aria-hidden="true" href="#contributors--course-staff"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors &amp;amp; course staff&lt;/h1&gt;
&lt;p&gt;Course materials and teaching performed by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://lena-voita.github.io" rel="nofollow"&gt;Elena Voita&lt;/a&gt; - course admin, lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kovarsky"&gt;Boris Kovarsky&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/drt7"&gt;David Talbot&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/esgv"&gt;Sergey Gubanov&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/justheuristic"&gt;Just Heuristic&lt;/a&gt; - lectures, seminars, homeworks&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yandexdataschool</author><guid isPermaLink="false">https://github.com/yandexdataschool/nlp_course</guid><pubDate>Fri, 08 Nov 2019 00:22:00 GMT</pubDate></item><item><title>fastai/course-v3 #23 in Jupyter Notebook, Today</title><link>https://github.com/fastai/course-v3</link><description>&lt;p&gt;&lt;i&gt;The 3rd edition of course.fast.ai&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-course-v3" class="anchor" aria-hidden="true" href="#course-v3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;course-v3&lt;/h1&gt;
&lt;p&gt;The 3rd edition of &lt;a href="https://course.fast.ai" rel="nofollow"&gt;course.fast.ai&lt;/a&gt;. See the &lt;code&gt;nbs&lt;/code&gt; folder for the notebooks.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/course-v3</guid><pubDate>Fri, 08 Nov 2019 00:23:00 GMT</pubDate></item><item><title>dragen1860/TensorFlow-2.x-Tutorials #24 in Jupyter Notebook, Today</title><link>https://github.com/dragen1860/TensorFlow-2.x-Tutorials</link><description>&lt;p&gt;&lt;i&gt;TensorFlow 2.x version's  Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT examples, etc. TF 2.0版入门实例代码，实战教程。&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-20-tutorials" class="anchor" aria-hidden="true" href="#tensorflow-20-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 Tutorials&lt;/h1&gt;
&lt;p&gt;Our repo. is the &lt;strong&gt;Winner&lt;/strong&gt; of &lt;a href="https://devpost.com/software/tensorflow-2-0-tutorials" rel="nofollow"&gt;⚡#PoweredByTF 2.0 Challenge!&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="res/tensorflow-2.0.gif"&gt;&lt;img src="res/tensorflow-2.0.gif" width="250" align="middle" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;Timeline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Oct. 1, 2019: TensorFlow 2.0 Stable!&lt;/li&gt;
&lt;li&gt;Aug. 24, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow 2.0 rc0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jun. 8, 2019: &lt;a href="https://twitter.com/fchollet/status/1134583289384120320" rel="nofollow"&gt;TensorFlow 2.0 Beta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mar. 7, 2019: &lt;a href="https://www.tensorflow.org/alpha" rel="nofollow"&gt;Tensorflow 2.0 Alpha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jan. 11, 2019: &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf" rel="nofollow"&gt;TensorFlow r2.0 preview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Aug. 14, 2018: &lt;a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A" rel="nofollow"&gt;TensorFlow 2.0 is coming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;make sure you are using python 3.x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;GPU install&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install &lt;code&gt;CUDA 10.0&lt;/code&gt;(or after) and &lt;code&gt;cudnn&lt;/code&gt; by yourself. and set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; up.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;pip install tensorflow&lt;span class="pl-k"&gt;-&lt;/span&gt;gpu  &lt;span class="pl-k"&gt;-&lt;/span&gt;U&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Test installation:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;In [&lt;span class="pl-c1"&gt;2&lt;/span&gt;]: &lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow  &lt;span class="pl-k"&gt;as&lt;/span&gt; tf

In [&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: tf.&lt;span class="pl-c1"&gt;__version__&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;3&lt;/span&gt;]: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;2.0.0&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
In [&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: tf.test.is_gpu_available()
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
totalMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;95GiB&lt;/span&gt; freeMemory: &lt;span class="pl-c1"&gt;3.&lt;/span&gt;&lt;span class="pl-ii"&gt;00GiB&lt;/span&gt;
&lt;span class="pl-c1"&gt;...&lt;/span&gt;
Out[&lt;span class="pl-c1"&gt;4&lt;/span&gt;]: &lt;span class="pl-c1"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-配套tf2视频教程" class="anchor" aria-hidden="true" href="#配套tf2视频教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;配套TF2视频教程&lt;/h1&gt;
&lt;p align="center"&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/cover.png" width="400" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;
    &lt;img src="res/TF_QR_163.png" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/p&gt; 
&lt;p&gt;TensorFlow 2.0的视频教程链接：&lt;a href="https://study.163.com/course/courseMain.htm?share=2&amp;amp;shareId=480000001847407&amp;amp;courseId=1209092816&amp;amp;_trace_c_p_k2_=dca16f8fd11a4525bac8c89f779b2cfa" rel="nofollow"&gt;深度学习与TensorFlow 2实战&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-acknowledgement" class="anchor" aria-hidden="true" href="#acknowledgement"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgement&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;爱可可-爱生活 友情推荐 &lt;a target="_blank" rel="noopener noreferrer" href="res/weibo.jpg"&gt;&lt;img src="res/weibo.jpg" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-includes" class="anchor" aria-hidden="true" href="#includes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Includes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow 2.0 Overview&lt;/li&gt;
&lt;li&gt;TensorFlow 2.0 Basic Usage&lt;/li&gt;
&lt;li&gt;Linear Regression&lt;/li&gt;
&lt;li&gt;MNIST, FashionMNIST&lt;/li&gt;
&lt;li&gt;CIFAR10&lt;/li&gt;
&lt;li&gt;Fully Connected Layer&lt;/li&gt;
&lt;li&gt;VGG16&lt;/li&gt;
&lt;li&gt;Inception Network&lt;/li&gt;
&lt;li&gt;ResNet18&lt;/li&gt;
&lt;li&gt;Naive RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;ColorBot&lt;/li&gt;
&lt;li&gt;Auto-Encoders&lt;/li&gt;
&lt;li&gt;Variational Auto-Encoders&lt;/li&gt;
&lt;li&gt;DCGAN&lt;/li&gt;
&lt;li&gt;CycleGAN&lt;/li&gt;
&lt;li&gt;WGAN&lt;/li&gt;
&lt;li&gt;Pixel2Pixel&lt;/li&gt;
&lt;li&gt;Faster RCNN&lt;/li&gt;
&lt;li&gt;A2C&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;GCN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to submit a &lt;strong&gt;PR&lt;/strong&gt; request to make this repo. more complete!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-refered-repos" class="anchor" aria-hidden="true" href="#refered-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Refered Repos.&lt;/h1&gt;
&lt;p&gt;Our work is not built from scratch. Great appreciation to these open works！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/madalinabuzau/tensorflow-eager-tutorials"&gt;https://github.com/madalinabuzau/tensorflow-eager-tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/herbiebradley/CycleGAN-Tensorflow"&gt;https://github.com/herbiebradley/CycleGAN-Tensorflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/moono/tf-eager-on-GAN"&gt;https://github.com/moono/tf-eager-on-GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Viredery/tf-eager-fasterrcnn"&gt;https://github.com/Viredery/tf-eager-fasterrcnn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/github/gitignore/blob/master/Python.gitignore"&gt;https://github.com/github/gitignore/blob/master/Python.gitignore&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dragen1860</author><guid isPermaLink="false">https://github.com/dragen1860/TensorFlow-2.x-Tutorials</guid><pubDate>Fri, 08 Nov 2019 00:24:00 GMT</pubDate></item><item><title>Pierian-Data/Complete-Python-3-Bootcamp #25 in Jupyter Notebook, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Fri, 08 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>