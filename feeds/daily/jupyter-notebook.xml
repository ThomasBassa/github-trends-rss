<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Jupyter Notebook, Today</title><link>https://github.com/trending/jupyter-notebook?since=daily</link><description>The top repositories on GitHub for jupyter-notebook, measured daily</description><pubDate>Mon, 10 Feb 2020 01:18:41 GMT</pubDate><lastBuildDate>Mon, 10 Feb 2020 01:18:41 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>Pierian-Data/Complete-Python-3-Bootcamp #1 in Jupyter Notebook, Today</title><link>https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</link><description>&lt;p&gt;&lt;i&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-complete-python-3-bootcamp" class="anchor" aria-hidden="true" href="#complete-python-3-bootcamp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Complete-Python-3-Bootcamp&lt;/h1&gt;
&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;
&lt;p&gt;Get it now for 95% off with the link:
&lt;a href="https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB" rel="nofollow"&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Pierian-Data</author><guid isPermaLink="false">https://github.com/Pierian-Data/Complete-Python-3-Bootcamp</guid><pubDate>Mon, 10 Feb 2020 00:01:00 GMT</pubDate></item><item><title>pollen-robotics/reachy #2 in Jupyter Notebook, Today</title><link>https://github.com/pollen-robotics/reachy</link><description>&lt;p&gt;&lt;i&gt;Open source interactive robot to explore real-world applications!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-reachy-open-source-interactive-robot-to-explore-real-world-applications" class="anchor" aria-hidden="true" href="#reachy-open-source-interactive-robot-to-explore-real-world-applications"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reachy: Open source interactive robot to explore real-world applications!&lt;/h1&gt;
&lt;p&gt;Reachy is a unique open prototyping platform. It makes AI &amp;amp; robotics accessible to researchers, innovation professionals and creatives.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=iSL39WFxCLE" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5c68c26b0ed6d280f98e4a3924ec34a4f7dfacf4/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f69534c3339574678434c452f302e6a7067" alt="Hello I'm Reachy, presentation video" data-canonical-src="https://img.youtube.com/vi/iSL39WFxCLE/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this repository, you will find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the software to control and program your Reachy,&lt;/li&gt;
&lt;li&gt;the 3d models,&lt;/li&gt;
&lt;li&gt;documentation and examples to get you started.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The software is open-sourced and released under an &lt;a href="./software/LICENSE"&gt;Apache License v2.0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Visit &lt;a href="https://pollen-robotics.com" rel="nofollow"&gt;pollen-robotics.com&lt;/a&gt; to learn more or visit &lt;a href="https://forum.pollen-robotics.com" rel="nofollow"&gt;our forum&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Follow &lt;a href="https://twitter.com/pollenrobotics" rel="nofollow"&gt;@PollenRobotics&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt;
&lt;p&gt;This project adheres to the Contributor &lt;a href="CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href="mailto:contact@pollen-robotics.com"&gt;contact@pollen-robotics.com&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;WIP&lt;/strong&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pollen-robotics</author><guid isPermaLink="false">https://github.com/pollen-robotics/reachy</guid><pubDate>Mon, 10 Feb 2020 00:02:00 GMT</pubDate></item><item><title>aamini/introtodeeplearning #3 in Jupyter Notebook, Today</title><link>https://github.com/aamini/introtodeeplearning</link><description>&lt;p&gt;&lt;i&gt;Lab Materials for MIT 6.S191: Introduction to Deep Learning&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;&lt;img src="assets/banner.png" alt="banner" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This repository contains all of the code and software labs for &lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;MIT 6.S191: Introduction to Deep Learning&lt;/a&gt;! All lecture slides and videos are available on the course website.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-opening-the-labs-in-google-colaboratory" class="anchor" aria-hidden="true" href="#opening-the-labs-in-google-colaboratory"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Opening the labs in Google Colaboratory:&lt;/h2&gt;
&lt;p&gt;The 2020 6.S191 labs will be run in Google's Colaboratory, a Jupyter notebook environment that runs entirely in the cloud, you don't need to download anything. To run these labs, you must have a Google account.&lt;/p&gt;
&lt;p&gt;On this Github repo, navigate to the lab folder you want to run (&lt;code&gt;lab1&lt;/code&gt;, &lt;code&gt;lab2&lt;/code&gt;, &lt;code&gt;lab3&lt;/code&gt;) and open the appropriate python notebook (*.ipynb). Click the "Run in Colab" link on the top of the lab. That's it!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-the-labs" class="anchor" aria-hidden="true" href="#running-the-labs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running the labs&lt;/h2&gt;
&lt;p&gt;Now, to run the labs, open the Jupyter notebook on Colab. Navigate to the "Runtime" tab --&amp;gt; "Change runtime type". In the pop-up window, under "Runtime type" select "Python 3", and under "Hardware accelerator" select "GPU". Go through the notebooks and fill in the &lt;code&gt;#TODO&lt;/code&gt; cells to get the code to compile for yourself!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-mit-deep-learning-package" class="anchor" aria-hidden="true" href="#mit-deep-learning-package"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MIT Deep Learning package&lt;/h3&gt;
&lt;p&gt;You might notice that inside the labs we install the &lt;code&gt;mitdeeplearning&lt;/code&gt; python package from the Python Package repository:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install mitdeeplearning&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This package contains convienence functions that we use throughout the course and can be imported like any other Python package.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import mitdeeplearning as mdl&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We do this for you in each of the labs, but the package is also open source under the same license so you can also use it outside the class.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-lecture-videos" class="anchor" aria-hidden="true" href="#lecture-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lecture Videos&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=5v1JnYv_yWs&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=1" rel="nofollow"&gt;&lt;img src="assets/video_play.png" width="500" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All lecture videos are available publicly online and linked above! Use and/or modification of lecture slides outside of 6.S191 must reference:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;© MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;
&lt;p&gt;&lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;All code in this repository is copyright 2020 &lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;MIT 6.S191 Introduction to Deep Learning&lt;/a&gt;. All Rights Reserved.&lt;/p&gt;
&lt;p&gt;Licensed under the MIT License. You may not use this file except in compliance with the License. Use and/or modification of this code outside of 6.S191 must reference:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;© MIT 6.S191: Introduction to Deep Learning&lt;/p&gt;
&lt;p&gt;&lt;a href="http://introtodeeplearning.com" rel="nofollow"&gt;http://introtodeeplearning.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>aamini</author><guid isPermaLink="false">https://github.com/aamini/introtodeeplearning</guid><pubDate>Mon, 10 Feb 2020 00:03:00 GMT</pubDate></item><item><title>jantic/DeOldify #4 in Jupyter Notebook, Today</title><link>https://github.com/jantic/DeOldify</link><description>&lt;p&gt;&lt;i&gt;A Deep Learning based project for colorizing and restoring old images (and video!)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deoldify" class="anchor" aria-hidden="true" href="#deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeOldify&lt;/h1&gt;
&lt;p&gt;Image &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; |
Video &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt; For those of you who are looking for a quick and easy way to run the open source version of DeOldify, for free, try this!  I love this implementation: &lt;a href="https://deepai.org/machine-learning-model/colorizer" rel="nofollow"&gt;DeOldify Image Colorization on DeepAI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instructions on how to use the Colabs above have been kindly provided in video tutorial form by Old Ireland in Colour's John Breslin.  It's great! Click video image below to watch.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=VaEl0faDw38" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9d812131195cc524d5fe03696fdc284208bedbde/687474703a2f2f696d672e796f75747562652e636f6d2f76692f5661456c306661447733382f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/VaEl0faDw38/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Get more updates on &lt;a href="https://twitter.com/citnaj" rel="nofollow"&gt;Twitter &lt;img src="resource_images/Twitter_Social_Icon_Rounded_Square_Color.svg" width="16" style="max-width:100%;"&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#about-deoldify"&gt;About DeOldify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#example-videos"&gt;Example Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#example-images"&gt;Example Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stuff-that-should-probably-be-in-a-paper"&gt;Stuff That Should Probably Be In A Paper&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#how-to-achieve-stable-video"&gt;How to Achieve Stable Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-is-nogan"&gt;What is NoGAN?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#why-three-models"&gt;Why Three Models?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-technical-details"&gt;Technical Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#this-project-going-forward"&gt;Going Forward&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#getting-started-yourself"&gt;Getting Started Yourself&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#easiest-approach"&gt;Easiest Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#your-own-machine-not-as-easy"&gt;Your Own Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pretrained-weights"&gt;Pretrained Weights&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-about-deoldify" class="anchor" aria-hidden="true" href="#about-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About DeOldify&lt;/h2&gt;
&lt;p&gt;Simply put, the mission of this project is to colorize and restore old images and film footage.
We'll get into the details in a bit, but first let's see some pretty pictures and videos!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-new-and-exciting-stuff-in-deoldify" class="anchor" aria-hidden="true" href="#new-and-exciting-stuff-in-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New and Exciting Stuff in DeOldify&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Glitches and artifacts are almost entirely eliminated&lt;/li&gt;
&lt;li&gt;Better skin (less zombies)&lt;/li&gt;
&lt;li&gt;More highly detailed and photorealistic renders&lt;/li&gt;
&lt;li&gt;Much less "blue bias"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt; - it actually looks good!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NoGAN&lt;/strong&gt; - a new and weird but highly effective way to do GAN training for image to image.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-example-videos" class="anchor" aria-hidden="true" href="#example-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Videos&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;  Click images to watch&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facebook-f8-demo" class="anchor" aria-hidden="true" href="#facebook-f8-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facebook F8 Demo&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=l3UXXid04Ys" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/95e149f839667ddcd87e0a1970e3870f6a61c24a/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6c335558586964303459732f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/l3UXXid04Ys/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-silent-movie-examples" class="anchor" aria-hidden="true" href="#silent-movie-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Silent Movie Examples&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=EXn-n2iqEjI" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24d210457f7e8b57ef701788f013f2f72d2eda1c/687474703a2f2f696d672e796f75747562652e636f6d2f76692f45586e2d6e326971456a492f302e6a7067" alt="" data-canonical-src="http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-example-images" class="anchor" aria-hidden="true" href="#example-images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example Images&lt;/h2&gt;
&lt;p&gt;"Migrant Mother" by Dorothea Lange (1936)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/cf0b5cd16cd934cba884172370a78b40b28db00a/68747470733a2f2f692e696d6775722e636f6d2f427430766e6b652e6a7067" alt="Migrant Mother" data-canonical-src="https://i.imgur.com/Bt0vnke.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Woman relaxing in her livingroom in Sweden (1920)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/8ae04c8fc773e163705fd8ec24d3a9271806980c/68747470733a2f2f692e696d6775722e636f6d2f31353864306f552e6a7067" alt="Sweden Living Room" data-canonical-src="https://i.imgur.com/158d0oU.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Toffs and Toughs" by Jimmy Sime (1937)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/0e3d002bbc787b75359789f8ade0c43b637cded3/68747470733a2f2f692e696d6775722e636f6d2f565975617634492e6a7067" alt="Class Divide" data-canonical-src="https://i.imgur.com/VYuav4I.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanksgiving Maskers (1911)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/ba7b6ae2cc2e908346ba56f06ea54061b9b1ee6e/68747470733a2f2f692e696d6775722e636f6d2f6e3871564a35632e6a7067" alt="Thanksgiving Maskers" data-canonical-src="https://i.imgur.com/n8qVJ5c.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Glen Echo Madame Careta Gypsy Camp in Maryland (1925)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/83d69aafb3b306643f99566d08d805099c741e98/68747470733a2f2f692e696d6775722e636f6d2f316f59724a52492e6a7067" alt="Gypsy Camp" data-canonical-src="https://i.imgur.com/1oYrJRI.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Mr. and Mrs. Lemuel Smith and their younger children in their farm house, Carroll County, Georgia." (1941)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/f016893e9d37cab0175d218547699364d9c30f76/68747470733a2f2f692e696d6775722e636f6d2f49326a38796e6d2e6a7067" alt="Georgia Farmhouse" data-canonical-src="https://i.imgur.com/I2j8ynm.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Building the Golden Gate Bridge" (est 1937)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/3b1aca12e6009a5b8a47bcfbbc84cd533b22a1de/68747470733a2f2f692e696d6775722e636f6d2f365362466a66712e6a7067" alt="Golden Gate Bridge" data-canonical-src="https://i.imgur.com/6SbFjfq.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;  What you might be wondering is while this render looks cool, are the colors accurate? The original photo certainly makes it look like the towers of the bridge could be white. We looked into this and it turns out the answer is no - the towers were already covered in red primer by this time. So that's something to keep in mind- historical accuracy remains a huge challenge!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;"Terrasse de café, Paris" (1925)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/ae76951da1b7106193d81c44d7da2a0b74d60077/68747470733a2f2f692e696d6775722e636f6d2f577072517750352e6a7067" alt="Cafe Paris" data-canonical-src="https://i.imgur.com/WprQwP5.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Norwegian Bride (est late 1890s)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/03ab876e5b758529725e98bceea87f0e610106df/68747470733a2f2f692e696d6775722e636f6d2f4d6d7476725a6d2e6a7067" alt="Norwegian Bride" data-canonical-src="https://i.imgur.com/MmtvrZm.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zitkála-Šá (Lakota: Red Bird), also known as Gertrude Simmons Bonnin (1898)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/60080246c37e01c042194b2d87f4360a25637a7b/68747470733a2f2f692e696d6775722e636f6d2f7a49474d3034332e6a7067" alt="Native Woman" data-canonical-src="https://i.imgur.com/zIGM043.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chinese Opium Smokers (1880)&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/5a05086ca8215de683081c6fb29998045fee0ddf/68747470733a2f2f692e696d6775722e636f6d2f6c5647713856712e6a7067" alt="Opium Real" data-canonical-src="https://i.imgur.com/lVGq8Vq.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stuff-that-should-probably-be-in-a-paper" class="anchor" aria-hidden="true" href="#stuff-that-should-probably-be-in-a-paper"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stuff That Should Probably Be In A Paper&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-how-to-achieve-stable-video" class="anchor" aria-hidden="true" href="#how-to-achieve-stable-video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to Achieve Stable Video&lt;/h3&gt;
&lt;p&gt;NoGAN training is crucial to getting the kind of stable and colorful images seen in this iteration of DeOldify. NoGAN training combines the benefits of GAN training (wonderful colorization) while eliminating the nasty side effects (like flickering objects in video). Believe it or not, video is rendered using isolated image generation without any sort of temporal modeling tacked on. The process performs 30-60 minutes of the GAN portion of "NoGAN" training, using 1% to 3% of imagenet data once.  Then, as with still image colorization, we "DeOldify" individual frames before rebuilding the video.&lt;/p&gt;
&lt;p&gt;In addition to improved video stability, there is an interesting thing going on here worth mentioning. It turns out the models I run, even different ones and with different training structures, keep arriving at more or less the same solution.  That's even the case for the colorization of things you may think would be arbitrary and unknowable, like the color of clothing, cars, and even special effects (as seen in "Metropolis").&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/ea1738479cfd9811faa49b7dc78bb59606e74cfb/68747470733a2f2f7468756d62732e6766796361742e636f6d2f48656176794c6f6e65426c6f77666973682d73697a655f726573747269637465642e676966" alt="Metropolis Special FX" data-canonical-src="https://thumbs.gfycat.com/HeavyLoneBlowfish-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My best guess is that the models are learning some interesting rules about how to colorize based on subtle cues present in the black and white images that I certainly wouldn't expect to exist.  This result leads to nicely deterministic and consistent results, and that means you don't have track model colorization decisions because they're not arbitrary.  Additionally, they seem remarkably robust so that even in moving scenes the renders are very consistent.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/007128e9e871429b96bca83aae7f2dfa9f3d9ecc/68747470733a2f2f7468756d62732e6766796361742e636f6d2f46616d696c6961724a7562696c616e744173702d73697a655f726573747269637465642e676966" alt="Moving Scene Example" data-canonical-src="https://thumbs.gfycat.com/FamiliarJubilantAsp-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Other ways to stabilize video add up as well. First, generally speaking rendering at a higher resolution (higher render_factor) will increase stability of colorization decisions.  This stands to reason because the model has higher fidelity image information to work with and will have a greater chance of making the "right" decision consistently.  Closely related to this is the use of resnet101 instead of resnet34 as the backbone of the generator- objects are detected more consistently and correctly with this. This is especially important for getting good, consistent skin rendering.  It can be particularly visually jarring if you wind up with "zombie hands", for example.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/2b18ba56365c70078a0672e7aaa2b402e2a25eea/68747470733a2f2f7468756d62732e6766796361742e636f6d2f54687269667479496e666572696f7249736162656c6c696e6577686561746561722d73697a655f726573747269637465642e676966" alt="Zombie Hand Example" data-canonical-src="https://thumbs.gfycat.com/ThriftyInferiorIsabellinewheatear-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Additionally, gaussian noise augmentation during training appears to help but at this point the conclusions as to just how much are bit more tenuous (I just haven't formally measured this yet).  This is loosely based on work done in style transfer video, described here:  &lt;a href="https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42" rel="nofollow"&gt;https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Special thanks go to Rani Horev for his contributions in implementing this noise augmentation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-is-nogan" class="anchor" aria-hidden="true" href="#what-is-nogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is NoGAN?&lt;/h3&gt;
&lt;p&gt;This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model. It provides the benefits of GAN training while spending minimal time doing direct GAN training.  Instead, most of the training time is spent pretraining the generator and critic separately with more straight-forward, fast and reliable conventional methods.  A key insight here is that those more "conventional" methods generally get you most of the results you need, and that GANs can be used to close the gap on realism. During the very short amount of actual GAN training the generator not only gets the full realistic colorization capabilities that used to take days of progressively resized GAN training, but it also doesn't accrue nearly as much of the artifacts and other ugly baggage of GANs. In fact, you can pretty much eliminate glitches and artifacts almost entirely depending on your approach. As far as I know this is a new technique. And it's incredibly effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original DeOldify Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/5f92319233179b2f204b8739173abf98a69ef39a/68747470733a2f2f7468756d62732e6766796361742e636f6d2f436f6f7264696e6174656456656e657261746564486f676765742d73697a655f726573747269637465642e676966" alt="Before Flicker" data-canonical-src="https://thumbs.gfycat.com/CoordinatedVeneratedHogget-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NoGAN-Based DeOldify Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966"&gt;&lt;img src="https://camo.githubusercontent.com/410aabdcd548bde894635617caf09eaa678a7e80/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4f696c79426c61636b417263746963686172652d73697a655f726573747269637465642e676966" alt="After Flicker" data-canonical-src="https://thumbs.gfycat.com/OilyBlackArctichare-size_restricted.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The steps are as follows: First train the generator in a conventional way by itself with just the feature loss. Next, generate images from that, and train the critic on distinguishing between those outputs and real images as a basic binary classifier. Finally, train the generator and critic together in a GAN setting (starting right at the target size of 192px in this case).  Now for the weird part:  All the useful GAN training here only takes place within a very small window of time.  There's an inflection point where it appears the critic has transferred everything it can that is useful to the generator. Past this point, image quality oscillates between the best that you can get at the inflection point, or bad in a predictable way (orangish skin, overly red lips, etc).  There appears to be no productive training after the inflection point.  And this point lies within training on just 1% to 3% of the Imagenet Data!  That amounts to about 30-60 minutes of training at 192px.&lt;/p&gt;
&lt;p&gt;The hard part is finding this inflection point.  So far, I've accomplished this by making a whole bunch of model save checkpoints (every 0.1% of data iterated on) and then just looking for the point where images look great before they go totally bonkers with orange skin (always the first thing to go). Additionally, generator rendering starts immediately getting glitchy and inconsistent at this point, which is no good particularly for video. What I'd really like to figure out is what the tell-tale sign of the inflection point is that can be easily automated as an early stopping point.  Unfortunately, nothing definitive is jumping out at me yet.  For one, it's happening in the middle of training loss decreasing- not when it flattens out, which would seem more reasonable on the surface.&lt;/p&gt;
&lt;p&gt;Another key thing about NoGAN training is you can repeat pretraining the critic on generated images after the initial GAN training, then repeat the GAN training itself in the same fashion.  This is how I was able to get extra colorful results with the "artistic" model.  But this does come at a cost currently- the output of the generator becomes increasingly inconsistent and you have to experiment with render resolution (render_factor) to get the best result.  But the renders are still glitch free and way more consistent than I was ever able to achieve with the original DeOldify model. You can do about five of these repeat cycles, give or take, before you get diminishing returns, as far as I can tell.&lt;/p&gt;
&lt;p&gt;Keep in mind- I haven't been entirely rigorous in figuring out what all is going on in NoGAN- I'll save that for a paper. That means there's a good chance I'm wrong about something.  But I think it's definitely worth putting out there now because I'm finding it very useful- it's solving basically much of my remaining problems I had in DeOldify.&lt;/p&gt;
&lt;p&gt;This builds upon a technique developed in collaboration with Jeremy Howard and Sylvain Gugger for Fast.AI's Lesson 7 in version 3 of Practical Deep Learning for Coders Part I. The particular lesson notebook can be found here: &lt;a href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb"&gt;https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-why-three-models" class="anchor" aria-hidden="true" href="#why-three-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why Three Models?&lt;/h2&gt;
&lt;p&gt;There are now three models to choose from in DeOldify. Each of these has key strengths and weaknesses, and so have different use cases.  Video is for video of course.  But stable and artistic are both for images, and sometimes one will do images better than the other.&lt;/p&gt;
&lt;p&gt;More details:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Artistic&lt;/strong&gt; - This model achieves the highest quality results in image coloration, in terms of interesting details and vibrance. The most notable drawback however is that it's a bit of a pain to fiddle around with to get the best results (you have to adjust the rendering resolution or render_factor to achieve this).  Additionally, the model does not do as well as stable in a few key common scenarios- nature scenes and portraits.  The model uses a resnet34 backbone on a UNet with an emphasis on depth of layers on the decoder side.  This model was trained with 5 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 32% of Imagenet data trained once (12.5 hours of direct GAN training).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stable&lt;/strong&gt; - This model achieves the best results with landscapes and portraits. Notably, it produces less "zombies"- where faces or limbs stay gray rather than being colored in properly.  It generally has less weird miscolorations than artistic, but it's also less colorful in general.  This model uses a resnet101 backbone on a UNet with an emphasis on width of layers on the decoder side.  This model was trained with 3 critic pretrain/GAN cycle repeats via NoGAN, in addition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.  This adds up to a total of 7% of Imagenet data trained once (3 hours of direct GAN training).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt; - This model is optimized for smooth, consistent and flicker-free video.  This would definitely be the least colorful of the three models, but it's honestly not too far off from "stable". The model is the same as "stable" in terms of architecture, but differs in training.  It's trained for a mere 2.2% of Imagenet data once at 192px, using only the initial generator/critic pretrain/GAN NoGAN training (1 hour of direct GAN training).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because the training of the artistic and stable models was done before the "inflection point" of NoGAN training described in "What is NoGAN???" was discovered,  I believe this amount of training on them can be knocked down considerably. As far as I can tell, the models were stopped at "good points" that were well beyond where productive training was taking place.  I'll be looking into this in the future.&lt;/p&gt;
&lt;p&gt;Ideally, eventually these three models will be consolidated into one that has all these good desirable unified.  I think there's a path there, but it's going to require more work!  So for now, the most practical solution appears to be to maintain multiple models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-technical-details" class="anchor" aria-hidden="true" href="#the-technical-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Technical Details&lt;/h2&gt;
&lt;p&gt;This is a deep learning based model.  More specifically, what I've done is combined the following approaches:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-self-attention-generative-adversarial-network" class="anchor" aria-hidden="true" href="#self-attention-generative-adversarial-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://arxiv.org/abs/1805.08318" rel="nofollow"&gt;Self-Attention Generative Adversarial Network&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Except the generator is a &lt;strong&gt;pretrained U-Net&lt;/strong&gt;, and I've just modified it to have the spectral normalization and self-attention.  It's a pretty straightforward translation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-two-time-scale-update-rule" class="anchor" aria-hidden="true" href="#two-time-scale-update-rule"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://arxiv.org/abs/1706.08500" rel="nofollow"&gt;Two Time-Scale Update Rule&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This is also very straightforward – it's just one to one generator/critic iterations and higher critic learning rate.
This is modified to incorporate a "threshold" critic loss that makes sure that the critic is "caught up" before moving on to generator training.
This is particularly useful for the "NoGAN" method described below.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nogan" class="anchor" aria-hidden="true" href="#nogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NoGAN&lt;/h3&gt;
&lt;p&gt;There's no paper here! This is a new type of GAN training that I've developed to solve some key problems in the previous DeOldify model.
The gist is that you get the benefits of GAN training while spending minimal time doing direct GAN training.
More details are in the &lt;a href="#what-is-nogan"&gt;What is NoGAN?&lt;/a&gt; section (it's a doozy).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-generator-loss" class="anchor" aria-hidden="true" href="#generator-loss"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generator Loss&lt;/h3&gt;
&lt;p&gt;Loss during NoGAN learning is two parts:  One is a basic Perceptual Loss (or Feature Loss) based on VGG16 – this just biases the generator model to replicate the input image.
The second is the loss score from the critic.  For the curious – Perceptual Loss isn't sufficient by itself to produce good results.
It tends to just encourage a bunch of brown/green/blue – you know, cheating to the test, basically, which neural networks are really good at doing!
Key thing to realize here is that GANs essentially are learning the loss function for you – which is really one big step closer to toward the ideal that we're shooting for in machine learning.
And of course you generally get much better results when you get the machine to learn something you were previously hand coding.
That's certainly the case here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Of note:&lt;/strong&gt;  There's no longer any "Progressive Growing of GANs" type training going on here.  It's just not needed in lieu of the superior results obtained by the "NoGAN" technique described above.&lt;/p&gt;
&lt;p&gt;The beauty of this model is that it should be generally useful for all sorts of image modification, and it should do it quite well.
What you're seeing above are the results of the colorization model, but that's just one component in a pipeline that I'm developing with the exact same approach.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-this-project-going-forward" class="anchor" aria-hidden="true" href="#this-project-going-forward"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;This Project, Going Forward&lt;/h2&gt;
&lt;p&gt;So that's the gist of this project – I'm looking to make old photos and film look reeeeaaally good with GANs, and more importantly, make the project &lt;em&gt;useful&lt;/em&gt;.
In the meantime though this is going to be my baby and I'll be actively updating and improving the code over the foreseeable future.
I'll try to make this as user-friendly as possible, but I'm sure there's going to be hiccups along the way.&lt;/p&gt;
&lt;p&gt;Oh and I swear I'll document the code properly...eventually.  Admittedly I'm &lt;em&gt;one of those&lt;/em&gt; people who believes in "self documenting code" (LOL).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started-yourself" class="anchor" aria-hidden="true" href="#getting-started-yourself"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Started Yourself&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-easiest-approach" class="anchor" aria-hidden="true" href="#easiest-approach"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Easiest Approach&lt;/h3&gt;
&lt;p&gt;The easiest way to get started is to go straight to the Colab notebooks:&lt;/p&gt;
&lt;p&gt;Image &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
| Video &lt;a href="https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/52feade06f2fecbf006889a904d221e6a730c194/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" align="center" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Special thanks to Matt Robinson and María Benavente for their image Colab notebook contributions, and Robert Bell for the video Colab notebook work!&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-your-own-machine-not-as-easy" class="anchor" aria-hidden="true" href="#your-own-machine-not-as-easy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Your Own Machine (not as easy)&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-hardware-and-operating-system-requirements" class="anchor" aria-hidden="true" href="#hardware-and-operating-system-requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Hardware and Operating System Requirements&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(Training Only) BEEFY Graphics card&lt;/strong&gt;.  I'd really like to have more memory than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.  The Generators and Critic are ridiculously large.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Colorization Alone) A decent graphics card&lt;/strong&gt;. Approximately 4GB+ memory video cards should be sufficient.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linux (or maybe Windows 10)&lt;/strong&gt;  I'm using Ubuntu 16.04, but nothing about this precludes Windows 10 support as far as I know.  I just haven't tested it and am not going to make it a priority for now.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-easy-install" class="anchor" aria-hidden="true" href="#easy-install"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Easy Install&lt;/h4&gt;
&lt;p&gt;You should now be able to do a simple install with Anaconda. Here are the steps:&lt;/p&gt;
&lt;p&gt;Open the command line and navigate to the root folder you wish to install.  Then type the following commands&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;cd DeOldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;conda env create -f environment.yml&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then start running with these commands:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;source activate deoldify&lt;/span&gt;
&lt;span class="pl-c1"&gt;jupyter lab&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From there you can start running the notebooks in Jupyter Lab, via the url they provide you in the console.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can also now do "conda activate deoldify" if you have the latest version of conda and in fact that's now recommended. But a lot of people don't have that yet so I'm not going to make it the default instruction here yet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;&lt;a id="user-content-note-on-test_images-folder" class="anchor" aria-hidden="true" href="#note-on-test_images-folder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note on test_images Folder&lt;/h4&gt;
&lt;p&gt;The images in the &lt;code&gt;test_images&lt;/code&gt; folder have been removed because they were using Git LFS and that costs a lot of money when GitHub actually charges for bandwidth on a popular open source project (they had a billing bug for while that was recently fixed).  The notebooks that use them (the image test ones) still point to images in that directory that I (Jason) have personally and I'd like to keep it that way because, after all, I'm by far the primary and most active developer.  But they won't work for you.  Still, those notebooks are a convenient template for making your own tests if you're so inclined.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-typical-training" class="anchor" aria-hidden="true" href="#typical-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typical training&lt;/h4&gt;
&lt;p&gt;The notebook &lt;code&gt;ColorizeTrainingWandb&lt;/code&gt; has been created to log and monitor results through &lt;a href="https://www.wandb.com/" rel="nofollow"&gt;Weights &amp;amp; Biases&lt;/a&gt;. You can find a description of typical training by consulting &lt;a href="https://app.wandb.ai/borisd13/DeOldify/reports?view=borisd13%2FDeOldify" rel="nofollow"&gt;W&amp;amp;B Report&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-docker" class="anchor" aria-hidden="true" href="#docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-docker-for-jupyter" class="anchor" aria-hidden="true" href="#docker-for-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker for Jupyter&lt;/h3&gt;
&lt;p&gt;You can build and run the docker using the following process:&lt;/p&gt;
&lt;p&gt;Cloning&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd DeOldify &amp;amp;&amp;amp; docker build -t deoldify_jupyter -f Dockerfile .&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;echo "http://$(curl ifconfig.io):8888" &amp;amp;&amp;amp; nvidia-docker run --ipc=host --env NOTEBOOK_PASSWORD="pass123" -p 8888:8888 -it deoldify_jupyter&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-docker-for-api" class="anchor" aria-hidden="true" href="#docker-for-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Docker for API&lt;/h3&gt;
&lt;p&gt;You can build and run the docker using the following process:&lt;/p&gt;
&lt;p&gt;Cloning&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/jantic/DeOldify.git DeOldify&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;cd DeOldify &amp;amp;&amp;amp; docker build -t deoldify_api -f Dockerfile-api .&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running Docker&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;echo "http://$(curl ifconfig.io):5000" &amp;amp;&amp;amp; nvidia-docker run --ipc=host -p 5000:5000 -d deoldify_api&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Calling the API for image processing&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: image/png" -H "Content-Type: application/json" -d "{\"source_url\":\"http://www.afrikanheritage.com/wp-content/uploads/2015/08/slave-family-P.jpeg\", \"render_factor\":35}" --output colorized_image.png&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Calling the API for video processing&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;curl -X POST "http://MY_SUPER_API_IP:5000/process" -H "accept: application/octet-stream" -H "Content-Type: application/json" -d "{\"source_url\":\"https://v.redd.it/d1ku57kvuf421/HLSPlaylist.m3u8\", \"render_factor\":35}" --output colorized_video.mp4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you don't have Nvidia Docker, &lt;a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#installing-version-20"&gt;here&lt;/a&gt; is the installation guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;a id="user-content-installation-details" class="anchor" aria-hidden="true" href="#installation-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation Details&lt;/h3&gt;
&lt;p&gt;This project is built around the wonderful Fast.AI library.  Prereqs, in summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.AI 1.0.51&lt;/strong&gt; (and its dependencies).  If you use any higher version you'll see grid artifacts in rendering and tensorboard will malfunction. So yeah...don't do that.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 1.0.1&lt;/strong&gt; Not the latest version of PyTorch- that will not play nicely with the version of FastAI above.  Note however that the conda install of FastAI 1.0.51 grabs the latest PyTorch, which doesn't work.  This is patched over by our own conda install but fyi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; &lt;code&gt;conda install -c conda-forge jupyterlab&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensorboard&lt;/strong&gt; (i.e. install Tensorflow) and &lt;strong&gt;TensorboardX&lt;/strong&gt; (&lt;a href="https://github.com/lanpa/tensorboardX"&gt;https://github.com/lanpa/tensorboardX&lt;/a&gt;).  I guess you don't &lt;em&gt;have&lt;/em&gt; to but man, life is so much better with it.  FastAI now comes with built in support for this- you just  need to install the prereqs: &lt;code&gt;conda install -c anaconda tensorflow-gpu&lt;/code&gt; and &lt;code&gt;pip install tensorboardX&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ImageNet&lt;/strong&gt; – Only if you're training, of course. It has proven to be a great dataset for my purposes.  &lt;a href="http://www.image-net.org/download-images" rel="nofollow"&gt;http://www.image-net.org/download-images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pretrained-weights" class="anchor" aria-hidden="true" href="#pretrained-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained Weights&lt;/h2&gt;
&lt;p&gt;To start right away on your own machine with your own images or videos without training the models yourself, you'll need to download the "Completed Generator Weights" listed below and drop them in the /models/ folder.&lt;/p&gt;
&lt;p&gt;The colorization inference notebooks should be able to guide you from here. The notebooks to use are named ImageColorizerArtistic.ipynb, ImageColorizerStable.ipynb, and VideoColorizer.ipynb.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-completed-generator-weights" class="anchor" aria-hidden="true" href="#completed-generator-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Completed Generator Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/336vn9y4qwyg9yz/ColorizeVideo_gen.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-completed-critic-weights" class="anchor" aria-hidden="true" href="#completed-critic-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Completed Critic Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/8g5txfzt2fw8mf5/ColorizeArtistic_crit.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/7a8u20e7xdu1dtd/ColorizeStable_crit.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/0401djgo1dfxdzt/ColorizeVideo_crit.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pretrain-only-generator-weights" class="anchor" aria-hidden="true" href="#pretrain-only-generator-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrain Only Generator Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/9zexurvrve141n9/ColorizeArtistic_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/mdnuo1563bb8nh4/ColorizeStable_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/avzixh1ujf86e8x/ColorizeVideo_PretrainOnly_gen.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pretrain-only-critic-weights" class="anchor" aria-hidden="true" href="#pretrain-only-critic-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrain Only Critic Weights&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/lakxe8akzjgjnmh/ColorizeArtistic_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Artistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/b3wka56iyv1fvdc/ColorizeStable_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Stable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.dropbox.com/s/j7og84cbhpa94gs/ColorizeVideo_PretrainOnly_crit.pth?dl=0" rel="nofollow"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-want-the-old-deoldify" class="anchor" aria-hidden="true" href="#want-the-old-deoldify"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want the Old DeOldify?&lt;/h2&gt;
&lt;p&gt;We suspect some of you are going to want access to the original DeOldify model for various reasons.  We have that archived here:  &lt;a href="https://github.com/dana-kelley/DeOldify"&gt;https://github.com/dana-kelley/DeOldify&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-want-more" class="anchor" aria-hidden="true" href="#want-more"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Want More?&lt;/h2&gt;
&lt;p&gt;Follow &lt;a href="https://twitter.com/search?q=%23Deoldify" rel="nofollow"&gt;#DeOldify&lt;/a&gt; or &lt;a href="https://twitter.com/citnaj" rel="nofollow"&gt;Jason Antic&lt;/a&gt; on Twitter.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;All code in this repository is under the MIT license as specified by the LICENSE file.&lt;/p&gt;
&lt;p&gt;The model weights listed in this readme under the "Pretrained Weights" section are trained by ourselves and are released under the MIT license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jantic</author><guid isPermaLink="false">https://github.com/jantic/DeOldify</guid><pubDate>Mon, 10 Feb 2020 00:04:00 GMT</pubDate></item><item><title>realpython/materials #5 in Jupyter Notebook, Today</title><link>https://github.com/realpython/materials</link><description>&lt;p&gt;&lt;i&gt;Bonus materials, exercises, and example projects for our Python tutorials&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-python-materials" class="anchor" aria-hidden="true" href="#real-python-materials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real Python Materials&lt;/h1&gt;
&lt;p&gt;Bonus materials, exercises, and example projects for our &lt;a href="https://realpython.com" rel="nofollow"&gt;Python tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Build Status: &lt;a href="https://circleci.com/gh/realpython/materials" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/991534e54579264db6c49f9225646cb77dd8fd6a/68747470733a2f2f636972636c6563692e636f6d2f67682f7265616c707974686f6e2f6d6174657269616c732e7376673f7374796c653d737667" alt="CircleCI" data-canonical-src="https://circleci.com/gh/realpython/materials.svg?style=svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-code-style-checks" class="anchor" aria-hidden="true" href="#running-code-style-checks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Code Style Checks&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://flake8.pycqa.org/en/latest/" rel="nofollow"&gt;flake8&lt;/a&gt; and &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; to ensure a consistent code style for all of our sample code in this repository.&lt;/p&gt;
&lt;p&gt;Run the following commands to validate your code against the linters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ flake8
$ black --check &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-running-python-code-formatter" class="anchor" aria-hidden="true" href="#running-python-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running Python Code Formatter&lt;/h2&gt;
&lt;p&gt;We're using a tool called &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; on this repo to ensure consistent formatting. On CI it runs in "check" mode to ensure any new files added to the repo are following PEP 8. If you see linter warnings that say something like "would reformat some_file.py" it means black disagrees with your formatting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The easiest way to resolve these errors is to just run Black locally on the code and then committing those changes, as explained below.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To automatically re-format your code to be consistent with our code style guidelines, run &lt;a href="https://github.com/ambv/black"&gt;black&lt;/a&gt; in the repository root folder:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ black &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>realpython</author><guid isPermaLink="false">https://github.com/realpython/materials</guid><pubDate>Mon, 10 Feb 2020 00:05:00 GMT</pubDate></item><item><title>simoninithomas/Deep_reinforcement_learning_Course #6 in Jupyter Notebook, Today</title><link>https://github.com/simoninithomas/Deep_reinforcement_learning_Course</link><description>&lt;p&gt;&lt;i&gt;Implementations from the free course Deep Reinforcement Learning with Tensorflow&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-reinforcement-learning-course" class="anchor" aria-hidden="true" href="#deep-reinforcement-learning-course"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="nofollow"&gt;Deep Reinforcement Learning Course&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-️-im-currently-updating-the-implementations-january-and-february-some-delay-due-to-job-interviews-with-tensorflow-and-pytorch" class="anchor" aria-hidden="true" href="#️-im-currently-updating-the-implementations-january-and-february-some-delay-due-to-job-interviews-with-tensorflow-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png"&gt;⚠️&lt;/g-emoji&gt; I'm currently updating the implementations (January and February (some delay due to job interviews)) with Tensorflow and PyTorch.&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png"&gt;&lt;img src="https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png" alt="Deep Reinforcement Course with Tensorflow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;  Deep Reinforcement Learning Course is a free series of blog posts and videos &lt;g-emoji class="g-emoji" alias="new" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f195.png"&gt;🆕&lt;/g-emoji&gt; about Deep Reinforcement Learning, where we'll &lt;b&gt;learn the main algorithms, and how to implement them with Tensorflow.&lt;/b&gt;
&lt;/p&gt;&lt;p&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt;The articles explain the concept from the big picture to the mathematical details behind it.&lt;/p&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; The videos explain how to create the agent with Tensorflow &lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="nofollow"&gt;Syllabus&lt;/a&gt;&lt;br&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content--part-1-introduction-to-reinforcement-learning-article-" class="anchor" aria-hidden="true" href="#-part-1-introduction-to-reinforcement-learning-article-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; Part 1: Introduction to Reinforcement Learning &lt;a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419" rel="nofollow"&gt;ARTICLE&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-part-2-q-learning-with-frozenlake-" class="anchor" aria-hidden="true" href="#part-2-q-learning-with-frozenlake-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 2: Q-learning with FrozenLake &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article--frozenlake-implementation" class="anchor" aria-hidden="true" href="#-article--frozenlake-implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe" rel="nofollow"&gt;ARTICLE&lt;/a&gt; // &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb"&gt;FROZENLAKE IMPLEMENTATION&lt;/a&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--implementing-a-q-learning-agent-that-plays-taxi-v2--" class="anchor" aria-hidden="true" href="#-implementing-a-q-learning-agent-that-plays-taxi-v2--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/q2ZOEFAaaI0" rel="nofollow"&gt;Implementing a Q-learning agent that plays Taxi-v2 &lt;g-emoji class="g-emoji" alias="taxi" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f695.png"&gt;🚕&lt;/g-emoji&gt;&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-3-deep-q-learning-with-doom-" class="anchor" aria-hidden="true" href="#part-3-deep-q-learning-with-doom-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 3: Deep Q-learning with Doom &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article----doom-implementation" class="anchor" aria-hidden="true" href="#-article----doom-implementation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8" rel="nofollow"&gt;ARTICLE&lt;/a&gt;  //  &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb"&gt;DOOM IMPLEMENTATION&lt;/a&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-a-dqn-agent-that-learns-to-play-atari-space-invaders--" class="anchor" aria-hidden="true" href="#-create-a-dqn-agent-that-learns-to-play-atari-space-invaders--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/gCJyVX98KJ4" rel="nofollow"&gt;Create a DQN Agent that learns to play Atari Space Invaders &lt;g-emoji class="g-emoji" alias="space_invader" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f47e.png"&gt;👾&lt;/g-emoji&gt;&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-4-policy-gradients-with-doom-" class="anchor" aria-hidden="true" href="#part-4-policy-gradients-with-doom-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 4: Policy Gradients with Doom &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article---cartpole-implementation--doom-implementation-" class="anchor" aria-hidden="true" href="#-article---cartpole-implementation--doom-implementation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f" rel="nofollow"&gt;ARTICLE&lt;/a&gt; //  &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb"&gt;CARTPOLE IMPLEMENTATION&lt;/a&gt; // &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Doom/Doom%20REINFORCE%20Monte%20Carlo%20Policy%20gradients.ipynb"&gt;DOOM IMPLEMENTATION&lt;/a&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-doom-deathmatch-" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-doom-deathmatch-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://www.youtube.com/watch?v=wLTQRuizVyE" rel="nofollow"&gt;Create an Agent that learns to play Doom deathmatch&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-3-improvments-in-deep-q-learning-" class="anchor" aria-hidden="true" href="#part-3-improvments-in-deep-q-learning-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 3+: Improvments in Deep Q-Learning &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article--doom-deadly-corridor-implementation-" class="anchor" aria-hidden="true" href="#-article--doom-deadly-corridor-implementation-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682" rel="nofollow"&gt;ARTICLE&lt;/a&gt;//  &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20(%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay).ipynb"&gt;Doom Deadly corridor IMPLEMENTATION&lt;/a&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-doom-deadly-corridor-" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-doom-deadly-corridor-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/-Ynjw0Vl3i4" rel="nofollow"&gt;Create an Agent that learns to play Doom Deadly corridor&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-5-advantage-advantage-actor-critic-a2c-" class="anchor" aria-hidden="true" href="#part-5-advantage-advantage-actor-critic-a2c-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 5: Advantage Advantage Actor Critic (A2C) &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article-" class="anchor" aria-hidden="true" href="#-article-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d" rel="nofollow"&gt;ARTICLE&lt;/a&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-sonic-" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-sonic-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="video_camera" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4f9.png"&gt;📹&lt;/g-emoji&gt; &lt;a href="https://youtu.be/GCfUdkCL7FQ" rel="nofollow"&gt;Create an Agent that learns to play Sonic&lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-6-proximal-policy-gradients-" class="anchor" aria-hidden="true" href="#part-6-proximal-policy-gradients-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 6: Proximal Policy Gradients &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article" class="anchor" aria-hidden="true" href="#-article"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e" rel="nofollow"&gt;ARTICLE&lt;/a&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content--create-an-agent-that-learns-to-play-sonic-the-hedgehog-2-and-3--" class="anchor" aria-hidden="true" href="#-create-an-agent-that-learns-to-play-sonic-the-hedgehog-2-and-3--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="man_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png"&gt;👨‍💻&lt;/g-emoji&gt; &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/PPO%20with%20Sonic%20the%20Hedgehog"&gt;Create an Agent that learns to play Sonic the Hedgehog 2 and 3 &lt;/a&gt; &lt;br&gt;&lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-7-curiosity-driven-learning-made-easy-part-i--" class="anchor" aria-hidden="true" href="#part-7-curiosity-driven-learning-made-easy-part-i--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 7: Curiosity Driven Learning made easy Part I &lt;br&gt; &lt;br&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content--article--" class="anchor" aria-hidden="true" href="#-article--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="scroll" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png"&gt;📜&lt;/g-emoji&gt; &lt;a href="https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359" rel="nofollow"&gt;ARTICLE&lt;/a&gt; &lt;br&gt; &lt;br&gt;&lt;/h3&gt;
&lt;h2&gt;&lt;a id="user-content-part-8-random-network-distillation-with-pytorch--" class="anchor" aria-hidden="true" href="#part-8-random-network-distillation-with-pytorch--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Part 8: Random Network Distillation with PyTorch  &lt;br&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content--a-trained-rnd-agent-that-learned-to-play-montezumas-revenge-21-hours-of-training-with-a-tesla-k80---" class="anchor" aria-hidden="true" href="#-a-trained-rnd-agent-that-learned-to-play-montezumas-revenge-21-hours-of-training-with-a-tesla-k80---"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="man_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png"&gt;👨‍💻&lt;/g-emoji&gt; &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/RND%20Montezuma's%20revenge%20PyTorch"&gt;A trained RND agent that learned to play Montezuma's revenge (21 hours of training with a Tesla K80&lt;/a&gt;  &lt;br&gt; &lt;br&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-any-questions-" class="anchor" aria-hidden="true" href="#any-questions-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Any questions &lt;g-emoji class="g-emoji" alias="man_technologist" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png"&gt;👨‍💻&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt; If you have any questions, feel free to ask me: &lt;/p&gt;
&lt;p&gt; &lt;g-emoji class="g-emoji" alias="e-mail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e7.png"&gt;📧&lt;/g-emoji&gt;: &lt;a href="mailto:hello@simoninithomas.com"&gt;hello@simoninithomas.com&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt; Github: &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course"&gt;https://github.com/simoninithomas/Deep_reinforcement_learning_Course&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;g-emoji class="g-emoji" alias="globe_with_meridians" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png"&gt;🌐&lt;/g-emoji&gt; : &lt;a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" rel="nofollow"&gt;https://simoninithomas.github.io/Deep_reinforcement_learning_Course/&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; Twitter: &lt;a href="https://twitter.com/ThomasSimonini" rel="nofollow"&gt;@ThomasSimonini&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; Don't forget to &lt;b&gt; follow me on &lt;a href="https://twitter.com/ThomasSimonini" rel="nofollow"&gt;twitter&lt;/a&gt;, &lt;a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course"&gt;github&lt;/a&gt; and &lt;a href="https://medium.com/@thomassimonini" rel="nofollow"&gt;Medium&lt;/a&gt; to be alerted of the new articles that I publish &lt;/b&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-help--" class="anchor" aria-hidden="true" href="#how-to-help--"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to help  &lt;g-emoji class="g-emoji" alias="raised_hands" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64c.png"&gt;🙌&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt;3 ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Clap our articles and like our videos a lot&lt;/strong&gt;:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Share and speak about our articles and videos&lt;/strong&gt;: By sharing our articles and videos you help us to spread the word.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improve our notebooks&lt;/strong&gt;: if you found a bug or &lt;strong&gt;a better implementation&lt;/strong&gt; you can send a pull request.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>simoninithomas</author><guid isPermaLink="false">https://github.com/simoninithomas/Deep_reinforcement_learning_Course</guid><pubDate>Mon, 10 Feb 2020 00:06:00 GMT</pubDate></item><item><title>MLEveryday/100-Days-Of-ML-Code #7 in Jupyter Notebook, Today</title><link>https://github.com/MLEveryday/100-Days-Of-ML-Code</link><description>&lt;p&gt;&lt;i&gt;100-Days-Of-ML-Code中文版&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-机器学习100天" class="anchor" aria-hidden="true" href="#机器学习100天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;机器学习100天&lt;/h1&gt;
&lt;p&gt;英文原版请移步&lt;a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code"&gt;Avik-Jain&lt;/a&gt;。数据在&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/tree/master/datasets"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;翻译前请先阅读&lt;a href="Translation%20specification.MD"&gt;规范&lt;/a&gt;。常见问题解答见&lt;a href="FAQ.MD"&gt;FAQ&lt;/a&gt;。&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;有监督学习
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86--%E7%AC%AC1%E5%A4%A9"&gt;数据预处理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E7%AE%80%E5%8D%95%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92--%E7%AC%AC2%E5%A4%A9"&gt;简单线性回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92--%E7%AC%AC3%E5%A4%A9"&gt;多元线性回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92--%E7%AC%AC4%E5%A4%A9"&gt;逻辑回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#k%E8%BF%91%E9%82%BB%E6%B3%95k-nn--%E7%AC%AC7%E5%A4%A9"&gt;k近邻法(k-NN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsvm--%E7%AC%AC12%E5%A4%A9"&gt;支持向量机(SVM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%86%B3%E7%AD%96%E6%A0%91--%E7%AC%AC23%E5%A4%A9"&gt;决策树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97--%E7%AC%AC33%E5%A4%A9"&gt;随机森林&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;无监督学习
&lt;ul&gt;
&lt;li&gt;&lt;a href="#k-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB--%E7%AC%AC43%E5%A4%A9"&gt;K-均值聚类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB--%E7%AC%AC54%E5%A4%A9"&gt;层次聚类&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-数据预处理--第1天" class="anchor" aria-hidden="true" href="#数据预处理--第1天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数据预处理 | 第1天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%201_Data_Preprocessing.md"&gt;数据预处理实现&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%201.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简单线性回归--第2天" class="anchor" aria-hidden="true" href="#简单线性回归--第2天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简单线性回归 | 第2天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%202_Simple_Linear_Regression.md"&gt;简单线性回归实现&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%202.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-多元线性回归--第3天" class="anchor" aria-hidden="true" href="#多元线性回归--第3天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;多元线性回归 | 第3天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%203_Multiple_Linear_Regression.md"&gt;多元线性回归实现&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%203.png"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%203.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-逻辑回归--第4天" class="anchor" aria-hidden="true" href="#逻辑回归--第4天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;逻辑回归 | 第4天&lt;/h2&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%204.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%204.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-逻辑回归--第5天" class="anchor" aria-hidden="true" href="#逻辑回归--第5天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;逻辑回归 | 第5天&lt;/h2&gt;
&lt;p&gt;今天我深入研究了逻辑回归到底是什么，以及它背后的数学是什么。学习了如何计算代价函数，以及如何使用梯度下降法来将代价函数降低到最小。&lt;br&gt;
由于时间关系，我将隔天发布信息图。如果有人在机器学习领域有一定经验，并愿意帮我编写代码文档，也了解github的Markdown语法，请在领英联系我。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-逻辑回归--第6天" class="anchor" aria-hidden="true" href="#逻辑回归--第6天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;逻辑回归 | 第6天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%206_Logistic_Regression.md"&gt;逻辑回归实现&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k近邻法k-nn--第7天" class="anchor" aria-hidden="true" href="#k近邻法k-nn--第7天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;K近邻法(k-NN) | 第7天&lt;/h2&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%207.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%207.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-逻辑回归背后的数学--第8天" class="anchor" aria-hidden="true" href="#逻辑回归背后的数学--第8天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;逻辑回归背后的数学 | 第8天&lt;/h2&gt;
&lt;p&gt;为了使我对逻辑回归的见解更加清晰，我在网上搜索了一些资源或文章，然后我就发现了Saishruthi Swaminathan的&lt;a href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc" rel="nofollow"&gt;这篇文章&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;它给出了逻辑回归的详细描述。请务必看一看。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-支持向量机svm--第9天" class="anchor" aria-hidden="true" href="#支持向量机svm--第9天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;支持向量机(SVM) | 第9天&lt;/h2&gt;
&lt;p&gt;直观了解SVM是什么以及如何使用它来解决分类问题。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-支持向量机和k近邻法--第10天" class="anchor" aria-hidden="true" href="#支持向量机和k近邻法--第10天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;支持向量机和K近邻法 | 第10天&lt;/h2&gt;
&lt;p&gt;了解更多关于SVM如何工作和实现knn算法的知识。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k近邻法k-nn--第11天" class="anchor" aria-hidden="true" href="#k近邻法k-nn--第11天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;K近邻法(k-NN) | 第11天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2011_K-NN.md"&gt;K近邻法(k-NN)实现&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-支持向量机svm--第12天" class="anchor" aria-hidden="true" href="#支持向量机svm--第12天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;支持向量机(SVM) | 第12天&lt;/h2&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2012.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%2012.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-支持向量机svm--第13天" class="anchor" aria-hidden="true" href="#支持向量机svm--第13天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;支持向量机(SVM) | 第13天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.md"&gt;SVM实现&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-支持向量机svm的实现--第14天" class="anchor" aria-hidden="true" href="#支持向量机svm的实现--第14天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;支持向量机(SVM)的实现 | 第14天&lt;/h2&gt;
&lt;p&gt;今天我在线性相关数据上实现了SVM。使用Scikit-Learn库。在scikit-learn中我们有SVC分类器，我们用它来完成这个任务。将在下一次实现时使用kernel-trick。Python代码见&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.py"&gt;此处&lt;/a&gt;,Jupyter notebook见&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.ipynb"&gt;此处&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-朴素贝叶斯分类器naive-bayes-classifier和黑盒机器学习black-box-machine-learning--第15天" class="anchor" aria-hidden="true" href="#朴素贝叶斯分类器naive-bayes-classifier和黑盒机器学习black-box-machine-learning--第15天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;朴素贝叶斯分类器(Naive Bayes Classifier)和黑盒机器学习(Black Box Machine Learning) | 第15天&lt;/h2&gt;
&lt;p&gt;学习不同类型的朴素贝叶斯分类器同时开始&lt;a href="https://bloomberg.github.io/foml/#home" rel="nofollow"&gt;Bloomberg&lt;/a&gt;的课程。课程列表中的第一个是黑盒机器学习。它给出了预测函数，特征提取，学习算法，性能评估，交叉验证，样本偏差，非平稳性，过度拟合和超参数调整的整体观点。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-通过内核技巧实现支持向量机--第16天" class="anchor" aria-hidden="true" href="#通过内核技巧实现支持向量机--第16天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;通过内核技巧实现支持向量机 | 第16天&lt;/h2&gt;
&lt;p&gt;使用Scikit-Learn库实现了SVM算法以及内核函数，该函数将我们的数据点映射到更高维度以找到最佳超平面。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-在coursera开始深度学习的专业课程--第17天" class="anchor" aria-hidden="true" href="#在coursera开始深度学习的专业课程--第17天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;在Coursera开始深度学习的专业课程 | 第17天&lt;/h2&gt;
&lt;p&gt;在1天内完成第1周和第2周内容以及学习课程中的逻辑回归神经网络。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-继续coursera上的深度学习专业课程--第18天" class="anchor" aria-hidden="true" href="#继续coursera上的深度学习专业课程--第18天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;继续Coursera上的深度学习专业课程 | 第18天&lt;/h2&gt;
&lt;p&gt;完成课程1。用Python自己实现一个神经网络。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-学习问题和yaser-abu-mostafa教授--第19天" class="anchor" aria-hidden="true" href="#学习问题和yaser-abu-mostafa教授--第19天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;学习问题和Yaser Abu-Mostafa教授 | 第19天&lt;/h2&gt;
&lt;p&gt;开始Yaser Abu-Mostafa教授的Caltech机器学习课程-CS156中的课程1。这基本上是对即将到来的课程的一种介绍。他也介绍了感知算法。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深度学习专业课程2--第20天" class="anchor" aria-hidden="true" href="#深度学习专业课程2--第20天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深度学习专业课程2 | 第20天&lt;/h2&gt;
&lt;p&gt;完成改进深度神经网络第1周内容：参数调整，正则化和优化。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-网页搜罗--第21天" class="anchor" aria-hidden="true" href="#网页搜罗--第21天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;网页搜罗 | 第21天&lt;/h2&gt;
&lt;p&gt;观看了一些关于如何使用Beautiful Soup进行网络爬虫的教程，以便收集用于构建模型的数据。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-学习还可行吗--第22天" class="anchor" aria-hidden="true" href="#学习还可行吗--第22天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;学习还可行吗? | 第22天&lt;/h2&gt;
&lt;p&gt;完成Yaser Abu-Mostafa教授的Caltech机器学习课程-CS156中的课程2。学习Hoeffding不等式。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-决策树--第23天" class="anchor" aria-hidden="true" href="#决策树--第23天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;决策树 | 第23天&lt;/h2&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2023%20-%20Chinese.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%2023%20-%20Chinese.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-统计学习理论的介绍--第24天" class="anchor" aria-hidden="true" href="#统计学习理论的介绍--第24天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;统计学习理论的介绍 | 第24天&lt;/h2&gt;
&lt;p&gt;Bloomberg ML课程的第3课介绍了一些核心概念，如输入空间，动作空间，结果空间，预测函数，损失函数和假设空间。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-决策树--第25天" class="anchor" aria-hidden="true" href="#决策树--第25天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;决策树 | 第25天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2025_Decision_Tree.md"&gt;决策树实现&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-跳到复习线性代数--第26天" class="anchor" aria-hidden="true" href="#跳到复习线性代数--第26天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;跳到复习线性代数 | 第26天&lt;/h2&gt;
&lt;p&gt;发现YouTube一个神奇的频道&lt;a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" rel="nofollow"&gt;3Blue1Brown&lt;/a&gt;，它有一个播放列表《线性代数的本质》。看完了4个视频，包括了向量，线性组合，跨度，基向量，线性变换和矩阵乘法。&lt;/p&gt;
&lt;p&gt;B站播放列表在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=9450" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-跳到复习线性代数--第27天" class="anchor" aria-hidden="true" href="#跳到复习线性代数--第27天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;跳到复习线性代数 | 第27天&lt;/h2&gt;
&lt;p&gt;继续观看了4个视频，内容包括三维变换、行列式、逆矩阵、列空间、零空间和非方矩阵。&lt;/p&gt;
&lt;p&gt;B站播放列表在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=9450" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-跳到复习线性代数--第28天" class="anchor" aria-hidden="true" href="#跳到复习线性代数--第28天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;跳到复习线性代数 | 第28天&lt;/h2&gt;
&lt;p&gt;继续观看了3个视频，内容包括点积和叉积。&lt;/p&gt;
&lt;p&gt;B站播放列表在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=9450" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-跳到复习线性代数--第29天" class="anchor" aria-hidden="true" href="#跳到复习线性代数--第29天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;跳到复习线性代数 | 第29天&lt;/h2&gt;
&lt;p&gt;观看了剩余的视频12到14，内容包括特征向量和特征值，以及抽象向量空间。&lt;/p&gt;
&lt;p&gt;B站播放列表在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=9450" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-微积分的本质--第30天" class="anchor" aria-hidden="true" href="#微积分的本质--第30天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;微积分的本质 | 第30天&lt;/h2&gt;
&lt;p&gt;完成上一播放列表后，YouTube推荐了新内容《微积分的本质》，今天看完了其中的3个视频，包括导数、链式法则、乘积法则和指数导数。&lt;/p&gt;
&lt;p&gt;B站播放列表在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=13407" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-微积分的本质--第31天" class="anchor" aria-hidden="true" href="#微积分的本质--第31天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;微积分的本质 | 第31天&lt;/h2&gt;
&lt;p&gt;观看了2个视频，内容包括隐分化与极限。&lt;/p&gt;
&lt;p&gt;B站播放列表在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=13407" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-微积分的本质--第32天" class="anchor" aria-hidden="true" href="#微积分的本质--第32天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;微积分的本质 | 第32天&lt;/h2&gt;
&lt;p&gt;观看了剩余的4个视频，内容包括积分与高阶导数。&lt;/p&gt;
&lt;p&gt;B站播放列表在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=13407" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-随机森林--第33天" class="anchor" aria-hidden="true" href="#随机森林--第33天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;随机森林 | 第33天&lt;/h2&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2033.png"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%2033.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-随机森林--第34天" class="anchor" aria-hidden="true" href="#随机森林--第34天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;随机森林 | 第34天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2034_Random_Forests.md"&gt;随机森林实现&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-什么是神经网络--深度学习第1章--第-35天" class="anchor" aria-hidden="true" href="#什么是神经网络--深度学习第1章--第-35天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;什么是神经网络？ | 深度学习，第1章 | 第 35天&lt;/h2&gt;
&lt;p&gt;Youtube频道3Blue1Brown中有精彩的视频介绍神经网络。这个视频提供了很好的解释，并使用手写数字数据集演示基本概念。&lt;/p&gt;
&lt;p&gt;B站视频在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=26587" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-梯度下降法神经网络如何学习--深度学习第2章--第36天" class="anchor" aria-hidden="true" href="#梯度下降法神经网络如何学习--深度学习第2章--第36天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;梯度下降法，神经网络如何学习 | 深度学习，第2章 | 第36天&lt;/h2&gt;
&lt;p&gt;Youtube频道3Blue1Brown关于神经网络的第2部分，这个视频用有趣的方式解释了梯度下降法。推荐必须观看169.&lt;/p&gt;
&lt;p&gt;B站视频在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=26587" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-反向传播法究竟做什么--深度学习第3章--第37天" class="anchor" aria-hidden="true" href="#反向传播法究竟做什么--深度学习第3章--第37天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;反向传播法究竟做什么？ | 深度学习，第3章 | 第37天&lt;/h2&gt;
&lt;p&gt;Youtube频道3Blue1Brown关于神经网络的第3部分，这个视频主要介绍了偏导数和反向传播法。&lt;/p&gt;
&lt;p&gt;B站视频在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=26587" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-反向传播法演算--深度学习第4章--第38天" class="anchor" aria-hidden="true" href="#反向传播法演算--深度学习第4章--第38天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;反向传播法演算 | 深度学习，第4章 | 第38天&lt;/h2&gt;
&lt;p&gt;Youtube频道3Blue1Brown关于神经网络的第3部分，这个视频主要介绍了偏导数和反向传播法。&lt;/p&gt;
&lt;p&gt;B站视频在&lt;a href="https://space.bilibili.com/88461692/#/channel/detail?cid=26587" rel="nofollow"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-第1部分--深度学习基础pythontensorflow和keras--第39天" class="anchor" aria-hidden="true" href="#第1部分--深度学习基础pythontensorflow和keras--第39天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第1部分 | 深度学习基础Python，TensorFlow和Keras | 第39天&lt;/h2&gt;
&lt;p&gt;视频地址在&lt;a href="https://www.youtube.com/watch?v=wQ8BIBpya2k&amp;amp;t=19s&amp;amp;index=2&amp;amp;list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN" rel="nofollow"&gt;这里&lt;/a&gt;。
&lt;br&gt;中文文字版&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2039.ipynb"&gt;notebook&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-第2部分--深度学习基础pythontensorflow和keras--第40天" class="anchor" aria-hidden="true" href="#第2部分--深度学习基础pythontensorflow和keras--第40天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第2部分 | 深度学习基础Python，TensorFlow和Keras | 第40天&lt;/h2&gt;
&lt;p&gt;视频地址在&lt;a href="https://www.youtube.com/watch?v=wQ8BIBpya2k&amp;amp;t=19s&amp;amp;index=2&amp;amp;list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN" rel="nofollow"&gt;这里&lt;/a&gt;。
&lt;br&gt;中文文字版&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2040.ipynb"&gt;notebook&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-第3部分--深度学习基础pythontensorflow和keras--第41天" class="anchor" aria-hidden="true" href="#第3部分--深度学习基础pythontensorflow和keras--第41天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第3部分 | 深度学习基础Python，TensorFlow和Keras | 第41天&lt;/h2&gt;
&lt;p&gt;视频地址在&lt;a href="https://www.youtube.com/watch?v=wQ8BIBpya2k&amp;amp;t=19s&amp;amp;index=2&amp;amp;list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN" rel="nofollow"&gt;这里&lt;/a&gt;。
&lt;br&gt;中文文字版&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2041.ipynb"&gt;notebook&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-第4部分--深度学习基础pythontensorflow和keras--第42天" class="anchor" aria-hidden="true" href="#第4部分--深度学习基础pythontensorflow和keras--第42天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第4部分 | 深度学习基础Python，TensorFlow和Keras | 第42天&lt;/h2&gt;
&lt;p&gt;视频地址在&lt;a href="https://www.youtube.com/watch?v=wQ8BIBpya2k&amp;amp;t=19s&amp;amp;index=2&amp;amp;list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN" rel="nofollow"&gt;这里&lt;/a&gt;。
&lt;br&gt;中文文字版&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2042.ipynb"&gt;notebook&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k-均值聚类--第43天" class="anchor" aria-hidden="true" href="#k-均值聚类--第43天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;K-均值聚类 | 第43天&lt;/h2&gt;
&lt;p&gt;转到无监督学习，并研究了聚类。可在&lt;a href="http://www.avikjain.me/" rel="nofollow"&gt;作者网站&lt;/a&gt;查询。发现一个奇妙的&lt;a href="http://shabal.in/visuals/kmeans/6.html" rel="nofollow"&gt;动画&lt;/a&gt;有助于理解K-均值聚类。&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2043.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%2043.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-k-均值聚类--第44天" class="anchor" aria-hidden="true" href="#k-均值聚类--第44天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;K-均值聚类 | 第44天&lt;/h2&gt;
&lt;p&gt;实现（待添加代码）&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--numpy--第45天" class="anchor" aria-hidden="true" href="#深入研究--numpy--第45天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | NUMPY | 第45天&lt;/h2&gt;
&lt;p&gt;得到JK VanderPlas写的书《Python数据科学手册（Python Data Science HandBook）》，Jupyter notebooks在&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook"&gt;这里&lt;/a&gt;。
&lt;br&gt;&lt;strong&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other%20Docs/Python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E6%89%8B%E5%86%8C.zip"&gt;高清中文版pdf&lt;/a&gt;&lt;/strong&gt;
&lt;br&gt;第2章：NumPy介绍，包括数据类型、数组和数组计算。
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb"&gt;2 NumPy入门&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.01-Understanding-Data-Types.ipynb"&gt;2.1 理解Python中的数据类型&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb"&gt;2.2 NumPy数组基础&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.03-Computation-on-arrays-ufuncs.ipynb"&gt;2.3 NumPy数组的计算：通用函数&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--numpy--第46天" class="anchor" aria-hidden="true" href="#深入研究--numpy--第46天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | NUMPY | 第46天&lt;/h2&gt;
&lt;p&gt;第2章： 聚合, 比较运算符和广播。
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.04-Computation-on-arrays-aggregates.ipynb"&gt;2.4 聚合：最小值、最大值和其他值&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.05-Computation-on-arrays-broadcasting.ipynb"&gt;2.5 数组的计算：广播&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.06-Boolean-Arrays-and-Masks.ipynb"&gt;2.6 比较、掩码和布尔运算&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--numpy--第47天" class="anchor" aria-hidden="true" href="#深入研究--numpy--第47天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | NUMPY | 第47天&lt;/h2&gt;
&lt;p&gt;第2章： 花哨的索引，数组排序，结构化数据。
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.07-Fancy-Indexing.ipynb"&gt;2.7 花哨的索引&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb"&gt;2.8 数组的排序&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.09-%3Cbr%3EStructured-Data-NumPy.ipynb"&gt;2.9 结构化数据：NumPy的结构化数组&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--pandas--第48天" class="anchor" aria-hidden="true" href="#深入研究--pandas--第48天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | PANDAS | 第48天&lt;/h2&gt;
&lt;p&gt;第3章：Pandas数据处理
&lt;br&gt;包含Pandas对象，数据取值与选择，数值运算方法，处理缺失值，层级索引，合并数据集。
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb"&gt;3 Pandas数据处理&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.01-Introducing-Pandas-Objects.ipynb"&gt;3.1 Pandas对象简介&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.02-Data-Indexing-and-Selection.ipynb"&gt;3.2 数据取值与选择&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.03-Operations-in-Pandas.ipynb"&gt;3.3 Pandas数值运算方法&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.04-Missing-Values.ipynb"&gt;3.4 处理缺失值&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb"&gt;3.5 层级索引&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.06-Concat-And-Append.ipynb"&gt;3.6 合并数据集：ConCat和Append方法&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--pandas--第49天" class="anchor" aria-hidden="true" href="#深入研究--pandas--第49天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | PANDAS | 第49天&lt;/h2&gt;
&lt;p&gt;第3章：完成剩余内容-合并与连接，累计与分组，数据透视表。
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb"&gt;3.7 合并数据集：合并与连接&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb"&gt;3.8 累计与分组&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.09-Pivot-Tables.ipynb"&gt;3.9 数据透视表&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--pandas--第50天" class="anchor" aria-hidden="true" href="#深入研究--pandas--第50天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | PANDAS | 第50天&lt;/h2&gt;
&lt;p&gt;第3章：向量化字符串操作，处理时间序列。
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb"&gt;3.10 向量化字符串操作&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb"&gt;3.11 处理时间序列&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb"&gt;3.12 高性能Pandas：eval()与query()&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--matplotlib--第51天" class="anchor" aria-hidden="true" href="#深入研究--matplotlib--第51天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | MATPLOTLIB | 第51天&lt;/h2&gt;
&lt;p&gt;第4章：Matplotlib数据可视化
&lt;br&gt;学习简易线形图, 简易散点图，密度图与等高线图.
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb"&gt;4 Matplotlib数据可视化&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.01-Simple-Line-Plots.ipynb"&gt;4.1 简易线形图&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.02-Simple-Scatter-Plots.ipynb"&gt;4.2 简易散点图&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.03-Errorbars.ipynb"&gt;4.3 可视化异常处理&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.04-Density-and-Contour-Plots.ipynb"&gt;4.4 密度图与等高线图&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--matplotlib--第52天" class="anchor" aria-hidden="true" href="#深入研究--matplotlib--第52天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | MATPLOTLIB | 第52天&lt;/h2&gt;
&lt;p&gt;第4章：Matplotlib数据可视化
&lt;br&gt;学习直方图，配置图例，配置颜色条，多子图。
&lt;br&gt;代码如下：
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.05-Histograms-and-Binnings.ipynb"&gt;4.5 直方图&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.06-Customizing-Legends.ipynb"&gt;4.6 配置图例&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.07-Customizing-Colorbars.ipynb"&gt;4.7 配置颜色条&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.08-Multiple-Subplots.ipynb"&gt;4.8 多子图&lt;/a&gt;
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.09-Text-and-Annotation.ipynb"&gt;4.9 文字与注释&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-深入研究--matplotlib--第53天" class="anchor" aria-hidden="true" href="#深入研究--matplotlib--第53天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;深入研究 | MATPLOTLIB | 第53天&lt;/h2&gt;
&lt;p&gt;第4章：Matplotlib数据可视化
&lt;br&gt;学习三维绘图。
&lt;br&gt;&lt;a href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.12-Three-Dimensional-Plotting.ipynb"&gt;4.12 画三维图&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-层次聚类--第54天" class="anchor" aria-hidden="true" href="#层次聚类--第54天"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;层次聚类 | 第54天&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other%20Docs/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB.gif"&gt;动画演示&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2054.jpg"&gt;&lt;img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/raw/master/Info-graphs/Day%2054.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>MLEveryday</author><guid isPermaLink="false">https://github.com/MLEveryday/100-Days-Of-ML-Code</guid><pubDate>Mon, 10 Feb 2020 00:07:00 GMT</pubDate></item><item><title>wesm/pydata-book #8 in Jupyter Notebook, Today</title><link>https://github.com/wesm/pydata-book</link><description>&lt;p&gt;&lt;i&gt;Materials and IPython notebooks for "Python for Data Analysis" by Wes McKinney, published by O'Reilly Media&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-python-for-data-analysis-2nd-edition" class="anchor" aria-hidden="true" href="#python-for-data-analysis-2nd-edition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python for Data Analysis, 2nd Edition&lt;/h1&gt;
&lt;p&gt;Materials and IPython notebooks for "Python for Data Analysis" by Wes McKinney,
published by O'Reilly Media&lt;/p&gt;
&lt;p&gt;&lt;a href="http://amzn.to/2vvBijB" rel="nofollow"&gt;Buy the book on Amazon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notebooks.azure.com/import/gh/wesm/pydata-book" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2c33d8af3d101ffcd6ea73a8d02290b8d829ac52/68747470733a2f2f6e6f7465626f6f6b732e617a7572652e636f6d2f6c61756e63682e706e67" data-canonical-src="https://notebooks.azure.com/launch.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Follow Wes on Twitter: &lt;a href="https://twitter.com/wesmckinn" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e5ee948ef48fbfa31f868c9dbd301bbd5cf38097/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7765736d636b696e6e2e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/wesmckinn.svg?style=social&amp;amp;label=Follow" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-1st-edition-readers" class="anchor" aria-hidden="true" href="#1st-edition-readers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1st Edition Readers&lt;/h1&gt;
&lt;p&gt;If you are reading the &lt;a href="http://amzn.to/2vvBijB" rel="nofollow"&gt;1st Edition&lt;/a&gt; (published in 2012), please find the
reorganized book materials on the &lt;a href="https://github.com/wesm/pydata-book/tree/1st-edition"&gt;&lt;code&gt;1st-edition&lt;/code&gt; branch&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/BrambleXu/pydata-notebook"&gt;Chinese&lt;/a&gt; by Xu Liang&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ipython-notebooks" class="anchor" aria-hidden="true" href="#ipython-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IPython Notebooks:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch02.ipynb" rel="nofollow"&gt;Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch03.ipynb" rel="nofollow"&gt;Chapter 3: Built-in Data Structures, Functions, and Files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch04.ipynb" rel="nofollow"&gt;Chapter 4: NumPy Basics: Arrays and Vectorized Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch05.ipynb" rel="nofollow"&gt;Chapter 5: Getting Started with pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch06.ipynb" rel="nofollow"&gt;Chapter 6: Data Loading, Storage, and File Formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch07.ipynb" rel="nofollow"&gt;Chapter 7: Data Cleaning and Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch08.ipynb" rel="nofollow"&gt;Chapter 8: Data Wrangling: Join, Combine, and Reshape&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch09.ipynb" rel="nofollow"&gt;Chapter 9: Plotting and Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch10.ipynb" rel="nofollow"&gt;Chapter 10: Data Aggregation and Group Operations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch11.ipynb" rel="nofollow"&gt;Chapter 11: Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch12.ipynb" rel="nofollow"&gt;Chapter 12: Advanced pandas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch13.ipynb" rel="nofollow"&gt;Chapter 13: Introduction to Modeling Libraries in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch14.ipynb" rel="nofollow"&gt;Chapter 14: Data Analysis Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/appa.ipynb" rel="nofollow"&gt;Appendix A: Advanced NumPy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h3&gt;
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed
above, is released under the &lt;a href="LICENSE-CODE"&gt;MIT license&lt;/a&gt;. Read more at the
&lt;a href="https://opensource.org/licenses/MIT" rel="nofollow"&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wesm</author><guid isPermaLink="false">https://github.com/wesm/pydata-book</guid><pubDate>Mon, 10 Feb 2020 00:08:00 GMT</pubDate></item><item><title>Hvass-Labs/TensorFlow-Tutorials #9 in Jupyter Notebook, Today</title><link>https://github.com/Hvass-Labs/TensorFlow-Tutorials</link><description>&lt;p&gt;&lt;i&gt;TensorFlow Tutorials with YouTube Videos&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-tensorflow-tutorials" class="anchor" aria-hidden="true" href="#tensorflow-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow Tutorials&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials"&gt;Original repository on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Original author is &lt;a href="http://www.hvass-labs.org" rel="nofollow"&gt;Magnus Erik Hvass Pedersen&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;These tutorials are intended for beginners in Deep Learning and TensorFlow.&lt;/li&gt;
&lt;li&gt;Each tutorial covers a single topic.&lt;/li&gt;
&lt;li&gt;The source-code is well-documented.&lt;/li&gt;
&lt;li&gt;There is a &lt;a href="https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ" rel="nofollow"&gt;YouTube video&lt;/a&gt; for each tutorial.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Simple Linear Model
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/01_Simple_Linear_Model.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/01_Simple_Linear_Model.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Convolutional Neural Network
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;del&gt;Pretty Tensor&lt;/del&gt;
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/03_PrettyTensor.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/03_PrettyTensor.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;3-B. Layers API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/03B_Layers_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/03B_Layers_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;3-C. Keras API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/03C_Keras_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/03C_Keras_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;p&gt;Save &amp;amp; Restore
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/04_Save_Restore.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/04_Save_Restore.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensemble Learning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/05_Ensemble_Learning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/05_Ensemble_Learning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CIFAR-10
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/06_CIFAR-10.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/06_CIFAR-10.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inception Model
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/07_Inception_Model.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/07_Inception_Model.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transfer Learning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/08_Transfer_Learning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Video Data
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/09_Video_Data.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/09_Video_Data.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fine-Tuning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adversarial Examples
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/11_Adversarial_Examples.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/11_Adversarial_Examples.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adversarial Noise for MNIST
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/12_Adversarial_Noise_MNIST.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/12_Adversarial_Noise_MNIST.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Visual Analysis
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/13_Visual_Analysis.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/13_Visual_Analysis.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;13-B. Visual Analysis for MNIST
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/13B_Visual_Analysis_MNIST.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/13B_Visual_Analysis_MNIST.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;ol start="14"&gt;
&lt;li&gt;
&lt;p&gt;DeepDream
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/14_DeepDream.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/14_DeepDream.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Style Transfer
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement Learning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/16_Reinforcement_Learning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/16_Reinforcement_Learning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimator API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/17_Estimator_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/17_Estimator_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TFRecords &amp;amp; Dataset API
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/18_TFRecords_Dataset_API.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/18_TFRecords_Dataset_API.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hyper-Parameter Optimization
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Natural Language Processing
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Machine Translation
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image Captioning
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Time-Series Prediction
(&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/23_Time-Series-Prediction.ipynb"&gt;Notebook&lt;/a&gt;)
(&lt;a href="https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/23_Time-Series-Prediction.ipynb" rel="nofollow"&gt;Google Colab&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-videos" class="anchor" aria-hidden="true" href="#videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Videos&lt;/h2&gt;
&lt;p&gt;These tutorials are also available as &lt;a href="https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ" rel="nofollow"&gt;YouTube videos&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-obsolete-tutorials" class="anchor" aria-hidden="true" href="#obsolete-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Obsolete Tutorials&lt;/h2&gt;
&lt;p&gt;Some of these tutorials use an API called PrettyTensor for creating
Neural Networks in TensorFlow, but the PrettyTensor API is now obsolete.
Some of the Notebooks are therefore also obsolete and they are clearly
marked at the top of each Notebook. It is recommended that you
instead use the Keras API for creating Neural Networks in TensorFlow.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-translations" class="anchor" aria-hidden="true" href="#translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Translations&lt;/h2&gt;
&lt;p&gt;These tutorials have been translated to the following languages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials-Chinese"&gt;Chinese&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-new-translations" class="anchor" aria-hidden="true" href="#new-translations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New Translations&lt;/h3&gt;
&lt;p&gt;You can help by translating the remaining tutorials or reviewing the ones that have already been translated. You can also help by translating to other languages.&lt;/p&gt;
&lt;p&gt;It is a very big job to translate all the tutorials, so you should just start with Tutorials #01, #02 and #03-C which are the most important for beginners.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-new-videos" class="anchor" aria-hidden="true" href="#new-videos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New Videos&lt;/h3&gt;
&lt;p&gt;You are also very welcome to record your own YouTube videos in other languages. It is strongly recommended that you get a decent microphone because good sound quality is very important. I used &lt;code&gt;vokoscreen&lt;/code&gt; for recording the videos and the free &lt;a href="https://www.blackmagicdesign.com/products/davinciresolve/" rel="nofollow"&gt;DaVinci Resolve&lt;/a&gt; for editing the videos.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-forks" class="anchor" aria-hidden="true" href="#forks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Forks&lt;/h2&gt;
&lt;p&gt;See the &lt;a href="forks.md"&gt;selected list of forks&lt;/a&gt; for community modifications to these tutorials.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;There are different ways of installing and running TensorFlow. This section describes how I did it
for these tutorials. You may want to do it differently and you can search the internet for instructions.&lt;/p&gt;
&lt;p&gt;If you are new to using Python and Linux then this may be challenging
to get working and you may need to do internet searches for error-messages, etc.
It will get easier with practice. You can also run the tutorials without installing
anything by using Google Colab, see further below.&lt;/p&gt;
&lt;p&gt;Some of the Python Notebooks use source-code located in different files to allow for easy re-use
across multiple tutorials. It is therefore recommended that you download the whole repository
from GitHub, instead of just downloading the individual Python Notebooks.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-git" class="anchor" aria-hidden="true" href="#git"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git&lt;/h3&gt;
&lt;p&gt;The easiest way to download and install these tutorials is by using git from the command-line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create the directory &lt;code&gt;TensorFlow-Tutorials&lt;/code&gt; and download all the files to it.&lt;/p&gt;
&lt;p&gt;This also makes it easy to update the tutorials, simply by executing this command inside that directory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git pull
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-download-zip-file" class="anchor" aria-hidden="true" href="#download-zip-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Download Zip-File&lt;/h3&gt;
&lt;p&gt;You can also &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/archive/master.zip"&gt;download&lt;/a&gt;
the contents of the GitHub repository as a Zip-file and extract it manually.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-environment" class="anchor" aria-hidden="true" href="#environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Environment&lt;/h3&gt;
&lt;p&gt;I use &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt; because it comes with many Python
packages already installed and it is easy to work with. After installing Anaconda,
you should create a &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;conda environment&lt;/a&gt;
so you do not destroy your main installation in case you make a mistake somewhere:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name tf python=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When Python gets updated to a new version, it takes a while before TensorFlow also
uses the new Python version. So if the TensorFlow installation fails, then you may
have to specify an older Python version for your new environment, such as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name tf python=3.6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can switch to the new environment by running the following (on Linux):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source activate tf
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-required-packages" class="anchor" aria-hidden="true" href="#required-packages"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Required Packages&lt;/h3&gt;
&lt;p&gt;The tutorials require several Python packages to be installed. The packages are listed in
&lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;
First you need to edit this file and select whether you want to install the CPU or GPU
version of TensorFlow.&lt;/p&gt;
&lt;p&gt;To install the required Python packages and dependencies you first have to activate the
conda-environment as described above, and then you run the following command
in a terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the GPU-version of TensorFlow also requires the installation of various
NVIDIA drivers, which is not described here.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-version-35-or-later" class="anchor" aria-hidden="true" href="#python-version-35-or-later"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Version 3.5 or Later&lt;/h3&gt;
&lt;p&gt;These tutorials were developed on Linux using &lt;strong&gt;Python 3.5 / 3.6&lt;/strong&gt; (the &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt; distribution) and &lt;a href="https://www.jetbrains.com/pycharm/" rel="nofollow"&gt;PyCharm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are reports that Python 2.7 gives error messages with these tutorials. Please make sure you are using &lt;strong&gt;Python 3.5&lt;/strong&gt; or later!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-run" class="anchor" aria-hidden="true" href="#how-to-run"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How To Run&lt;/h2&gt;
&lt;p&gt;If you have followed the above installation instructions, you should
now be able to run the tutorials in the Python Notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd ~/development/TensorFlow-Tutorials/  # Your installation directory.
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should start a web-browser that shows the list of tutorials. Click on a tutorial to load it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run-in-google-colab" class="anchor" aria-hidden="true" href="#run-in-google-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run in Google Colab&lt;/h3&gt;
&lt;p&gt;If you do not want to install anything on your own computer, then the Notebooks
can be viewed, edited and run entirely on the internet by using
&lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Google Colab&lt;/a&gt;. There is a
&lt;a href="https://www.youtube.com/watch?v=Hs6HI2YWchM" rel="nofollow"&gt;YouTube video&lt;/a&gt; explaining how to do this.
You click the "Google Colab"-link next to each tutorial listed above.
You can view the Notebook on Colab but in order to run it you need to login using
your Google account.
Then you need to execute the following commands at the top of the Notebook,
which clones the contents of this repository to your work-directory on Colab.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Clone the repository from GitHub to Google Colab's temporary drive.
import os
work_dir = "/content/TensorFlow-Tutorials/"
if not os.path.exists(work_dir):
    !git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git
os.chdir(work_dir)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All required packages should already be installed on Colab, otherwise you
can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-older-versions" class="anchor" aria-hidden="true" href="#older-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Older Versions&lt;/h2&gt;
&lt;p&gt;Sometimes the source-code has changed from that shown in the YouTube videos. This may be due to
bug-fixes, improvements, or because code-sections are moved to separate files for easy re-use.&lt;/p&gt;
&lt;p&gt;If you want to see the exact versions of the source-code that were used in the YouTube videos,
then you can &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/commits/master"&gt;browse the history&lt;/a&gt;
of commits to the GitHub repository.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license-mit" class="anchor" aria-hidden="true" href="#license-mit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License (MIT)&lt;/h2&gt;
&lt;p&gt;These tutorials and source-code are published under the &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/LICENSE"&gt;MIT License&lt;/a&gt;
which allows very broad use for both academic and commercial purposes.&lt;/p&gt;
&lt;p&gt;A few of the images used for demonstration purposes may be under copyright. These images are included under the "fair usage" laws.&lt;/p&gt;
&lt;p&gt;You are very welcome to modify these tutorials and use them in your own projects.
Please keep a link to the &lt;a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials"&gt;original repository&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Hvass-Labs</author><guid isPermaLink="false">https://github.com/Hvass-Labs/TensorFlow-Tutorials</guid><pubDate>Mon, 10 Feb 2020 00:09:00 GMT</pubDate></item><item><title>nfmcclure/tensorflow_cookbook #10 in Jupyter Notebook, Today</title><link>https://github.com/nfmcclure/tensorflow_cookbook</link><description>&lt;p&gt;&lt;i&gt;Code for Tensorflow Machine Learning Cookbook&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="readme.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/nfmcclure/tensorflow_cookbook/raw/master/images/book_covers.jpg"&gt;&lt;img src="https://github.com/nfmcclure/tensorflow_cookbook/raw/master/images/book_covers.jpg" width="400" height="250" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-tensorflow-machine-learning-cookbook" class="anchor" aria-hidden="true" href="#tensorflow-machine-learning-cookbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook" rel="nofollow"&gt;TensorFlow Machine Learning Cookbook&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-a-packt-publishing-book" class="anchor" aria-hidden="true" href="#a-packt-publishing-book"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook" rel="nofollow"&gt;A Packt Publishing Book&lt;/a&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-by-nick-mcclure" class="anchor" aria-hidden="true" href="#by-nick-mcclure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;By Nick McClure&lt;/h3&gt;
&lt;p&gt;=================&lt;/p&gt;
&lt;p&gt;Build: &lt;a href="https://travis-ci.org/nfmcclure/tensorflow_cookbook" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/737d325288c2818c84a603bc9ac6d63c24637330/68747470733a2f2f7472617669732d63692e6f72672f6e666d63636c7572652f74656e736f72666c6f775f636f6f6b626f6f6b2e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/nfmcclure/tensorflow_cookbook.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;=================&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#ch-1-getting-started-with-tensorflow"&gt;Ch 1: Getting Started with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-2-the-tensorflow-way"&gt;Ch 2: The TensorFlow Way&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-3-linear-regression"&gt;Ch 3: Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-4-support-vector-machines"&gt;Ch 4: Support Vector Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-5-nearest-neighbor-methods"&gt;Ch 5: Nearest Neighbor Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-6-neural-networks"&gt;Ch 6: Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-7-natural-language-processing"&gt;Ch 7: Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-8-convolutional-neural-networks"&gt;Ch 8: Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-9-recurrent-neural-networks"&gt;Ch 9: Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-10-taking-tensorflow-to-production"&gt;Ch 10: Taking TensorFlow to Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ch-11-more-with-tensorflow"&gt;Ch 11: More with TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-ch-1-getting-started-with-tensorflow" class="anchor" aria-hidden="true" href="#ch-1-getting-started-with-tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="01_Introduction#ch-1-getting-started-with-tensorflow"&gt;Ch 1: Getting Started with TensorFlow&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/01_How_TensorFlow_Works#introduction-to-how-tensorflow-graphs-work"&gt;
    &lt;img src="01_Introduction/images/01_outline.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/02_Creating_and_Using_Tensors#creating-and-using-tensors"&gt;
    &lt;img src="01_Introduction/images/02_variable.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/03_Using_Variables_and_Placeholders#variables-and-placeholders"&gt;
    &lt;img src="01_Introduction/images/03_placeholder.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/06_Implementing_Activation_Functions#activation-functions"&gt;
    &lt;img src="01_Introduction/images/06_activation_funs1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="01_Introduction/06_Implementing_Activation_Functions#activation-functions"&gt;
    &lt;img src="01_Introduction/images/06_activation_funs2.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;This chapter intends to introduce the main objects and concepts in TensorFlow.  We also introduce how to access the data for the rest of the book and provide additional resources for learning about TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="01_Introduction/01_How_TensorFlow_Works#introduction-to-how-tensorflow-graphs-work"&gt;General Outline of TF Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we introduce TensorFlow and the general outline of how most TensorFlow algorithms work.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="01_Introduction/02_Creating_and_Using_Tensors#creating-and-using-tensors"&gt;Creating and Using Tensors&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to create and initialize tensors in TensorFlow.  We also depict how these operations appear in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="01_Introduction/03_Using_Variables_and_Placeholders#variables-and-placeholders"&gt;Using Variables and Placeholders&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to create and use variables and placeholders in TensorFlow.  We also depict how these operations appear in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="01_Introduction/04_Working_with_Matrices#working-with-matrices"&gt;Working with Matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Understanding how TensorFlow can work with matrices is crucial to understanding how the algorithms work.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="01_Introduction/05_Declaring_Operations#declaring-operations"&gt;Declaring Operations&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to use various mathematical operations in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="01_Introduction/06_Implementing_Activation_Functions#activation-functions"&gt;Implementing Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Activation functions are unique functions that TensorFlow has built in for your use in algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="01_Introduction/07_Working_with_Data_Sources#data-source-information"&gt;Working with Data Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to access all the various required data sources in the book.  There are also links describing the data sources and where they come from.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="01_Introduction/08_Additional_Resources#additional-resources"&gt;Additional Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Mostly official resources and papers.  The papers are TensorFlow papers or Deep Learning resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-2-the-tensorflow-way" class="anchor" aria-hidden="true" href="#ch-2-the-tensorflow-way"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="02_TensorFlow_Way#ch-2-the-tensorflow-way"&gt;Ch 2: The TensorFlow Way&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/01_Operations_as_a_Computational_Graph#operations-as-a-computational-graph"&gt;
    &lt;img src="02_TensorFlow_Way/images/01_Operations_on_a_Graph.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/02_Layering_Nested_Operations#multiple-operations-on-a-computational-graph"&gt;
    &lt;img src="02_TensorFlow_Way/images/02_Multiple_Operations.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/03_Working_with_Multiple_Layers#working-with-multiple-layers"&gt;
    &lt;img src="02_TensorFlow_Way/images/03_Multiple_Layers.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/04_Implementing_Loss_Functions#implementing-loss-functions"&gt;
    &lt;img src="02_TensorFlow_Way/images/04_loss_fun1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/05_Implementing_Back_Propagation#implementing-back-propagation"&gt;
    &lt;img src="02_TensorFlow_Way/images/04_loss_fun2.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/06_Working_with_Batch_and_Stochastic_Training#working-with-batch-and-stochastic-training"&gt;
    &lt;img src="02_TensorFlow_Way/images/06_Back_Propagation.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/07_Combining_Everything_Together#combining-everything-together"&gt;
    &lt;img src="02_TensorFlow_Way/images/07_Combing_Everything_Together.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="02_TensorFlow_Way/08_Evaluating_Models#evaluating-models"&gt;
    &lt;img src="02_TensorFlow_Way/images/08_Evaluating_Models.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;After we have established the basic objects and methods in TensorFlow, we now want to establish the components that make up TensorFlow algorithms.  We start by introducing computational graphs, and then move to loss functions and back propagation.  We end with creating a simple classifier and then show an example of evaluating regression and classification algorithms.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/01_Operations_as_a_Computational_Graph#operations-as-a-computational-graph"&gt;One Operation as a Computational Graph&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to create an operation on a computational graph and how to visualize it using Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/02_Layering_Nested_Operations#multiple-operations-on-a-computational-graph"&gt;Layering Nested Operations&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to create multiple operations on a computational graph and how to visualize them using Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/03_Working_with_Multiple_Layers#working-with-multiple-layers"&gt;Working with Multiple Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we extend the usage of the computational graph to create multiple layers and show how they appear in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/04_Implementing_Loss_Functions#implementing-loss-functions"&gt;Implementing Loss Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In order to train a model, we must be able to evaluate how well it is doing. This is given by loss functions. We plot various loss functions and talk about the benefits and limitations of some.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/05_Implementing_Back_Propagation#implementing-back-propagation"&gt;Implementing Back Propagation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to use loss functions to iterate through data and back propagate errors for regression and classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/06_Working_with_Batch_and_Stochastic_Training#working-with-batch-and-stochastic-training"&gt;Working with Stochastic and Batch Training&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow makes it easy to use both batch and stochastic training. We show how to implement both and talk about the benefits and limitations of each.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/07_Combining_Everything_Together#combining-everything-together"&gt;Combining Everything Together&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We now combine everything together that we have learned and create a simple classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="02_TensorFlow_Way/08_Evaluating_Models#evaluating-models"&gt;Evaluating Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Any model is only as good as it's evaluation.  Here we show two examples of (1) evaluating a regression algorithm and (2) a classification algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-3-linear-regression" class="anchor" aria-hidden="true" href="#ch-3-linear-regression"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="03_Linear_Regression#ch-3-linear-regression"&gt;Ch 3: Linear Regression&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/01_Using_the_Matrix_Inverse_Method#using-the-matrix-inverse-method"&gt;
    &lt;img src="03_Linear_Regression/images/01_Inverse_Matrix_Method.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/02_Implementing_a_Decomposition_Method#using-the-cholesky-decomposition-method"&gt;
    &lt;img src="03_Linear_Regression/images/02_Cholesky_Decomposition.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/03_TensorFlow_Way_of_Linear_Regression#learning-the-tensorflow-way-of-regression"&gt;
    &lt;img src="03_Linear_Regression/images/03_lin_reg_fit.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/04_Loss_Functions_in_Linear_Regressions#loss-functions-in-linear-regression"&gt;
    &lt;img src="03_Linear_Regression/images/04_L1_L2_learningrates.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/05_Implementing_Deming_Regression#implementing-deming-regression"&gt;
    &lt;img src="03_Linear_Regression/images/05_demming_vs_linear_reg.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/06_Implementing_Lasso_and_Ridge_Regression#implementing-lasso-and-ridge-regression"&gt;
    &lt;img src="03_Linear_Regression/images/07_elasticnet_reg_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/07_Implementing_Elasticnet_Regression#implementing-elasticnet-regression"&gt;
    &lt;img src="03_Linear_Regression/images/07_elasticnet_reg_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="03_Linear_Regression/08_Implementing_Logistic_Regression#implementing-logistic-regression"&gt;
    &lt;img src="03_Linear_Regression/images/08_logistic_reg_acc.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Here we show how to implement various linear regression techniques in TensorFlow.  The first two sections show how to do standard matrix linear regression solving in TensorFlow.  The remaining six sections depict how to implement various types of regression using computational graphs in TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/01_Using_the_Matrix_Inverse_Method#using-the-matrix-inverse-method"&gt;Using the Matrix Inverse Method&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to solve a 2D regression with a matrix inverse in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/02_Implementing_a_Decomposition_Method#using-the-cholesky-decomposition-method"&gt;Implementing a Decomposition Method&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Solving a 2D linear regression with Cholesky decomposition.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/03_TensorFlow_Way_of_Linear_Regression#learning-the-tensorflow-way-of-regression"&gt;Learning the TensorFlow Way of Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression iterating through a computational graph with L2 Loss.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/04_Loss_Functions_in_Linear_Regressions#loss-functions-in-linear-regression"&gt;Understanding Loss Functions in Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;L2 vs L1 loss in linear regression.  We talk about the benefits and limitations of both.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/05_Implementing_Deming_Regression#implementing-deming-regression"&gt;Implementing Deming Regression (Total Regression)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Deming (total) regression implemented in TensorFlow by changing the loss function.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/06_Implementing_Lasso_and_Ridge_Regression#implementing-lasso-and-ridge-regression"&gt;Implementing Lasso and Ridge Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Lasso and Ridge regression are ways of regularizing the coefficients. We implement both of these in TensorFlow via changing the loss functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/07_Implementing_Elasticnet_Regression#implementing-elasticnet-regression"&gt;Implementing Elastic Net Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Elastic net is a regularization technique that combines the L2 and L1 loss for coefficients.  We show how to implement this in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="03_Linear_Regression/08_Implementing_Logistic_Regression#implementing-logistic-regression"&gt;Implementing Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We implement logistic regression by the use of an activation function in our computational graph.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-4-support-vector-machines" class="anchor" aria-hidden="true" href="#ch-4-support-vector-machines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="04_Support_Vector_Machines#ch-4-support-vector-machines"&gt;Ch 4: Support Vector Machines&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/01_Introduction#support-vector-machine-introduction"&gt;
    &lt;img src="04_Support_Vector_Machines/images/01_introduction.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/02_Working_with_Linear_SVMs#working-with-linear-svms"&gt;
    &lt;img src="04_Support_Vector_Machines/images/02_linear_svm_output.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/03_Reduction_to_Linear_Regression#svm-reduction-to-linear-regression"&gt;
    &lt;img src="04_Support_Vector_Machines/images/03_svm_regression_output.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/04_Working_with_Kernels#working-with-kernels"&gt;
    &lt;img src="04_Support_Vector_Machines/images/04_linear_svm_gaussian.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/05_Implementing_Nonlinear_SVMs#implementing-nonlinear-svms"&gt;
    &lt;img src="04_Support_Vector_Machines/images/05_non_linear_svms.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="04_Support_Vector_Machines/06_Implementing_Multiclass_SVMs#implementing-multiclass-svms"&gt;
    &lt;img src="04_Support_Vector_Machines/images/06_multiclass_svm.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;This chapter shows how to implement various SVM methods with TensorFlow.  We first create a linear SVM and also show how it can be used for regression.  We then introduce kernels (RBF Gaussian kernel) and show how to use it to split up non-linear data. We finish with a multi-dimensional implementation of non-linear SVMs to work with multiple classes.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/01_Introduction#support-vector-machine-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce the concept of SVMs and how we will go about implementing them in the TensorFlow framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/02_Working_with_Linear_SVMs#working-with-linear-svms"&gt;Working with Linear SVMs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We create a linear SVM to separate I. setosa based on sepal length and pedal width in the Iris data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/03_Reduction_to_Linear_Regression#svm-reduction-to-linear-regression"&gt;Reduction to Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;The heart of SVMs is separating classes with a line.  We change tweek the algorithm slightly to perform SVM regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/04_Working_with_Kernels#working-with-kernels"&gt;Working with Kernels in TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In order to extend SVMs into non-linear data, we explain and show how to implement different kernels in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/05_Implementing_Nonlinear_SVMs#implementing-nonlinear-svms"&gt;Implementing Non-Linear SVMs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We use the Gaussian kernel (RBF) to separate non-linear classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="04_Support_Vector_Machines/06_Implementing_Multiclass_SVMs#implementing-multiclass-svms"&gt;Implementing Multi-class SVMs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;SVMs are inherently binary predictors.  We show how to extend them in a one-vs-all strategy in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-5-nearest-neighbor-methods" class="anchor" aria-hidden="true" href="#ch-5-nearest-neighbor-methods"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="05_Nearest_Neighbor_Methods#ch-5-nearest-neighbor-methods"&gt;Ch 5: Nearest Neighbor Methods&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/01_Introduction#nearest-neighbor-methods-introduction"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/nearest_neighbor_intro.jpg" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/02_Working_with_Nearest_Neighbors#working-with-nearest-neighbors"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/02_nn_histogram.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/03_Working_with_Text_Distances#working-with-text-distances"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/04_Computing_with_Mixed_Distance_Functions#computing-with-mixed-distance-functions"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/04_pred_vs_actual.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/05_An_Address_Matching_Example#an-address-matching-example"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="05_Nearest_Neighbor_Methods/06_Nearest_Neighbors_for_Image_Recognition#nearest-neighbors-for-image-recognition"&gt;
    &lt;img src="05_Nearest_Neighbor_Methods/images/06_nn_image_recognition.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Nearest Neighbor methods are a very popular ML algorithm.  We show how to implement k-Nearest Neighbors, weighted k-Nearest Neighbors, and k-Nearest Neighbors with mixed distance functions.  In this chapter we also show how to use the Levenshtein distance (edit distance) in TensorFlow, and use it to calculate the distance between strings. We end this chapter with showing how to use k-Nearest Neighbors for categorical prediction with the MNIST handwritten digit recognition.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/01_Introduction#nearest-neighbor-methods-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce the concepts and methods needed for performing k-Nearest Neighbors in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/02_Working_with_Nearest_Neighbors#working-with-nearest-neighbors"&gt;Working with Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We create a nearest neighbor algorithm that tries to predict housing worth (regression).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/03_Working_with_Text_Distances#working-with-text-distances"&gt;Working with Text Based Distances&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In order to use a distance function on text, we show how to use edit distances in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/04_Computing_with_Mixed_Distance_Functions#computing-with-mixed-distance-functions"&gt;Computing Mixing Distance Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we implement scaling of the distance function by the standard deviation of the input feature for k-Nearest Neighbors.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/05_An_Address_Matching_Example#an-address-matching-example"&gt;Using Address Matching&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We use a mixed distance function to match addresses. We use numerical distance for zip codes, and string edit distance for street names. The street names are allowed to have typos.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="05_Nearest_Neighbor_Methods/06_Nearest_Neighbors_for_Image_Recognition#nearest-neighbors-for-image-recognition"&gt;Using Nearest Neighbors for Image Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;The MNIST digit image collection is a great data set for illustration of how to perform k-Nearest Neighbors for an image classification task.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-6-neural-networks" class="anchor" aria-hidden="true" href="#ch-6-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="06_Neural_Networks#ch-6-neural-networks"&gt;Ch 6: Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/01_Introduction#neural-networks-introduction"&gt;
    &lt;img src="06_Neural_Networks/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/02_Implementing_an_Operational_Gate#implementing-an-operational-gate"&gt;
    &lt;img src="06_Neural_Networks/images/02_operational_gates.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/03_Working_with_Activation_Functions#working-with-activation-functions"&gt;
    &lt;img src="06_Neural_Networks/images/03_activation1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/04_Single_Hidden_Layer_Network#implementing-a-one-layer-neural-network"&gt;
    &lt;img src="06_Neural_Networks/images/04_nn_layout.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/05_Implementing_Different_Layers#implementing-different-layers"&gt;
    &lt;img src="06_Neural_Networks/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/06_Using_Multiple_Layers#using-multiple-layers"&gt;
    &lt;img src="06_Neural_Networks/images/06_nn_multiple_layers_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/07_Improving_Linear_Regression#improving-linear-regression"&gt;
    &lt;img src="06_Neural_Networks/images/07_lin_reg_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="06_Neural_Networks/08_Learning_Tic_Tac_Toe#learning-to-play-tic-tac-toe"&gt;
    &lt;img src="06_Neural_Networks/images/08_tictactoe_layout.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Neural Networks are very important in machine learning and growing in popularity due to the major breakthroughs in prior unsolved problems.  We must start with introducing 'shallow' neural networks, which are very powerful and can help us improve our prior ML algorithm results.  We start by introducing the very basic NN unit, the operational gate.  We gradually add more and more to the neural network and end with training a model to play tic-tac-toe.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/01_Introduction#neural-networks-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce the concept of neural networks and how TensorFlow is built to easily handle these algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/02_Implementing_an_Operational_Gate#implementing-an-operational-gate"&gt;Implementing Operational Gates&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We implement an operational gate with one operation. Then we show how to extend this to multiple nested operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/03_Working_with_Activation_Functions#working-with-activation-functions"&gt;Working with Gates and Activation Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Now we have to introduce activation functions on the gates.  We show how different activation functions operate.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/04_Single_Hidden_Layer_Network#implementing-a-one-layer-neural-network"&gt;Implementing a One Layer Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We have all the pieces to start implementing our first neural network.  We do so here with regression on the Iris data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/05_Implementing_Different_Layers#implementing-different-layers"&gt;Implementing Different Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This section introduces the convolution layer and the max-pool layer.  We show how to chain these together in a 1D and 2D example with fully connected layers as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/06_Using_Multiple_Layers#using-multiple-layers"&gt;Using Multi-layer Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to functionalize different layers and variables for a cleaner multi-layer neural network.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/07_Improving_Linear_Regression#improving-linear-regression"&gt;Improving Predictions of Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how we can improve the convergence of our prior logistic regression with a set of hidden layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="8"&gt;
&lt;li&gt;&lt;a href="06_Neural_Networks/08_Learning_Tic_Tac_Toe#learning-to-play-tic-tac-toe"&gt;Learning to Play Tic-Tac-Toe&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Given a set of tic-tac-toe boards and corresponding optimal moves, we train a neural network classification model to play.  At the end of the script, you can attempt to play against the trained model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-7-natural-language-processing" class="anchor" aria-hidden="true" href="#ch-7-natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="07_Natural_Language_Processing#ch-7-natural-language-processing"&gt;Ch 7: Natural Language Processing&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/01_Introduction#natural-language-processing-introduction"&gt;
    &lt;img src="07_Natural_Language_Processing/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/02_Working_with_Bag_of_Words#working-with-bag-of-words"&gt;
    &lt;img src="07_Natural_Language_Processing/images/02_bag_of_words.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/03_Implementing_tf_idf#implementing-tf-idf"&gt;
    &lt;img src="07_Natural_Language_Processing/images/03_tfidf_acc.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/04_Working_With_Skip_Gram_Embeddings#working-with-skip-gram-embeddings"&gt;
    &lt;img src="07_Natural_Language_Processing/images/04_skipgram_model.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/05_Working_With_CBOW_Embeddings#working-with-cbow-embeddings"&gt;
    &lt;img src="07_Natural_Language_Processing/images/05_cbow_model.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/06_Using_Word2Vec_Embeddings#using-word2vec-embeddings"&gt;
    &lt;img src="07_Natural_Language_Processing/images/06_word2vec_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="07_Natural_Language_Processing/07_Sentiment_Analysis_With_Doc2Vec#sentiment-analysis-with-doc2vec"&gt;
    &lt;img src="07_Natural_Language_Processing/images/07_sentiment_doc2vec_loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Natural Language Processing (NLP) is a way of processing textual information into numerical summaries, features, or models. In this chapter we will motivate and explain how to best deal with text in TensorFlow.  We show how to implement the classic 'Bag-of-Words' and show that there may be better ways to embed text based on the problem at hand. There are neural network embeddings called Word2Vec (CBOW and Skip-Gram) and Doc2Vec.  We show how to implement all of these in TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/01_Introduction#natural-language-processing-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce methods for turning text into numerical vectors. We introduce the TensorFlow 'embedding' feature as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/02_Working_with_Bag_of_Words#working-with-bag-of-words"&gt;Working with Bag-of-Words&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we use TensorFlow to do a one-hot-encoding of words called bag-of-words.  We use this method and logistic regression to predict if a text message is spam or ham.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/03_Implementing_tf_idf#implementing-tf-idf"&gt;Implementing TF-IDF&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We implement Text Frequency - Inverse Document Frequency (TFIDF) with a combination of Sci-kit Learn and TensorFlow. We perform logistic regression on TFIDF vectors to improve on our spam/ham text-message predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/04_Working_With_Skip_Gram_Embeddings#working-with-skip-gram-embeddings"&gt;Working with Skip-Gram&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Our first implementation of Word2Vec called, "skip-gram" on a movie review database.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/05_Working_With_CBOW_Embeddings#working-with-cbow-embeddings"&gt;Working with CBOW&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Next, we implement a form of Word2Vec called, "CBOW" (Continuous Bag of Words) on a movie review database.  We also introduce method to saving and loading word embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/06_Using_Word2Vec_Embeddings#using-word2vec-embeddings"&gt;Implementing Word2Vec Example&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we use the prior saved CBOW word embeddings to improve on our TF-IDF logistic regression of movie review sentiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="7"&gt;
&lt;li&gt;&lt;a href="07_Natural_Language_Processing/07_Sentiment_Analysis_With_Doc2Vec#sentiment-analysis-with-doc2vec"&gt;Performing Sentiment Analysis with Doc2Vec&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we introduce a Doc2Vec method (concatenation of doc and word embeddings) to improve out logistic model of movie review sentiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-8-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#ch-8-convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="08_Convolutional_Neural_Networks#ch-8-convolutional-neural-networks"&gt;Ch 8: Convolutional Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/01_Intro_to_CNN#introduction-to-convolutional-neural-networks"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/01_intro_cnn.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/02_Intro_to_CNN_MNIST#introduction-to-cnn-with-mnist"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/02_cnn1_mnist_output.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/03_CNN_CIFAR10#cifar-10-cnn"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/03_cnn2_loss_acc.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/04_Retraining_Current_Architectures#retraining-fine-tuning-current-cnn-architectures"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/05_Stylenet_NeuralStyle#stylenet--neural-style"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/05_stylenet_ex.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="08_Convolutional_Neural_Networks/06_Deepdream#deepdream-in-tensorflow"&gt;
    &lt;img src="08_Convolutional_Neural_Networks/images/06_deepdream_ex.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) are ways of getting neural networks to deal with image data. CNN derive their name from the use of a convolutional layer that applies a fixed size filter across a larger image, recognizing a pattern in any part of the image. There are many other tools that they use (max-pooling, dropout, etc...) that we show how to implement with TensorFlow.  We also show how to retrain an existing architecture and take CNNs further with Stylenet and Deep Dream.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/01_Intro_to_CNN#introduction-to-convolutional-neural-networks"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce convolutional neural networks (CNN), and how we can use them in TensorFlow.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/02_Intro_to_CNN_MNIST#introduction-to-cnn-with-mnist"&gt;Implementing a Simple CNN.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we show how to create a CNN architecture that performs well on the MNIST digit recognition task.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/03_CNN_CIFAR10#cifar-10-cnn"&gt;Implementing an Advanced CNN.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we show how to replicate an architecture for the CIFAR-10 image recognition task.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/04_Retraining_Current_Architectures#retraining-fine-tuning-current-cnn-architectures"&gt;Retraining an Existing Architecture.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to download and setup the CIFAR-10 data for the TensorFlow retraining/fine-tuning tutorial.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/05_Stylenet_NeuralStyle#stylenet--neural-style"&gt;Using Stylenet/NeuralStyle.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this recipe, we show a basic implementation of using Stylenet or Neuralstyle.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="08_Convolutional_Neural_Networks/06_Deepdream#deepdream-in-tensorflow"&gt;Implementing Deep Dream.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This script shows a line-by-line explanation of TensorFlow's deepdream tutorial. Taken from &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream"&gt;Deepdream on TensorFlow&lt;/a&gt;. Note that the code here is converted to Python 3.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-9-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#ch-9-recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="09_Recurrent_Neural_Networks#ch-9-recurrent-neural-networks"&gt;Ch 9: Recurrent Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/01_Introduction#introduction-to-rnns-in-tensorflow"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/01_RNN_Seq2Seq.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction#implementing-an-rnn-for-spam-prediction"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/02_RNN_Spam_Acc_Loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/03_Implementing_LSTM#implementing-an-lstm-model"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/03_LSTM_Loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/04_Stacking_Multiple_LSTM_Layers#stacking-multiple-lstm-layers"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/04_MultipleRNN_Architecture.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/05_Creating_A_Sequence_To_Sequence_Model#creating-a-sequence-to-sequence-model-with-tensorflow-seq2seq"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/05_Seq2Seq_Loss.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="09_Recurrent_Neural_Networks/06_Training_A_Siamese_Similarity_Measure#training-a-siamese-similarity-measure-rnns"&gt;
    &lt;img src="09_Recurrent_Neural_Networks/images/06_Similarity_RNN.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Recurrent Neural Networks (RNNs) are very similar to regular neural networks except that they allow 'recurrent' connections, or loops that depend on the prior states of the network. This allows RNNs to efficiently deal with sequential data, whereas other types of networks cannot. We then motivate the usage of LSTM (Long Short Term Memory) networks as a way of addressing regular RNN problems. Then we show how easy it is to implement these RNN types in TensorFlow.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/01_Introduction#introduction-to-rnns-in-tensorflow"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We introduce Recurrent Neural Networks and how they are able to feed in a sequence and predict either a fixed target (categorical/numerical) or another sequence (sequence to sequence).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction#implementing-an-rnn-for-spam-prediction"&gt;Implementing an RNN Model for Spam Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we create an RNN model to improve on our spam/ham SMS text predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/03_Implementing_LSTM#implementing-an-lstm-model"&gt;Implementing an LSTM Model for Text Generation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to implement a LSTM (Long Short Term Memory) RNN for Shakespeare language generation. (Word level vocabulary)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/04_Stacking_Multiple_LSTM_Layers#stacking-multiple-lstm-layers"&gt;Stacking Multiple LSTM Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We stack multiple LSTM layers to improve on our Shakespeare language generation. (Character level vocabulary)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/05_Creating_A_Sequence_To_Sequence_Model#creating-a-sequence-to-sequence-model-with-tensorflow-seq2seq"&gt;Creating a Sequence to Sequence Translation Model (Seq2Seq)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we use TensorFlow's sequence-to-sequence models to train an English-German translation model.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="09_Recurrent_Neural_Networks/06_Training_A_Siamese_Similarity_Measure#training-a-siamese-similarity-measure-rnns"&gt;Training a Siamese Similarity Measure&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we implement a Siamese RNN to predict the similarity of addresses and use it for record matching.  Using RNNs for record matching is very versatile, as we do not have a fixed set of target categories and can use the trained model to predict similarities across new addresses.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-10-taking-tensorflow-to-production" class="anchor" aria-hidden="true" href="#ch-10-taking-tensorflow-to-production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="10_Taking_TensorFlow_to_Production#ch-10-taking-tensorflow-to-production"&gt;Ch 10: Taking TensorFlow to Production&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/01_Implementing_Unit_Tests#implementing-unit-tests"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/02_Using_Multiple_Devices#using-multiple-devices"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/03_Parallelizing_TensorFlow#parallelizing-tensorflow"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/04_Production_Tips#production-tips-with-tensorflow"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="10_Taking_TensorFlow_to_Production/05_Production_Example#a-production-example"&gt;
    &lt;img src="10_Taking_TensorFlow_to_Production/images/image.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;Of course there is more to TensorFlow than just creating and fitting machine learning models.  Once we have a model that we want to use, we have to move it towards production usage.  This chapter will provide tips and examples of implementing unit tests, using multiple processors, using multiple machines (TensorFlow distributed), and finish with a full production example.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/01_Implementing_Unit_Tests#implementing-unit-tests"&gt;Implementing Unit Tests&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to implement different types of unit tests on tensors (placeholders and variables).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/02_Using_Multiple_Devices#using-multiple-devices"&gt;Using Multiple Executors (Devices)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to use a machine with multiple devices.  E.g., a machine with a CPU, and one or more GPUs.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/03_Parallelizing_TensorFlow#parallelizing-tensorflow"&gt;Parallelizing TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to setup and use TensorFlow distributed on multiple machines.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/04_Production_Tips#production-tips-with-tensorflow"&gt;Tips for TensorFlow in Production&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Various tips for developing with TensorFlow&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="10_Taking_TensorFlow_to_Production/05_Production_Example#a-production-example"&gt;An Example of Productionalizing TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We show how to do take the RNN model for predicting ham/spam (from Chapter 9, recipe #2) and put it in two production level files: training and evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ch-11-more-with-tensorflow" class="anchor" aria-hidden="true" href="#ch-11-more-with-tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="11_More_with_TensorFlow#ch-11-more-with-tensorflow"&gt;Ch 11: More with TensorFlow&lt;/a&gt;&lt;/h2&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/01_Visualizing_Computational_Graphs#visualizing-computational-graphs-wtensorboard"&gt;
    &lt;img src="11_More_with_TensorFlow/images/01_tensorboard1.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/02_Working_with_a_Genetic_Algorithm"&gt;
    &lt;img src="11_More_with_TensorFlow/images/02_genetic_algorithm.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/03_Clustering_Using_KMeans#clustering-using-k-means"&gt;
    &lt;img src="11_More_with_TensorFlow/images/03_kmeans.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;kbd&gt;
  &lt;a href="11_More_with_TensorFlow/04_Solving_A_System_of_ODEs#solving-a-system-of-odes"&gt;
    &lt;img src="11_More_with_TensorFlow/images/04_ode_system.png" align="center" height="45" width="90" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/kbd&gt;
&lt;p&gt;To illustrate how versatile TensorFlow is, we will show additional examples in this chapter. We start with showing how to use the logging/visualizing tool Tensorboard.  Then we illustrate how to do k-means clustering, use a genetic algorithm, and solve a system of ODEs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/01_Visualizing_Computational_Graphs#visualizing-computational-graphs-wtensorboard"&gt;Visualizing Computational Graphs (with Tensorboard)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;An example of using histograms, scalar summaries, and creating images in Tensorboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/02_Working_with_a_Genetic_Algorithm#working-with-a-genetic-algorithm"&gt;Working with a Genetic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We create a genetic algorithm to optimize an individual (array of 50 numbers) toward the ground truth function.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/03_Clustering_Using_KMeans#clustering-using-k-means"&gt;Clustering Using K-means&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;How to use TensorFlow to do k-means clustering.  We use the Iris data set, set k=3, and use k-means to make predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/04_Solving_A_System_of_ODEs#solving-a-system-of-odes"&gt;Solving a System of ODEs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here, we show how to use TensorFlow to solve a system of ODEs.  The system of concern is the Lotka-Volterra predator-prey system.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/05_Using_a_Random_Forest#using-a-random-forest"&gt;Using a Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;We illustrate how to use TensorFlow's gradient boosted regression and classification trees.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="6"&gt;
&lt;li&gt;&lt;a href="11_More_with_TensorFlow/06_Using_TensorFlow_with_Keras#using-tensorflow-with-keras"&gt;Using TensorFlow with Keras&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Here we show how to use the Keras sequential model building for a fully connected neural network and a CNN model with callbacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>nfmcclure</author><guid isPermaLink="false">https://github.com/nfmcclure/tensorflow_cookbook</guid><pubDate>Mon, 10 Feb 2020 00:10:00 GMT</pubDate></item><item><title>zergtant/pytorch-handbook #11 in Jupyter Notebook, Today</title><link>https://github.com/zergtant/pytorch-handbook</link><description>&lt;p&gt;&lt;i&gt;pytorch handbook是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门，其中包含的Pytorch教程全部通过测试保证可以成功运行&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-中文手册pytorch-handbook" class="anchor" aria-hidden="true" href="#pytorch-中文手册pytorch-handbook"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch 中文手册（pytorch handbook）&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png"&gt;&lt;img src="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png" alt="pytorch" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-书籍介绍" class="anchor" aria-hidden="true" href="#书籍介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;书籍介绍&lt;/h2&gt;
&lt;p&gt;这是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门。&lt;/p&gt;
&lt;p&gt;由于本人水平有限，在写此教程的时候参考了一些网上的资料，在这里对他们表示敬意，我会在每个引用中附上原文地址，方便大家参考。&lt;/p&gt;
&lt;p&gt;深度学习的技术在飞速的发展，同时PyTorch也在不断更新，且本人会逐步完善相关内容。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-版本说明" class="anchor" aria-hidden="true" href="#版本说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;版本说明&lt;/h2&gt;
&lt;p&gt;由于PyTorch版本更迭，教程的版本会与PyTorch版本，保持一致。&lt;/p&gt;
&lt;p&gt;2020.1.16 PyTorch已经发布1.4的稳定版。&lt;/p&gt;
&lt;p&gt;Java bindings 被列入了支持列表，这几天准备出一个springboot的集成教程&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-qq-3群" class="anchor" aria-hidden="true" href="#qq-3群"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;QQ 3群&lt;/h2&gt;
&lt;p&gt;群号：773681699&lt;/p&gt;
&lt;p&gt;扫描二维码&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="Pytorch-Handbook-3.png"&gt;&lt;img src="Pytorch-Handbook-3.png" alt="QR" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="//shang.qq.com/wpa/qunwpa?idkey=ee402d5f0e7732b2171e643d729177ce55ac404eafda5edd9b740d73fabe6a96" rel="nofollow"&gt;点击链接加入群聊 『PyTorch Handbook 交流3群』&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1群(985896536)已满，2群(681980831)已满&lt;/p&gt;
&lt;p&gt;不要再加了&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-说明" class="anchor" aria-hidden="true" href="#说明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;说明&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;修改错别字请直接提issue或PR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PR时请注意版本&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;有问题也请直接提issue&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;感谢&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-第一章pytorch-入门" class="anchor" aria-hidden="true" href="#第一章pytorch-入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一章：PyTorch 入门&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter1/1.1-pytorch-introduction.md"&gt;PyTorch 简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.2-pytorch-installation.md"&gt;PyTorch 环境搭建&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md"&gt;PyTorch 深度学习：60分钟快速入门（官方）&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="chapter1/1_tensor_tutorial.ipynb"&gt;张量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/2_autograd_tutorial.ipynb"&gt;Autograd：自动求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/3_neural_networks_tutorial.ipynb"&gt;神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/4_cifar10_tutorial.ipynb"&gt;训练一个分类器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/5_data_parallel_tutorial.ipynb"&gt;选读：数据并行处理（多GPU）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter1/1.4-pytorch-resource.md"&gt;相关资源介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-第二章-基础" class="anchor" aria-hidden="true" href="#第二章-基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二章 基础&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-pytorch-基础" class="anchor" aria-hidden="true" href="#第一节-pytorch-基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 PyTorch 基础&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.1.pytorch-basics-tensor.ipynb"&gt;张量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.2-pytorch-basics-autograd.ipynb"&gt;自动求导&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.3-pytorch-basics-nerual-network.ipynb"&gt;神经网络包nn和优化器optm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="chapter2/2.1.4-pytorch-basics-data-loader.ipynb"&gt;数据的加载和预处理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-深度学习基础及数学原理" class="anchor" aria-hidden="true" href="#第二节-深度学习基础及数学原理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 深度学习基础及数学原理&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.2-deep-learning-basic-mathematics.ipynb"&gt;深度学习基础及数学原理&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-神经网络简介" class="anchor" aria-hidden="true" href="#第三节-神经网络简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 神经网络简介&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.3-deep-learning-neural-network-introduction.ipynb"&gt;神经网络简介&lt;/a&gt;  注：本章在本地使用微软的Edge打开会崩溃，请使Chrome Firefox打开查看&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-卷积神经网络" class="anchor" aria-hidden="true" href="#第四节-卷积神经网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 卷积神经网络&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.4-cnn.ipynb"&gt;卷积神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-循环神经网络" class="anchor" aria-hidden="true" href="#第五节-循环神经网络"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 循环神经网络&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter2/2.5-rnn.ipynb"&gt;循环神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第三章-实践" class="anchor" aria-hidden="true" href="#第三章-实践"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三章 实践&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-logistic回归二元分类" class="anchor" aria-hidden="true" href="#第一节-logistic回归二元分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 logistic回归二元分类&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.1-logistic-regression.ipynb"&gt;logistic回归二元分类&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-cnnmnist数据集手写数字识别" class="anchor" aria-hidden="true" href="#第二节-cnnmnist数据集手写数字识别"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 CNN:MNIST数据集手写数字识别&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.2-mnist.ipynb"&gt;CNN:MNIST数据集手写数字识别&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-rnn实例通过sin预测cos" class="anchor" aria-hidden="true" href="#第三节-rnn实例通过sin预测cos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 RNN实例：通过Sin预测Cos&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter3/3.3-rnn.ipynb"&gt;RNN实例：通过Sin预测Cos&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第四章-提高" class="anchor" aria-hidden="true" href="#第四章-提高"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四章 提高&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-fine-tuning" class="anchor" aria-hidden="true" href="#第一节-fine-tuning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 Fine-tuning&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.1-fine-tuning.ipynb"&gt;Fine-tuning&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-可视化" class="anchor" aria-hidden="true" href="#第二节-可视化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 可视化&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.1-visdom.ipynb"&gt;visdom&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.2-tensorboardx.ipynb"&gt;tensorboardx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="chapter4/4.2.3-cnn-visualizing.ipynb"&gt;可视化理解卷积神经网络&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-fastai" class="anchor" aria-hidden="true" href="#第三节-fastai"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 Fast.ai&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.3-fastai.ipynb"&gt;Fast.ai&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-训练的一些技巧" class="anchor" aria-hidden="true" href="#第四节-训练的一些技巧"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 训练的一些技巧&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-多gpu并行训练" class="anchor" aria-hidden="true" href="#第五节-多gpu并行训练"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 多GPU并行训练&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter4/4.5-multiply-gpu-parallel-training.ipynb"&gt;多GPU并行计算&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-第五章-应用" class="anchor" aria-hidden="true" href="#第五章-应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五章 应用&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第一节-kaggle介绍" class="anchor" aria-hidden="true" href="#第一节-kaggle介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第一节 Kaggle介绍&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.1-kaggle.md"&gt;Kaggle介绍&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第二节-结构化数据" class="anchor" aria-hidden="true" href="#第二节-结构化数据"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第二节 结构化数据&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第三节-计算机视觉" class="anchor" aria-hidden="true" href="#第三节-计算机视觉"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第三节 计算机视觉&lt;/h4&gt;
&lt;p&gt;&lt;a href="chapter5/5.3-Fashion-MNIST.ipynb"&gt;Fashion MNIST 图像分类&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-第四节-自然语言处理" class="anchor" aria-hidden="true" href="#第四节-自然语言处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第四节 自然语言处理&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第五节-协同过滤" class="anchor" aria-hidden="true" href="#第五节-协同过滤"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第五节 协同过滤&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-第六章-资源" class="anchor" aria-hidden="true" href="#第六章-资源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第六章 资源&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-第七章-附录" class="anchor" aria-hidden="true" href="#第七章-附录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第七章 附录&lt;/h3&gt;
&lt;p&gt;&lt;a href="pi/"&gt;树莓派编译安装 pytorch 1.4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;transforms的常用操作总结&lt;/p&gt;
&lt;p&gt;pytorch的损失函数总结&lt;/p&gt;
&lt;p&gt;pytorch的优化器总结&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/60543937b5e790e3bca35357ccc1313f4b5f52b3/68747470733a2f2f692e6372656174697665636f6d6d6f6e732e6f72672f6c2f62792d6e632d73612f332e302f38387833312e706e67" alt="" data-canonical-src="https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn" rel="nofollow"&gt;本作品采用知识共享署名-非商业性使用-相同方式共享 3.0  中国大陆许可协议进行许可&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zergtant</author><guid isPermaLink="false">https://github.com/zergtant/pytorch-handbook</guid><pubDate>Mon, 10 Feb 2020 00:11:00 GMT</pubDate></item><item><title>robmarkcole/satellite-image-deep-learning #12 in Jupyter Notebook, Today</title><link>https://github.com/robmarkcole/satellite-image-deep-learning</link><description>&lt;p&gt;&lt;i&gt; Resources for deep learning with satellite &amp; aerial imagery&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://github.com/sponsors/robmarkcole"&gt;&lt;img src="https://camo.githubusercontent.com/71d78e24fcd75bb7723712c87325766d6b95acd6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73706f6e736f722d2546302539462539322539362d677265656e" alt="Sponsor" data-canonical-src="https://img.shields.io/badge/sponsor-%F0%9F%92%96-green" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This document primarily lists resources for performing deep learning (DL) on satellite imagery. To a lesser extent Machine learning (ML, e.g. random forests, stochastic gradient descent) are also discussed, as are classical image processing techniques.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-top-links" class="anchor" aria-hidden="true" href="#top-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Top links&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chrieke/awesome-satellite-imagery-datasets"&gt;https://github.com/chrieke/awesome-satellite-imagery-datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/jacquestardie/0d1c0cb413b3b9b06edf"&gt;A modern geospatial workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/deepVector/geospatial-machine-learning"&gt;geospatial-machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.satimagingcorp.com/satellite-sensors/" rel="nofollow"&gt;Long list of satellite missions with example imagery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://registry.opendata.aws/" rel="nofollow"&gt;AWS datasets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#datasets"&gt;Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#online-computing-resources"&gt;Online computing resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#interesting-dl-projects"&gt;Interesting dl projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#production"&gt;Production&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#image-formats--catalogues"&gt;Image formats and catalogues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#state-of-the-art"&gt;State of the art&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#online-platforms-for-geo-analysis"&gt;Online platforms for Geo analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#techniques"&gt;Techniques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning#useful-references"&gt;Useful references&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Warning&lt;/strong&gt; satellite image files can be LARGE, even a small data set may comprise 50 GB of imagery&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.maptiler.com/gallery/satellite/" rel="nofollow"&gt;Various datasets listed here&lt;/a&gt; and at &lt;a href="https://github.com/chrieke/awesome-satellite-imagery-datasets"&gt;awesome-satellite-imagery-datasets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-worldview---spacenet" class="anchor" aria-hidden="true" href="#worldview---spacenet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WorldView - SpaceNet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/WorldView-3" rel="nofollow"&gt;https://en.wikipedia.org/wiki/WorldView-3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;0.3m PAN, 1.24 MS, 3.7m SWIR. Off-Nadir (stereo) available.&lt;/li&gt;
&lt;li&gt;Owned by &lt;a href="https://www.digitalglobe.com/" rel="nofollow"&gt;DigitalGlobe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@sumit.arora/getting-started-with-aws-spacenet-and-spacenet-dataset-visualization-basics-7ddd2e5809a2" rel="nofollow"&gt;Intro to SpaceNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spacenetchallenge.github.io/datasets/datasetHomePage.html" rel="nofollow"&gt;SpaceNet dataset on AWS&lt;/a&gt; -&amp;gt; see &lt;a href="https://medium.com/the-downlinq/getting-started-with-spacenet-data-827fd2ec9f53" rel="nofollow"&gt;this getting started notebook&lt;/a&gt; and this notebook on the &lt;a href="https://medium.com/the-downlinq/introducing-the-spacenet-off-nadir-imagery-and-buildings-dataset-e4a3c1cb4ce3" rel="nofollow"&gt;off-Nadir dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://menthe.ovh.hw.ipol.im/IARPA_data/cloud_optimized_geotif/" rel="nofollow"&gt;cloud_optimized_geotif here&lt;/a&gt; used in the 3D modelling notebook &lt;a href="https://gfacciol.github.io/IS18/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SpaceNetChallenge/utilities"&gt;Package of utilities&lt;/a&gt; to assist working with the SpaceNet dataset.&lt;/li&gt;
&lt;li&gt;For more Worldview imagery see Kaggle DSTL competition.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-sentinel" class="anchor" aria-hidden="true" href="#sentinel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentinel&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;As part of the &lt;a href="https://en.wikipedia.org/wiki/Copernicus_Programme" rel="nofollow"&gt;EU Copernicus program&lt;/a&gt;, multiple Sentinel satellites are capturing imagery -&amp;gt; see &lt;a href="https://en.wikipedia.org/wiki/Copernicus_Programme#Sentinel_missions" rel="nofollow"&gt;wikipedia&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;13 bands, Spatial resolution of 10 m, 20 m and 60 m, 290 km swath, the temporal resolution is 5 days&lt;/li&gt;
&lt;li&gt;&lt;a href="https://console.cloud.google.com/storage/browser/gcp-public-data-sentinel-2?prefix=tiles%2F31%2FT%2FCJ%2F" rel="nofollow"&gt;Open access data on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paid access via &lt;a href="https://www.sentinel-hub.com/" rel="nofollow"&gt;sentinel-hub&lt;/a&gt; and &lt;a href="https://github.com/sentinel-hub/sentinelhub-py"&gt;python-api&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/binder-examples/getting-data/blob/master/Sentinel2.ipynb"&gt;Example loading sentinel data in a notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/so2sat" rel="nofollow"&gt;so2sat on Tensorflow datasets&lt;/a&gt; - So2Sat LCZ42 is a dataset consisting of co-registered synthetic aperture radar and multispectral optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) label. The dataset is distributed over 42 cities across different continents and cultural regions of the world.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/eurosat" rel="nofollow"&gt;eurosat&lt;/a&gt; - EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/bigearthnet" rel="nofollow"&gt;bigearthnet&lt;/a&gt; - The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. The image patch size on the ground is 1.2 x 1.2 km with variable image size depending on the channel resolution. This is a multi-label dataset with 43 imbalanced labels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-landsat" class="anchor" aria-hidden="true" href="#landsat"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Landsat&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Long running US program -&amp;gt; see &lt;a href="https://en.wikipedia.org/wiki/Landsat_program" rel="nofollow"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;8 bands, 15 to 60 meters, 185km swath, the temporal resolution is 16 days&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloud.google.com/storage/docs/public-datasets/landsat" rel="nofollow"&gt;Imagery on GCP&lt;/a&gt;, see &lt;a href="https://console.cloud.google.com/storage/browser/gcp-public-data-landsat/" rel="nofollow"&gt;the GCP bucket here&lt;/a&gt;, with imagery analysed in &lt;a href="https://github.com/pangeo-data/pangeo-example-notebooks/blob/master/landsat8-cog-ndvi.ipynb"&gt;this notebook&lt;/a&gt; on Pangeo&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-shuttle-radar-topography-mission-digital-elevation-maps" class="anchor" aria-hidden="true" href="#shuttle-radar-topography-mission-digital-elevation-maps"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Shuttle Radar Topography Mission (digital elevation maps)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://srtm.csi.cgiar.org/srtmdata/" rel="nofollow"&gt;Data - open access&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aerial-imagery-drones" class="anchor" aria-hidden="true" href="#aerial-imagery-drones"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial imagery (drones)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cvgl.stanford.edu/projects/uav_data/" rel="nofollow"&gt;Stanford Drone Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-kaggle" class="anchor" aria-hidden="true" href="#kaggle"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle&lt;/h2&gt;
&lt;p&gt;Kaggle hosts several large satellite image datasets (&lt;a href="https://www.kaggle.com/datasets?sortBy=relevance&amp;amp;group=public&amp;amp;search=image&amp;amp;page=1&amp;amp;pageSize=20&amp;amp;size=large&amp;amp;filetype=all&amp;amp;license=all" rel="nofollow"&gt;&amp;gt; 1 GB&lt;/a&gt;). A list if general image datasets is &lt;a href="https://gisgeography.com/free-satellite-imagery-data-list/" rel="nofollow"&gt;here&lt;/a&gt;. A list of land-use datasets is &lt;a href="https://gisgeography.com/free-global-land-cover-land-use-data/" rel="nofollow"&gt;here&lt;/a&gt;. The &lt;a href="http://blog.kaggle.com" rel="nofollow"&gt;kaggle blog&lt;/a&gt; is an interesting read.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---amazon-from-space---classification-challenge" class="anchor" aria-hidden="true" href="#kaggle---amazon-from-space---classification-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Amazon from space - classification challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data" rel="nofollow"&gt;https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3-5 meter resolution GeoTIFF images from planet Dove satellite constellation&lt;/li&gt;
&lt;li&gt;12 classes including - &lt;strong&gt;cloudy, primary + waterway&lt;/strong&gt; etc&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2017/10/17/planet-understanding-the-amazon-from-space-1st-place-winners-interview/" rel="nofollow"&gt;1st place winner interview - used 11 custom CNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---dstl---segmentation-challenge" class="anchor" aria-hidden="true" href="#kaggle---dstl---segmentation-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - DSTL - segmentation challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection" rel="nofollow"&gt;https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rating - medium, many good examples (see the Discussion as well as kernels), but as this competition was run a couple of years ago many examples use python 2&lt;/li&gt;
&lt;li&gt;WorldView 3 - 45 satellite images covering 1km x 1km in both 3 (i.e. RGB) and 16-band (400nm - SWIR) images&lt;/li&gt;
&lt;li&gt;10 Labelled classes include - &lt;strong&gt;Buildings, Road, Trees, Crops, Waterway, Vehicles&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kaggle.com/2017/04/26/dstl-satellite-imagery-competition-1st-place-winners-interview-kyle-lee/" rel="nofollow"&gt;Interview with 1st place winner who used segmentation networks&lt;/a&gt; - 40+ models, each tweaked for particular target (e.g. roads, trees)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/" rel="nofollow"&gt;Deepsense 4th place solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My analysis &lt;a href="https://github.com/robmarkcole/Useful-python/tree/master/Kaggle/dstl-satellite-imagery-feature-detection"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---airbus-ship-detection-challenge" class="anchor" aria-hidden="true" href="#kaggle---airbus-ship-detection-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Airbus Ship Detection Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/airbus-ship-detection/overview" rel="nofollow"&gt;https://www.kaggle.com/c/airbus-ship-detection/overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rating - medium, most solutions using deep-learning, many kernels, &lt;a href="https://www.kaggle.com/kmader/baseline-u-net-model-part-1" rel="nofollow"&gt;good example kernel&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I believe there was a problem with this dataset, which led to many complaints that the competition was ruined.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---draper---place-images-in-order-of-time" class="anchor" aria-hidden="true" href="#kaggle---draper---place-images-in-order-of-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Draper - place images in order of time&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/draper-satellite-image-chronology/data" rel="nofollow"&gt;https://www.kaggle.com/c/draper-satellite-image-chronology/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rating - hard. Not many useful kernels.&lt;/li&gt;
&lt;li&gt;Images are grouped into sets of five, each of which have the same setId. Each image in a set was taken on a different day (but not necessarily at the same time each day). The images for each set cover approximately the same area but are not exactly aligned.&lt;/li&gt;
&lt;li&gt;Kaggle interviews for entrants who &lt;a href="http://blog.kaggle.com/2016/09/15/draper-satellite-image-chronology-machine-learning-solution-vicens-gaitan/" rel="nofollow"&gt;used XGBOOST&lt;/a&gt; and a &lt;a href="http://blog.kaggle.com/2016/09/08/draper-satellite-image-chronology-damien-soukhavong/" rel="nofollow"&gt;hybrid human/ML approach&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---deepsat---classification-challenge" class="anchor" aria-hidden="true" href="#kaggle---deepsat---classification-challenge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - Deepsat - classification challenge&lt;/h3&gt;
&lt;p&gt;Not satellite but airborne imagery. Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared. The training and test labels are one-hot encoded 1x6 vectors. Each image patch is size normalized to 28x28 pixels. Data in &lt;code&gt;.mat&lt;/code&gt; Matlab format. JPEG?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://csc.lsu.edu/~saikat/deepsat/" rel="nofollow"&gt;Imagery source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/crawford/deepsat-sat4" rel="nofollow"&gt;Sat4&lt;/a&gt; 500,000 image patches covering four broad land cover classes - &lt;strong&gt;barren land, trees, grassland and a class that consists of all land cover classes other than the above three&lt;/strong&gt; &lt;a href="https://www.kaggle.com/robmarkcole/satellite-image-classification" rel="nofollow"&gt;Example notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/crawford/deepsat-sat6" rel="nofollow"&gt;Sat6&lt;/a&gt; 405,000 image patches each of size 28x28 and covering 6 landcover classes - &lt;strong&gt;barren land, trees, grassland, roads, buildings and water bodies.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://alan.do/deep-gradient-boosted-learning-4e33adaf2969" rel="nofollow"&gt;Deep Gradient Boosted Learning article&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---other" class="anchor" aria-hidden="true" href="#kaggle---other"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - other&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Satellite + loan data -&amp;gt; &lt;a href="https://www.kaggle.com/reubencpereira/spatial-data-repo" rel="nofollow"&gt;https://www.kaggle.com/reubencpereira/spatial-data-repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-alternative-datasets" class="anchor" aria-hidden="true" href="#alternative-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Alternative datasets&lt;/h2&gt;
&lt;p&gt;There are a variety of datasets suitable for land classification problems.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-datasets" class="anchor" aria-hidden="true" href="#tensorflow-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tensorflow datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There are a number of remote sensing datasets&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/resisc45" rel="nofollow"&gt;resisc45&lt;/a&gt; - RESISC45 dataset is a publicly available benchmark for Remote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/eurosat" rel="nofollow"&gt;eurosat&lt;/a&gt; - EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/bigearthnet" rel="nofollow"&gt;bigearthnet&lt;/a&gt; - The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. The image patch size on the ground is 1.2 x 1.2 km with variable image size depending on the channel resolution. This is a multi-label dataset with 43 imbalanced labels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-uc-merced" class="anchor" aria-hidden="true" href="#uc-merced"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;UC Merced&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html" rel="nofollow"&gt;http://weegee.vision.ucmerced.edu/datasets/landuse.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Available as a Tensorflow dataset -&amp;gt; &lt;a href="https://www.tensorflow.org/datasets/catalog/uc_merced" rel="nofollow"&gt;https://www.tensorflow.org/datasets/catalog/uc_merced&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;This is a 21 class land use image dataset meant for research purposes.&lt;/li&gt;
&lt;li&gt;There are 100 RGB TIFF images for each class&lt;/li&gt;
&lt;li&gt;Each image measures 256x256 pixels with a pixel resolution of 1 foot&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-aws-datasets" class="anchor" aria-hidden="true" href="#aws-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Landsat -&amp;gt; free viewer at &lt;a href="https://viewer.remotepixel.ca/#3/40/-70.5" rel="nofollow"&gt;remotepixel&lt;/a&gt; and &lt;a href="https://libra.developmentseed.org/" rel="nofollow"&gt;libra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optical, radar, segmented etc. &lt;a href="https://aws.amazon.com/earth/" rel="nofollow"&gt;https://aws.amazon.com/earth/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spacenetchallenge.github.io/datasets/datasetHomePage.html" rel="nofollow"&gt;SpaceNet - WorldView-3&lt;/a&gt; and &lt;a href="https://spark-in.me/post/spacenet-three-challenge" rel="nofollow"&gt;article here&lt;/a&gt;. Also example &lt;a href="https://docs.rastervision.io/en/0.8/quickstart.html" rel="nofollow"&gt;semantic segmentation using Raster Vision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-quilt" class="anchor" aria-hidden="true" href="#quilt"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quilt&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Several people have uploaded datasets to &lt;a href="https://quiltdata.com/search/?q=satellite" rel="nofollow"&gt;Quilt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-google-earth-engine" class="anchor" aria-hidden="true" href="#google-earth-engine"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Earth Engine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/earth-engine/" rel="nofollow"&gt;https://developers.google.com/earth-engine/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Various imagery and climate datasets, including Landsat &amp;amp; Sentinel imagery&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/earth-engine/python_install" rel="nofollow"&gt;Python API&lt;/a&gt; but  all compute happens on Googles servers&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-weather-datasets" class="anchor" aria-hidden="true" href="#weather-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Weather Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UK met-odffice -&amp;gt; &lt;a href="https://www.metoffice.gov.uk/datapoint" rel="nofollow"&gt;https://www.metoffice.gov.uk/datapoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NASA (make request and emailed when ready) -&amp;gt; &lt;a href="https://search.earthdata.nasa.gov" rel="nofollow"&gt;https://search.earthdata.nasa.gov&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NOAA (requires BigQuery) -&amp;gt; &lt;a href="https://www.kaggle.com/noaa/goes16/home" rel="nofollow"&gt;https://www.kaggle.com/noaa/goes16/home&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Time series weather data for several US cities -&amp;gt; &lt;a href="https://www.kaggle.com/selfishgene/historical-hourly-weather-data" rel="nofollow"&gt;https://www.kaggle.com/selfishgene/historical-hourly-weather-data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-online-computing-resources" class="anchor" aria-hidden="true" href="#online-computing-resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online computing resources&lt;/h1&gt;
&lt;p&gt;Generally a GPU is required for DL, and this section lists Jupyter environments with GPU available. There is a good overview of online Jupyter envs &lt;a href="https://course-v3.fast.ai/index.html" rel="nofollow"&gt;on the fast.at site&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-google-colab" class="anchor" aria-hidden="true" href="#google-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Colab&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Collaboratory &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;notebooks&lt;/a&gt; with GPU as a backend for free for 12 hours at a time. Note that the GPU may be shared with other users, so if you aren't getting good performance try reloading.&lt;/li&gt;
&lt;li&gt;Tensorflow available &amp;amp; pytorch can be installed, &lt;a href="https://towardsdatascience.com/fast-ai-lesson-1-on-google-colab-free-gpu-d2af89f53604" rel="nofollow"&gt;useful articles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-kaggle---also-google" class="anchor" aria-hidden="true" href="#kaggle---also-google"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kaggle - also Google!&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Free to use&lt;/li&gt;
&lt;li&gt;GPU Kernels - may run for 1 hour&lt;/li&gt;
&lt;li&gt;Tensorflow, pytorch &amp;amp; fast.ai available&lt;/li&gt;
&lt;li&gt;Advantage that many datasets are already available&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@hortonhearsafoo/announcing-fast-ai-part-1-now-available-as-kaggle-kernels-8ef4ca3b9ce6" rel="nofollow"&gt;Read&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-floydhub" class="anchor" aria-hidden="true" href="#floydhub"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Floydhub&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.floydhub.com/" rel="nofollow"&gt;https://www.floydhub.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pricing -&amp;gt; &lt;a href="https://www.floydhub.com/pricing" rel="nofollow"&gt;https://www.floydhub.com/pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Free plan allows 1 job and 10GB storage, but pay for GPU.&lt;/li&gt;
&lt;li&gt;Cloud GPUs (AWS backend)&lt;/li&gt;
&lt;li&gt;Tensorboard&lt;/li&gt;
&lt;li&gt;Version Control for DL&lt;/li&gt;
&lt;li&gt;Deploy Models as REST APIs&lt;/li&gt;
&lt;li&gt;Public Datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;### Clouderizer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://clouderizer.com/" rel="nofollow"&gt;https://clouderizer.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Clouderizer $5 month for 200 hours (Robbie plan)&lt;/li&gt;
&lt;li&gt;Run projects locally, on cloud or both.&lt;/li&gt;
&lt;li&gt;SSH terminal, Jupyter Notebooks and Tensorboard are securely accessible from Clouderizer Web Console.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-paperspace" class="anchor" aria-hidden="true" href="#paperspace"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Paperspace&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.paperspace.com/" rel="nofollow"&gt;https://www.paperspace.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1-Click Jupyter Notebooks, GPU on demand&lt;/li&gt;
&lt;li&gt;Pay as you go -&amp;gt; &lt;a href="https://www.paperspace.com/pricing" rel="nofollow"&gt;https://www.paperspace.com/pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Paperspace/paperspace-python"&gt;Python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-crestle" class="anchor" aria-hidden="true" href="#crestle"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Crestle&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.crestle.com/" rel="nofollow"&gt;https://www.crestle.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pricing -&amp;gt; &lt;a href="https://www.crestle.com/pricing" rel="nofollow"&gt;https://www.crestle.com/pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Min plan is $5 per month with 200 hours per month. Pay $0.59/hour for GPU and storage $0.014/GB/day&lt;/li&gt;
&lt;li&gt;Cloud GPU &amp;amp; persistent file store&lt;/li&gt;
&lt;li&gt;Fast.ai lessons pre-installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-interesting-dl-projects" class="anchor" aria-hidden="true" href="#interesting-dl-projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Interesting DL projects&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-raster-vision-by-azavea" class="anchor" aria-hidden="true" href="#raster-vision-by-azavea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Raster Vision by Azavea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.azavea.com/projects/raster-vision/" rel="nofollow"&gt;https://www.azavea.com/projects/raster-vision/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An open source Python framework for building computer vision models on aerial, satellite, and other large imagery sets.&lt;/li&gt;
&lt;li&gt;Accessible through the &lt;a href="https://www.rasterfoundry.com/" rel="nofollow"&gt;Raster Foundry&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/azavea/raster-vision-examples"&gt;Example use cases on open data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-robosat" class="anchor" aria-hidden="true" href="#robosat"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RoboSat&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mapbox/robosat"&gt;https://github.com/mapbox/robosat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generic ecosystem for feature extraction from aerial and satellite imagery.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-robosatpink" class="anchor" aria-hidden="true" href="#robosatpink"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RoboSat.Pink&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A fork of robotsat&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datapink/robosat.pink"&gt;https://github.com/datapink/robosat.pink&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deeposm" class="anchor" aria-hidden="true" href="#deeposm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepOSM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/trailbehind/DeepOSM"&gt;https://github.com/trailbehind/DeepOSM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Train a deep learning net with OpenStreetMap features and satellite imagery.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deepnetsforeo---segmentation" class="anchor" aria-hidden="true" href="#deepnetsforeo---segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepNetsForEO - segmentation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nshaud/DeepNetsForEO"&gt;https://github.com/nshaud/DeepNetsForEO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Uses SegNET for working on remote sensing images using deep learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-skynet-data" class="anchor" aria-hidden="true" href="#skynet-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Skynet-data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/developmentseed/skynet-data"&gt;https://github.com/developmentseed/skynet-data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data pipeline for machine learning with OpenStreetMap&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-production" class="anchor" aria-hidden="true" href="#production"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Production&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-custom-rest-api" class="anchor" aria-hidden="true" href="#custom-rest-api"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom REST API&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Basic &lt;a href="https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html" rel="nofollow"&gt;https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html&lt;/a&gt; with code &lt;a href="https://github.com/jrosebr1/simple-keras-rest-api"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Advanced &lt;a href="https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/" rel="nofollow"&gt;https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/galiboo/olympus"&gt;https://github.com/galiboo/olympus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-serving" class="anchor" aria-hidden="true" href="#tensorflow-serving"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tensorflow Serving&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/serving/" rel="nofollow"&gt;https://www.tensorflow.org/serving/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Official version is python 2 but python 3 build &lt;a href="https://github.com/illagrenan/tensorflow-serving-api-python3"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Another approach is &lt;a href="https://www.tensorflow.org/serving/docker" rel="nofollow"&gt;to use Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. Multiple models, or indeed multiple versions of the same model, can be served simultaneously.  TensorFlow Serving comes with a scheduler that groups individual inference requests into batches for joint execution on a GPU&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-floydhub-1" class="anchor" aria-hidden="true" href="#floydhub-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Floydhub&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Allows exposing model via rest API&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-modeldepot" class="anchor" aria-hidden="true" href="#modeldepot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;modeldepot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://modeldepot.io" rel="nofollow"&gt;https://modeldepot.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ML models hosted&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-image-formats--catalogues" class="anchor" aria-hidden="true" href="#image-formats--catalogues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image formats &amp;amp; catalogues&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We certainly want to consider cloud optimised GeoTiffs &lt;a href="https://www.cogeo.org/" rel="nofollow"&gt;https://www.cogeo.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://terria.io/" rel="nofollow"&gt;https://terria.io/&lt;/a&gt; for pretty catalogues&lt;/li&gt;
&lt;li&gt;&lt;a href="https://remotepixel.ca/projects/index.html#satsearch" rel="nofollow"&gt;Remote pixel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://apps.sentinel-hub.com/eo-browser/" rel="nofollow"&gt;Sentinel-hub eo-browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Large datasets may come in HDF5 format, can view with -&amp;gt; &lt;a href="https://www.hdfgroup.org/downloads/hdfview/" rel="nofollow"&gt;https://www.hdfgroup.org/downloads/hdfview/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Climate data is often in netcdf format, which can be &lt;a href="https://moderndata.plot.ly/weather-maps-in-python-with-mapbox-gl-xarray-and-netcdf4/" rel="nofollow"&gt;opened using xarray&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The xarray docs list a number of ways that data &lt;a href="http://xarray.pydata.org/en/latest/io.html#" rel="nofollow"&gt;can be stored and loaded&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stac---spatiotemporal-asset-catalog" class="anchor" aria-hidden="true" href="#stac---spatiotemporal-asset-catalog"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;STAC - SpatioTemporal Asset Catalog&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Specification describing the layout of a catalogue comprising of static files. The aim is that the catalogue is crawlable so it can be indexed by a search engine and make imagery discoverable, without requiring yet another API interface.&lt;/li&gt;
&lt;li&gt;An initiative of &lt;a href="https://www.radiant.earth/" rel="nofollow"&gt;https://www.radiant.earth/&lt;/a&gt; in particular &lt;a href="https://github.com/cholmes"&gt;https://github.com/cholmes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spec at &lt;a href="https://github.com/radiantearth/stac-spec"&gt;https://github.com/radiantearth/stac-spec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browser at &lt;a href="https://github.com/radiantearth/stac-browser"&gt;https://github.com/radiantearth/stac-browser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Talk at &lt;a href="https://docs.google.com/presentation/d/1O6W0lMeXyUtPLl-k30WPJIyH1ecqrcWk29Np3bi6rl0/edit#slide=id.p" rel="nofollow"&gt;https://docs.google.com/presentation/d/1O6W0lMeXyUtPLl-k30WPJIyH1ecqrcWk29Np3bi6rl0/edit#slide=id.p&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example catalogue at &lt;a href="https://landsat-stac.s3.amazonaws.com/catalog.json" rel="nofollow"&gt;https://landsat-stac.s3.amazonaws.com/catalog.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chat &lt;a href="https://gitter.im/SpatioTemporal-Asset-Catalog/Lobby" rel="nofollow"&gt;https://gitter.im/SpatioTemporal-Asset-Catalog/Lobby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Several useful repos on &lt;a href="https://github.com/sat-utils"&gt;https://github.com/sat-utils&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-state-of-the-art" class="anchor" aria-hidden="true" href="#state-of-the-art"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;State of the art&lt;/h1&gt;
&lt;p&gt;What are companies doing?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overall trend to using AWS S3 backend for image storage. There are a variety of tools for exploring and having teams collaborate on data on S3, e.g. &lt;a href="https://github.com/quiltdata/t4"&gt;T4&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Bucking the trend, &lt;a href="https://spacenews.com/descartes-labs-platform-adds-airbus-imagery/" rel="nofollow"&gt;Descartes &amp;amp; Airbus are using a google backend&lt;/a&gt; -&amp;gt; checkout &lt;a href="https://github.com/dask/gcsfs"&gt;gcsts for google cloud storage sile-system&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Just speculating, but a &lt;a href="https://github.com/aws-samples/amazon-rekognition-video-analyzer"&gt;serverless pipeline&lt;/a&gt; appears to be where companies are headed for routine compute tasks, whilst providing a Jupyter notebook approach for custom analysis.&lt;/li&gt;
&lt;li&gt;Traditional data formats aren't designed for processing, so new standards are developing such as &lt;a href="http://blog.digitalglobe.com/developers/cloud-optimized-geotiffs-and-the-path-to-accessible-satellite-imagery-analytics/" rel="nofollow"&gt;cloud optimised geotiffs&lt;/a&gt; and &lt;a href="https://github.com/zarr-developers/zarr"&gt;zarr&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-online-platforms-for-geo-analysis" class="anchor" aria-hidden="true" href="#online-platforms-for-geo-analysis"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online platforms for Geo analysis&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/pangeo/cloud-native-geoprocessing-of-earth-observation-satellite-data-with-pangeo-997692d91ca2" rel="nofollow"&gt;This article discusses some of the available platforms&lt;/a&gt; -&amp;gt; TLDR Pangeo rocks, but must BYO imagery&lt;/li&gt;
&lt;li&gt;Pangeo - open source resources for parallel processing using Dask and Xarray &lt;a href="http://pangeo.io/index.html" rel="nofollow"&gt;http://pangeo.io/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://sandbox.intelligence-airbusds.com/web/" rel="nofollow"&gt;Airbus Sandbox&lt;/a&gt; -&amp;gt; will provide access to imagery&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.descarteslabs.com/" rel="nofollow"&gt;Descartes Labs&lt;/a&gt; -&amp;gt; access to EO imagery from a variety of providers via python API -&amp;gt; not clear which imagery is available (Airbus + others?) or pricing&lt;/li&gt;
&lt;li&gt;DigitalGlobe have a cloud hosted Jupyter notebook platform called &lt;a href="https://platform.digitalglobe.com/gbdx/" rel="nofollow"&gt;GBDX&lt;/a&gt;. Cloud hosting means they can guarantee the infrastructure supports their algorithms, and they appear to be close/closer to deploying DL. &lt;a href="https://notebooks.geobigdata.io/hub/tutorials/list" rel="nofollow"&gt;Tutorial notebooks here&lt;/a&gt;. Only Sentinel-2 and Landsat data on free tier.&lt;/li&gt;
&lt;li&gt;Planet have a &lt;a href="https://developers.planet.com/" rel="nofollow"&gt;Jupyter notebook platform&lt;/a&gt; which can be deployed locally and requires an &lt;a href="https://developers.planet.com/docs/quickstart/getting-started/" rel="nofollow"&gt;API key&lt;/a&gt; (14 days free). They have a python wrapper (2.7..) to their rest API. No price after 14 day trial.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-techniques" class="anchor" aria-hidden="true" href="#techniques"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Techniques&lt;/h1&gt;
&lt;p&gt;This section explores the different techniques (DL, ML &amp;amp; classical) people are applying to common problems in satellite imagery analysis. Classification problems are the most simply addressed via DL, object detection is harder, and cloud detection harder still (niche interest).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-land-classification" class="anchor" aria-hidden="true" href="#land-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Land classification&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Very common problem, assign land classification to a pixel based on pixel value, can be addressed via &lt;a href="https://github.com/acgeospatial/Satellite_Imagery_Python/blob/master/Clustering_KMeans-Sentinel2.ipynb"&gt;simple sklearn cluster algorithm&lt;/a&gt; or &lt;a href="https://towardsdatascience.com/land-use-land-cover-classification-with-deep-learning-9a5041095ddb" rel="nofollow"&gt;deep learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Land use is related to classification, but we are trying to detect a scene, e.g. housing, forestry. I have tried CNN -&amp;gt; &lt;a href="https://github.com/robmarkcole/satellite-image-deep-learning/tree/master/land_classification"&gt;See my notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/tavgreen/landuse_classification"&gt;Land Use Classification using Convolutional Neural Network in Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1709.00201.pdf" rel="nofollow"&gt;Sea-Land segmentation using DL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Azure/pixel_level_land_classification"&gt;Pixel level segmentation on Azure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hantek/deeplearn_hsi"&gt;Deep Learning-Based Classification of Hyperspectral Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rogerxujiang/dstl_unet"&gt;A U-net based on Tensorflow for objection detection (or segmentation) of satellite images - DSTL dataset but python 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/whats-growing-there-a5618a2e6933" rel="nofollow"&gt;What’s growing there? Using eo-learn and fastai to identify crops from multi-spectral remote sensing data (Sentinel 2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-semantic-segmentation" class="anchor" aria-hidden="true" href="#semantic-segmentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semantic segmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pixel-wise classification&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/matterport/Mask_RCNN"&gt;Instance segmentation with keras - links to satellite examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-change-detection" class="anchor" aria-hidden="true" href="#change-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Monitor water levels, coast lines, size of urban areas, wildfire damage. Note, clouds change often too..!&lt;/li&gt;
&lt;li&gt;Using PCA (python 2, requires updating) -&amp;gt; &lt;a href="https://appliedmachinelearning.blog/2017/11/25/unsupervised-changed-detection-in-multi-temporal-satellite-images-using-pca-k-means-python-code/" rel="nofollow"&gt;https://appliedmachinelearning.blog/2017/11/25/unsupervised-changed-detection-in-multi-temporal-satellite-images-using-pca-k-means-python-code/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Using CNN -&amp;gt; &lt;a href="https://github.com/vbhavank/Unstructured-change-detection-using-CNN"&gt;https://github.com/vbhavank/Unstructured-change-detection-using-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vbhavank/Siamese-neural-network-for-change-detection"&gt;Siamese neural network to detect changes in aerial images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.spaceknow.com/" rel="nofollow"&gt;https://www.spaceknow.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/agr-ayush/Landsat-Time-Series-Analysis-for-Multi-Temporal-Land-Cover-Classification"&gt;LANDSAT Time Series Analysis for Multi-temporal Land Cover Classification using Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.planet.com/pulse/publications/change-detection-in-3d-generating-digital-elevation-models-from-dove-imagery/" rel="nofollow"&gt;Change Detection in 3D: Generating Digital Elevation Models from Dove Imagery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mdpi.com/2072-4292/10/11/1827" rel="nofollow"&gt;Change Detection in Hyperspectral Images Using Recurrent 3D Fully Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hfattahi/PySAR"&gt;PySAR - InSAR (Interferometric Synthetic Aperture Radar) timeseries analysis in python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-image-registration" class="anchor" aria-hidden="true" href="#image-registration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Image registration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Image_registration" rel="nofollow"&gt;Wikipedia article on registration&lt;/a&gt; -&amp;gt; register for change detection or &lt;a href="https://mono.software/2018/03/14/Image-stitching/" rel="nofollow"&gt;image stitching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Traditional approach -&amp;gt; define control points, employ RANSAC algorithm&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_correlation" rel="nofollow"&gt;Phase correlation&lt;/a&gt; used to estimate the translation between two images with sub-pixel accuracy, useful for &lt;a href="https://onlinelibrary.wiley.com/doi/10.1002/9781118724194.ch11" rel="nofollow"&gt;allows accurate registration of low resolution imagery onto high resolution imagery&lt;/a&gt;, or register a &lt;a href="https://www.mathworks.com/help/images/registering-an-image-using-normalized-cross-correlation.html" rel="nofollow"&gt;sub-image on a full image&lt;/a&gt; -&amp;gt; Unlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects. &lt;a href="https://github.com/JamieTurrin/Phase-Correlation"&gt;Applied to Landsat images here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-object-detection" class="anchor" aria-hidden="true" href="#object-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Object detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A typical task is detecting boats on the ocean, which should be simpler than land based challenges owing to blank background in images, but is still challenging and no convincing robust solutions available.&lt;/li&gt;
&lt;li&gt;Intro articles &lt;a href="https://medium.com/earthcube-stories/how-hard-it-is-for-an-ai-to-detect-ships-on-satellite-images-7265e34aadf0" rel="nofollow"&gt;here&lt;/a&gt; and &lt;a href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gbdxstories.digitalglobe.com/boats/" rel="nofollow"&gt;DigitalGlobe article&lt;/a&gt; - they use a combination classical techniques (masks, erodes) to reduce the search space (identifying water via &lt;a href="https://en.wikipedia.org/wiki/Normalized_difference_water_index" rel="nofollow"&gt;NDWI&lt;/a&gt; which requires SWIR) then apply a binary DL classifier on candidate regions of interest. They deploy the final algo &lt;a href="https://github.com/platformstories/boat-detector"&gt;as a task&lt;/a&gt; on their GBDX platform. They propose that in the future an R-CNN may be suitable for the whole process.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/planetlabs/notebooks/blob/master/jupyter-notebooks/ship-detector/01_ship_detector.ipynb"&gt;Planet use non DL felzenszwalb algorithm to detect ships&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/kmader/synthetic-word-ocr/kernels" rel="nofollow"&gt;Segmentation of buildings on kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jyamaoka/LandUse"&gt;Identifying Buildings in Satellite Images with Machine Learning and Quilt&lt;/a&gt; -&amp;gt; NDVI &amp;amp; edge detection via gaussian blur as features, fed to TPOT for training with labels from OpenStreetMap, modelled as a two class problem, “Buildings” and “Nature”.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/" rel="nofollow"&gt;Deep learning for satellite imagery via image segmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/the-downlinq/building-extraction-with-yolt2-and-spacenet-data-a926f9ffac4f" rel="nofollow"&gt;Building Extraction with YOLT2 and SpaceNet Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jremillard/images-to-osm"&gt;Find sports fields using Mask R-CNN and overlay on open-street-map&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-cloud-detection" class="anchor" aria-hidden="true" href="#cloud-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cloud detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A subset of the object detection problem, but surprisingly challenging&lt;/li&gt;
&lt;li&gt;From &lt;a href="https://medium.com/sentinel-hub/improving-cloud-detection-with-machine-learning-c09dc5d7cf13" rel="nofollow"&gt;this article on sentinelhub&lt;/a&gt; there are three popular classical algorithms that detects thresholds in multiple bands in order to identify clouds. In the same article they propose using semantic segmentation combined with a CNN for a cloud classifier (excellent review paper &lt;a href="https://arxiv.org/pdf/1704.06857.pdf" rel="nofollow"&gt;here&lt;/a&gt;), but state that this requires too much compute resources.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mdpi.com/2072-4292/8/8/666" rel="nofollow"&gt;This article&lt;/a&gt; compares a number of ML algorithms, random forests, stochastic gradient descent, support vector machines, Bayesian method.&lt;/li&gt;
&lt;li&gt;DL..&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-super-resolution" class="anchor" aria-hidden="true" href="#super-resolution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Super resolution&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/the-downlinq/super-resolution-on-satellite-imagery-using-deep-learning-part-1-ec5c5cd3cd2" rel="nofollow"&gt;https://medium.com/the-downlinq/super-resolution-on-satellite-imagery-using-deep-learning-part-1-ec5c5cd3cd2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://modeldepot.io/joe/vdsr-for-super-resolution" rel="nofollow"&gt;https://modeldepot.io/joe/vdsr-for-super-resolution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pansharpening" class="anchor" aria-hidden="true" href="#pansharpening"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pansharpening&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Image fusion of low res multispectral with high res pan band. Several algorithms described &lt;a href="http://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/fundamentals-of-panchromatic-sharpening.htm" rel="nofollow"&gt;in the ArcGIS docs&lt;/a&gt;, with the simplest being taking the mean of the pan and RGB pixel value.&lt;/li&gt;
&lt;li&gt;Does not require DL, classical algos suffice, &lt;a href="http://nbviewer.jupyter.org/github/HyperionAnalytics/PyDataNYC2014/blob/master/panchromatic_sharpening.ipynb" rel="nofollow"&gt;see this notebook&lt;/a&gt; and &lt;a href="https://www.kaggle.com/resolut/panchromatic-sharpening" rel="nofollow"&gt;this kaggle kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mapbox/rio-pansharpen"&gt;https://github.com/mapbox/rio-pansharpen&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-stereo-imaging-for-terrain-mapping--dems" class="anchor" aria-hidden="true" href="#stereo-imaging-for-terrain-mapping--dems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stereo imaging for terrain mapping &amp;amp; DEMs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Digital_elevation_model" rel="nofollow"&gt;Wikipedia DEM article&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Phase_correlation" rel="nofollow"&gt;phase correlation&lt;/a&gt; article&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/depth-from-stereo.md"&gt;Intro to depth from stereo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Map terrain from stereo images to produce a digital elevation model (DEM) -&amp;gt; high resolution &amp;amp; paired images required, typically 0.3 m, e.g. &lt;a href="https://dg-cms-uploads-production.s3.amazonaws.com/uploads/document/file/37/DG-WV2ELEVACCRCY-WP.pdf" rel="nofollow"&gt;Worldview&lt;/a&gt; or &lt;a href="https://www.pobonline.com/articles/100233-when-is-satellite-stereo-imagery-the-best-option-for-3d-modeling" rel="nofollow"&gt;GeoEye&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Process of creating a DEM &lt;a href="https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B1/327/2016/isprs-archives-XLI-B1-327-2016.pdf" rel="nofollow"&gt;here&lt;/a&gt; and &lt;a href="https://www.geoimage.com.au/media/brochure_pdfs/Geoimage_DEM_brochure_Oct10_LR.pdf" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pro.arcgis.com/en/pro-app/help/data/imagery/generate-elevation-data-using-the-dems-wizard.htm" rel="nofollow"&gt;ArcGIS can generate DEMs from stereo images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MISS3D/s2p"&gt;https://github.com/MISS3D/s2p&lt;/a&gt; -&amp;gt; produces elevation models from images taken by high resolution optical satellites -&amp;gt; demo code on &lt;a href="https://gfacciol.github.io/IS18/" rel="nofollow"&gt;https://gfacciol.github.io/IS18/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dev.ipol.im/~facciolo/pub/CVPRW2017.pdf" rel="nofollow"&gt;Automatic 3D Reconstruction from Multi-Date Satellite Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Seki_SGM-Nets_Semi-Global_Matching_CVPR_2017_paper.pdf" rel="nofollow"&gt;Semi-global matching with neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/geohackweek/glacierhack_2018"&gt;Predict the fate of glaciers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mrharicot/monodepth"&gt;monodepth - Unsupervised single image depth prediction with CNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jzbontar/mc-cnn"&gt;Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/giswqs/lidar"&gt;Terrain and hydrological analysis based on LiDAR-derived digital elevation models (DEM) - Python package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-image.org/docs/dev/auto_examples/transform/plot_register_translation.html?highlight=cross%20correlation" rel="nofollow"&gt;Phase correlation in scikit-image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-lidar" class="anchor" aria-hidden="true" href="#lidar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lidar&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/geoai/reconstructing-3d-buildings-from-aerial-lidar-with-ai-details-6a81cb3079c0" rel="nofollow"&gt;Reconstructing 3D buildings from aerial LiDAR with Mask R-CNN)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nvdi---vegetation-index" class="anchor" aria-hidden="true" href="#nvdi---vegetation-index"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVDI - vegetation index&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Simple band math &lt;code&gt;ndvi = np.true_divide((ir - r), (ir + r))&lt;/code&gt; but challenging due to the size of the imagery.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/HyperionAnalytics/PyDataNYC2014/blob/master/ndvi_calculation.ipynb" rel="nofollow"&gt;Example notebook local&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pangeo-data/pangeo-example-notebooks/blob/master/landsat8-cog-ndvi.ipynb"&gt;Landsat data in cloud optimised format analysed for NVDI&lt;/a&gt; with &lt;a href="https://medium.com/pangeo/cloud-native-geoprocessing-of-earth-observation-satellite-data-with-pangeo-997692d91ca2" rel="nofollow"&gt;medium article here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-sar" class="anchor" aria-hidden="true" href="#sar"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SAR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/upstream/denoising-sentinel-1-radar-images-5f764faffb3e" rel="nofollow"&gt;Removing speckle noise from Sentinel-1 SAR using a CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A dataset which is specifically made for deep learning on SAR and optical imagery is the SEN1-2 dataset, which contains corresponding patch pairs of Sentinel 1 (VV) and 2 (RGB) data. It is the largest manually curated dataset of S1 and S2 products, with corresponding labels for land use/land cover mapping, SAR-optical fusion, segmentation and classification tasks. Paper: &lt;a href="https://elib.dlr.de/128117/1/SEN12MS_Preprint.pdf" rel="nofollow"&gt;https://elib.dlr.de/128117/1/SEN12MS_Preprint.pdf&lt;/a&gt; Data: &lt;a href="https://mediatum.ub.tum.de/1474000" rel="nofollow"&gt;https://mediatum.ub.tum.de/1474000&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/datasets/catalog/so2sat" rel="nofollow"&gt;so2sat on Tensorflow datasets&lt;/a&gt; - So2Sat LCZ42 is a dataset consisting of co-registered synthetic aperture radar and multispectral optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) label. The dataset is distributed over 42 cities across different continents and cultural regions of the world.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019JB017519" rel="nofollow"&gt;Using Machine Learning to Automatically Detect Volcanic Unrest in a Time Series of Interferograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-aerial-imagery-drones-1" class="anchor" aria-hidden="true" href="#aerial-imagery-drones-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Aerial imagery (drones)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/pedestrian-detection-in-aerial-images-using-retinanet-9053e8a72c6" rel="nofollow"&gt;RetinaNet for pedestrian detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-for-fun" class="anchor" aria-hidden="true" href="#for-fun"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;For fun&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/jacquestardie/6227891818625e4c19c1b1d5bebe4fe4"&gt;Style transfer - see the world in a new way&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-useful-open-source-software" class="anchor" aria-hidden="true" href="#useful-open-source-software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful open source software&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://qgis.org/en/site/" rel="nofollow"&gt;QGIS&lt;/a&gt;- Create, edit, visualise, analyse and publish geospatial information. &lt;a href="https://docs.qgis.org/testing/en/docs/pyqgis_developer_cookbook/intro.html#scripting-in-the-python-console" rel="nofollow"&gt;Python scripting and plugins&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.orfeo-toolbox.org/" rel="nofollow"&gt;Orfeo toolbox&lt;/a&gt; - remote sensing toolbox with python API (just a wrapper to the C code). Do activites such as &lt;a href="https://www.orfeo-toolbox.org/CookBook/Applications/app_Pansharpening.html" rel="nofollow"&gt;pansharpening&lt;/a&gt;, ortho-rectification, image registration, image segmentation &amp;amp; classification. Not much documentation.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://appliedimagery.com/download/" rel="nofollow"&gt;QUICK TERRAIN READER - view DEMS, Windows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-useful-github-repos" class="anchor" aria-hidden="true" href="#useful-github-repos"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful github repos&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/sshuair/torchvision-enhance"&gt;torchvision-enhance&lt;/a&gt; -&amp;gt; Enhance PyTorch vision for semantic segmentation, multi-channel images and TIF file,...&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sshuair/dl-satellite-docker"&gt;dl-satellite-docker&lt;/a&gt; -&amp;gt; docker files for geospatial analysis, including tensorflow, pytorch, gdal, xgboost...&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-useful-references" class="anchor" aria-hidden="true" href="#useful-references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0" rel="nofollow"&gt;https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/taspinar/sidl/blob/master/notebooks/2_Detecting_road_and_roadtypes_in_sattelite_images.ipynb"&gt;https://github.com/taspinar/sidl/blob/master/notebooks/2_Detecting_road_and_roadtypes_in_sattelite_images.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenGeoscience/geonotebook"&gt;Geonotebooks&lt;/a&gt; with &lt;a href="https://github.com/OpenGeoscience/geonotebook/tree/master/devops/docker"&gt;Docker container&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/acgeospatial/Sentinel-5P/blob/master/Sentinel_5P.ipynb"&gt;Sentinel NetCDF data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open Data Cube - serve up cubes of data &lt;a href="https://www.opendatacube.org/" rel="nofollow"&gt;https://www.opendatacube.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/RemotePixel/remotepixel-api"&gt;Process Satellite data using AWS Lambda functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenDroneMap/ODM"&gt;OpenDroneMap&lt;/a&gt; - generate maps, point clouds, 3D models and DEMs from drone, balloon or kite images.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content--support-this-work" class="anchor" aria-hidden="true" href="#-support-this-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt; Support this work&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/sponsors/robmarkcole"&gt;https://github.com/sponsors/robmarkcole&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you or your business find this work useful please consider becoming a sponsor at the link above, this really helps justify the time I invest in maintaining this repo. As we say in England, 'every little helps' - thanks in advance!&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>robmarkcole</author><guid isPermaLink="false">https://github.com/robmarkcole/satellite-image-deep-learning</guid><pubDate>Mon, 10 Feb 2020 00:12:00 GMT</pubDate></item><item><title>udacity/deep-learning-v2-pytorch #13 in Jupyter Notebook, Today</title><link>https://github.com/udacity/deep-learning-v2-pytorch</link><description>&lt;p&gt;&lt;i&gt;Projects and exercises for the latest Deep Learning ND program https://www.udacity.com/course/deep-learning-nanodegree--nd101&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-pytorch" class="anchor" aria-hidden="true" href="#deep-learning-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Learning (PyTorch)&lt;/h1&gt;
&lt;p&gt;This repository contains material related to Udacity's &lt;a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="nofollow"&gt;Deep Learning Nanodegree program&lt;/a&gt;. It consists of a bunch of tutorial notebooks for various deep learning topics. In most cases, the notebooks lead you through implementing models such as convolutional networks, recurrent networks, and GANs. There are other topics covered such as weight initialization and batch normalization.&lt;/p&gt;
&lt;p&gt;There are also notebooks used as projects for the Nanodegree program. In the program itself, the projects are reviewed by real people (Udacity reviewers), but the starting code is available here, as well.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table Of Contents&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-tutorials" class="anchor" aria-hidden="true" href="#tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-introduction-to-neural-networks" class="anchor" aria-hidden="true" href="#introduction-to-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction to Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks"&gt;Introduction to Neural Networks&lt;/a&gt;: Learn how to implement gradient descent and apply it to predicting patterns in student admissions data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-analysis-network"&gt;Sentiment Analysis with NumPy&lt;/a&gt;: &lt;a href="http://iamtrask.github.io/" rel="nofollow"&gt;Andrew Trask&lt;/a&gt; leads you through building a sentiment analysis model, predicting if some text is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch"&gt;Introduction to PyTorch&lt;/a&gt;: Learn how to build neural networks in PyTorch and use pre-trained networks for state-of-the-art image classifiers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-convolutional-neural-networks" class="anchor" aria-hidden="true" href="#convolutional-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/convolutional-neural-networks"&gt;Convolutional Neural Networks&lt;/a&gt;: Visualize the output of layers that make up a CNN. Learn how to define and train a CNN for classifying &lt;a href="https://en.wikipedia.org/wiki/MNIST_database" rel="nofollow"&gt;MNIST data&lt;/a&gt;, a handwritten digit database that is notorious in the fields of machine and deep learning. Also, define and train a CNN for classifying images in the &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="nofollow"&gt;CIFAR10 dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/transfer-learning"&gt;Transfer Learning&lt;/a&gt;. In practice, most people don't train their own networks on huge datasets; they use &lt;strong&gt;pre-trained&lt;/strong&gt; networks such as VGGnet. Here you'll use VGGnet to help classify images of flowers without training an end-to-end network from scratch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/weight-initialization"&gt;Weight Initialization&lt;/a&gt;: Explore how initializing network weights affects performance.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/autoencoder"&gt;Autoencoders&lt;/a&gt;: Build models for image compression and de-noising, using feedforward and convolutional networks in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/style-transfer"&gt;Style Transfer&lt;/a&gt;: Extract style and content features from images, using a pre-trained network. Implement style transfer according to the paper, &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="nofollow"&gt;Image Style Transfer Using Convolutional Neural Networks&lt;/a&gt; by Gatys et. al. Define appropriate losses for iteratively creating a target, style-transferred image of your own design!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recurrent-neural-networks" class="anchor" aria-hidden="true" href="#recurrent-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/recurrent-neural-networks"&gt;Intro to Recurrent Networks (Time series &amp;amp; Character-level RNN)&lt;/a&gt;: Recurrent neural networks are able to use information about the sequence of data, such as the sequence of characters in text; learn how to implement these in PyTorch for a variety of tasks.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/word2vec-embeddings"&gt;Embeddings (Word2Vec)&lt;/a&gt;: Implement the Word2Vec model to find semantic representations of words for use in natural language processing.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn"&gt;Sentiment Analysis RNN&lt;/a&gt;: Implement a recurrent neural network that can predict if the text of a moview review is positive or negative.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/attention"&gt;Attention&lt;/a&gt;: Implement attention and apply it to annotation vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-generative-adversarial-networks" class="anchor" aria-hidden="true" href="#generative-adversarial-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Generative Adversarial Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/gan-mnist"&gt;Generative Adversarial Network on MNIST&lt;/a&gt;: Train a simple generative adversarial network on the MNIST dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/batch-norm"&gt;Batch Normalization&lt;/a&gt;: Learn how to improve training rates and network stability with batch normalizations.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/dcgan-svhn"&gt;Deep Convolutional GAN (DCGAN)&lt;/a&gt;: Implement a DCGAN to generate new images based on the Street View House Numbers (SVHN) dataset.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/cycle-gan"&gt;CycleGAN&lt;/a&gt;: Implement a CycleGAN that is designed to learn from unpaired and unlabeled data; use trained generators to transform images from summer to winter and vice versa.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-deploying-a-model-with-aws-sagemaker" class="anchor" aria-hidden="true" href="#deploying-a-model-with-aws-sagemaker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploying a Model (with AWS SageMaker)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/sagemaker-deployment"&gt;All exercise and project notebooks&lt;/a&gt; for the lessons on model deployment can be found in the linked, Github repo. Learn to deploy pre-trained models using AWS SageMaker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-bikesharing"&gt;Predicting Bike-Sharing Patterns&lt;/a&gt;: Implement a neural network in NumPy to predict bike rentals.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-dog-classification"&gt;Dog Breed Classifier&lt;/a&gt;: Build a convolutional neural network with PyTorch to classify any image (even an image of a face) as a specific dog breed.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-tv-script-generation"&gt;TV Script Generation&lt;/a&gt;: Train a recurrent neural network to generate scripts in the style of dialogue from Seinfeld.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-face-generation"&gt;Face Generation&lt;/a&gt;: Use a DCGAN on the CelebA dataset to generate images of new and realistic human faces.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-elective-material" class="anchor" aria-hidden="true" href="#elective-material"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Elective Material&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/tensorflow/intro-to-tensorflow"&gt;Intro to TensorFlow&lt;/a&gt;: Starting building neural networks with TensorFlow.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/keras"&gt;Keras&lt;/a&gt;: Learn to build neural networks and convolutional neural networks with Keras.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-configure-and-manage-your-environment-with-anaconda" class="anchor" aria-hidden="true" href="#configure-and-manage-your-environment-with-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure and Manage Your Environment with Anaconda&lt;/h2&gt;
&lt;p&gt;Per the Anaconda &lt;a href="http://conda.pydata.org/docs" rel="nofollow"&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conda is an open source package management system and environment management system
for installing multiple versions of software packages and their dependencies and
switching easily between them. It works on Linux, OS X and Windows, and was created
for Python programs but can package and distribute any software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Using Anaconda consists of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt; on your computer, by selecting the latest Python version for your operating system. If you already have &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;miniconda&lt;/code&gt; installed, you should be able to skip this step and move on to step 2.&lt;/li&gt;
&lt;li&gt;Create and activate * a new &lt;code&gt;conda&lt;/code&gt; &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;environment&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;* Each time you wish to work on any exercises, activate your &lt;code&gt;conda&lt;/code&gt; environment!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-1-installation" class="anchor" aria-hidden="true" href="#1-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the latest version of &lt;code&gt;miniconda&lt;/code&gt; that matches your system.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Linux&lt;/th&gt;
&lt;th&gt;Mac&lt;/th&gt;
&lt;th&gt;Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe" rel="nofollow"&gt;64-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh" rel="nofollow"&gt;32-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe" rel="nofollow"&gt;32-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt; on your machine. Detailed instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-create-and-activate-the-environment" class="anchor" aria-hidden="true" href="#2-create-and-activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Create and Activate the Environment&lt;/h2&gt;
&lt;p&gt;For Windows users, these following commands need to be executed from the &lt;strong&gt;Anaconda prompt&lt;/strong&gt; as opposed to a Windows terminal window. For Mac, a normal terminal window will work.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-git-and-version-control" class="anchor" aria-hidden="true" href="#git-and-version-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git and version control&lt;/h4&gt;
&lt;p&gt;These instructions also assume you have &lt;code&gt;git&lt;/code&gt; installed for working with Github from a terminal window, but if you do not, you can download that first with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you'd like to learn more about version control and using &lt;code&gt;git&lt;/code&gt; from the command line, take a look at our &lt;a href="https://www.udacity.com/course/version-control-with-git--ud123" rel="nofollow"&gt;free course: Version Control with Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, we're ready to create our local environment!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository, and navigate to the downloaded folder. This may take a minute or two to clone due to the included image data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/udacity/deep-learning-v2-pytorch.git
cd deep-learning-v2-pytorch
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Create (and activate) a new environment, named &lt;code&gt;deep-learning&lt;/code&gt; with Python 3.6. If prompted to proceed with the install &lt;code&gt;(Proceed [y]/n)&lt;/code&gt; type y.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create -n deep-learning python=3.6
source activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create --name deep-learning python=3.6
activate deep-learning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point your command line should look something like: &lt;code&gt;(deep-learning) &amp;lt;User&amp;gt;:deep-learning-v2-pytorch &amp;lt;user&amp;gt;$&lt;/code&gt;. The &lt;code&gt;(deep-learning)&lt;/code&gt; indicates that your environment has been activated, and you can proceed with further package installations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install PyTorch and torchvision; this should install the latest version of PyTorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch -c pytorch
pip install torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install a few required pip packages, which are specified in the requirements text file (including OpenCV).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="7"&gt;
&lt;li&gt;That's it!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now most of the &lt;code&gt;deep-learning&lt;/code&gt; libraries are available to you. Very occasionally, you will see a repository with an addition requirements file, which exists should you want to use TensorFlow and Keras, for example. In this case, you're encouraged to install another library to your existing environment, or create a new environment for a specific project.&lt;/p&gt;
&lt;p&gt;Now, assuming your &lt;code&gt;deep-learning&lt;/code&gt; environment is still activated, you can navigate to the main repo and start looking at the notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd
cd deep-learning-v2-pytorch
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit the environment when you have completed your work session, simply close the terminal window.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>udacity</author><guid isPermaLink="false">https://github.com/udacity/deep-learning-v2-pytorch</guid><pubDate>Mon, 10 Feb 2020 00:13:00 GMT</pubDate></item><item><title>Varal7/ml-tutorial #14 in Jupyter Notebook, Today</title><link>https://github.com/Varal7/ml-tutorial</link><description>&lt;p&gt;&lt;i&gt;Introduction to ML packages for the 6.86x course&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This repo does not have a README.&lt;/i&gt;&lt;/p&gt;</description><author>Varal7</author><guid isPermaLink="false">https://github.com/Varal7/ml-tutorial</guid><pubDate>Mon, 10 Feb 2020 00:14:00 GMT</pubDate></item><item><title>yandexdataschool/Practical_DL #15 in Jupyter Notebook, Today</title><link>https://github.com/yandexdataschool/Practical_DL</link><description>&lt;p&gt;&lt;i&gt;DL course co-developed by YSDA, HSE and Skoltech&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deep-learning-course" class="anchor" aria-hidden="true" href="#deep-learning-course"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep learning course&lt;/h1&gt;
&lt;p&gt;This repo supplements Deep Learning course taught at YSDA and HSE @fall'19. &lt;em&gt;For previous iteration visit the &lt;a href="https://github.com/yandexdataschool/Practical_DL/tree/spring19"&gt;spring19 branch&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Lecture and seminar materials for each week are in ./week* folders. Homeworks are in ./homework* folders.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-general-info" class="anchor" aria-hidden="true" href="#general-info"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;General info&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Create cloud jupyter session from this repo - &lt;a href="https://mybinder.org/v2/gh/yandexdataschool/Practical_DL/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/24c94be25a8a8b5703a34466825bbfdd6147d9d0/68747470733a2f2f6d7962696e6465722e6f72672f62616467652e737667" alt="Binder" data-canonical-src="https://mybinder.org/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Telegram &lt;a href="https://t.me/hsedeeplearning2019" rel="nofollow"&gt;chat room&lt;/a&gt; (russian).&lt;/li&gt;
&lt;li&gt;Deadlines &amp;amp; grading rules can be found at &lt;a href="https://github.com/yandexdataschool/Practical_DL/wiki/Homeworks-and-grading-(HSE)"&gt;this page&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Any technical issues, ideas, bugs in course materials, contribution ideas - add an &lt;a href="https://github.com/yandexdataschool/practical_dl/issues"&gt;issue&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-syllabus" class="anchor" aria-hidden="true" href="#syllabus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;week01&lt;/strong&gt; Intro to deep learning&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Lecture: Deep learning -- introduction, backpropagation algorithm, adaptive optimization methods&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Seminar: Neural networks in numpy&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Homework 1 is out!&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Please begin worrying about &lt;a href="https://github.com/yandexdataschool/Practical_DL/issues/6"&gt;installing pytorch&lt;/a&gt;. You will need it next week!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;week02&lt;/strong&gt; Catch-all lecture about deep learning tricks&lt;/p&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Lecture: Deep learning as a language, dropout, batch/layer normalization, other tricks, deep learning frameworks&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox"&gt; Seminar: PyTorch basics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-contributors--course-staff" class="anchor" aria-hidden="true" href="#contributors--course-staff"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors &amp;amp; course staff&lt;/h1&gt;
&lt;p&gt;Course materials and teaching performed by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sites.skoltech.ru/compvision/members/vilem/" rel="nofollow"&gt;Victor Lempitsky&lt;/a&gt; - all main track lectures (1-11)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/simflin"&gt;Victor Yurchenko&lt;/a&gt; - intro notebooks, admin stuff&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/vadim-v-lebedev"&gt;Vadim Lebedev&lt;/a&gt; - notebooks, admin stuff&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/DmitryUlyanov"&gt;Dmitry Ulyanov&lt;/a&gt; - notebooks on generative models &amp;amp; autoencoders&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/justheuristic/"&gt;Fedor Ratnikov&lt;/a&gt; - pytorch &amp;amp; nlp notebooks, one bonus lecture&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Omrigan"&gt;Oleg Vasilev&lt;/a&gt; - notebooks, technical issue resolution&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ars-ashuha"&gt;Arseniy Ashukha&lt;/a&gt; - image captioning materials&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mihaha"&gt;Mikhail Khalman&lt;/a&gt; - variational autoencoder materials&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>yandexdataschool</author><guid isPermaLink="false">https://github.com/yandexdataschool/Practical_DL</guid><pubDate>Mon, 10 Feb 2020 00:15:00 GMT</pubDate></item><item><title>abhinavsagar/Cryptocurrency-Price-Prediction #16 in Jupyter Notebook, Today</title><link>https://github.com/abhinavsagar/Cryptocurrency-Price-Prediction</link><description>&lt;p&gt;&lt;i&gt;Predicting cryptocurrency prices using LSTM neural network&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-cryptocurrency-price-prediction" class="anchor" aria-hidden="true" href="#cryptocurrency-price-prediction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cryptocurrency-Price-Prediction&lt;/h1&gt;
&lt;p&gt;Predicting Cryptocurrency Price using LSTM neural network&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-important" class="anchor" aria-hidden="true" href="#important"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IMPORTANT&lt;/h2&gt;
&lt;p&gt;This code was made public to share our research for the benefit of the scientific community. Do NOT use it for immoral purposes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-if-this-repository-helps-you-in-anyway-show-your-love-heart-by-putting-a-star-on-this-project-v" class="anchor" aria-hidden="true" href="#if-this-repository-helps-you-in-anyway-show-your-love-heart-by-putting-a-star-on-this-project-v"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;If this repository helps you in anyway, show your love &lt;g-emoji class="g-emoji" alias="heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png"&gt;❤️&lt;/g-emoji&gt; by putting a &lt;g-emoji class="g-emoji" alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png"&gt;⭐️&lt;/g-emoji&gt; on this project &lt;g-emoji class="g-emoji" alias="v" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/270c.png"&gt;✌️&lt;/g-emoji&gt;&lt;/h2&gt;
&lt;p&gt;Check out the corresponding medium blog post &lt;a href="https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a?source=friends_link&amp;amp;sk=331d27e1be556a0803f34b746f505467" rel="nofollow"&gt;https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-cryptocurrency-line-plot" class="anchor" aria-hidden="true" href="#cryptocurrency-line-plot"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cryptocurrency line plot&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="i13.png"&gt;&lt;img src="i13.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-actual-vs-predicted-prices" class="anchor" aria-hidden="true" href="#actual-vs-predicted-prices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Actual vs predicted prices&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="i14.png"&gt;&lt;img src="i14.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@misc{Abhinav:2019,
  Author = {Abhinav Sagar},
  Title = {Cryptocurrency-Price-Prediction},
  Year = {2019},
  Publisher = {GitHub},
  Journal = {GitHub repository},
  Howpublished = {\url{https://github.com/abhinavsagar/Cryptocurrency-Price-Prediction}}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>abhinavsagar</author><guid isPermaLink="false">https://github.com/abhinavsagar/Cryptocurrency-Price-Prediction</guid><pubDate>Mon, 10 Feb 2020 00:16:00 GMT</pubDate></item><item><title>quantopian/pyfolio #17 in Jupyter Notebook, Today</title><link>https://github.com/quantopian/pyfolio</link><description>&lt;p&gt;&lt;i&gt;Portfolio and risk analytics in Python&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/3b820de5af1d3e62ecdd614349abd46f4d46d7d6/68747470733a2f2f6d656469612e7175616e746f7069616e2e636f6d2f6c6f676f732f6f70656e5f736f757263652f7079666f6c696f2d6c6f676f2d30332e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/3b820de5af1d3e62ecdd614349abd46f4d46d7d6/68747470733a2f2f6d656469612e7175616e746f7069616e2e636f6d2f6c6f676f732f6f70656e5f736f757263652f7079666f6c696f2d6c6f676f2d30332e706e67" alt="pyfolio" title="pyfolio" data-canonical-src="https://media.quantopian.com/logos/open_source/pyfolio-logo-03.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-pyfolio" class="anchor" aria-hidden="true" href="#pyfolio"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyfolio&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://gitter.im/quantopian/pyfolio?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" alt="Join the chat at https://gitter.im/quantopian/pyfolio" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/quantopian/pyfolio" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/322e7b31553e8f91ca29ca32255265bc1e0a8c94/68747470733a2f2f7472617669732d63692e6f72672f7175616e746f7069616e2f7079666f6c696f2e706e673f6272616e63683d6d6173746572" alt="build status" data-canonical-src="https://travis-ci.org/quantopian/pyfolio.png?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;pyfolio is a Python library for performance and risk analysis of
financial portfolios developed by
&lt;a href="https://www.quantopian.com" rel="nofollow"&gt;Quantopian Inc&lt;/a&gt;. It works well with the
&lt;a href="https://www.zipline.io/" rel="nofollow"&gt;Zipline&lt;/a&gt; open source backtesting library.
Quantopian also offers a &lt;a href="https://factset.quantopian.com" rel="nofollow"&gt;fully managed service for professionals&lt;/a&gt;
that includes Zipline, Alphalens, Pyfolio, FactSet data, and more.&lt;/p&gt;
&lt;p&gt;At the core of pyfolio is a so-called tear sheet that consists of
various individual plots that provide a comprehensive image of the
performance of a trading algorithm. Here's an example of a simple tear
sheet analyzing a strategy:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/quantopian/pyfolio/raw/master/docs/simple_tear_0.png"&gt;&lt;img src="https://github.com/quantopian/pyfolio/raw/master/docs/simple_tear_0.png" alt="simple tear 0" title="Example tear sheet created from a Zipline algo" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/quantopian/pyfolio/raw/master/docs/simple_tear_1.png"&gt;&lt;img src="https://github.com/quantopian/pyfolio/raw/master/docs/simple_tear_1.png" alt="simple tear 1" title="Example tear sheet created from a Zipline algo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also see &lt;a href="https://nbviewer.jupyter.org/format/slides/github/quantopian/pyfolio/blob/master/pyfolio/examples/pyfolio_talk_slides.ipynb#/" rel="nofollow"&gt;slides of a talk about
pyfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;To install pyfolio, run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install pyfolio&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h4&gt;
&lt;p&gt;For development, you may want to use a &lt;a href="https://docs.python-guide.org/en/latest/dev/virtualenvs/" rel="nofollow"&gt;virtual environment&lt;/a&gt; to avoid dependency conflicts between pyfolio and other Python projects you have. To get set up with a virtual env, run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;mkvirtualenv pyfolio&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, clone this git repository and run &lt;code&gt;python setup.py develop&lt;/code&gt;
and edit the library files directly.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-matplotlib-on-osx" class="anchor" aria-hidden="true" href="#matplotlib-on-osx"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Matplotlib on OSX&lt;/h4&gt;
&lt;p&gt;If you are on OSX and using a non-framework build of Python, you may need to set your backend:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;backend: TkAgg&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-k"&gt;~&lt;/span&gt;/.matplotlib/matplotlibrc&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;A good way to get started is to run the pyfolio examples in
a &lt;a href="https://jupyter.org/" rel="nofollow"&gt;Jupyter notebook&lt;/a&gt;. To do this, you first want to
start a Jupyter notebook server:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;jupyter notebook&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From the notebook list page, navigate to the pyfolio examples directory
and open a notebook. Execute the code in a notebook cell by clicking on it
and hitting Shift+Enter.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-questions" class="anchor" aria-hidden="true" href="#questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Questions?&lt;/h2&gt;
&lt;p&gt;If you find a bug, feel free to &lt;a href="https://github.com/quantopian/pyfolio/issues"&gt;open an issue&lt;/a&gt; in this repository.&lt;/p&gt;
&lt;p&gt;You can also join our &lt;a href="https://groups.google.com/forum/#!forum/pyfolio" rel="nofollow"&gt;mailing list&lt;/a&gt; or
our &lt;a href="https://gitter.im/quantopian/pyfolio" rel="nofollow"&gt;Gitter channel&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-support" class="anchor" aria-hidden="true" href="#support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support&lt;/h2&gt;
&lt;p&gt;Please &lt;a href="https://github.com/quantopian/pyfolio/issues/new"&gt;open an issue&lt;/a&gt; for support.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;If you'd like to contribute, a great place to look is the &lt;a href="https://github.com/quantopian/pyfolio/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;issues marked with help-wanted&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a list of core developers and outside collaborators, see &lt;a href="https://github.com/quantopian/pyfolio/graphs/contributors"&gt;the GitHub contributors list&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>quantopian</author><guid isPermaLink="false">https://github.com/quantopian/pyfolio</guid><pubDate>Mon, 10 Feb 2020 00:17:00 GMT</pubDate></item><item><title>fastai/numerical-linear-algebra #18 in Jupyter Notebook, Today</title><link>https://github.com/fastai/numerical-linear-algebra</link><description>&lt;p&gt;&lt;i&gt;Free online textbook of Jupyter notebooks for fast.ai Computational Linear Algebra course&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-computational-linear-algebra-for-coders" class="anchor" aria-hidden="true" href="#computational-linear-algebra-for-coders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computational Linear Algebra for Coders&lt;/h2&gt;
&lt;p&gt;This course is focused on the question: &lt;strong&gt;How do we do matrix computations with acceptable speed and acceptable accuracy?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This course was taught in the &lt;a href="https://www.usfca.edu/arts-sciences/graduate-programs/analytics" rel="nofollow"&gt;University of San Francisco's Masters of Science in Analytics&lt;/a&gt; program, summer 2017 (for graduate students studying to become data scientists).  The course is taught in Python with Jupyter Notebooks, using libraries such as Scikit-Learn and Numpy for most lessons, as well as Numba (a library that compiles Python to C for faster performance) and PyTorch (an alternative to Numpy for the GPU) in a few lessons.&lt;/p&gt;
&lt;p&gt;Accompanying the notebooks is a &lt;a href="https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;playlist of lecture videos, available on YouTube&lt;/a&gt;.  If you are ever confused by a lecture or it goes too quickly, check out the beginning of the next video, where I review concepts from the previous lecture, often explaining things from a new perspective or with different illustrations, and answer questions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-help" class="anchor" aria-hidden="true" href="#getting-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Help&lt;/h2&gt;
&lt;p&gt;You can ask questions or share your thoughts and resources using the &lt;a href="http://forums.fast.ai/c/lin-alg" rel="nofollow"&gt;&lt;strong&gt;Computational Linear Algebra&lt;/strong&gt; category on our fast.ai discussion forums&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;p&gt;The following listing links to the notebooks in this repository, rendered through the &lt;a href="http://nbviewer.jupyter.org" rel="nofollow"&gt;nbviewer&lt;/a&gt; service.  Topics Covered:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-0-course-logistics-video-1" class="anchor" aria-hidden="true" href="#0-course-logistics-video-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb" rel="nofollow"&gt;0. Course Logistics&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=8iGzBMboA0I&amp;amp;index=1&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 1&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Intro" rel="nofollow"&gt;My background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Teaching" rel="nofollow"&gt;Teaching Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Writing-Assignment" rel="nofollow"&gt;Importance of Technical Writing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Excellent-Technical-Blogs" rel="nofollow"&gt;List of Excellent Technical Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb#Linear-Algebra" rel="nofollow"&gt;Linear Algebra Review Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-1-why-are-we-here-video-1" class="anchor" aria-hidden="true" href="#1-why-are-we-here-video-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb" rel="nofollow"&gt;1. Why are we here?&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=8iGzBMboA0I&amp;amp;index=1&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 1&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;We start with a high level overview of some foundational concepts in numerical linear algebra.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Matrix-and-Tensor-Products" rel="nofollow"&gt;Matrix and Tensor Products&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Matrix-Decompositions" rel="nofollow"&gt;Matrix Decompositions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Accuracy" rel="nofollow"&gt;Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Memory-Use" rel="nofollow"&gt;Memory use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Speed" rel="nofollow"&gt;Speed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Scalability-/-parallelization" rel="nofollow"&gt;Parallelization &amp;amp; Vectorization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-2-topic-modeling-with-nmf-and-svd-video-2-and-video-3" class="anchor" aria-hidden="true" href="#2-topic-modeling-with-nmf-and-svd-video-2-and-video-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb" rel="nofollow"&gt;2. Topic Modeling with NMF and SVD&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=kgd40iDT8yY&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=2" rel="nofollow"&gt;Video 2&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=C8KEtrWjjyo&amp;amp;index=3&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 3&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;We will use the newsgroups dataset to try to identify the topics of different posts.  We use a term-document matrix that represents the frequency of the vocabulary in the documents.  We factor it using NMF, and then with SVD.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#TF-IDF" rel="nofollow"&gt;Topic Frequency-Inverse Document Frequency (TF-IDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Singular-Value-Decomposition-(SVD)" rel="nofollow"&gt;Singular Value Decomposition (SVD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Non-negative-Matrix-Factorization-(NMF)" rel="nofollow"&gt;Non-negative Matrix Factorization (NMF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Gradient-Descent" rel="nofollow"&gt;Stochastic Gradient Descent (SGD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#PyTorch" rel="nofollow"&gt;Intro to PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Truncated-SVD" rel="nofollow"&gt;Truncated SVD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-3-background-removal-with-robust-pca-video-3-video-4-and-video-5" class="anchor" aria-hidden="true" href="#3-background-removal-with-robust-pca-video-3-video-4-and-video-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb" rel="nofollow"&gt;3. Background Removal with Robust PCA&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=C8KEtrWjjyo&amp;amp;index=3&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 3&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=Ys8R2nUTOAk&amp;amp;index=4&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 4&lt;/a&gt;, and &lt;a href="https://www.youtube.com/watch?v=O2x5KPJr5ag&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=5" rel="nofollow"&gt;Video 5&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;Another application of SVD is to identify the people and remove the background of a surveillance video.  We will cover robust PCA, which uses randomized SVD.  And Randomized SVD uses the LU factorization.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Load-and-view-the-data" rel="nofollow"&gt;Load and View Video Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#SVD" rel="nofollow"&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb"&gt;Principal Component Analysis (PCA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#L1-norm-induces-sparsity" rel="nofollow"&gt;L1 Norm Induces Sparsity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Robust-PCA-(via-Primary-Component-Pursuit)" rel="nofollow"&gt;Robust PCA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#LU-Factorization" rel="nofollow"&gt;LU factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Stability" rel="nofollow"&gt;Stability of LU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#LU-factorization-with-Partial-Pivoting" rel="nofollow"&gt;LU factorization with Pivoting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#History-of-Gaussian-Elimination" rel="nofollow"&gt;History of Gaussian Elimination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb#Block-Matrices" rel="nofollow"&gt;Block Matrix Multiplication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-4-compressed-sensing-with-robust-regression-video-6-and-video-7" class="anchor" aria-hidden="true" href="#4-compressed-sensing-with-robust-regression-video-6-and-video-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression" rel="nofollow"&gt;4. Compressed Sensing with Robust Regression&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=YY9_EYNj5TY&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=6" rel="nofollow"&gt;Video 6&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=ZUGkvIM6ehM&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=7" rel="nofollow"&gt;Video 7&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;Compressed sensing is critical to allowing CT scans with lower radiation-- the image can be reconstructed with less data.  Here we will learn the technique and apply it to CT images.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Broadcasting" rel="nofollow"&gt;Broadcasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Sparse-Matrices-(in-Scipy)" rel="nofollow"&gt;Sparse matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Sparse-Matrices-(in-Scipy)" rel="nofollow"&gt;CT Scans and Compressed Sensing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#Regresssion" rel="nofollow"&gt;L1 and L2 regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-5-predicting-health-outcomes-with-linear-regressions-video-8" class="anchor" aria-hidden="true" href="#5-predicting-health-outcomes-with-linear-regressions-video-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb" rel="nofollow"&gt;5. Predicting Health Outcomes with Linear Regressions&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=SjX55V8zDXI&amp;amp;index=8&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 8&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Linear-regression-in-Scikit-Learn" rel="nofollow"&gt;Linear regression in sklearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Polynomial-Features" rel="nofollow"&gt;Polynomial Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Speeding-up-feature-generation" rel="nofollow"&gt;Speeding up with Numba&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb#Regularization-and-noise" rel="nofollow"&gt;Regularization and Noise&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-6-how-to-implement-linear-regressionvideo-8" class="anchor" aria-hidden="true" href="#6-how-to-implement-linear-regressionvideo-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb" rel="nofollow"&gt;6. How to Implement Linear Regression&lt;/a&gt;(&lt;a href="https://www.youtube.com/watch?v=SjX55V8zDXI&amp;amp;index=8&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 8&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#How-did-sklearn-do-it?" rel="nofollow"&gt;How did Scikit Learn do it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Naive-Solution" rel="nofollow"&gt;Naive solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Normal-Equations-(Cholesky)" rel="nofollow"&gt;Normal equations and Cholesky factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#QR-Factorization" rel="nofollow"&gt;QR factorization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#SVD" rel="nofollow"&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Timing-Comparison" rel="nofollow"&gt;Timing Comparison&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Conditioning-&amp;amp;-stability" rel="nofollow"&gt;Conditioning &amp;amp; Stability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Full-vs-Reduced-Factorizations" rel="nofollow"&gt;Full vs Reduced Factorizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/6.%20How%20to%20Implement%20Linear%20Regression.ipynb#Matrix-Inversion-is-Unstable" rel="nofollow"&gt;Matrix Inversion is Unstable&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-7-pagerank-with-eigen-decompositions-video-9-and-video-10" class="anchor" aria-hidden="true" href="#7-pagerank-with-eigen-decompositions-video-9-and-video-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb" rel="nofollow"&gt;7. PageRank with Eigen Decompositions&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=AbB-w77yxD0&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&amp;amp;index=9" rel="nofollow"&gt;Video 9&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=1kw8bpA9QmQ&amp;amp;index=10&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 10&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;We have applied SVD to topic modeling, background removal, and linear regression. SVD is intimately connected to the eigen decomposition, so we will now learn how to calculate eigenvalues for a large matrix.  We will use DBpedia data, a large dataset of Wikipedia links, because here the principal eigenvector gives the relative importance of different Wikipedia pages (this is the basic idea of Google's PageRank algorithm).  We will look at 3 different methods for calculating eigenvectors, of increasing complexity (and increasing usefulness!).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#Motivation" rel="nofollow"&gt;SVD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#DBpedia" rel="nofollow"&gt;DBpedia Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#Power-method" rel="nofollow"&gt;Power Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#QR-Algorithm" rel="nofollow"&gt;QR Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#A-Two-Phase-Approach" rel="nofollow"&gt;Two-phase approach to finding eigenvalues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/7.%20PageRank%20with%20Eigen%20Decompositions.ipynb#Arnoldi-Iteration" rel="nofollow"&gt;Arnoldi Iteration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-8-implementing-qr-factorization-video-10" class="anchor" aria-hidden="true" href="#8-implementing-qr-factorization-video-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb" rel="nofollow"&gt;8. Implementing QR Factorization&lt;/a&gt; (&lt;a href="https://www.youtube.com/watch?v=1kw8bpA9QmQ&amp;amp;index=10&amp;amp;list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY" rel="nofollow"&gt;Video 10&lt;/a&gt;)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb#Gram-Schmidt" rel="nofollow"&gt;Gram-Schmidt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb#Householder" rel="nofollow"&gt;Householder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/8.%20Implementing%20QR%20Factorization.ipynb#Ex-9.2:-Classical-vs-Modified-Gram-Schmidt" rel="nofollow"&gt;Stability Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-why-is-this-course-taught-in-such-a-weird-order" class="anchor" aria-hidden="true" href="#why-is-this-course-taught-in-such-a-weird-order"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why is this course taught in such a weird order?&lt;/h2&gt;
&lt;p&gt;This course is structured with a &lt;em&gt;top-down&lt;/em&gt; teaching method, which is different from how most math courses operate.  Typically, in a &lt;em&gt;bottom-up&lt;/em&gt; approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures.  The problems with this are that students often lose motivation, don't have a sense of the "big picture", and don't know what they'll need.&lt;/p&gt;
&lt;p&gt;Harvard Professor David Perkins has a book, &lt;a href="https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719" rel="nofollow"&gt;Making Learning Whole&lt;/a&gt; in which he uses baseball as an analogy.  We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game.  Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.&lt;/p&gt;
&lt;p&gt;If you took the fast.ai deep learning course, that is what we used.  You can hear more about my teaching philosophy &lt;a href="http://www.fast.ai/2016/10/08/teaching-philosophy/" rel="nofollow"&gt;in this blog post&lt;/a&gt; or &lt;a href="https://vimeo.com/214233053" rel="nofollow"&gt;this talk I gave at the San Francisco Machine Learning meetup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All that to say, don't worry if you don't understand everything at first!  You're not supposed to.  We will start using some "black boxes" or matrix decompositions that haven't yet been explained, and then we'll dig into the lower level details later.&lt;/p&gt;
&lt;p&gt;To start, focus on what things DO, not what they ARE.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>fastai</author><guid isPermaLink="false">https://github.com/fastai/numerical-linear-algebra</guid><pubDate>Mon, 10 Feb 2020 00:18:00 GMT</pubDate></item><item><title>GoogleCloudPlatform/training-data-analyst #19 in Jupyter Notebook, Today</title><link>https://github.com/GoogleCloudPlatform/training-data-analyst</link><description>&lt;p&gt;&lt;i&gt;Labs and demos for courses for GCP Training (http://cloud.google.com/training).&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-training-data-analyst" class="anchor" aria-hidden="true" href="#training-data-analyst"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;training-data-analyst&lt;/h1&gt;
&lt;p&gt;Labs and demos for Google Cloud Platform courses (&lt;a href="http://cloud.google.com/training" rel="nofollow"&gt;http://cloud.google.com/training&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-this-repo" class="anchor" aria-hidden="true" href="#contributing-to-this-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to this repo&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Small edits are welcome! Please submit a Pull-Request. See also &lt;a href="./CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;For larger edits, please submit an issue, and we will create a branch for you. Then, get the code reviewed (in the branch) before submitting.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-organization-of-this-repo" class="anchor" aria-hidden="true" href="#organization-of-this-repo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Organization of this repo&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-try-out-the-code-on-google-cloud-platform" class="anchor" aria-hidden="true" href="#try-out-the-code-on-google-cloud-platform"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Try out the code on Google Cloud Platform&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/cloudshell/open/?git_repo=https://github.com/GoogleCloudPlatform/training-data-analyst.git" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c6304dc5972cf1d3766a7697b6c74f49d72fbb61/687474703a2f2f677374617469632e636f6d2f636c6f75647373682f696d616765732f6f70656e2d62746e2e706e67" alt="Open in Cloud Shell" data-canonical-src="http://gstatic.com/cloudssh/images/open-btn.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-courses" class="anchor" aria-hidden="true" href="#courses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Courses&lt;/h2&gt;
&lt;p&gt;Code for the following courses is included in this repo:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-google-cloud-platform-big-data-and-machine-learning-fundamentals" class="anchor" aria-hidden="true" href="#google-cloud-platform-big-data-and-machine-learning-fundamentals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Google Cloud Platform Big Data and Machine Learning Fundamentals&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-course" class="anchor" aria-hidden="true" href="#course"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/training/courses/data-ml-fundamentals" rel="nofollow"&gt;https://cloud.google.com/training/courses/data-ml-fundamentals&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-code" class="anchor" aria-hidden="true" href="#code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="CPB100"&gt;GCP Big Data &amp;amp; Machine Learning Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-data-engineering-on-google-cloud-platform" class="anchor" aria-hidden="true" href="#data-engineering-on-google-cloud-platform"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Data Engineering on Google Cloud Platform&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-course-1" class="anchor" aria-hidden="true" href="#course-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/training/courses/data-engineering" rel="nofollow"&gt;https://cloud.google.com/training/courses/data-engineering&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-code-1" class="anchor" aria-hidden="true" href="#code-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Code&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="courses/unstructured"&gt;Leveraging unstructured data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/data_analysis"&gt;Serverless Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning"&gt;Serverless Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/streaming"&gt;Resilient streaming systems&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-machine-learning-on-google-cloud-platform--advanced-ml-on-gcp" class="anchor" aria-hidden="true" href="#machine-learning-on-google-cloud-platform--advanced-ml-on-gcp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning on Google Cloud Platform (&amp;amp; Advanced ML on GCP)&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-courses-1" class="anchor" aria-hidden="true" href="#courses-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Courses&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/google-machine-learning" rel="nofollow"&gt;https://www.coursera.org/learn/google-machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/specializations/advanced-machine-learning-tensorflow-gcp" rel="nofollow"&gt;https://www.coursera.org/specializations/advanced-machine-learning-tensorflow-gcp&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-codes" class="anchor" aria-hidden="true" href="#codes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Codes&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/01_googleml"&gt;How Google Does ML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/02_generalization"&gt;Launching into ML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/03_tensorflow"&gt;Introduction to TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/04_features"&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/05_artandscience"&gt;Art and Science of ML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/06_structured"&gt;End-to-end machine learning on Structured Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Production ML models&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/08_image"&gt;Image Classification Models in TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/09_sequence"&gt;Sequence Models for Time-Series and Text problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="courses/machine_learning/deepdive/10_recommend"&gt;Recommendation Engines using TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-blog-posts" class="anchor" aria-hidden="true" href="#blog-posts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Blog posts&lt;/h3&gt;
&lt;p&gt;blogs/&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>GoogleCloudPlatform</author><guid isPermaLink="false">https://github.com/GoogleCloudPlatform/training-data-analyst</guid><pubDate>Mon, 10 Feb 2020 00:19:00 GMT</pubDate></item><item><title>deepmind/deepmind-research #20 in Jupyter Notebook, Today</title><link>https://github.com/deepmind/deepmind-research</link><description>&lt;p&gt;&lt;i&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-deepmind-research" class="anchor" aria-hidden="true" href="#deepmind-research"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepMind Research&lt;/h1&gt;
&lt;p&gt;This repository contains implementations and illustrative code to accompany
DeepMind publications. Along with publishing papers to accompany research
conducted at DeepMind, we release open-source
&lt;a href="https://deepmind.com/research/open-source/open-source-environments/" rel="nofollow"&gt;environments&lt;/a&gt;,
&lt;a href="https://deepmind.com/research/open-source/open-source-datasets/" rel="nofollow"&gt;data sets&lt;/a&gt;,
and &lt;a href="https://deepmind.com/research/open-source/open-source-code/" rel="nofollow"&gt;code&lt;/a&gt; to
enable the broader research community to engage with our work and build upon it,
with the ultimate goal of accelerating scientific progress to benefit society.
For example, you can build on our implementations of the
&lt;a href="https://github.com/deepmind/dqn"&gt;Deep Q-Network&lt;/a&gt; or
&lt;a href="https://github.com/deepmind/dnc"&gt;Differential Neural Computer&lt;/a&gt;, or experiment
in the same environments we use for our research, such as
&lt;a href="https://github.com/deepmind/lab"&gt;DeepMind Lab&lt;/a&gt; or
&lt;a href="https://github.com/deepmind/pysc2"&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you enjoy building tools, environments, software libraries, and other
infrastructure of the kind listed below, you can view open positions to work in
related areas on our &lt;a href="https://deepmind.com/careers/" rel="nofollow"&gt;careers page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For a full list of our publications, please see
&lt;a href="https://deepmind.com/research/publications/" rel="nofollow"&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects" class="anchor" aria-hidden="true" href="#projects"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="alphafold_casp13"&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt;
&lt;li&gt;&lt;a href="unrestricted_advx"&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="hierarchical_probabilistic_unet"&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="scratchgan"&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="tvt"&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="curl"&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="transporter"&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="bigbigan"&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="cs_gan"&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="side_effects_penalties"&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="PrediNet"&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="unsupervised_adversarial_training"&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="graph_matching_networks"&gt;Graph Matching Networks for Learning the Similarity of Graph Structured
Objects&lt;/a&gt;, ICML 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="regal"&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>deepmind</author><guid isPermaLink="false">https://github.com/deepmind/deepmind-research</guid><pubDate>Mon, 10 Feb 2020 00:20:00 GMT</pubDate></item><item><title>udacity/CVND_Exercises #21 in Jupyter Notebook, Today</title><link>https://github.com/udacity/CVND_Exercises</link><description>&lt;p&gt;&lt;i&gt;Exercise notebooks for CVND.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-computer-vision-nanodegree-program-exercises" class="anchor" aria-hidden="true" href="#computer-vision-nanodegree-program-exercises"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer Vision Nanodegree Program, Exercises&lt;/h1&gt;
&lt;p&gt;This repository contains code exercises and materials for Udacity's &lt;a href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" rel="nofollow"&gt;Computer Vision Nanodegree&lt;/a&gt; program. It consists of tutorial notebooks that demonstrate, or challenge you to complete, various computer vision applications and techniques. These notebooks depend on a number of software packages to run, and so, we suggest that you create a local environment with these dependencies by following the instructions below.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-configure-and-manage-your-environment-with-anaconda" class="anchor" aria-hidden="true" href="#configure-and-manage-your-environment-with-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure and Manage Your Environment with Anaconda&lt;/h1&gt;
&lt;p&gt;Per the Anaconda &lt;a href="http://conda.pydata.org/docs" rel="nofollow"&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Conda is an open source package management system and environment management system
for installing multiple versions of software packages and their dependencies and
switching easily between them. It works on Linux, OS X and Windows, and was created
for Python programs but can package and distribute any software.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h2&gt;
&lt;p&gt;Using Anaconda consists of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt; on your computer, by selecting the latest Python version for your operating system. If you already have &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;miniconda&lt;/code&gt; installed, you should be able to skip this step and move on to step 2.&lt;/li&gt;
&lt;li&gt;Create and activate * a new &lt;code&gt;conda&lt;/code&gt; &lt;a href="http://conda.pydata.org/docs/using/envs.html" rel="nofollow"&gt;environment&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;* Each time you wish to work on any exercises, activate your &lt;code&gt;conda&lt;/code&gt; environment!&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-1-installation" class="anchor" aria-hidden="true" href="#1-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1. Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the latest version of &lt;code&gt;miniconda&lt;/code&gt; that matches your system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: There have been reports of issues creating an environment using miniconda &lt;code&gt;v4.3.13&lt;/code&gt;. If it gives you issues try versions &lt;code&gt;4.3.11&lt;/code&gt; or &lt;code&gt;4.2.12&lt;/code&gt; from &lt;a href="https://repo.continuum.io/miniconda/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Linux&lt;/th&gt;
&lt;th&gt;Mac&lt;/th&gt;
&lt;th&gt;Windows&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh" rel="nofollow"&gt;64-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe" rel="nofollow"&gt;64-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;32-bit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh" rel="nofollow"&gt;32-bit (bash installer)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe" rel="nofollow"&gt;32-bit (exe installer)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; &lt;a href="http://conda.pydata.org/miniconda.html" rel="nofollow"&gt;miniconda&lt;/a&gt; on your machine. Detailed instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; &lt;a href="http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install" rel="nofollow"&gt;http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-2-create-and-activate-the-environment" class="anchor" aria-hidden="true" href="#2-create-and-activate-the-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2. Create and Activate the Environment&lt;/h2&gt;
&lt;p&gt;For Windows users, these following commands need to be executed from the &lt;strong&gt;Anaconda prompt&lt;/strong&gt; as opposed to a Windows terminal window. For Mac, a normal terminal window will work.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-git-and-version-control" class="anchor" aria-hidden="true" href="#git-and-version-control"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Git and version control&lt;/h4&gt;
&lt;p&gt;These instructions also assume you have &lt;code&gt;git&lt;/code&gt; installed for working with Github from a terminal window, but if you do not, you can download that first with the command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you'd like to learn more about version control and using &lt;code&gt;git&lt;/code&gt; from the command line, take a look at our &lt;a href="https://www.udacity.com/course/version-control-with-git--ud123" rel="nofollow"&gt;free course: Version Control with Git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Now, we're ready to create our local environment!&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository, and navigate to the downloaded folder. This may take a minute or two to clone due to the included image data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/udacity/CVND_Exercises.git
cd CVND_Exercises
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Create (and activate) a new environment, named &lt;code&gt;cv-nd&lt;/code&gt; with Python 3.6. If prompted to proceed with the install &lt;code&gt;(Proceed [y]/n)&lt;/code&gt; type y.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create -n cv-nd python=3.6
source activate cv-nd
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda create --name cv-nd python=3.6
activate cv-nd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point your command line should look something like: &lt;code&gt;(cv-nd) &amp;lt;User&amp;gt;:CVND_Exercises &amp;lt;user&amp;gt;$&lt;/code&gt;. The &lt;code&gt;(cv-nd)&lt;/code&gt; indicates that your environment has been activated, and you can proceed with further package installations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install PyTorch and torchvision; this should install the latest version of PyTorch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision -c pytorch 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;conda install pytorch-cpu -c pytorch
pip install torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install a few required pip packages, which are specified in the requirements text file (including OpenCV).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="7"&gt;
&lt;li&gt;That's it!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now all of the &lt;code&gt;cv-nd&lt;/code&gt; libraries are available to you. Assuming you're environment is still activated, you can navigate to the Exercises repo and start looking at the notebooks:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd
cd CVND_Exercises
jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To exit the environment when you have completed your work session, simply close the terminal window.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-notes-on-environment-creation-and-deletion" class="anchor" aria-hidden="true" href="#notes-on-environment-creation-and-deletion"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notes on environment creation and deletion&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Verify&lt;/strong&gt; that the &lt;code&gt;cv-nd&lt;/code&gt; environment was created in your environments:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda info --envs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Cleanup&lt;/strong&gt; downloaded libraries (remove tarballs, zip files, etc):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda clean -tp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Uninstall&lt;/strong&gt; the environment (if you want); you can remove it by name:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda env remove -n cv-nd
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>udacity</author><guid isPermaLink="false">https://github.com/udacity/CVND_Exercises</guid><pubDate>Mon, 10 Feb 2020 00:21:00 GMT</pubDate></item><item><title>robertness/causalML #22 in Jupyter Notebook, Today</title><link>https://github.com/robertness/causalML</link><description>&lt;p&gt;&lt;i&gt;A course on causal machine learning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-causal-modeling-in-machine-learning" class="anchor" aria-hidden="true" href="#causal-modeling-in-machine-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Causal Modeling in Machine Learning&lt;/h1&gt;
&lt;p&gt;CS 7290 Special Topics in Data Science
Spring 2020
Prof. Robert Osazuwa Ness
Northeastern University, Khoury College of Computer Sciences&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-time-and-place-thursdays-6-915pm-room-richards-hall-227-ri-227" class="anchor" aria-hidden="true" href="#time-and-place-thursdays-6-915pm-room-richards-hall-227-ri-227"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Time and Place: Thursdays, 6-9:15pm, room: Richards Hall 227 (RI 227)&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-syllabus-and-schedule" class="anchor" aria-hidden="true" href="#syllabus-and-schedule"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Syllabus and schedule&lt;/h2&gt;
&lt;p&gt;The syllabus for the Spring 2020 class is &lt;a href="https://github.com/robertness/causalML/blob/master/syllabus.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you are thinking of signing up for the course and have questions, reach out to me at &lt;a href="mailto:robertness@gmail.com"&gt;robertness@gmail.com&lt;/a&gt;, rather than my NEU email.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-course-outcomes" class="anchor" aria-hidden="true" href="#course-outcomes"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course Outcomes&lt;/h2&gt;
&lt;p&gt;Upon completing this course, you will be capable of building causal reasoning algorithms into decision-making systems in data science and machine learning teams within top-tier technology organizations.  You will have implemented a portfolio project that demonstrates this competence.  You will use the PyTorch-based probabilistic deep learning framework &lt;a href="http://pyro.ai/" rel="nofollow"&gt;Pyro&lt;/a&gt; to implement the homework assignments and class project and will be an expert in using this framework by the end of the course.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-who-is-this-course-for" class="anchor" aria-hidden="true" href="#who-is-this-course-for"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Who is this course for?&lt;/h2&gt;
&lt;p&gt;Prerequisites include (DS5220 and DS5230) or (CS6140 and CS6220) or approval of the instructor&lt;/p&gt;
&lt;p&gt;You will gain the most from this course if you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You are familiar with random variables, joint probability distributions, conditional probabilities distributions, Baye's rule and basic ideas from Bayesian statistics, and expectation.&lt;/li&gt;
&lt;li&gt;You a good software engineer or aspire to be one.&lt;/li&gt;
&lt;li&gt;You work in or plan to work on a team running experiments in a top-tier tech company or a technically advanced retail company.&lt;/li&gt;
&lt;li&gt;You plan on working as an ML/AI research scientist and want to learn how to create agents that reason like humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-course-description" class="anchor" aria-hidden="true" href="#course-description"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Course Description&lt;/h2&gt;
&lt;p&gt;This course will cover the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Causality in the context of model-based machine learning, Bayesian modeling, and programmatic AI&lt;/li&gt;
&lt;li&gt;Reasoning about probability distributions with directed acyclic graphs&lt;/li&gt;
&lt;li&gt;Interventions and do-calculus, identification and estimation of causal effects, covariate adjustment, and other methods of causal inference&lt;/li&gt;
&lt;li&gt;Counterfactual reasoning and algorithmic counterfactuals&lt;/li&gt;
&lt;li&gt;Causal reasoning in the context of A/B tests, multi-armed bandits, sequential decision-making, and reinforcement learning&lt;/li&gt;
&lt;li&gt;Deep causal latent variable models&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-attendance" class="anchor" aria-hidden="true" href="#attendance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Attendance&lt;/h2&gt;
&lt;p&gt;There will be several ways to get attendance points.  These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proactively answering questions in class&lt;/li&gt;
&lt;li&gt;Completely online quizzes&lt;/li&gt;
&lt;li&gt;Contributing to the course wiki.&lt;/li&gt;
&lt;li&gt;Answering other students' questions in the course forum.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-online-course-materials-and-readings" class="anchor" aria-hidden="true" href="#online-course-materials-and-readings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online Course Materials and Readings&lt;/h2&gt;
&lt;p&gt;Students will be provided access to online course materials.  Much of the lecture notes will be provided in advance of the class.  Students should go through the online course in advance of in-person class.  This will increase the quality of the in-person classes and allow you to absorb more during class time.&lt;/p&gt;
&lt;p&gt;Readings will be assigned in advance of class.  Students are expected to read the assigned readings in advance of each lecture.&lt;/p&gt;
&lt;p&gt;This course does not require the purchase of textbooks.  However, it will rely heavily on the following two books:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pearl, Judea. Causality. Cambridge university press, 2009.&lt;/li&gt;
&lt;li&gt;Peters, Jonas, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. MIT Press, 2017.
While not necessary for the course, these books are worth buying just to have as a reference.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-homework" class="anchor" aria-hidden="true" href="#homework"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Homework&lt;/h2&gt;
&lt;p&gt;The homework in this class will consist of 5 problem sets, which will combine mathematical derivations with programming exercises in Python. Submissions must be made via blackboard by 11.59pm on the due date.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-project" class="anchor" aria-hidden="true" href="#project"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Project&lt;/h2&gt;
&lt;p&gt;The goal of the project is to gain experience in implementing, testing, and presenting one of the methods covered in the lectures. Students will collaborate in groups of 2-4. We will provide a list of project descriptions.  Students who want to pursue a unique project should speak to the instructor.  Unique projects done in collaboration with a company are encouraged.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-grading-and-academic-guidelines" class="anchor" aria-hidden="true" href="#grading-and-academic-guidelines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Grading and Academic Guidelines&lt;/h2&gt;
&lt;p&gt;The final grade for this course will be weighted as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Homework: 40%
Attendance: 25%
Course Project: 35%
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Refresh your knowledge of the university's &lt;a href="http://www.northeastern.edu/osccr/academic-integrity-policy/" rel="nofollow"&gt;policy&lt;/a&gt; about academic integrity and plagiarism (this includes plagarizing code). There is &lt;strong&gt;zero-tolerance&lt;/strong&gt; for cheating!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-self-evaluation" class="anchor" aria-hidden="true" href="#self-evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Self-evaluation&lt;/h2&gt;
&lt;p&gt;Students will be asked to indicate the amount of time spent on each homework, as well as the project. The will also be able to indicate what they think went well, and what they think did not go well.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>robertness</author><guid isPermaLink="false">https://github.com/robertness/causalML</guid><pubDate>Mon, 10 Feb 2020 00:22:00 GMT</pubDate></item><item><title>jackfrued/Python-100-Days #23 in Jupyter Notebook, Today</title><link>https://github.com/jackfrued/Python-100-Days</link><description>&lt;p&gt;&lt;i&gt;Python - 100天从新手到大师&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-python---100天从新手到大师" class="anchor" aria-hidden="true" href="#python---100天从新手到大师"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python - 100天从新手到大师&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;作者：骆昊&lt;/p&gt;
&lt;p&gt;最近有很多想学习Python的小伙伴陆陆续续加入我们的交流群，目前我们的交流群人数已经超过一万人。我们的目标是打造一个优质的Python交流社区，一方面为想学习Python的初学者扫平入门过程中的重重障碍；另一方为新入行的开发者提供问道的途径，帮助他们迅速成长为优秀的职业人；此外，有经验的开发者可以利用这个平台把自己的工作经验无偿分享或有偿提供出来，让大家都能够得到职业技能以及综合素质的全面提升。之前的公开课和线下技术交流活动因为工作的关系荒废了一段时间了，但是各位小伙伴仍然活跃在交流群并一如既往的支持我们，在此向大家表示感谢。近期开始持续更新前15天和最后10天的内容，前15天是写给初学者的，我希望把上手的难度进一步降低，例子程序更加简单清晰；最后10天是Python项目实战和面试相关的东西，我希望内容更详实和完整，尤其是第100天的面试题部分；创作不易，感谢大家的打赏支持，这些钱不会用于购买咖啡而是通过腾讯公益平台捐赠给需要帮助的人（&lt;a href="./%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97.md"&gt;点击&lt;/a&gt;了解捐赠情况）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-qq-group.png"&gt;&lt;img src="./res/python-qq-group.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python应用领域和就业形势分析" class="anchor" aria-hidden="true" href="#python应用领域和就业形势分析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python应用领域和就业形势分析&lt;/h3&gt;
&lt;p&gt;简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学习曲线低，非专业人士也能上手&lt;/li&gt;
&lt;li&gt;开源系统，拥有强大的生态圈&lt;/li&gt;
&lt;li&gt;解释型语言，完美的平台可移植性&lt;/li&gt;
&lt;li&gt;支持面向对象和函数式编程&lt;/li&gt;
&lt;li&gt;能够通过调用C/C++代码扩展功能&lt;/li&gt;
&lt;li&gt;代码规范程度高，可读性强&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前几个比较流行的领域，Python都有用武之地。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;云基础设施 - Python / Java / Go&lt;/li&gt;
&lt;li&gt;DevOps - Python / Shell / Ruby / Go&lt;/li&gt;
&lt;li&gt;网络爬虫 - Python / PHP / C++&lt;/li&gt;
&lt;li&gt;数据分析挖掘 - Python / R / Scala / Matlab&lt;/li&gt;
&lt;li&gt;机器学习 - Python / R / Java / Lisp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作为一名Python开发者，主要的就业领域包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python服务器后台开发 / 游戏服务器开发 / 数据接口开发工程师&lt;/li&gt;
&lt;li&gt;Python自动化运维工程师&lt;/li&gt;
&lt;li&gt;Python数据分析 / 数据可视化 / 大数据工程师&lt;/li&gt;
&lt;li&gt;Python爬虫工程师&lt;/li&gt;
&lt;li&gt;Python聊天机器人开发 / 图像识别和视觉算法 / 深度学习工程师&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图显示了主要城市Python招聘需求量及薪资待遇排行榜（截止到2018年5月）。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-top-10.png"&gt;&lt;img src="./res/python-top-10.png" alt="Python招聘需求及薪资待遇Top 10" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-bj-salary.png"&gt;&lt;img src="./res/python-bj-salary.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/python-salary-chengdu.png"&gt;&lt;img src="./res/python-salary-chengdu.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;给初学者的几个建议：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make English as your working language.&lt;/li&gt;
&lt;li&gt;Practice makes perfect.&lt;/li&gt;
&lt;li&gt;All experience comes from mistakes.&lt;/li&gt;
&lt;li&gt;Don't be one of the leeches.&lt;/li&gt;
&lt;li&gt;Either stand out or kicked out.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day0115---python语言基础" class="anchor" aria-hidden="true" href="#day0115---python语言基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01~15 - &lt;a href="./Day01-15"&gt;Python语言基础&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day01---初识python" class="anchor" aria-hidden="true" href="#day01---初识python"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day01 - &lt;a href="./Day01-15/01.%E5%88%9D%E8%AF%86Python.md"&gt;初识Python&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python简介 - Python的历史 / Python的优缺点 / Python的应用领域&lt;/li&gt;
&lt;li&gt;搭建编程环境 - Windows环境 / Linux环境 / MacOS环境&lt;/li&gt;
&lt;li&gt;从终端运行Python程序 - Hello, world / print函数 / 运行程序&lt;/li&gt;
&lt;li&gt;使用IDLE - 交互式环境(REPL) / 编写多行代码 / 运行程序 / 退出IDLE&lt;/li&gt;
&lt;li&gt;注释 - 注释的作用 / 单行注释 / 多行注释&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day02---语言元素" class="anchor" aria-hidden="true" href="#day02---语言元素"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day02 - &lt;a href="./Day01-15/02.%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0.md"&gt;语言元素&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;程序和进制 - 指令和程序 / 冯诺依曼机 / 二进制和十进制 / 八进制和十六进制&lt;/li&gt;
&lt;li&gt;变量和类型 - 变量的命名 / 变量的使用 / input函数 / 检查变量类型 / 类型转换&lt;/li&gt;
&lt;li&gt;数字和字符串 - 整数 / 浮点数 / 复数 / 字符串 / 字符串基本操作 / 字符编码&lt;/li&gt;
&lt;li&gt;运算符 - 数学运算符 / 赋值运算符 / 比较运算符 / 逻辑运算符 / 身份运算符 / 运算符的优先级&lt;/li&gt;
&lt;li&gt;应用案例 - 华氏温度转换成摄氏温度 / 输入圆的半径计算周长和面积 / 输入年份判断是否是闰年&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day03---分支结构" class="anchor" aria-hidden="true" href="#day03---分支结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day03 - &lt;a href="./Day01-15/03.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md"&gt;分支结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;分支结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图&lt;/li&gt;
&lt;li&gt;if语句 - 简单的if / if-else结构 / if-elif-else结构 / 嵌套的if&lt;/li&gt;
&lt;li&gt;应用案例 - 用户身份验证 / 英制单位与公制单位互换 / 掷骰子决定做什么 / 百分制成绩转等级制 / 分段函数求值 / 输入三条边的长度如果能构成三角形就计算周长和面积&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day04---循环结构" class="anchor" aria-hidden="true" href="#day04---循环结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day04 - &lt;a href="./Day01-15/04.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md"&gt;循环结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;循环结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图&lt;/li&gt;
&lt;li&gt;while循环 - 基本结构 / break语句 / continue语句&lt;/li&gt;
&lt;li&gt;for循环 - 基本结构 / range类型 / 循环中的分支结构 / 嵌套的循环 / 提前结束程序&lt;/li&gt;
&lt;li&gt;应用案例 - 1~100求和 / 判断素数 / 猜数字游戏 / 打印九九表 / 打印三角形图案 / 猴子吃桃 / 百钱百鸡&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day05---构造程序逻辑" class="anchor" aria-hidden="true" href="#day05---构造程序逻辑"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day05 - &lt;a href="./Day01-15/05.%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91.md"&gt;构造程序逻辑&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;经典案例：水仙花数 / 百钱百鸡 / Craps赌博游戏&lt;/li&gt;
&lt;li&gt;练习题目：斐波那契数列 / 完美数 / 素数&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day06---函数和模块的使用" class="anchor" aria-hidden="true" href="#day06---函数和模块的使用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day06 - &lt;a href="./Day01-15/06.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;函数和模块的使用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;函数的作用 - 代码的坏味道 / 用函数封装功能模块&lt;/li&gt;
&lt;li&gt;定义函数 - def语句 / 函数名 / 参数列表 / return语句 / 调用自定义函数&lt;/li&gt;
&lt;li&gt;调用函数 - Python内置函数 /  导入模块和函数&lt;/li&gt;
&lt;li&gt;函数的参数 - 默认参数 / 可变参数 / 关键字参数 / 命名关键字参数&lt;/li&gt;
&lt;li&gt;函数的返回值 - 没有返回值  / 返回单个值 / 返回多个值&lt;/li&gt;
&lt;li&gt;作用域问题 - 局部作用域 / 嵌套作用域 / 全局作用域 / 内置作用域 / 和作用域相关的关键字&lt;/li&gt;
&lt;li&gt;用模块管理函数 - 模块的概念 / 用自定义模块管理函数 / 命名冲突的时候会怎样（同一个模块和不同的模块）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day07---字符串和常用数据结构" class="anchor" aria-hidden="true" href="#day07---字符串和常用数据结构"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day07 - &lt;a href="./Day01-15/07.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.md"&gt;字符串和常用数据结构&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;字符串的使用 - 计算长度 / 下标运算 / 切片 / 常用方法&lt;/li&gt;
&lt;li&gt;列表基本用法 - 定义列表 / 用下表访问元素 / 下标越界 / 添加元素 / 删除元素 / 修改元素 / 切片 / 循环遍历&lt;/li&gt;
&lt;li&gt;列表常用操作 - 连接 / 复制(复制元素和复制数组) / 长度 / 排序 / 倒转 / 查找&lt;/li&gt;
&lt;li&gt;生成列表 - 使用range创建数字列表 / 生成表达式 / 生成器&lt;/li&gt;
&lt;li&gt;元组的使用 - 定义元组 / 使用元组中的值 / 修改元组变量 / 元组和列表转换&lt;/li&gt;
&lt;li&gt;集合基本用法 - 集合和列表的区别 /  创建集合 / 添加元素 / 删除元素 /  清空&lt;/li&gt;
&lt;li&gt;集合常用操作 - 交集 / 并集 / 差集 / 对称差 / 子集 / 超集&lt;/li&gt;
&lt;li&gt;字典的基本用法 - 字典的特点 / 创建字典 / 添加元素 / 删除元素 / 取值 / 清空&lt;/li&gt;
&lt;li&gt;字典常用操作 - keys()方法 / values()方法 / items()方法 / setdefault()方法&lt;/li&gt;
&lt;li&gt;基础练习 - 跑马灯效果 / 列表找最大元素 / 统计考试成绩的平均分 / Fibonacci数列 / 杨辉三角&lt;/li&gt;
&lt;li&gt;综合案例 - 双色球选号 / 井字棋&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day08---面向对象编程基础" class="anchor" aria-hidden="true" href="#day08---面向对象编程基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day08 - &lt;a href="./Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80.md"&gt;面向对象编程基础&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;类和对象 - 什么是类 / 什么是对象 / 面向对象其他相关概念&lt;/li&gt;
&lt;li&gt;定义类 - 基本结构 / 属性和方法 / 构造器 / 析构器 / __str__方法&lt;/li&gt;
&lt;li&gt;使用对象 - 创建对象 / 给对象发消息&lt;/li&gt;
&lt;li&gt;面向对象的四大支柱 - 抽象 / 封装 / 继承 / 多态&lt;/li&gt;
&lt;li&gt;基础练习 - 定义学生类 / 定义时钟类 / 定义图形类 / 定义汽车类&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day09---面向对象进阶" class="anchor" aria-hidden="true" href="#day09---面向对象进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day09 - &lt;a href="./Day01-15/09.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6.md"&gt;面向对象进阶&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;属性 - 类属性 / 实例属性 / 属性访问器 / 属性修改器 / 属性删除器 / 使用__slots__&lt;/li&gt;
&lt;li&gt;类中的方法 - 实例方法 / 类方法 / 静态方法&lt;/li&gt;
&lt;li&gt;运算符重载 - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__&lt;/li&gt;
&lt;li&gt;类(的对象)之间的关系 - 关联 / 继承 / 依赖&lt;/li&gt;
&lt;li&gt;继承和多态 - 什么是继承 / 继承的语法 / 调用父类方法 / 方法重写 / 类型判定 / 多重继承 / 菱形继承(钻石继承)和C3算法&lt;/li&gt;
&lt;li&gt;综合案例 - 工资结算系统 / 图书自动折扣系统 / 自定义分数类&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day10---图形用户界面和游戏开发" class="anchor" aria-hidden="true" href="#day10---图形用户界面和游戏开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day10 - &lt;a href="./Day01-15/10.%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91.md"&gt;图形用户界面和游戏开发&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;使用tkinter开发GUI程序&lt;/li&gt;
&lt;li&gt;使用pygame三方库开发游戏应用&lt;/li&gt;
&lt;li&gt;“大球吃小球”游戏&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day11---文件和异常" class="anchor" aria-hidden="true" href="#day11---文件和异常"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day11 - &lt;a href="./Day01-15/11.%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8.md"&gt;文件和异常&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;读文件 - 读取整个文件 / 逐行读取 / 文件路径&lt;/li&gt;
&lt;li&gt;写文件 - 覆盖写入 / 追加写入 / 文本文件 / 二进制文件&lt;/li&gt;
&lt;li&gt;异常处理 - 异常机制的重要性 / try-except代码块 / else代码块 / finally代码块 / 内置异常类型 / 异常栈 / raise语句&lt;/li&gt;
&lt;li&gt;数据持久化 - CSV文件概述 / csv模块的应用 / JSON数据格式 / json模块的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day12---字符串和正则表达式" class="anchor" aria-hidden="true" href="#day12---字符串和正则表达式"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day12 - &lt;a href="./Day01-15/12.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.md"&gt;字符串和正则表达式&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;字符串高级操作 - 转义字符 / 原始字符串 / 多行字符串 / in和 not in运算符 / is开头的方法 / join和split方法 / strip相关方法 / pyperclip模块 / 不变字符串和可变字符串 / StringIO的使用&lt;/li&gt;
&lt;li&gt;正则表达式入门 - 正则表达式的作用 / 元字符 / 转义 / 量词 / 分组 / 零宽断言 /贪婪匹配与惰性匹配懒惰 / 使用re模块实现正则表达式操作（匹配、搜索、替换、捕获）&lt;/li&gt;
&lt;li&gt;使用正则表达式 - re模块 / compile函数 / group和groups方法 / match方法 / search方法 / findall和finditer方法 / sub和subn方法 / split方法&lt;/li&gt;
&lt;li&gt;应用案例 - 使用正则表达式验证输入的字符串&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day13---进程和线程" class="anchor" aria-hidden="true" href="#day13---进程和线程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day13 - &lt;a href="./Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B.md"&gt;进程和线程&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;进程和线程的概念 - 什么是进程 / 什么是线程 / 多线程的应用场景&lt;/li&gt;
&lt;li&gt;使用进程 - fork函数 / multiprocessing模块 / 进程池 / 进程间通信&lt;/li&gt;
&lt;li&gt;使用线程 - thread模块 / threading模块 / Thread类 / Lock类 / Condition类 / 线程池&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day14---网络编程入门和网络应用开发" class="anchor" aria-hidden="true" href="#day14---网络编程入门和网络应用开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day14 - &lt;a href="./Day01-15/14.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91.md"&gt;网络编程入门和网络应用开发&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;计算机网络基础 - 计算机网络发展史 / “TCP-IP”模型 / IP地址 / 端口 / 协议 / 其他相关概念&lt;/li&gt;
&lt;li&gt;网络应用模式 - “客户端-服务器”模式 / “浏览器-服务器”模式&lt;/li&gt;
&lt;li&gt;基于HTTP协议访问网络资源 - 网络API概述 / 访问URL / requests模块 / 解析JSON格式数据&lt;/li&gt;
&lt;li&gt;Python网络编程 - 套接字的概念 / socket模块 /  socket函数 / 创建TCP服务器 / 创建TCP客户端 / 创建UDP服务器 / 创建UDP客户端 / SocketServer模块&lt;/li&gt;
&lt;li&gt;电子邮件 - SMTP协议 / POP3协议 / IMAP协议 / smtplib模块 / poplib模块 / imaplib模块&lt;/li&gt;
&lt;li&gt;短信服务 - 调用短信服务网关&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day15---图像和文档处理" class="anchor" aria-hidden="true" href="#day15---图像和文档处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day15 - &lt;a href="./Day01-15/15.%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86.md"&gt;图像和文档处理&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;用Pillow处理图片 - 图片读写 / 图片合成 / 几何变换 / 色彩转换 / 滤镜效果&lt;/li&gt;
&lt;li&gt;读写Word文档 - 文本内容的处理 / 段落 / 页眉和页脚 / 样式的处理&lt;/li&gt;
&lt;li&gt;读写Excel文件 - xlrd模块 / xlwt模块&lt;/li&gt;
&lt;li&gt;生成PDF文件 - pypdf2模块 / reportlab模块&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day16day20---python语言进阶-" class="anchor" aria-hidden="true" href="#day16day20---python语言进阶-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day16~Day20 - &lt;a href="./Day16-20/16-20.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md"&gt;Python语言进阶 &lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;常用数据结构&lt;/li&gt;
&lt;li&gt;函数的高级用法 - “一等公民” / 高阶函数 / Lambda函数 / 作用域和闭包 / 装饰器&lt;/li&gt;
&lt;li&gt;面向对象高级知识 - “三大支柱” / 类与类之间的关系 / 垃圾回收 / 魔术属性和方法 / 混入 / 元类 / 面向对象设计原则 / GoF设计模式&lt;/li&gt;
&lt;li&gt;迭代器和生成器 - 相关魔术方法 / 创建生成器的两种方式 /&lt;/li&gt;
&lt;li&gt;并发和异步编程 - 多线程 / 多进程 / 异步IO / async和await&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day2130---web前端入门" class="anchor" aria-hidden="true" href="#day2130---web前端入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day21~30 - &lt;a href="./Day21-30/21-30.Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0.md"&gt;Web前端入门&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用HTML标签承载页面内容&lt;/li&gt;
&lt;li&gt;用CSS渲染页面&lt;/li&gt;
&lt;li&gt;用JavaScript处理交互式行为&lt;/li&gt;
&lt;li&gt;jQuery入门和提高&lt;/li&gt;
&lt;li&gt;Vue.js入门&lt;/li&gt;
&lt;li&gt;Element的使用&lt;/li&gt;
&lt;li&gt;Bootstrap的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3135---玩转linux操作系统" class="anchor" aria-hidden="true" href="#day3135---玩转linux操作系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day31~35 - &lt;a href="./Day31-35/31-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md"&gt;玩转Linux操作系统&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;操作系统发展史和Linux概述&lt;/li&gt;
&lt;li&gt;Linux基础命令&lt;/li&gt;
&lt;li&gt;Linux中的实用程序&lt;/li&gt;
&lt;li&gt;Linux的文件系统&lt;/li&gt;
&lt;li&gt;Vim编辑器的应用&lt;/li&gt;
&lt;li&gt;环境变量和Shell编程&lt;/li&gt;
&lt;li&gt;软件的安装和服务的配置&lt;/li&gt;
&lt;li&gt;网络访问和管理&lt;/li&gt;
&lt;li&gt;其他相关内容&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day3640---数据库基础和进阶" class="anchor" aria-hidden="true" href="#day3640---数据库基础和进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day36~40 - &lt;a href="./Day36-40"&gt;数据库基础和进阶&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="./Day36-40/36-38.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL.md"&gt;关系型数据库MySQL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;关系型数据库概述&lt;/li&gt;
&lt;li&gt;MySQL的安装和使用&lt;/li&gt;
&lt;li&gt;SQL的使用
&lt;ul&gt;
&lt;li&gt;DDL - 数据定义语言 - create / drop / alter&lt;/li&gt;
&lt;li&gt;DML - 数据操作语言 - insert / delete / update / select&lt;/li&gt;
&lt;li&gt;DCL - 数据控制语言 - grant / revoke&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;相关知识
&lt;ul&gt;
&lt;li&gt;范式理论 - 设计二维表的指导思想&lt;/li&gt;
&lt;li&gt;数据完整性&lt;/li&gt;
&lt;li&gt;数据一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在Python中操作MySQL&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="./Day36-40/39-40.NoSQL%E5%85%A5%E9%97%A8.md"&gt;NoSQL入门&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;NoSQL概述&lt;/li&gt;
&lt;li&gt;Redis概述&lt;/li&gt;
&lt;li&gt;Mongo概述&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day4155---实战django" class="anchor" aria-hidden="true" href="#day4155---实战django"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41~55 - &lt;a href="./Day41-55"&gt;实战Django&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day41---快速上手" class="anchor" aria-hidden="true" href="#day41---快速上手"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day41 - &lt;a href="./Day41-55/41.%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md"&gt;快速上手&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Web应用工作原理和HTTP协议&lt;/li&gt;
&lt;li&gt;Django框架概述&lt;/li&gt;
&lt;li&gt;5分钟快速上手&lt;/li&gt;
&lt;li&gt;使用视图模板&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day42---深入模型" class="anchor" aria-hidden="true" href="#day42---深入模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day42 - &lt;a href="./Day41-55/42.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md"&gt;深入模型&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;关系型数据库配置&lt;/li&gt;
&lt;li&gt;管理后台的使用&lt;/li&gt;
&lt;li&gt;使用ORM完成对模型的CRUD操作&lt;/li&gt;
&lt;li&gt;Django模型最佳实践&lt;/li&gt;
&lt;li&gt;模型定义参考&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day43---静态资源和ajax请求" class="anchor" aria-hidden="true" href="#day43---静态资源和ajax请求"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day43 - &lt;a href="./Day41-55/43.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md"&gt;静态资源和Ajax请求&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;加载静态资源&lt;/li&gt;
&lt;li&gt;用Ajax请求获取数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day44---表单的应用" class="anchor" aria-hidden="true" href="#day44---表单的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day44 - &lt;a href="./Day41-55/44.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;表单的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;表单和表单控件&lt;/li&gt;
&lt;li&gt;跨站请求伪造和CSRF令牌&lt;/li&gt;
&lt;li&gt;Form和ModelForm&lt;/li&gt;
&lt;li&gt;表单验证&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day45---cookie和session" class="anchor" aria-hidden="true" href="#day45---cookie和session"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day45 - &lt;a href="./Day41-55/45.Cookie%E5%92%8CSession.md"&gt;Cookie和Session&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;实现用户跟踪&lt;/li&gt;
&lt;li&gt;cookie和session的关系&lt;/li&gt;
&lt;li&gt;Django框架对session的支持&lt;/li&gt;
&lt;li&gt;视图函数中的cookie读写操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day46---报表和日志" class="anchor" aria-hidden="true" href="#day46---报表和日志"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day46 - &lt;a href="./Day41-55/46.%E6%8A%A5%E8%A1%A8%E5%92%8C%E6%97%A5%E5%BF%97.md"&gt;报表和日志&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;通过HttpResponse修改响应头&lt;/li&gt;
&lt;li&gt;使用StreamingHttpResponse处理大文件&lt;/li&gt;
&lt;li&gt;使用xlwt生成Excel报表&lt;/li&gt;
&lt;li&gt;使用reportlab生成PDF报表&lt;/li&gt;
&lt;li&gt;使用ECharts生成前端图表&lt;/li&gt;
&lt;li&gt;配置日志和Django-Debug-Toolbar&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day47---中间件的应用" class="anchor" aria-hidden="true" href="#day47---中间件的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day47 - &lt;a href="./Day41-55/47.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;中间件的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;什么是中间件&lt;/li&gt;
&lt;li&gt;Django框架内置的中间件&lt;/li&gt;
&lt;li&gt;自定义中间件及其应用场景&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day48---前后端分离开发入门" class="anchor" aria-hidden="true" href="#day48---前后端分离开发入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day48 - &lt;a href="./Day41-55/48.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md"&gt;前后端分离开发入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;返回JSON格式的数据&lt;/li&gt;
&lt;li&gt;用Vue.js渲染页面&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day49---restful架构和drf入门" class="anchor" aria-hidden="true" href="#day49---restful架构和drf入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day49 - &lt;a href="./Day41-55/49.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md"&gt;RESTful架构和DRF入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day50---restful架构和drf进阶" class="anchor" aria-hidden="true" href="#day50---restful架构和drf进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day50 - &lt;a href="./Day41-55/50.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md"&gt;RESTful架构和DRF进阶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day51---使用缓存" class="anchor" aria-hidden="true" href="#day51---使用缓存"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day51 - &lt;a href="./Day41-55/51.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md"&gt;使用缓存&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;网站优化第一定律&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在Django项目中使用Redis提供缓存服务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在视图函数中读写缓存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用装饰器实现页面缓存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为数据接口提供缓存服务&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day52---文件上传和富文本编辑" class="anchor" aria-hidden="true" href="#day52---文件上传和富文本编辑"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day52 - &lt;a href="./Day41-55/52.%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8.md"&gt;文件上传和富文本编辑&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;文件上传表单控件和图片文件预览&lt;/li&gt;
&lt;li&gt;服务器端如何处理上传的文件&lt;/li&gt;
&lt;li&gt;富文本编辑器概述&lt;/li&gt;
&lt;li&gt;wangEditor的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day53---短信和邮件" class="anchor" aria-hidden="true" href="#day53---短信和邮件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day53 - &lt;a href="./Day41-55/53.%E7%9F%AD%E4%BF%A1%E5%92%8C%E9%82%AE%E4%BB%B6.md"&gt;短信和邮件&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;常用短信网关平台介绍&lt;/li&gt;
&lt;li&gt;使用螺丝帽发送短信&lt;/li&gt;
&lt;li&gt;Django框架对邮件服务的支持&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day54---异步任务和定时任务" class="anchor" aria-hidden="true" href="#day54---异步任务和定时任务"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day54 - &lt;a href="./Day41-55/54.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md"&gt;异步任务和定时任务&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;网站优化第二定律&lt;/li&gt;
&lt;li&gt;配置消息队列服务&lt;/li&gt;
&lt;li&gt;在项目中使用celery实现任务异步化&lt;/li&gt;
&lt;li&gt;在项目中使用celery实现定时任务&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day55---单元测试和项目上线" class="anchor" aria-hidden="true" href="#day55---单元测试和项目上线"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day55 - &lt;a href="./Day41-55/55.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md"&gt;单元测试和项目上线&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python中的单元测试&lt;/li&gt;
&lt;li&gt;Django框架对单元测试的支持&lt;/li&gt;
&lt;li&gt;使用版本控制系统&lt;/li&gt;
&lt;li&gt;配置和使用uWSGI&lt;/li&gt;
&lt;li&gt;动静分离和Nginx配置&lt;/li&gt;
&lt;li&gt;配置HTTPS&lt;/li&gt;
&lt;li&gt;配置域名解析&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day5660---实战flask" class="anchor" aria-hidden="true" href="#day5660---实战flask"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56~60 - &lt;a href="./Day56-65"&gt;实战Flask&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day56---flask入门" class="anchor" aria-hidden="true" href="#day56---flask入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day56 - &lt;a href="./Day56-60/56.Flask%E5%85%A5%E9%97%A8.md"&gt;Flask入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day57---模板的使用" class="anchor" aria-hidden="true" href="#day57---模板的使用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day57 - &lt;a href="./Day56-60/57.%E6%A8%A1%E6%9D%BF%E7%9A%84%E4%BD%BF%E7%94%A8.md"&gt;模板的使用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day58---表单的处理" class="anchor" aria-hidden="true" href="#day58---表单的处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day58 - &lt;a href="./Day56-60/58.%E8%A1%A8%E5%8D%95%E7%9A%84%E5%A4%84%E7%90%86.md"&gt;表单的处理&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day59---数据库操作" class="anchor" aria-hidden="true" href="#day59---数据库操作"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day59 - &lt;a href="./Day56-60/59.%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C.md"&gt;数据库操作&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day60---项目实战" class="anchor" aria-hidden="true" href="#day60---项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day60 - &lt;a href="./Day56-60/60.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day6165---实战tornado" class="anchor" aria-hidden="true" href="#day6165---实战tornado"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61~65 - &lt;a href="./Day61-65"&gt;实战Tornado&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day61---预备知识" class="anchor" aria-hidden="true" href="#day61---预备知识"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day61 - &lt;a href="./Day61-65/61.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86.md"&gt;预备知识&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;并发编程&lt;/li&gt;
&lt;li&gt;I/O模式和事件驱动&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day62---tornado入门" class="anchor" aria-hidden="true" href="#day62---tornado入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day62 - &lt;a href="./Day61-65/62.Tornado%E5%85%A5%E9%97%A8.md"&gt;Tornado入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Tornado概述&lt;/li&gt;
&lt;li&gt;5分钟上手Tornado&lt;/li&gt;
&lt;li&gt;路由解析&lt;/li&gt;
&lt;li&gt;请求处理器&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day63---异步化" class="anchor" aria-hidden="true" href="#day63---异步化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day63 - &lt;a href="./Day61-65/63.%E5%BC%82%E6%AD%A5%E5%8C%96.md"&gt;异步化&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;aiomysql和aioredis的使用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day64---websocket的应用" class="anchor" aria-hidden="true" href="#day64---websocket的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day64 - &lt;a href="./Day61-65/64.WebSocket%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;WebSocket的应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;WebSocket简介&lt;/li&gt;
&lt;li&gt;WebSocket服务器端编程&lt;/li&gt;
&lt;li&gt;WebSocket客户端编程&lt;/li&gt;
&lt;li&gt;项目：Web聊天室&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day65---项目实战" class="anchor" aria-hidden="true" href="#day65---项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day65 - &lt;a href="./Day61-65/65.%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;前后端分离开发和接口文档的撰写&lt;/li&gt;
&lt;li&gt;使用Vue.js实现前端渲染&lt;/li&gt;
&lt;li&gt;使用ECharts实现报表功能&lt;/li&gt;
&lt;li&gt;使用WebSocket实现推送服务&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day6675---爬虫开发" class="anchor" aria-hidden="true" href="#day6675---爬虫开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66~75 - &lt;a href="./Day66-75"&gt;爬虫开发&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day66---网络爬虫和相关工具" class="anchor" aria-hidden="true" href="#day66---网络爬虫和相关工具"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day66 - &lt;a href="./Day66-75/66.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7.md"&gt;网络爬虫和相关工具&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;网络爬虫的概念及其应用领域&lt;/li&gt;
&lt;li&gt;网络爬虫的合法性探讨&lt;/li&gt;
&lt;li&gt;开发网络爬虫的相关工具&lt;/li&gt;
&lt;li&gt;一个爬虫程序的构成&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day67---数据采集和解析" class="anchor" aria-hidden="true" href="#day67---数据采集和解析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day67 - &lt;a href="./Day66-75/67.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90.md"&gt;数据采集和解析&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;数据采集的标准和三方库&lt;/li&gt;
&lt;li&gt;页面解析的三种方式：正则表达式解析 / XPath解析 / CSS选择器解析&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day68---存储数据" class="anchor" aria-hidden="true" href="#day68---存储数据"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day68 - &lt;a href="./Day66-75/68.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE.md"&gt;存储数据&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;如何存储海量数据&lt;/li&gt;
&lt;li&gt;实现数据的缓存&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day69---并发下载" class="anchor" aria-hidden="true" href="#day69---并发下载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day69 - &lt;a href="./Day66-75/69.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD.md"&gt;并发下载&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;多线程和多进程&lt;/li&gt;
&lt;li&gt;异步I/O和协程&lt;/li&gt;
&lt;li&gt;async和await关键字的使用&lt;/li&gt;
&lt;li&gt;三方库aiohttp的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day70---解析动态内容" class="anchor" aria-hidden="true" href="#day70---解析动态内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day70 - &lt;a href="./Day66-75/70.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md"&gt;解析动态内容&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JavaScript逆向工程&lt;/li&gt;
&lt;li&gt;使用Selenium获取动态内容&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day71---表单交互和验证码处理" class="anchor" aria-hidden="true" href="#day71---表单交互和验证码处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day71 - &lt;a href="./Day66-75/71.%E8%A1%A8%E5%8D%95%E4%BA%A4%E4%BA%92%E5%92%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E5%A4%84%E7%90%86.md"&gt;表单交互和验证码处理&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;自动提交表单&lt;/li&gt;
&lt;li&gt;Cookie池的应用&lt;/li&gt;
&lt;li&gt;验证码处理&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day72---scrapy入门" class="anchor" aria-hidden="true" href="#day72---scrapy入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day72 - &lt;a href="./Day66-75/72.Scrapy%E5%85%A5%E9%97%A8.md"&gt;Scrapy入门&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Scrapy爬虫框架概述&lt;/li&gt;
&lt;li&gt;安装和使用Scrapy&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day73---scrapy高级应用" class="anchor" aria-hidden="true" href="#day73---scrapy高级应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day73 - &lt;a href="./Day66-75/73.Scrapy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md"&gt;Scrapy高级应用&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Spider的用法&lt;/li&gt;
&lt;li&gt;中间件的应用：下载中间件 / 蜘蛛中间件&lt;/li&gt;
&lt;li&gt;Scrapy对接Selenium抓取动态内容&lt;/li&gt;
&lt;li&gt;Scrapy部署到Docker&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day74---scrapy分布式实现" class="anchor" aria-hidden="true" href="#day74---scrapy分布式实现"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day74 - &lt;a href="./Day66-75/74.Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0.md"&gt;Scrapy分布式实现&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;分布式爬虫的原理&lt;/li&gt;
&lt;li&gt;Scrapy分布式实现&lt;/li&gt;
&lt;li&gt;使用Scrapyd实现分布式部署&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-day75---爬虫项目实战" class="anchor" aria-hidden="true" href="#day75---爬虫项目实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day75 - &lt;a href="./Day66-75/75.%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98.md"&gt;爬虫项目实战&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;爬取招聘网站数据&lt;/li&gt;
&lt;li&gt;爬取房地产行业数据&lt;/li&gt;
&lt;li&gt;爬取二手车交易平台数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-day7690---数据处理和机器学习" class="anchor" aria-hidden="true" href="#day7690---数据处理和机器学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76~90 - &lt;a href="./Day76-90"&gt;数据处理和机器学习&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-day76---机器学习基础" class="anchor" aria-hidden="true" href="#day76---机器学习基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day76 - &lt;a href="./Day76-90/76.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md"&gt;机器学习基础&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day77---pandas的应用" class="anchor" aria-hidden="true" href="#day77---pandas的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day77 - &lt;a href="./Day76-90/77.Pandas%E7%9A%84%E5%BA%94%E7%94%A8.md"&gt;Pandas的应用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day78---numpy和scipy的应用" class="anchor" aria-hidden="true" href="#day78---numpy和scipy的应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day78 - &lt;a href="./Day76-90/78.NumPy%E5%92%8CSciPy%E7%9A%84%E5%BA%94%E7%94%A8"&gt;NumPy和SciPy的应用&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day79---matplotlib和数据可视化" class="anchor" aria-hidden="true" href="#day79---matplotlib和数据可视化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day79 - &lt;a href="./Day76-90/79.Matplotlib%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"&gt;Matplotlib和数据可视化&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day80---k最近邻knn分类" class="anchor" aria-hidden="true" href="#day80---k最近邻knn分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day80 - &lt;a href="./Day76-90/80.k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB.md"&gt;k最近邻(KNN)分类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day81---决策树" class="anchor" aria-hidden="true" href="#day81---决策树"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day81 - &lt;a href="./Day76-90/81.%E5%86%B3%E7%AD%96%E6%A0%91.md"&gt;决策树&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day82---贝叶斯分类" class="anchor" aria-hidden="true" href="#day82---贝叶斯分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day82 - &lt;a href="./Day76-90/82.%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB.md"&gt;贝叶斯分类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day83---支持向量机svm" class="anchor" aria-hidden="true" href="#day83---支持向量机svm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day83 - &lt;a href="./Day76-90/83.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.md"&gt;支持向量机(SVM)&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day84---k-均值聚类" class="anchor" aria-hidden="true" href="#day84---k-均值聚类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day84 - &lt;a href="./Day76-90/84.K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB.md"&gt;K-均值聚类&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day85---回归分析" class="anchor" aria-hidden="true" href="#day85---回归分析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day85 - &lt;a href="./Day76-90/85.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90.md"&gt;回归分析&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day86---大数据分析入门" class="anchor" aria-hidden="true" href="#day86---大数据分析入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day86 - &lt;a href="./Day76-90/86.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8.md"&gt;大数据分析入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day87---大数据分析进阶" class="anchor" aria-hidden="true" href="#day87---大数据分析进阶"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day87 - &lt;a href="./Day76-90/87.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BF%9B%E9%98%B6.md"&gt;大数据分析进阶&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day88---tensorflow入门" class="anchor" aria-hidden="true" href="#day88---tensorflow入门"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day88 - &lt;a href="./Day76-90/88.Tensorflow%E5%85%A5%E9%97%A8.md"&gt;Tensorflow入门&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day89---tensorflow实战" class="anchor" aria-hidden="true" href="#day89---tensorflow实战"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day89 - &lt;a href="./Day76-90/89.Tensorflow%E5%AE%9E%E6%88%98.md"&gt;Tensorflow实战&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-day90---推荐系统" class="anchor" aria-hidden="true" href="#day90---推荐系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day90 - &lt;a href="./Day76-90/90.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.md"&gt;推荐系统&lt;/a&gt;&lt;/h4&gt;
&lt;h3&gt;&lt;a id="user-content-day91100---团队项目开发" class="anchor" aria-hidden="true" href="#day91100---团队项目开发"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Day91~100 - &lt;a href="./Day91-100"&gt;团队项目开发&lt;/a&gt;&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-第91天团队项目开发的问题和解决方案" class="anchor" aria-hidden="true" href="#第91天团队项目开发的问题和解决方案"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第91天：&lt;a href="./Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;团队项目开发的问题和解决方案&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;软件过程模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;经典过程模型（瀑布模型）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可行性分析（研究做还是不做），输出《可行性分析报告》。&lt;/li&gt;
&lt;li&gt;需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。&lt;/li&gt;
&lt;li&gt;概要设计和详细设计，输出概念模型图（ER图）、物理模型图、类图、时序图等。&lt;/li&gt;
&lt;li&gt;编码 / 测试。&lt;/li&gt;
&lt;li&gt;上线 / 维护。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;瀑布模型最大的缺点是无法拥抱需求变化，整套流程结束后才能看到产品，团队士气低落。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;产品的Backlog（用户故事、产品原型）。&lt;/li&gt;
&lt;li&gt;计划会议（评估和预算）。&lt;/li&gt;
&lt;li&gt;日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。&lt;/li&gt;
&lt;li&gt;修复bug（问题描述、重现步骤、测试人员、被指派人）。&lt;/li&gt;
&lt;li&gt;发布版本。&lt;/li&gt;
&lt;li&gt;评审会议（Showcase，用户需要参与）。&lt;/li&gt;
&lt;li&gt;回顾会议（对当前迭代周期做一个总结）。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;补充：敏捷软件开发宣言&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;个体和互动&lt;/strong&gt; 高于 流程和工具&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作的软件&lt;/strong&gt; 高于 详尽的文档&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;客户合作&lt;/strong&gt; 高于 合同谈判&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;响应变化&lt;/strong&gt; 高于 遵循计划&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/agile-scrum-sprint-cycle.png"&gt;&lt;img src="./res/agile-scrum-sprint-cycle.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;敏捷团队通常人数为8-10人。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在看板上面，看板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;项目团队组建&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;团队的构成和角色&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;说明：谢谢付祥英女士绘制了下面这张精美的公司组织架构图。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/company_architecture.png"&gt;&lt;img src="./res/company_architecture.png" alt="company_architecture" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编程规范和代码审查（flake8、pylint）&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/pylint.png"&gt;&lt;img src="./res/pylint.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python中的一些“惯例”（请参考&lt;a href="Python%E6%83%AF%E4%BE%8B.md"&gt;《Python惯例-如何编写Pythonic的代码》&lt;/a&gt;）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;影响代码可读性的原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码注释太少或者没有注释&lt;/li&gt;
&lt;li&gt;代码破坏了语言的最佳实践&lt;/li&gt;
&lt;li&gt;反模式编程（意大利面代码、复制-黏贴编程、自负编程、……）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;团队开发工具介绍&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;版本控制：Git、Mercury&lt;/li&gt;
&lt;li&gt;缺陷管理：&lt;a href="https://about.gitlab.com/" rel="nofollow"&gt;Gitlab&lt;/a&gt;、&lt;a href="http://www.redmine.org.cn/" rel="nofollow"&gt;Redmine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;敏捷闭环工具：&lt;a href="https://www.zentao.net/" rel="nofollow"&gt;禅道&lt;/a&gt;、&lt;a href="https://www.atlassian.com/software/jira/features" rel="nofollow"&gt;JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;持续集成：&lt;a href="https://jenkins.io/" rel="nofollow"&gt;Jenkins&lt;/a&gt;、&lt;a href="https://travis-ci.org/" rel="nofollow"&gt;Travis-CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;请参考&lt;a href="Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md"&gt;《团队项目开发的问题和解决方案》&lt;/a&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目选题和理解业务" class="anchor" aria-hidden="true" href="#项目选题和理解业务"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目选题和理解业务&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;选题范围设定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CMS（用户端）：新闻聚合网站、问答/分享社区、影评/书评网站等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他类型：自身行业背景和工作经验、业务容易理解和把控。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需求理解、模块划分和任务分配&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需求理解：头脑风暴和竞品分析。&lt;/li&gt;
&lt;li&gt;模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。&lt;/li&gt;
&lt;li&gt;任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/requirements_by_xmind.png"&gt;&lt;img src="./res/requirements_by_xmind.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;制定项目进度表（每日更新）&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;th&gt;人员&lt;/th&gt;
&lt;th&gt;状态&lt;/th&gt;
&lt;th&gt;完成&lt;/th&gt;
&lt;th&gt;工时&lt;/th&gt;
&lt;th&gt;计划开始&lt;/th&gt;
&lt;th&gt;实际开始&lt;/th&gt;
&lt;th&gt;计划结束&lt;/th&gt;
&lt;th&gt;实际结束&lt;/th&gt;
&lt;th&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;评论&lt;/td&gt;
&lt;td&gt;添加评论&lt;/td&gt;
&lt;td&gt;王大锤&lt;/td&gt;
&lt;td&gt;正在进行&lt;/td&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;删除评论&lt;/td&gt;
&lt;td&gt;王大锤&lt;/td&gt;
&lt;td&gt;等待&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;查看评论&lt;/td&gt;
&lt;td&gt;白元芳&lt;/td&gt;
&lt;td&gt;正在进行&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/7&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;需要进行代码审查&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;评论投票&lt;/td&gt;
&lt;td&gt;白元芳&lt;/td&gt;
&lt;td&gt;等待&lt;/td&gt;
&lt;td&gt;0%&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;2018/8/8&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OOAD和数据库设计&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;UML（统一建模语言）的类图&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/uml-class-diagram.png"&gt;&lt;img src="./res/uml-class-diagram.png" alt="uml" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过模型创建表（正向工程）&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py makemigrations app
python manage.py migrate&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用PowerDesigner绘制物理模型图。&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="./res/power-designer-pdm.png"&gt;&lt;img src="./res/power-designer-pdm.png" alt="" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过数据表创建模型（反向工程）&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python manage.py inspectdb &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; app/models.py&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-第92天docker容器详解" class="anchor" aria-hidden="true" href="#第92天docker容器详解"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第92天：&lt;a href="./Day91-100/92.Docker%E5%AE%B9%E5%99%A8%E8%AF%A6%E8%A7%A3.md"&gt;Docker容器详解&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Docker简介&lt;/li&gt;
&lt;li&gt;安装Docker&lt;/li&gt;
&lt;li&gt;使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins）&lt;/li&gt;
&lt;li&gt;构建Docker镜像（Dockerfile的编写和相关指令）&lt;/li&gt;
&lt;li&gt;容器编排（Docker-compose）&lt;/li&gt;
&lt;li&gt;集群管理&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第93天mysql性能优化" class="anchor" aria-hidden="true" href="#第93天mysql性能优化"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第93天：&lt;a href="./Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md"&gt;MySQL性能优化&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第94天网络api接口设计" class="anchor" aria-hidden="true" href="#第94天网络api接口设计"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第94天：&lt;a href="./Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md"&gt;网络API接口设计&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第95天使用django开发商业项目day91-10095使用django开发商业项目md" class="anchor" aria-hidden="true" href="#第95天使用django开发商业项目day91-10095使用django开发商业项目md"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项	目.md)&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-项目开发中的公共问题" class="anchor" aria-hidden="true" href="#项目开发中的公共问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目开发中的公共问题&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;数据库的配置（多数据库、主从复制、数据库路由）&lt;/li&gt;
&lt;li&gt;缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵））&lt;/li&gt;
&lt;li&gt;日志的配置&lt;/li&gt;
&lt;li&gt;分析和调试（Django-Debug-ToolBar）&lt;/li&gt;
&lt;li&gt;好用的Python模块（日期计算、图像处理、数据加密、三方API）&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-rest-api设计" class="anchor" aria-hidden="true" href="#rest-api设计"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;REST API设计&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;RESTful架构
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2011/09/restful.html" rel="nofollow"&gt;理解RESTful架构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2014/05/restful_api.html" rel="nofollow"&gt;RESTful API设计指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html" rel="nofollow"&gt;RESTful API最佳实践&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;API接口文档的撰写
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rap2.taobao.org/" rel="nofollow"&gt;RAP2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yapi.demo.qunar.com/" rel="nofollow"&gt;YAPI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.django-rest-framework.org/" rel="nofollow"&gt;django-REST-framework&lt;/a&gt;的应用&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目中的重点难点剖析" class="anchor" aria-hidden="true" href="#项目中的重点难点剖析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目中的重点难点剖析&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;使用缓存缓解数据库压力 - Redis&lt;/li&gt;
&lt;li&gt;使用消息队列做解耦合和削峰 - Celery + RabbitMQ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第96天软件测试和自动化测试" class="anchor" aria-hidden="true" href="#第96天软件测试和自动化测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第96天：&lt;a href="Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md"&gt;软件测试和自动化测试&lt;/a&gt;&lt;/h4&gt;
&lt;h5&gt;&lt;a id="user-content-单元测试" class="anchor" aria-hidden="true" href="#单元测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;单元测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;测试的种类&lt;/li&gt;
&lt;li&gt;编写单元测试（unittest、pytest、nose2、tox、ddt、……）&lt;/li&gt;
&lt;li&gt;测试覆盖率（coverage）&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-项目部署" class="anchor" aria-hidden="true" href="#项目部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目部署&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;部署前的准备工作
&lt;ul&gt;
&lt;li&gt;关键设置（SECRET_KEY / DEBUG / ALLOWED_HOSTS / 缓存 / 数据库）&lt;/li&gt;
&lt;li&gt;HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE&lt;/li&gt;
&lt;li&gt;日志相关配置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linux常用命令回顾&lt;/li&gt;
&lt;li&gt;Linux常用服务的安装和配置&lt;/li&gt;
&lt;li&gt;uWSGI/Gunicorn和Nginx的使用
&lt;ul&gt;
&lt;li&gt;Gunicorn和uWSGI的比较
&lt;ul&gt;
&lt;li&gt;对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。&lt;/li&gt;
&lt;li&gt;uWSGI支持异构部署。&lt;/li&gt;
&lt;li&gt;由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。&lt;/li&gt;
&lt;li&gt;在性能上，Gunicorn和uWSGI其实表现相当。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用虚拟化技术（Docker）部署测试环境和生产环境&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-性能测试" class="anchor" aria-hidden="true" href="#性能测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;性能测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;AB的使用&lt;/li&gt;
&lt;li&gt;SQLslap的使用&lt;/li&gt;
&lt;li&gt;sysbench的使用&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;&lt;a id="user-content-自动化测试" class="anchor" aria-hidden="true" href="#自动化测试"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自动化测试&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;使用Shell和Python进行自动化测试&lt;/li&gt;
&lt;li&gt;使用Selenium实现自动化测试
&lt;ul&gt;
&lt;li&gt;Selenium IDE&lt;/li&gt;
&lt;li&gt;Selenium WebDriver&lt;/li&gt;
&lt;li&gt;Selenium Remote Control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;测试工具Robot Framework介绍&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第97天电商网站技术要点剖析" class="anchor" aria-hidden="true" href="#第97天电商网站技术要点剖析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第97天：&lt;a href="./Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md"&gt;电商网站技术要点剖析&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第98天项目部署上线和性能调优" class="anchor" aria-hidden="true" href="#第98天项目部署上线和性能调优"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第98天：&lt;a href="./Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md"&gt;项目部署上线和性能调优&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;MySQL数据库调优&lt;/li&gt;
&lt;li&gt;Web服务器性能优化
&lt;ul&gt;
&lt;li&gt;Nginx负载均衡配置&lt;/li&gt;
&lt;li&gt;Keepalived实现高可用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;代码性能调优
&lt;ul&gt;
&lt;li&gt;多线程&lt;/li&gt;
&lt;li&gt;异步化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;静态资源访问优化
&lt;ul&gt;
&lt;li&gt;云存储&lt;/li&gt;
&lt;li&gt;CDN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-第99天面试中的公共问题" class="anchor" aria-hidden="true" href="#第99天面试中的公共问题"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第99天：&lt;a href="./Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md"&gt;面试中的公共问题&lt;/a&gt;&lt;/h4&gt;
&lt;h4&gt;&lt;a id="user-content-第100天python面试题集" class="anchor" aria-hidden="true" href="#第100天python面试题集"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;第100天：&lt;a href="./Day91-100/100.Python%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86.md"&gt;Python面试题集&lt;/a&gt;&lt;/h4&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>jackfrued</author><guid isPermaLink="false">https://github.com/jackfrued/Python-100-Days</guid><pubDate>Mon, 10 Feb 2020 00:23:00 GMT</pubDate></item><item><title>xavier-zy/Awesome-pytorch-list-CNVersion #24 in Jupyter Notebook, Today</title><link>https://github.com/xavier-zy/Awesome-pytorch-list-CNVersion</link><description>&lt;p&gt;&lt;i&gt;Awesome-pytorch-list 翻译工作进行中......&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-awesome-pytorch-list厉害的pytorch项目" class="anchor" aria-hidden="true" href="#awesome-pytorch-list厉害的pytorch项目"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Awesome-Pytorch-list｜厉害的Pytorch项目&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png"&gt;&lt;img src="https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png" alt="pytorch-logo-dark" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-english-version" class="anchor" aria-hidden="true" href="#english-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/bharathgs/Awesome-pytorch-list"&gt;English Version&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a id="user-content-contents内容" class="anchor" aria-hidden="true" href="#contents内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents｜内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#awesome-pytorch-list%EF%BD%9C%E5%8E%89%E5%AE%B3%E7%9A%84pytorch%E9%A1%B9%E7%9B%AE"&gt;Awesome-Pytorch-list｜厉害的Pytorch项目&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#english-version"&gt;English Version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contents%EF%BD%9C%E5%86%85%E5%AE%B9"&gt;Contents｜内容&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pytorch-related-libraries%EF%BD%9Cpytorch-%E7%9B%B8%E5%85%B3%E5%BA%93"&gt;Pytorch &amp;amp; related libraries｜Pytorch &amp;amp; 相关库&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#nlp-speech-processing%EF%BD%9C%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86"&gt;NLP &amp;amp; Speech Processing｜自然语言处理 &amp;amp; 语音处理:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cv%EF%BD%9C%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"&gt;CV｜计算机视觉:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#probabilisticgenerative-libraries%EF%BD%9C%E6%A6%82%E7%8E%87%E5%BA%93%E5%92%8C%E7%94%9F%E6%88%90%E5%BA%93"&gt;Probabilistic/Generative Libraries｜概率库和生成库:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#other-libraries%EF%BD%9C%E5%85%B6%E4%BB%96%E5%BA%93"&gt;Other libraries｜其他库:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#tutorials-examples%EF%BD%9C%E6%95%99%E7%A8%8B-%E7%A4%BA%E4%BE%8B"&gt;Tutorials &amp;amp; examples｜教程 &amp;amp; 示例&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#paper-implementations%EF%BD%9C%E8%AE%BA%E6%96%87%E5%AE%9E%E7%8E%B0"&gt;Paper implementations｜论文实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#talks-conferences%EF%BD%9C%E6%8A%A5%E5%91%8A-%E4%BC%9A%E8%AE%AE"&gt;Talks &amp;amp; conferences｜报告 &amp;amp; 会议&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pytorch-elsewhere-%EF%BD%9C-pytorch%E7%9B%B8%E5%85%B3"&gt;Pytorch elsewhere ｜ Pytorch相关&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch--related-librariespytorch--相关库" class="anchor" aria-hidden="true" href="#pytorch--related-librariespytorch--相关库"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pytorch &amp;amp; related libraries｜Pytorch &amp;amp; 相关库&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://pytorch.org" rel="nofollow"&gt;pytorch&lt;/a&gt;: Tensors and Dynamic neural networks in Python with strong GPU acceleration | 使用强GPU加速的Python张量计算和动态神经网络.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-nlp--speech-processing自然语言处理--语音处理" class="anchor" aria-hidden="true" href="#nlp--speech-processing自然语言处理--语音处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP &amp;amp; Speech Processing｜自然语言处理 &amp;amp; 语音处理:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;kbd&gt;2000+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/text"&gt;text&lt;/a&gt;: 针对文本数据和NLP数据集的数据加载和抽象。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/IBM/pytorch-seq2seq"&gt;pytorch-seq2seq&lt;/a&gt;: Pytorch中处理seq2seq的开源框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Sandeep42/anuvada"&gt;anuvada&lt;/a&gt;: NLP可解释模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/audio"&gt;audio&lt;/a&gt;: 简单的音频I/O。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/loop"&gt;loop&lt;/a&gt;:  一种跨多说话者的语音生成方法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;6000+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/fairseq-py"&gt;fairseq&lt;/a&gt;: Facebook开发的Sequence-to-Sequence python工具包。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/awni/speech"&gt;speech&lt;/a&gt;: 语音转文字的端到端模型实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3500+&lt;/kbd&gt; &lt;a href="https://github.com/OpenNMT/OpenNMT-py"&gt;OpenNMT-py&lt;/a&gt;: 开源神经机器翻译 &lt;a href="http://opennmt.net" rel="nofollow"&gt;http://opennmt.net&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/huggingface/neuralcoref"&gt;neuralcoref&lt;/a&gt;: 在spaCy中使用神经网络实现快速共指消解。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/sentiment-discovery"&gt;sentiment-discovery&lt;/a&gt;: 基于规模的无监督语言模型在稳健情绪分类中的应用。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2200+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/MUSE"&gt;MUSE&lt;/a&gt;: 一个多语言无监督或有监督词语嵌入库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/lium-lst/nmtpytorch"&gt;nmtpytorch&lt;/a&gt;: PyTorch中的Sequence-to-Sequence框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/vincentherrmann/pytorch-wavenet"&gt;pytorch-wavenet&lt;/a&gt;: 快速生成WaveNet的实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/soobinseo/Tacotron-pytorch"&gt;Tacotron-pytorch&lt;/a&gt;: Tacotron: 端到端语音合成。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;7400+&lt;/kbd&gt; &lt;a href="https://github.com/allenai/allennlp"&gt;AllenNLP&lt;/a&gt;: 开源NLP研究库，基于PyTorch。&lt;a href="https://allennlp.org" rel="nofollow"&gt;http://www.allennlp.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1500+&lt;/kbd&gt; &lt;a href="https://github.com/PetrochukM/PyTorch-NLP"&gt;PyTorch-NLP&lt;/a&gt;: 为加速NLP研究设立的一个库，包含神经网络层、文本处理模块和众多数据集。 pytorchnlp.readthedocs.io&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/outcastofmusic/quick-nlp"&gt;quick-nlp&lt;/a&gt;: 基于FastAI的Pytorch NLP库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1400+&lt;/kbd&gt; &lt;a href="https://github.com/mozilla/TTS"&gt;TTS&lt;/a&gt;: 文本转语音的深度学习框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2000+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/LASER"&gt;LASER&lt;/a&gt;: LASER是一个用来计算和使用多语言语句嵌入的库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pyannote/pyannote-audio"&gt;pyannote-audio&lt;/a&gt;: 用于说话人分类的神经构建块：语音活动检测, 说话人变化检测, 说话人嵌入。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Maluuba/gensen"&gt;gensen&lt;/a&gt;: 基于大规模多任务学习的通用句子表示。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/translate"&gt;translate&lt;/a&gt;: 翻译——一个PyTorch语言库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1500+&lt;/kbd&gt; &lt;a href="https://github.com/espnet/espnet"&gt;espnet&lt;/a&gt;: 端到端语音处理工具集。 espnet.github.io/espnet&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2800+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/pythia"&gt;pythia&lt;/a&gt;: 源于FAIR(Facebook AI Research)的视觉与语言多模态研究的模块化框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/UnsupervisedMT"&gt;UnsupervisedMT&lt;/a&gt;: 基于短语的神经无监督机器翻译。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jsalt18-sentence-repl/jiant"&gt;jiant&lt;/a&gt;: 通用文本理解模型的jiant工具包。&lt;a href="https://jiant.info" rel="nofollow"&gt;https://jiant.info&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2900+&lt;/kbd&gt; &lt;a href="https://github.com/codertimo/BERT-pytorch"&gt;BERT-PyTorch&lt;/a&gt;: Google AI 2018 BERT 的 Pytorch 实现，伴有简单注释。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1700+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/InferSent"&gt;InferSent&lt;/a&gt;: NLI的句子嵌入(InferSent)和训练代码。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000+&lt;/kbd&gt; &lt;a href="https://github.com/google/uis-rnn"&gt;uis-rnn&lt;/a&gt;:无限交错状态递归神经网络(UIS-RNN)算法，能够从嘈杂的环境中分辨声音，对应论文 Fully Supervised Speaker Diarization. arxiv.org/abs/1810.04719&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;7500+&lt;/kbd&gt; &lt;a href="https://github.com/zalandoresearch/flair"&gt;flair&lt;/a&gt;: 一个针对最先进的NLP的简单框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;5500+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/pytext"&gt;pytext&lt;/a&gt;: 基于PyTorch的自然语言建模框架。 fb.me/pytextdocs&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/mindslab-ai/voicefilter"&gt;voicefilter&lt;/a&gt;: 谷歌AI的VoiceFilter的非官方实现。 &lt;a href="http://swpark.me/voicefilter" rel="nofollow"&gt;http://swpark.me/voicefilter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kamalkraj/BERT-NER"&gt;BERT-NER&lt;/a&gt;: 基于BERT的命名体识别(Named-Entity-Recognition)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/feedly/transfer-nlp"&gt;transfer-nlp&lt;/a&gt;: 为可复制实验管理而设计的NLP库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/asyml/texar-pytorch"&gt;texar-pytorch&lt;/a&gt;: 机器学习和文本生成工具包。 texar.io&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/mravanelli/pytorch-kaldi"&gt;pytorch-kaldi&lt;/a&gt;: pytorch-kaldi 是一个开发中的最先进的dnn/rnn混合语音识别系统。其DNN部分由PyTorch实现，而特征提取、标签计算和解码由kaldi工具包完成。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NVIDIA/NeMo"&gt;NeMo&lt;/a&gt;: 神经模块：对话式AI（conversational AI）工具集 nvidia.github.io/NeMo&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/harvardnlp/pytorch-struct"&gt;pytorch-struct&lt;/a&gt;: 经过测试的GPU实现库，实现了深度学习中的一些核心的结构化算法，如HMM, Dep Trees, CKY, ...&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/freewym/espresso"&gt;espresso&lt;/a&gt;: Espresso: 快速的端到端神经语音识别工具集。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt;: huggingface Transformers: TensorFlow 2.0  和 PyTorch 上最先进的NLP工具。huggingface.co/transformers&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lucidrains/reformer-pytorch"&gt;reformer-pytorch&lt;/a&gt;: &lt;a href="https://openreview.net/pdf?id=rkgNKkHtvB" rel="nofollow"&gt;Reformer&lt;/a&gt; 的 PyTorch 版。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-cv计算机视觉" class="anchor" aria-hidden="true" href="#cv计算机视觉"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CV｜计算机视觉:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;kbd&gt;4800+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/vision"&gt;pytorch vision&lt;/a&gt;: TorchVision包含流行的数据集、模型架构、计算机视觉中常用的图像变换。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tymokvo/pt-styletransfer"&gt;pt-styletransfer&lt;/a&gt;: 作为PyTorch中一个类的神经风格转移。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/thnkim/OpenFacePytorch"&gt;OpenFacePytorch&lt;/a&gt;: 使用OpenFace的nn4.small2.v1.t7模型的PyTorch模块。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/felixgwu/img_classification_pk_pytorch"&gt;img_classification_pk_pytorch&lt;/a&gt;: 将你的图像分类模型和最先进的模型进行快速比较 (比如DenseNet, ResNet, ...)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/SparseConvNet"&gt;SparseConvNet&lt;/a&gt;: 子流形稀疏卷积神经网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/automan000/Convolution_LSTM_pytorch"&gt;Convolution_LSTM_pytorch&lt;/a&gt;: 多层卷积LSTM(长短期记忆网络)模块。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3200+&lt;/kbd&gt; &lt;a href="https://github.com/1adrianb/face-alignment"&gt;face-alignment&lt;/a&gt;: &lt;g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png"&gt;🔥&lt;/g-emoji&gt; 基于 PyTorch 的 2D 和 3D 面部对齐库。 adrianbulat.com&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/ZijunDeng/pytorch-semantic-segmentation"&gt;pytorch-semantic-segmentation&lt;/a&gt;: 语义分割。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/longcw/RoIAlign.pytorch"&gt;RoIAlign.pytorch&lt;/a&gt;: PyTorch版本的RoIAlign。其实现基于crop_and_resize，支持CPU和GPU上的前向和后向。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/creafz/pytorch-cnn-finetune"&gt;pytorch-cnn-finetune&lt;/a&gt;: 用PyTorch微调预训练卷积神经网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ignacio-rocco/detectorch"&gt;detectorch&lt;/a&gt;: Detectorch - PyTorch版detectron框架，目前仅有detectron的推断(inference)和评估(evalutaion)功能，无训练(training)功能。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3500+&lt;/kbd&gt; &lt;a href="https://github.com/mdbloice/Augmentor"&gt;Augmentor&lt;/a&gt;: 用于机器学习的图像增强库。 &lt;a href="http://augmentor.readthedocs.io" rel="nofollow"&gt;http://augmentor.readthedocs.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jonas-koehler/s2cnn"&gt;s2cnn&lt;/a&gt;: Spherical CNNs：球面卷积网络的PyTorch实现。 (e.g. 全方位图像、全球信号)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/donnyyou/torchcv"&gt;TorchCV&lt;/a&gt;: 基于PyTorch的计算机视觉深度学习框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;6800+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark"&gt;maskrcnn-benchmark&lt;/a&gt;: 实例分割与对象检测的快速模块化参考实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/osmr/imgclsmob"&gt;image-classification-mobile&lt;/a&gt;: 计算机视觉卷积网络训练沙盒，包含ImageNet-1K上的与训练分类模型集合。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/perone/medicaltorch"&gt;medicaltorch&lt;/a&gt;: 一个医学成像框架。&lt;a href="http://medicaltorch.readthedocs.io" rel="nofollow"&gt;http://medicaltorch.readthedocs.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3800+&lt;/kbd&gt; &lt;a href="https://github.com/albu/albumentations"&gt;albumentations&lt;/a&gt;: 快速图像增强库和其他库的易用包装器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1400+&lt;/kbd&gt; &lt;a href="https://github.com/arraiyopensource/kornia"&gt;kornia&lt;/a&gt;: 开源可微计算机视觉库。&lt;a href="https://kornia.org" rel="nofollow"&gt;https://kornia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/s3nh/text-detector"&gt;text-detector&lt;/a&gt;: 检测和翻译文本。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/timesler/facenet-pytorch"&gt;facenet-pytorch&lt;/a&gt;: 预训练Pytorch人脸检测与识别模型，从 &lt;a href="https://github.com/davidsandberg/facenet"&gt;davidsandberg/facenet&lt;/a&gt; 移植而来。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/detectron2"&gt;detectron2&lt;/a&gt;: Detectron2是FAIR的下一代目标检测和分割研究平台。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Media-Smart/vedaseg"&gt;vedaseg&lt;/a&gt;: 基于PyTorch的语义分割工具箱。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/ClassyVision"&gt;ClassyVision&lt;/a&gt;: A用于图像和视频分类的端到端PyTorch框架。&lt;a href="https://classyvision.ai" rel="nofollow"&gt;https://classyvision.ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/alankbi/detecto"&gt;detecto&lt;/a&gt;: 用 5 行代码构建功能完备的计算机视觉模型。&lt;a href="https://detecto.readthedocs.io/" rel="nofollow"&gt;https://detecto.readthedocs.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/pytorch3d"&gt;pytorch3d&lt;/a&gt;: PyTorch3d 是一个面向深度学习的高效、可复用的 3D 计算机视觉库。 &lt;a href="https://pytorch3d.org/" rel="nofollow"&gt;https://pytorch3d.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-probabilisticgenerative-libraries概率库和生成库" class="anchor" aria-hidden="true" href="#probabilisticgenerative-libraries概率库和生成库"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Probabilistic/Generative Libraries｜概率库和生成库:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/stepelu/ptstat"&gt;ptstat&lt;/a&gt;: 概率编程和统计推断。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;5700+&lt;/kbd&gt; &lt;a href="https://github.com/uber/pyro"&gt;pyro&lt;/a&gt;: 基于 Python 和 PyTorch 的深度通用概率编程库。 &lt;a href="http://pyro.ai" rel="nofollow"&gt;http://pyro.ai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/probtorch/probtorch"&gt;probtorch&lt;/a&gt;: Probabilistic Torch是一个扩展了PyTorch的深度生成模型的库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/drckf/paysage"&gt;paysage&lt;/a&gt;: 基于Python/PyTorch的非监督学习和生成模型库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ctallec/pyvarinf"&gt;pyvarinf&lt;/a&gt;: Python包，促进了带有变分推断的贝叶斯深度学习方法在pytorch中的应用。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/probprog/pyprob"&gt;pyprob&lt;/a&gt;: 一个基于PyTorch的概率编程与推断编译的库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/spring-epfl/mia"&gt;mia&lt;/a&gt;: 一个运行针对机器学习模型的成员推理攻击的库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/akanimax/pro_gan_pytorch"&gt;pro_gan_pytorch&lt;/a&gt;: 作为PyTorch nn.Module的扩展的ProGAN包。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1300+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/botorch"&gt;botorch&lt;/a&gt;: PyTorch中的贝叶斯优化。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-other-libraries其他库" class="anchor" aria-hidden="true" href="#other-libraries其他库"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other libraries｜其他库:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mrdrozdov/pytorch-extras"&gt;pytorch extras&lt;/a&gt;: PyTorch的额外特性。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/szagoruyko/functional-zoo"&gt;functional zoo&lt;/a&gt;: PyTorch和Tensorflow的模型定义和预训练权重。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1300+&lt;/kbd&gt; &lt;a href="https://github.com/ncullen93/torchsample"&gt;torch-sampling&lt;/a&gt;: Pytorch的采样、高级训练、数据增强和实用程序。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/deepcraft/torchcraft-py"&gt;torchcraft-py&lt;/a&gt;: TorchCraft的Python包装器，TorchCraft是连接Torch和StarCraft的桥梁。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ramon-oliveira/aorun"&gt;aorun&lt;/a&gt;: Aorun试图以PyTorch为后端实现类似于Keras的API。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/oval-group/logger"&gt;logger&lt;/a&gt;: 机器学习记录器（logger）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/iamaziz/PyTorch-docset"&gt;PyTorch-docset&lt;/a&gt;: PyTorch离线文档，结合Dash，Zeal，Velocity或者LovelyDocs使用。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/clcarwin/convert_torch_to_pytorch"&gt;convert_torch_to_pytorch&lt;/a&gt;: 将Torch t7模型转换为PyTorch模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;5500+&lt;/kbd&gt; &lt;a href="https://github.com/Cadene/pretrained-models.pytorch"&gt;pretrained-models.pytorch&lt;/a&gt;: PyTorch 预训练卷积神经网络：NASNet, ResNeXt, ResNet, InceptionV4, InceptionResnetV2, Xception, DPN 等等。该项目的目标是帮助复制研究论文结果。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/locuslab/pytorch_fft"&gt;pytorch_fft&lt;/a&gt;: CUDA FFTs的PyTorch包装器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/fanq15/caffe_to_torch_to_pytorch"&gt;caffe_to_torch_to_pytorch&lt;/a&gt;: Caffe模型转PyTorch/Torch模型，Torch模型转PyTorch模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/sniklaus/pytorch-extension"&gt;pytorch-extension&lt;/a&gt;: PyTorch的CUDA扩展示例，计算了两个张量的&lt;a href="https://baike.baidu.com/item/%E5%93%88%E8%BE%BE%E7%8E%9B%E7%A7%AF/18894493?fr=aladdin" rel="nofollow"&gt;哈达玛积(Hadamard product)&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;5700+&lt;/kbd&gt; &lt;a href="https://github.com/lanpa/tensorboard-pytorch"&gt;tensorboard-pytorch&lt;/a&gt;: 该模块以tensorboard格式保存PyTorch张量以供检查。目前支持tensorboard中的标量、图像、音频、直方图等特性。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/jrg365/gpytorch"&gt;gpytorch&lt;/a&gt;: GPyTorch是一个用PyTorch实现的高斯过程库。它可以轻松地创建可伸缩、灵活和模块化的高斯过程模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1900+&lt;/kbd&gt; &lt;a href="https://github.com/maciejkula/spotlight"&gt;spotlight&lt;/a&gt;: 深度推荐模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/awentzonline/pytorch-cns"&gt;pytorch-cns&lt;/a&gt;: 基于PyTorch的广义压缩网络搜索（Generalized &lt;a href="http://people.idsia.ch/~juergen/compressednetworksearch.html" rel="nofollow"&gt;Compressed Network Search&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/szagoruyko/pyinn"&gt;pyinn&lt;/a&gt;: CuPy实现融合PyTorch操作。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/nasimrahaman/inferno"&gt;inferno&lt;/a&gt;: 关于PyTorch的实用程序库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/henryre/pytorch-fitmodule"&gt;pytorch-fitmodule&lt;/a&gt;: 一种用于PyTorch模块的超简单拟合方法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2600+&lt;/kbd&gt; &lt;a href="https://github.com/dnouri/inferno"&gt;inferno-sklearn&lt;/a&gt;: 一个基于PyTorch封装且兼容scikit-learn的神经网络库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/marvis/pytorch-caffe-darknet-convert"&gt;pytorch-caffe-darknet-convert&lt;/a&gt;: 在 pytorch, caffe prototxt/weights 和 darknet cfg/weights 之间转换。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/longcw/pytorch2caffe"&gt;pytorch2caffe&lt;/a&gt;: 将PyTorch模型转换成Caffe模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/nearai/pytorch-tools"&gt;pytorch-tools&lt;/a&gt;: PyTorch工具。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1700+&lt;/kbd&gt; &lt;a href="https://github.com/taolei87/sru"&gt;sru&lt;/a&gt;: 训练RNNs和训练CNNs一样快。 (arxiv.org/abs/1709.02755)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/prisma-ai/torch2coreml"&gt;torch2coreml&lt;/a&gt;: Torch7 -&amp;gt; CoreML，该工具可将Torch7模型转换为&lt;a href="https://developer.apple.com/documentation/coreml" rel="nofollow"&gt;Apple CoreML&lt;/a&gt;格式以便在Apple设备上运行。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000+&lt;/kbd&gt; &lt;a href="https://github.com/zhanghang1989/PyTorch-Encoding"&gt;PyTorch-Encoding&lt;/a&gt;: PyTorch 深度纹理编码网络 (Deep Texture Encoding Network) &lt;a href="http://hangzh.com/PyTorch-Encoding" rel="nofollow"&gt;http://hangzh.com/PyTorch-Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ryanleary/pytorch-ctc"&gt;pytorch-ctc&lt;/a&gt;: PyTorch-CTC 实现了CTC(联结主义时间分类，Connectionist Temporal Classification)集束搜索（Beam Search）解码。C++代码借鉴了TensorFlow，并通过一些改进增加了灵活性。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/t-vi/candlegp"&gt;candlegp&lt;/a&gt;: Pytorch中的高斯过程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/loudinthecloud/dpwa"&gt;dpwa&lt;/a&gt;: 基于成对平均（Pair-Wise Averaging）的分布式学习。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/koz4k/dni-pytorch"&gt;dni-pytorch&lt;/a&gt;: 基于合成梯度的PyTorch解耦神经接口。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2600+&lt;/kbd&gt; &lt;a href="https://github.com/dnouri/skorch"&gt;skorch&lt;/a&gt;: 一个基于PyTorch封装且兼容scikit-learn的神经网络库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2300+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/ignite"&gt;ignite&lt;/a&gt;: Ignite是一个高级库，帮助你在PyTorch中训练神经网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/glample/Arnold"&gt;Arnold&lt;/a&gt;: Arnold - DOOM 游戏代理。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/albanie/pytorch-mcn"&gt;pytorch-mcn&lt;/a&gt;: 将MatConvNet模型转换为PyTorch模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1900+&lt;/kbd&gt; &lt;a href="https://github.com/chenyuntc/simple-faster-rcnn-pytorch"&gt;simple-faster-rcnn-pytorch&lt;/a&gt;: Faster R-CNN 的简化实现，性能与原始论文相当。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/DL-IT/generative_zoo"&gt;generative_zoo&lt;/a&gt;: generative_zoo提供了PyTorch中一些生成模型的工作实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000+&lt;/kbd&gt; &lt;a href="https://github.com/szagoruyko/pytorchviz"&gt;pytorchviz&lt;/a&gt;: 可视化PyTorch的运行图。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/cogitare-ai/cogitare"&gt;cogitare&lt;/a&gt;: Cogitare - 一个现代、快速、模块化的深度学习和机器学习框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dmarnerides/pydlt"&gt;pydlt&lt;/a&gt;: 基于PyTorch的深度学习工具箱。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wohlert/semi-supervised-pytorch"&gt;semi-supervised-pytorch&lt;/a&gt;: 各种基于VAE的半监督模型和生成模型的实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/rusty1s/pytorch_cluster"&gt;pytorch_cluster&lt;/a&gt;: 优化图簇算法的PyTorch扩展库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/aditya-khant/neural-assembly-compiler"&gt;neural-assembly-compiler&lt;/a&gt;: 基于自适应神经编译的PyTorch神经汇编编译器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/vadimkantorov/caffemodel2pytorch"&gt;caffemodel2pytorch&lt;/a&gt;: 将Caffe模型转换为PyTorch模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/extension-cpp"&gt;extension-cpp&lt;/a&gt;: PyTorch中的C++扩展。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/GRAAL-Research/pytoune"&gt;pytoune&lt;/a&gt;: 类Keras框架和实用程序。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dusty-nv/jetson-reinforcement"&gt;jetson-reinforcement&lt;/a&gt;: 使用PyTorch，OpenAI Gym和Gazebo机器人模拟的NVIDIA Jetson深度强化学习GPU库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/salesforce/matchbox"&gt;matchbox&lt;/a&gt;: 编写单个示例的PyTorch代码，然后小批量地高效运行。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/josipd/torch-two-sample"&gt;torch-two-sample&lt;/a&gt;: PyTorch双样本测试库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1700+&lt;/kbd&gt; &lt;a href="https://github.com/sksq96/pytorch-summary"&gt;pytorch-summary&lt;/a&gt;: PyTorch模型总结，类似于Keras中的&lt;code&gt;model.summary()&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/BelBES/mpl.pytorch"&gt;mpl.pytorch&lt;/a&gt;: MaxPoolingLoss的PyTorch实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/YosefLab/scVI-dev"&gt;scVI-dev&lt;/a&gt;: 链接失效。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2800+&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/apex"&gt;apex&lt;/a&gt;: 一个PyTorch扩展：面向精简混合精度和分布式训练。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2900+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/ELF"&gt;ELF&lt;/a&gt;: ELF: 游戏研究平台，复现了AlphaGoZero/AlphaZero。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/EKami/Torchlite"&gt;Torchlite&lt;/a&gt;: Pytorch建立在sklearn、Pytorch和Tensorflow等流行机器学习框架上的高水平库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Schlumberger/joint-vae"&gt;joint-vae&lt;/a&gt;: JointVAE的PyTorch实现，一个面向分离连续和离散变异因素的框架 &lt;g-emoji class="g-emoji" alias="star2" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png"&gt;🌟&lt;/g-emoji&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kengz/SLM-Lab"&gt;SLM-Lab&lt;/a&gt;: PyTorch模块化深度强化学习框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Hananel-Hazan/bindsnet"&gt;bindsnet&lt;/a&gt;: 一个Python包，可借助PyTorch &lt;code&gt;Tensor&lt;/code&gt; 功能在CPUs或GPUs上模拟脉冲神经网络(SNNs, Spiking Neural Networks)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/akanimax/pro_gan_pytorch"&gt;pro_gan_pytorch&lt;/a&gt;: 作为 PyTorch nn.Module 扩展的 ProGAN 包。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;5600+&lt;/kbd&gt; &lt;a href="https://github.com/rusty1s/pytorch_geometric"&gt;pytorch_geometric&lt;/a&gt;: PyTorch几何深度学习扩展库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/knighton/torchplus"&gt;torchplus&lt;/a&gt;: 在 PyTorch modules 上实现 + 运算符，返回序列。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zuoxingdong/lagom"&gt;lagom&lt;/a&gt;: lagom: 用于强化学习算法快速原型构建的轻量级PyTorch架构。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ecs-vlc/torchbearer"&gt;torchbearer&lt;/a&gt;: torchbearer: PyTorch模型拟合库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tristandeleu/pytorch-maml-rl"&gt;pytorch-maml-rl&lt;/a&gt;: 强化学习中的模型不可知元学习(MAML, Model-Agnostic Meta-Learning)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/bharathgs/NALU"&gt;NALU&lt;/a&gt;: 神经算术逻辑单元(Neural Arithmetic Logic Units)的PyTorch基本实现，论文：arxiv.org/pdf/1808.00508.pdf 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/PIQuIL/QuCumber"&gt;QuCumber&lt;/a&gt;: 神经网络多体波函数重构。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/MagNet-DL/magnet"&gt;magnet&lt;/a&gt;: 自我建立的深度学习项目。&lt;a href="http://magnet-dl.readthedocs.io/" rel="nofollow"&gt;http://magnet-dl.readthedocs.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jbohnslav/opencv_transforms"&gt;opencv_transforms&lt;/a&gt;: OpenCV实现Torchvision的图像分割。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;16100+&lt;/kbd&gt; &lt;a href="https://github.com/fastai/fastai"&gt;fastai&lt;/a&gt;: fast.ai 深度学习库、课程和教程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/RobotLocomotion/pytorch-dense-correspondence"&gt;pytorch-dense-correspondence&lt;/a&gt;: &lt;a href="arxiv.org/pdf/1806.08756.pdf"&gt;《Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation》&lt;/a&gt; 一文的代码。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/richzhang/colorization-pytorch"&gt;colorization-pytorch&lt;/a&gt;: PyTorch实现交互式深度着色(Interactive Deep Colorization)。 richzhang.github.io/ideepcolor&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/cms-flash/beauty-net"&gt;beauty-net&lt;/a&gt;: PyTorch一个简单、灵活、可扩展的PyTorch模板。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Mariewelt/OpenChem"&gt;OpenChem&lt;/a&gt;: OpenChem: 面向计算化学和药物设计研究的深度学习工具包 mariewelt.github.io/OpenChem 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/aiqm/torchani"&gt;torchani&lt;/a&gt;: PyTorch精确神经网络电位。 aiqm.github.io/torchani&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hjmshi/PyTorch-LBFGS"&gt;PyTorch-LBFGS&lt;/a&gt;: PyTorch实现L-BFGS。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/cornellius-gp/gpytorch"&gt;gpytorch&lt;/a&gt;: PyTorch中对高斯过程的高效且模块化的实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mariogeiger/hessian"&gt;hessian&lt;/a&gt;: PyTorch版hessian。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/MillionIntegrals/vel"&gt;vel&lt;/a&gt;: 深度学习研究中的速度。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/msamogh/nonechucks"&gt;nonechucks&lt;/a&gt;: 动态地处理数据集中的坏样本，使用转换作为过滤器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Swall0w/torchstat"&gt;torchstat&lt;/a&gt;: PyTorch中的模型分析器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/QNNPACK"&gt;QNNPACK&lt;/a&gt;: 量化神经网络包—量化神经网络算子的移动优化实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2600+&lt;/kbd&gt; &lt;a href="https://github.com/rtqichen/torchdiffeq"&gt;torchdiffeq&lt;/a&gt;: PyTorch解常微分方程（ODE），使用的是全GPU支持、O(1)内存复杂度的反向传播算法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/BachiLi/redner"&gt;redner&lt;/a&gt;: 可微的 Monte Carlo 路径跟踪器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/masa-su/pixyz"&gt;pixyz&lt;/a&gt;: 一个库，用来以更简洁、直观和可扩展的方式开发深层生成模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/perone/euclidesdb"&gt;euclidesdb&lt;/a&gt;: 一种多模型机器学习特征嵌入数据库。 &lt;a href="http://euclidesdb.readthedocs.io" rel="nofollow"&gt;http://euclidesdb.readthedocs.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/nerox8664/pytorch2keras"&gt;pytorch2keras&lt;/a&gt;: 将PyTorch模型转换为Keras模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/domainadaptation/salad"&gt;salad&lt;/a&gt;: 域适应和半监督学习工具箱。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Erotemic/netharn"&gt;netharn&lt;/a&gt;: PyTorch的参数化拟合和预测线束（Prediction Harnesses）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3200+&lt;/kbd&gt; &lt;a href="https://github.com/dmlc/dgl"&gt;dgl&lt;/a&gt;: Python包，基于现有的DL框架，用于简化对图形的深度学习。&lt;a href="http://dgl.ai" rel="nofollow"&gt;http://dgl.ai&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1400+&lt;/kbd&gt; &lt;a href="https://github.com/CSAILVision/gandissect"&gt;gandissect&lt;/a&gt;: 基于PyTorch的工具，用于可视化和理解GAN的神经元。gandissect.csail.mit.edu&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/justusschock/delira"&gt;delira&lt;/a&gt;: 基于PyTorch和Tensorlow的快速原型和训练深层神经网络的轻量级框架，用于医疗成像。 delira.rtfd.io&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/AIRLab-POLIMI/mushroom"&gt;mushroom&lt;/a&gt;: 强化学习实验的Python库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/thuml/Xlearn"&gt;Xlearn&lt;/a&gt;: 迁移学习库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ferrine/geoopt"&gt;geoopt&lt;/a&gt;: 基于PyTorch优化的黎曼自适应优化方法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/unit8co/vegans"&gt;vegans&lt;/a&gt;: 包含多种现有的GANs。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1400+&lt;/kbd&gt; &lt;a href="https://github.com/arraiyopensource/kornia"&gt;kornia&lt;/a&gt;: PyTorch开源可微计算机视觉库。 &lt;a href="https://kornia.org" rel="nofollow"&gt;https://kornia.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/BorealisAI/advertorch"&gt;AdverTorch&lt;/a&gt;: 研究对抗鲁棒性的工具箱。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2500+&lt;/kbd&gt; &lt;a href="https://github.com/Luolc/AdaBound"&gt;AdaBound&lt;/a&gt;: 一个优化器，训练速度和Adam一样快，和SGD一样好。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mblondel/fenchel-young-losses"&gt;fenchel-young-losses&lt;/a&gt;: 在PyTorch/TensorFlow/scikit-learn中使用Fenchel-Young损失作为概率分类的损失函数。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/Lyken17/pytorch-OpCounter"&gt;pytorch-OpCounter&lt;/a&gt;: 统计PyTorch模型的MACs/FLOPs。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kaihsin/Tor10"&gt;Tor10&lt;/a&gt;: 基于PyTorch，为量子模拟设计的通用张量网络库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/catalyst-team/catalyst"&gt;Catalyst&lt;/a&gt;: PyTorch DL&amp;amp;RL 研究的高级实用程序。它的开发重点是可重复性、快速实验和代码/思想重用。能够研究/开发新的东西，而不是编写另一个常规的训练循环。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/facebook/Ax"&gt;Ax&lt;/a&gt;: 自适应实验平台。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/achaiah/pywick"&gt;pywick&lt;/a&gt;: 高水平的PyTorch神经网络训练库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kakaobrain/torchgpipe"&gt;torchgpipe&lt;/a&gt;: PyTorch实现GPipe。 torchgpipe.readthedocs.io&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/hub"&gt;hub&lt;/a&gt;: Pytorch Hub 是一个预训练模型库，用来提升研究的可重复性。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2700+&lt;/kbd&gt; &lt;a href="https://github.com/williamFalcon/pytorch-lightning"&gt;pytorch-lightning&lt;/a&gt;: 面向ML研究人员的轻量级PyTorch包装器。缩放模型，少写样板。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kaihsin/Tor10"&gt;Tor10&lt;/a&gt;: 基于pytorch为量子模拟设计的通用张量网络库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2500+&lt;/kbd&gt; &lt;a href="https://github.com/microsoft/tensorwatch"&gt;tensorwatch&lt;/a&gt;: 针对Python机器学习与数据科学的调试、监控与可视化。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/fancompute/wavetorch"&gt;wavetorch&lt;/a&gt;: 波动方程的数值求解与反传播。 arxiv.org/abs/1904.12831&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ag14774/diffdist"&gt;diffdist&lt;/a&gt;: diffdist是一个面向PyTorch的Python库。它扩展了&lt;code&gt;torch.autograd&lt;/code&gt;的默认功能，并增加了对进程间可微通信的支持。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/awwong1/torchprof"&gt;torchprof&lt;/a&gt;: 用于Pytorch模型逐层分析的最小依赖库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/oxfordcontrol/osqpth"&gt;osqpth&lt;/a&gt;: PyTorch可微OSQP求解器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mctorch/mctorch"&gt;mctorch&lt;/a&gt;: 面向深度学习的流形优化库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/noahgolmant/pytorch-hessian-eigenthings"&gt;pytorch-hessian-eigenthings&lt;/a&gt;: 使用Hessian向量积和随机幂迭代的高效PyTorch Hessian特征分解。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/StanfordVL/MinkowskiEngine"&gt;MinkowskiEngine&lt;/a&gt;: 闵可夫斯基引擎是一个用于广义稀疏卷积和高维稀疏张量的自动微分方法库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Omegastick/pytorch-cpp-rl"&gt;pytorch-cpp-rl&lt;/a&gt;: CppRl是一个强化学习框架，用 PyTorch C++ 前端编写。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/BloodAxe/pytorch-toolbelt"&gt;pytorch-toolbelt&lt;/a&gt;: PyTorch扩展，用来进行快速R&amp;amp;D原型开发和Kaggle代码收集。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Fonbet/argus-tensor-stream"&gt;argus-tensor-stream&lt;/a&gt;: 一个库，用来将实时视频流解码至CUDA内存。tensorstream.argus-ai.com&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hal3/macarico"&gt;macarico&lt;/a&gt;: 在 PyTorch 中学习搜索。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/astooke/rlpyt"&gt;rlpyt&lt;/a&gt;: PyTorch 中的强化学习。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/blue-season/pywarm"&gt;pywarm&lt;/a&gt;: 为 PyTorch 建立神经网络的一种更清洁的方法。&lt;a href="https://blue-season.github.io/pywarm/" rel="nofollow"&gt;https://blue-season.github.io/pywarm/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/learnables/learn2learn"&gt;learn2learn&lt;/a&gt;: PyTorch元学习框架。&lt;a href="http://learn2learn.net" rel="nofollow"&gt;http://learn2learn.net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/torchbeast"&gt;torchbeast&lt;/a&gt;: 分布式强化学习的PyTorch平台。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/higher"&gt;higher&lt;/a&gt;: higher 是一个PyTorch库，允许用户获得跨越训练循环而不是单个训练步骤的损失的高阶梯度。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Vermeille/Torchelie/"&gt;Torchelie&lt;/a&gt;: Torchélie 是面向PyTorch的一系列工具函数、层、损失、模型、训练器等的合集。 &lt;a href="https://torchelie.readthedocs.org/" rel="nofollow"&gt;https://torchelie.readthedocs.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/CrypTen"&gt;CrypTen&lt;/a&gt;: CrypTen 是一个隐私保护机器学习框架，它使用PyTorch编写，允许研究人员和开发人员使用加密数据训练模型。CrypTen目前支持将安全的多方计算（&lt;a href="https://en.wikipedia.org/wiki/Secure_multi-party_computation" rel="nofollow"&gt;Secure Multiparty Computation&lt;/a&gt;）作为其加密机制。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cvxgrp/cvxpylayers"&gt;cvxpylayers&lt;/a&gt;: cvxpylayers 是一个 Python 库，用于在PyTorch中构造可微凸优化层。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/HobbitLong/RepDistiller"&gt;RepDistiller&lt;/a&gt;: 对比表示蒸馏（CRD）和最新知识蒸馏方法的基准。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NVIDIAGameWorks/kaolin"&gt;kaolin&lt;/a&gt;: 一个旨在加速3D深度学习研究的PyTorch库。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/BasBuller/PySNN"&gt;PySNN&lt;/a&gt;: 高效的尖峰神经网络框架，建立在PyTorch之上，用于GPU加速。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dmmiller612/sparktorch"&gt;sparktorch&lt;/a&gt;: 在 Apache Spark 上训练和运行 PyTorch 模型。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/KevinMusgrave/pytorch-metric-learning"&gt;pytorch-metric-learning&lt;/a&gt;: 在应用程序中使用度量学习的最简单方法。模块化，灵活，可扩展。用 PyTorch 构建。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cpnota/autonomous-learning-library"&gt;autonomous-learning-library&lt;/a&gt;: 用于建立深度强化学习代理的 PyTorch 库。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/asappresearch/flambe"&gt;flambe&lt;/a&gt;: 一个用于加速研究及其生产路径的ML框架。&lt;a href="https://flambe.ai" rel="nofollow"&gt;https://flambe.ai&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-tutorials--examples教程--示例" class="anchor" aria-hidden="true" href="#tutorials--examples教程--示例"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tutorials &amp;amp; examples｜教程 &amp;amp; 示例&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;kbd&gt;3500+&lt;/kbd&gt; &lt;strong&gt;&lt;a href="https://github.com/spro/practical-pytorch"&gt;Practical Pytorch&lt;/a&gt;&lt;/strong&gt;: 该教程对不同的RNN模型进行了解释。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html" rel="nofollow"&gt;DeepLearningForNLPInPytorch&lt;/a&gt;: IPython Notebook 深度学习教程，包含对自然语言处理的强调。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;13100+&lt;/kbd&gt; &lt;a href="https://github.com/yunjey/pytorch-tutorial"&gt;pytorch-tutorial&lt;/a&gt;: 面向研究人员的深度学习教程，其中大部分模型的实现代码都少于30行。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/keon/pytorch-exercises"&gt;pytorch-exercises&lt;/a&gt;: PyTorch练习集合。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2600+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/tutorials"&gt;pytorch tutorials&lt;/a&gt;: 各种PyTorch教程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;9900+&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/examples"&gt;pytorch examples&lt;/a&gt;:  PyTorch使用示例，应用场景包括视觉、文本、强化学习等。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/napsternxg/pytorch-practice"&gt;pytorch practice&lt;/a&gt;: PyTorch示例。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/vinhkhuc/PyTorch-Mini-Tutorials"&gt;pytorch mini tutorials&lt;/a&gt;: PyTorch极简教程，改编自Alec Radford的&lt;a href="https://github.com/Newmu/Theano-Tutorials"&gt;Theano教程&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/xiayandi/Pytorch_text_classification"&gt;pytorch text classification&lt;/a&gt;: PyTorch实现基于CNN的文本分类。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/desimone/pytorch-cat-vs-dogs"&gt;cats vs dogs&lt;/a&gt;: Kaggle 竞赛 Dogs vs. Cats Redux: Kernels Edition 的网络微调示例。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/eladhoffer/convNet.pytorch"&gt;convnet&lt;/a&gt;: 深度卷积网络在不同数据集(ImageNet, Cifar10, Cifar100, MNIST)上的完整训练示例。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mailmahee/pytorch-generative-adversarial-networks"&gt;pytorch-generative-adversarial-networks&lt;/a&gt;: 一个简单的对抗生成网络(GAN) 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/amdegroot/pytorch-containers"&gt;pytorch containers&lt;/a&gt;: PyTorch中简化的Torch容器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/cemoody/topicsne"&gt;T-SNE in pytorch&lt;/a&gt;: t-SNE实验。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/fducau/AAE_pytorch"&gt;AAE_pytorch&lt;/a&gt;: PyTorch版对抗自编码器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/GunhoChoi/Kind_PyTorch_Tutorial"&gt;Kind_PyTorch_Tutorial&lt;/a&gt;: PyTorch新手教程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/justdark/pytorch-poetry-gen"&gt;pytorch-poetry-gen&lt;/a&gt;: 基于PyTorch的char-RNN（字符级循环神经网络）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/JamesChuanggg/pytorch-REINFORCE"&gt;pytorch-REINFORCE&lt;/a&gt;: PyTorch 实现了 OpenAI gym 下离散和连续控制的 REINFORCE。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3600+&lt;/kbd&gt; &lt;strong&gt;&lt;a href="https://github.com/MorvanZhou/PyTorch-Tutorial"&gt;PyTorch-Tutorial&lt;/a&gt;&lt;/strong&gt;: 简单而快速地搭建你自己的神经网络。 &lt;a href="https://morvanzhou.github.io/tutorials/" rel="nofollow"&gt;https://morvanzhou.github.io/tutorials/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/joansj/pytorch-intro"&gt;pytorch-intro&lt;/a&gt;: 演示如何在PyTorch中实现CNNs和RNNs。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/bearpaw/pytorch-classification"&gt;pytorch-classification&lt;/a&gt;: 一个CIFAR-10/100和ImageNet数据集上的分类框架。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hardmaru/pytorch_notebooks"&gt;pytorch_notebooks - hardmaru&lt;/a&gt;: 用NumPy和PyTorch编写的随机教程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/soravux/pytorch_tutorial"&gt;pytorch_tutoria-quick&lt;/a&gt;: PyTorch介绍和教程。面向计算机视觉、图形和机器学习领域的研究人员，要求对神经网络理论知识和常用神经网络框架由基本的了解。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Spandan-Madan/Pytorch_fine_tuning_Tutorial"&gt;Pytorch_fine_tuning_Tutorial&lt;/a&gt;: 在PyTorch中进行微调或转移学习的简短教程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Kyubyong/pytorch_exercises"&gt;pytorch_exercises&lt;/a&gt;: PyTorch练习。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/soumith/traffic-sign-detection-homework"&gt;traffic-sign-detection&lt;/a&gt;: 纽约大学2018年计算机视觉秋季课程示例。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Js-Mim/mss_pytorch"&gt;mss_pytorch&lt;/a&gt;: 无需进行滤波后处理，利用循环推断算法实现歌唱语音分离 - PyTorch 实现。 演示: js-mim.github.io/mss_pytorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2100+&lt;/kbd&gt; &lt;a href="https://github.com/DSKSD/DeepNLP-models-Pytorch"&gt;DeepNLP-models-Pytorch&lt;/a&gt; cs-224n课程中的各种深度NLP模型的PyTorch实现。(Stanford Univ: NLP with Deep Learning)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mila-udem/welcome_tutorials"&gt;Mila introductory tutorials&lt;/a&gt;: 面向MILA新生的各种教程。（&lt;a href="https://mila.quebec/en/mila/" rel="nofollow"&gt;MILA：加拿大蒙特利尔人工智能研究中心&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/moskomule/pytorch.rl.learning"&gt;pytorch.rl.learning&lt;/a&gt;: 使用PyTorch学习强化学习。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/keon/seq2seq"&gt;minimal-seq2seq&lt;/a&gt;: 关注神经机器翻译的最小Seq2Seq模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/JeanKossaifi/tensorly-notebooks"&gt;tensorly-notebooks&lt;/a&gt;: 利用Python和TensorLy实现张量方法。 tensorly.github.io/dev&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jpeg729/pytorch_bits"&gt;pytorch_bits&lt;/a&gt;: 时序预测的相关示例。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/sanyam5/skip-thoughts"&gt;skip-thoughts&lt;/a&gt;: PyTorch实现Skip-Thought词向量模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/xiadingZ/video-caption-pytorch"&gt;video-caption-pytorch&lt;/a&gt;: 利用PyTorch为视频添加字幕。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/higgsfield/Capsule-Network-Tutorial"&gt;Capsule-Network-Tutorial&lt;/a&gt;: 简单易学的胶囊网络（Capsule Network）教程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1400+&lt;/kbd&gt; &lt;a href="https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch"&gt;code-of-learn-deep-learning-with-pytorch&lt;/a&gt;: 《深度学习入门之PyTorch》书中代码。 item.jd.com/17915495606.html&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/higgsfield/RL-Adventure"&gt;RL-Adventure&lt;/a&gt;: Pytorch 版 Deep Q Learning 教程，简单、易学、代码可读性强，包含 DQN / DDQN / Prioritized replay/ noisy networks/ distributional values/ Rainbow/ hierarchical RL 的 PyTorch 实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hpcgarage/accelerated_dl_pytorch"&gt;accelerated_dl_pytorch&lt;/a&gt;: Jupyter Day Atlanta II 会议上的加速深度学习算法，包含 PyTorch 教程和会议演讲文稿。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1800+&lt;/kbd&gt; &lt;a href="https://github.com/higgsfield/RL-Adventure-2"&gt;RL-Adventure-2&lt;/a&gt;: 以下内容的 PyTorch0.4 版本教程: actor critic / proximal policy optimization / acer / ddpg / twin dueling ddpg / soft actor critic / generative adversarial imitation learning / hindsight experience replay。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f" rel="nofollow"&gt;Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)&lt;/a&gt;: 50行生成对抗网络。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/" rel="nofollow"&gt;adversarial-autoencoders-with-pytorch&lt;/a&gt;: PyTorch对抗自编码器。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@vishnuvig/transfer-learning-using-pytorch-4c3475f4495" rel="nofollow"&gt;transfer learning using pytorch&lt;/a&gt;: PyTorch迁移学习。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/" rel="nofollow"&gt;how-to-implement-a-yolo-object-detector-in-pytorch&lt;/a&gt;: 如何使用PyTorch实现一个YOLO (v3)物体检测器。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.fastforwardlabs.com/2018/04/10/pytorch-for-recommenders-101.html" rel="nofollow"&gt;pytorch-for-recommenders-101&lt;/a&gt;: 使用PyTorch构建推荐系统。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wkentaro/pytorch-for-numpy-users"&gt;pytorch-for-numpy-users&lt;/a&gt;: 面向Numpy用户的PyTorch。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pytorchtutorial.com/" rel="nofollow"&gt;PyTorch Tutorial&lt;/a&gt;: PyTorch中文教程（PyTorch中文网）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Kaixhin/grokking-pytorch"&gt;grokking-pytorch&lt;/a&gt;: 手把手教你学会PyTorch。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/Atcold/PyTorch-Deep-Learning-Minicourse"&gt;PyTorch-Deep-Learning-Minicourse&lt;/a&gt;: PyTorch深度学习微型课程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/utkuozbulak/pytorch-custom-dataset-examples"&gt;pytorch-custom-dataset-examples&lt;/a&gt;: PyTorch的一些自定义数据集示例。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://florianwilhelm.info/2018/08/multiplicative_LSTM_for_sequence_based_recos/" rel="nofollow"&gt;Multiplicative LSTM for sequence-based Recommenders&lt;/a&gt;: 面向基于序列的推荐器的乘法LSTM。/基于LSTM的序列推荐实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/furkanu/deeplearning.ai-pytorch"&gt;deeplearning.ai-pytorch&lt;/a&gt;: Coursera深度学习课程(deeplearning.ai)任务的PyTorch实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tobiascz/MNIST_Pytorch_python_and_capi"&gt;MNIST_Pytorch_python_and_capi&lt;/a&gt;: 示例：如何在Python中训练一个MNIST网络并在C++中用PyTorch1.0运行。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ne7ermore/torch_light"&gt;torch_light&lt;/a&gt;: 教程和示例，包括强化学习、NLP、CV。Logistic、CNN、RNN、LSTM等神经网络模型由数行代码实现，一些高级示例由复杂模型实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dribnet/portrain-gan"&gt;portrain-gan&lt;/a&gt;: 编码（解码尚未实现）art-DCGAN 生成的肖像油画。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/omarsar/mri-analysis-pytorch"&gt;mri-analysis-pytorch&lt;/a&gt;: 使用PyTorch和MedicalTorch进行核磁共振（MRI）分析。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/davidcpage/cifar10-fast"&gt;cifar10-fast&lt;/a&gt;: 在79秒内完成CIFAR10数据集上的ResNet模型的训练并达到94%的测试准确率，相关内容参见 &lt;a href="https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet/" rel="nofollow"&gt;blog series&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://in.udacity.com/course/deep-learning-pytorch--ud188" rel="nofollow"&gt;Intro to Deep Learning with PyTorch&lt;/a&gt;: Udacity和Facebook联合推出的免费课程，包括对PyTorch的介绍和对PyTorch作者之一的Soumith Chintala的采访。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000+&lt;/kbd&gt; &lt;a href="https://github.com/bentrevett/pytorch-sentiment-analysis"&gt;pytorch-sentiment-analysis&lt;/a&gt;: PyTorch和TorchText语义分析教程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/rwightman/pytorch-image-models"&gt;pytorch-image-models&lt;/a&gt;: PyTorch图像模型、脚本、与训练权重—— (SE)ResNet/ResNeXT, DPN, EfficientNet, MobileNet-V3/V2/V1, MNASNet, Single-Path NAS, FBNet等等。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/BIGBALLON/CIFAR-ZOO"&gt;CIFAR-ZOO&lt;/a&gt;: 以CIFAR为基准的多种CNN架构的PyTorch实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dsgiitr/d2l-pytorch"&gt;d2l-pytorch&lt;/a&gt;: 本项目尝试复制《动手深度学习（Dive into Deep Learning）》(&lt;a href="http://www.d2l.ai" rel="nofollow"&gt;www.d2l.ai&lt;/a&gt;) 一书，将MXnet代码改编为PyTorch版。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/stared/thinking-in-tensors-writing-in-pytorch"&gt;thinking-in-tensors-writing-in-pytorch&lt;/a&gt;:  张量思维，PyTorch实践 (深度学习入门)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/lemonhu/NER-BERT-pytorch"&gt;NER-BERT-pytorch&lt;/a&gt;: 命名试题识别的PyTorch解决方案，使用了Google AI的预训练BERT模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dougsouza/pytorch-sync-batchnorm-example"&gt;pytorch-sync-batchnorm-example&lt;/a&gt;: 如何在 PyTorch 中使用交叉复制（Cross Replica）/同步批标准化（Synchronized Batchnorm）。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/barissayil/SentimentAnalysis"&gt;SentimentAnalysis&lt;/a&gt;: 情绪分析神经网络，在斯坦福情绪树库上用微调BERT训练得到。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-paper-implementations论文实现" class="anchor" aria-hidden="true" href="#paper-implementations论文实现"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Paper implementations｜论文实现&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/neuralix/google_evolution"&gt;google_evolution&lt;/a&gt;: 实现了 &lt;a href="https://arxiv.org/abs/1703.01041" rel="nofollow"&gt;Large-scale evolution of image classifiers&lt;/a&gt; 一文的结果网络之一。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/edouardoyallon/pyscatwave"&gt;pyscatwave&lt;/a&gt;: 基于CuPy/PyTorch的快速散射变换，&lt;a href="https://arxiv.org/abs/1703.08961" rel="nofollow"&gt;Scaling the Scattering Transform: Deep Hybrid Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/edouardoyallon/scalingscattering"&gt;scalingscattering&lt;/a&gt;: 该仓库包含 &lt;a href="https://arxiv.org/abs/1703.08961" rel="nofollow"&gt;Scaling The Scattering Transform : Deep Hybrid Networks&lt;/a&gt; 一文中的实验。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/episodeyang/deep-auto-punctuation"&gt;deep-auto-punctuation&lt;/a&gt;: 通过逐字符学习实现自动添加标点。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation"&gt;Realtime_Multi-Person_Pose_Estimation&lt;/a&gt;: 基于PyTorch的多人人体姿态估计，&lt;a href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation"&gt;原始代码&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/onlytailei/PyTorch-value-iteration-networks"&gt;PyTorch-value-iteration-networks&lt;/a&gt;: PyTorch实现价值迭代网络（&lt;a href="https://arxiv.org/abs/1602.02867" rel="nofollow"&gt;Value Iteration Networks&lt;/a&gt;）（NIPS2016最佳论文奖）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/analvikingur/pytorch_Highway"&gt;pytorch_Highway&lt;/a&gt;: PyTorch实现高速公路网络（&lt;a href="https://arxiv.org/abs/1505.00387" rel="nofollow"&gt;Highway Networks&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/analvikingur/pytorch_NEG_loss"&gt;pytorch_NEG_loss&lt;/a&gt;: PyTorch实现负采样损失（&lt;a href="https://arxiv.org/abs/1310.4546" rel="nofollow"&gt;Negative Sampling Loss&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/analvikingur/pytorch_RVAE"&gt;pytorch_RVAE&lt;/a&gt;: 用PyTorch实现的产生序列数据的递归变分自动编码器，相关论文：&lt;a href="https://arxiv.org/abs/1511.06349#" rel="nofollow"&gt;Generating Sentences from a Continuous Space&lt;/a&gt;，&lt;a href="https://arxiv.org/abs/1508.06615" rel="nofollow"&gt;Character-Aware Neural Language Models&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/analvikingur/pytorch_TDNN"&gt;pytorch_TDNN&lt;/a&gt;: 用PyTorch实现时间延迟神经网络（Time Delayed NN）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/moskomule/eve.pytorch"&gt;eve.pytorch&lt;/a&gt;: 一个Eve优化器的实现，相关论文：&lt;a href="https://arxiv.org/abs/1611.01505" rel="nofollow"&gt;Imploving Stochastic Gradient Descent with Feedback&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/locuslab/e2e-model-learning"&gt;e2e-model-learning&lt;/a&gt;: 随机优化中的基于任务的端到端模型，&lt;a href="https://arxiv.org/abs/1703.04529" rel="nofollow"&gt;https://arxiv.org/abs/1703.04529&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mrzhu-cool/pix2pix-pytorch"&gt;pix2pix-pytorch&lt;/a&gt;: PyTorch实现“基于条件对抗网络的图像到图像翻译”。 论文：&lt;a href="https://arxiv.org/pdf/1611.07004v1.pdf" rel="nofollow"&gt;Image-to-Image Translation Using Conditional Adversarial Networks&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2900+&lt;/kbd&gt; &lt;a href="https://github.com/amdegroot/ssd.pytorch"&gt;Single Shot MultiBox Detector&lt;/a&gt;: 单发多盒探测器，论文：&lt;a href="http://arxiv.org/abs/1512.02325" rel="nofollow"&gt;Single Shot MultiBox Detector&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/carpedm20/DiscoGAN-pytorch"&gt;DiscoGAN&lt;/a&gt;: 学习利用生成性对抗网络发现跨域关系。论文：&lt;a href="https://arxiv.org/abs/1703.05192" rel="nofollow"&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/SKTBrain/DiscoGAN"&gt;official DiscoGAN implementation&lt;/a&gt;: 官方实现“学习利用生成性对抗网络发现跨域关系”。 论文：&lt;a href="https://arxiv.org/abs/1703.05192" rel="nofollow"&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/atgambardella/pytorch-es"&gt;pytorch-es&lt;/a&gt;: 进化策略。论文：&lt;a href="https://arxiv.org/abs/1703.03864" rel="nofollow"&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/bodokaiser/piwise"&gt;piwise&lt;/a&gt;: 使用PyTorch对VOC2012数据集进行像素切割。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/transedward/pytorch-dqn"&gt;pytorch-dqn&lt;/a&gt;: 深度Q学习网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ruotianluo/neuraltalk2.pytorch"&gt;neuraltalk2-pytorch&lt;/a&gt;: PyTorch图像字幕代码库(在分支“with_finetune”中有可微调CNN)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mattmacy/vnet.pytorch"&gt;vnet.pytorch&lt;/a&gt;: PyTorch实现V-Net：全卷积神经网络在体医学图像分割中的应用。 &lt;a href="http://mattmacy.io/vnet.pytorch/" rel="nofollow"&gt;http://mattmacy.io/vnet.pytorch/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wkentaro/pytorch-fcn"&gt;pytorch-fcn&lt;/a&gt;: PyTorch 实现完全卷积网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/xternalz/WideResNet-pytorch"&gt;WideResNets&lt;/a&gt;: PyTorch实现WideResNets。该实现比官方Torch实现花费更少的GPU内存。实现: &lt;a href="https://github.com/szagoruyko/wide-residual-networks"&gt;https://github.com/szagoruyko/wide-residual-networks&lt;/a&gt; .&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/c0nn3r/pytorch_highway_networks"&gt;pytorch_highway_networks&lt;/a&gt;: PyTorch实现高速公路网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ypxie/pytorch-NeuCom"&gt;pytorch-NeuCom&lt;/a&gt;: Pytorch实现DeepMind的可微神经计算机&lt;a href="http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz" rel="nofollow"&gt;论文&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/eladhoffer/captionGen"&gt;captionGen&lt;/a&gt;: 使用PyTorch为图像生成标注。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jayleicn/animeGAN"&gt;AnimeGAN&lt;/a&gt;: 生成对抗网络的PyTorch简单实现，关注于动漫脸谱绘画。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Shawn1993/cnn-text-classification-pytorch"&gt;Cnn-text classification&lt;/a&gt;: PyTorch 实现 &lt;a href="https://arxiv.org/abs/1408.5882" rel="nofollow"&gt;Kim的基于卷积神经网络的句子分类&lt;/a&gt; 论文。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/SeanNaren/deepspeech.pytorch"&gt;deepspeech2&lt;/a&gt;: 使用 Baidu Warp-CTC 实现DeepSpeech2。创造一个基于 DeepSpeech2 架构的网络，用 CTC 激活函数训练。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/MaximumEntropy/Seq2Seq-PyTorch"&gt;seq2seq&lt;/a&gt;: 包含PyTorch中的Seq2Seq模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/rarilurelo/pytorch_a3c"&gt;Asynchronous Advantage Actor-Critic in PyTorch&lt;/a&gt;: PyTorch实现A3C(Asynchronous Advantage Actor-Critic)，论文：&lt;a href="https://arxiv.org/pdf/1602.01783v1.pdf" rel="nofollow"&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt;。由于 PyTorch 可以轻松地在多进程内控制共享内存，我们可以轻易实现A3C这样的异步算法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/bamos/densenet.pytorch"&gt;densenet&lt;/a&gt;: This is a PyTorch 实现 DenseNet-BC 架构，相关论文 &lt;a href="https://arxiv.org/abs/1608.06993" rel="nofollow"&gt;Densely Connected Convolutional Networks&lt;/a&gt;。该实现的 CIFAR-10+ 100层错误率为 4.77 增长率为 12。官方实现和许多第三方库的链接参见 &lt;a href="https://github.com/liuzhuang13/DenseNet"&gt;liuzhuang13/DenseNet&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/alykhantejani/nninit"&gt;nninit&lt;/a&gt;: PyTorch神经网络模块的权值初始化方案，这是 &lt;a href="https://github.com/Kaixhin/nninit"&gt;nninit&lt;/a&gt; 的流行端口。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/longcw/faster_rcnn_pytorch"&gt;faster rcnn&lt;/a&gt;: PyTorch 实现 Faster RCNN。该项目主要基于 py-faster-rcnn 和 TFFRCNN。更多关于 R-CNN 的细节请参考论文 Faster R-CNN：&lt;a href="https://arxiv.org/abs/1506.01497" rel="nofollow"&gt;Towards Real-Time Object Detection with Region Proposal Network&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/akolishchak/doom-net-pytorch"&gt;doomnet&lt;/a&gt;: PyTorch版Doom-net，实现了ViZDoom环境下的RL模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ClementPinard/FlowNetPytorch"&gt;flownet&lt;/a&gt;: 通过Dosovitskiy等完成FlowNet的Pytorch实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/gsp-27/pytorch_Squeezenet"&gt;sqeezenet&lt;/a&gt;: 在CIFAR10数据集上用PyTorch实现Squeezenet模型，&lt;a href="https://arxiv.org/abs/1602.07360" rel="nofollow"&gt;论文&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2400+&lt;/kbd&gt; &lt;a href="https://github.com/martinarjovsky/WassersteinGAN"&gt;WassersteinGAN&lt;/a&gt;: PyTorch实现&lt;a href="https://arxiv.org/abs/1701.07875" rel="nofollow"&gt;WassersteinGAN&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/locuslab/optnet"&gt;optnet&lt;/a&gt;: 该仓库包含PyTorch源码，重现了论文&lt;a href="https://arxiv.org/abs/1703.00443" rel="nofollow"&gt;OptNet: Differentiable Optimization as a Layer in Neural Networks&lt;/a&gt;中的实验。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/locuslab/qpth"&gt;qp solver&lt;/a&gt;: PyTorch的一个快速和可微分的QP求解器。&lt;a href="https://locuslab.github.io/qpth/" rel="nofollow"&gt;https://locuslab.github.io/qpth/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ikostrikov/pytorch-naf"&gt;Continuous Deep Q-Learning with Model-based Acceleration &lt;/a&gt;: &lt;a href="https://arxiv.org/pdf/1603.00748v1.pdf" rel="nofollow"&gt;基于模型加速的连续深度Q学习&lt;/a&gt;的再实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ikostrikov/pytorch-meta-optimizer"&gt;Learning to learn by gradient descent by gradient descent&lt;/a&gt;: PyTorch实现&lt;a href="https://arxiv.org/abs/1606.04474" rel="nofollow"&gt;Learning to learn by gradient descent by gradient descent&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/darkstar112358/fast-neural-style"&gt;fast-neural-style&lt;/a&gt;: PyTorch实现fast-neural-style，论文：&lt;a href="https://arxiv.org/abs/1603.08155" rel="nofollow"&gt;Perceptual Losses for Real-Time Style Transfer and Super-Resolution&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/leongatys/PytorchNeuralStyleTransfer"&gt;PytorchNeuralStyleTransfer&lt;/a&gt;: Pytorch中的神经风格转换。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/bengxy/FastNeuralStyle"&gt;Fast Neural Style for Image Style Transform by Pytorch&lt;/a&gt;: 使用快速神经风格进行图像风格转换。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/alexis-jacq/Pytorch-Tutorials"&gt;neural style transfer&lt;/a&gt;: 通过神经风格算法介绍PyTorch，&lt;a href="https://arxiv.org/abs/1508.06576" rel="nofollow"&gt;Neural-Style algorithm&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zuoxingdong/VIN_PyTorch_Visdom"&gt;VIN_PyTorch_Visdom&lt;/a&gt;: PyTorch实现价值迭代网络(VIN):干净、简单、模块化。利用Visdom进行可视化。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/longcw/yolo2-pytorch"&gt;YOLO2&lt;/a&gt;: PyTorch中的YOLOv2。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000+&lt;/kbd&gt; &lt;a href="https://github.com/szagoruyko/attention-transfer"&gt;attention-transfer&lt;/a&gt;: 通过注意转移改善卷积网络，&lt;a href="https://arxiv.org/abs/1612.03928" rel="nofollow"&gt;ICLR2017会议论文&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/potterhsu/SVHNClassifier-PyTorch"&gt;SVHNClassifier&lt;/a&gt;: PyTorch实现&lt;a href="https://arxiv.org/pdf/1312.6082.pdf" rel="nofollow"&gt;基于深度卷积神经网络的街景图像多位数识别&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/oeway/pytorch-deform-conv"&gt;pytorch-deform-conv&lt;/a&gt;: PyTorch实现可变形卷积(Deformable Convolution)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/carpedm20/BEGAN-pytorch"&gt;BEGAN-pytorch&lt;/a&gt;: PyTorch实现&lt;a href="https://arxiv.org/abs/1703.10717" rel="nofollow"&gt;边界均衡生成对抗网络（BEGAN）&lt;/a&gt;: Boundary Equilibrium Generative Adversarial Networks.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dasguptar/treelstm.pytorch"&gt;treelstm.pytorch&lt;/a&gt;: PyTorch实现树形结构LSTM。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/DmitryUlyanov/AGE"&gt;AGE&lt;/a&gt;: 论文代码，原文：对抗生成编码器网络（&lt;a href="http://sites.skoltech.ru/app/data/uploads/sites/25/2017/04/AGE.pdf" rel="nofollow"&gt;Adversarial Generator-Encoder Networks&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/prlz77/ResNeXt.pytorch"&gt;ResNeXt.pytorch&lt;/a&gt;: 再现 ResNet-V3 (深度神经网络的聚集残差变换)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jingweiz/pytorch-rl"&gt;pytorch-rl&lt;/a&gt;: 基于PyTorch和Visdom的深度强化学习。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/sujithv28/Deep-Leafsnap"&gt;Deep-Leafsnap&lt;/a&gt;: 对比传统的计算机视觉方法，使用深度神经网络的&lt;a href="https://neerajkumar.org/base/papers/nk_eccv2012_leafsnap.pdf" rel="nofollow"&gt;LeafSnap&lt;/a&gt;能有效提高测试准确率。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;10100+&lt;/kbd&gt; &lt;a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"&gt;pytorch-CycleGAN-and-pix2pix&lt;/a&gt;: PyTorch 实现图像风格迁移。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/onlytailei/A3C-PyTorch"&gt;A3C-PyTorch&lt;/a&gt;:PyTorch 实现 A3C(Advantage async actor-critic)算法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kentsommer/pytorch-value-iteration-networks"&gt;pytorch-value-iteration-networks&lt;/a&gt;: PyTorch实现价值迭代网络Value Iteration Networks (NIPS 2016 最佳论文)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zhanghang1989/PyTorch-Style-Transfer"&gt;PyTorch-Style-Transfer&lt;/a&gt;: PyTorch实现实时转换多风格生成网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/isht7/pytorch-deeplab-resnet"&gt;pytorch-deeplab-resnet&lt;/a&gt;: PyTorch实现 &lt;a href="https://arxiv.org/abs/1606.00915" rel="nofollow"&gt;DeepLab resnet v2&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/fxia22/pointnet.pytorch"&gt;pointnet.pytorch&lt;/a&gt;: PyTorch实现 "PointNet: 基于深度学习的3D点分类和分割模型" &lt;a href="https://arxiv.org/abs/1612.00593" rel="nofollow"&gt;https://arxiv.org/abs/1612.00593&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/aaron-xichen/pytorch-playground"&gt;pytorch-playground&lt;/a&gt;: 包含常见的预训练模型和数据集(MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)**.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jingweiz/pytorch-dnc"&gt;pytorch-dnc&lt;/a&gt;: PyTorch/Visdom实现的神经机器翻译(NTM)&amp;amp;可微神经计算机(DNC)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jinfagang/pytorch_image_classifier"&gt;pytorch_image_classifier&lt;/a&gt;: 使用PyTorch的最小但实用的图像分类器管道，在ResNet18上进行细化，在自己的小型数据集上获得99%的准确率。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/yunjey/mnist-svhn-transfer"&gt;mnist-svhn-transfer&lt;/a&gt;: PyTorch实现CycleGAN和SGAN。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/marvis/pytorch-yolo2"&gt;pytorch-yolo2&lt;/a&gt;: pytorch-yolo2&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/andrewliao11/dni.pytorch"&gt;dni&lt;/a&gt;: PyTorch实现使用合成梯度的解耦神经接口，论文：&lt;a href="https://arxiv.org/abs/1608.05343" rel="nofollow"&gt;Decoupled Neural Interfaces using Synthetic Gradients&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/caogang/wgan-gp"&gt;wgan-gp&lt;/a&gt;: PyTorch实现论文"&lt;a href="https://arxiv.org/abs/1704.00028v3" rel="nofollow"&gt;Improved Training of Wasserstein GANs&lt;/a&gt;".&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/spro/pytorch-seq2seq-intent-parsing"&gt;pytorch-seq2seq-intent-parsing&lt;/a&gt;:  PyTorch使用seq2seq和注意力模型进行意图分析和空位填充。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/demelin/pyTorch_NCE"&gt;pyTorch_NCE&lt;/a&gt;: 复现噪音对比估计算法，论文：&lt;a href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf" rel="nofollow"&gt;Noise-contrastive estimation: A new estimation principle for unnormalized statistical models&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/cxhernandez/molencoder"&gt;molencoder&lt;/a&gt;: 分子自动编码器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/stormraiser/GAN-weight-norm"&gt;GAN-weight-norm&lt;/a&gt;: 论文代码，"&lt;a href="https://arxiv.org/abs/1704.03971" rel="nofollow"&gt;生成对抗网络中批量和权重归一化的影响&lt;/a&gt;"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/rachtsingh/lgamma"&gt;lgamma&lt;/a&gt;: 实现polygamma、lgamma和beta函数。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/eladhoffer/bigBatch"&gt;bigBatch&lt;/a&gt;: 论文代码，论文：“&lt;a href="https://arxiv.org/abs/1705.08741" rel="nofollow"&gt;训练越久，泛化越好：关闭神经网络大批量训练的泛化间隙&lt;/a&gt;”。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dgriff777/rl_a3c_pytorch"&gt;rl_a3c_pytorch&lt;/a&gt;: 针对 Atari 2600 的强化学习，实现了 A3C LSTM 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ahirner/pytorch-retraining"&gt;pytorch-retraining&lt;/a&gt;: PyTorch动物园模型转移学习(torchvision)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/priba/nmp_qc"&gt;nmp_qc&lt;/a&gt;: 用于计算机视觉的神经消息传递。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jacobgil/pytorch-grad-cam"&gt;grad-cam&lt;/a&gt;: PyTorch 实现&lt;a href="https://arxiv.org/pdf/1610.02391v1.pdf" rel="nofollow"&gt;Grad-CAM&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mjacar/pytorch-trpo"&gt;pytorch-trpo&lt;/a&gt;: PyTorch s实现置信域策略优化（&lt;a href="https://arxiv.org/abs/1502.05477" rel="nofollow"&gt;Trust Region Policy Optimization (TRPO)&lt;/a&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jacobgil/pytorch-explain-black-box"&gt;pytorch-explain-black-box&lt;/a&gt;: PyTorch通过有意义扰动实现黑箱的可解释性解释，&lt;a href="https://arxiv.org/abs/1704.03296" rel="nofollow"&gt;论文&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jmtomczak/vae_vpflows"&gt;vae_vpflows&lt;/a&gt;: 凸组合线性IAF与Householder流 &lt;a href="https://jmtomczak.github.io/deebmed.html" rel="nofollow"&gt;https://jmtomczak.github.io/deebmed.html&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kimhc6028/relational-networks"&gt;relational-networks&lt;/a&gt;: Pytorch实现"&lt;a href="https://arxiv.org/pdf/1706.01427.pdf" rel="nofollow"&gt;用一个简单的神经网络模块来做关系推理&lt;/a&gt;"(关系网络)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Cadene/vqa.pytorch"&gt;vqa.pytorch&lt;/a&gt;: 视觉问答。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/end-to-end-negotiator"&gt;end-to-end-negotiator&lt;/a&gt;: 成交还是不成交？谈判对话的端到端学习。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ShiyuLiang/odin-pytorch"&gt;odin-pytorch&lt;/a&gt;: 神经网络失配实例的原则性检测。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ajbrock/FreezeOut"&gt;FreezeOut&lt;/a&gt;: 一种通过逐步冻结层加速神经网络训练的简单技术。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jakezhaojb/ARAE"&gt;ARAE&lt;/a&gt;: 论文代码，"&lt;a href="https://arxiv.org/abs/1706.04223" rel="nofollow"&gt;对抗性正则化的自动编码器, ARAE&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kimhc6028/forward-thinking-pytorch"&gt;forward-thinking-pytorch&lt;/a&gt;: PyTorch实现"&lt;a href="https://arxiv.org/pdf/1706.02480.pdf" rel="nofollow"&gt;前向思考：一次一层地建立和训练神经网络&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/BoyuanJiang/context_encoder_pytorch"&gt;context_encoder_pytorch&lt;/a&gt;: PyTorch实现上下文编码器(Context Encoders)，可用于图像修复。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3300+&lt;/kbd&gt; &lt;a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch"&gt;attention-is-all-you-need-pytorch&lt;/a&gt;: PyTorch在"Attention is All You Need"中实现转换模型，&lt;a href="https://github.com/thnkim/OpenFacePytorch%E3%80%82"&gt;https://github.com/thnkim/OpenFacePytorch。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/thnkim/OpenFacePytorch"&gt;OpenFacePytorch&lt;/a&gt;: 使用 OpenFace's nn4.small2.v1.t7 模型的PyTorch模块。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pemami4911/neural-combinatorial-rl-pytorch"&gt;neural-combinatorial-rl-pytorch&lt;/a&gt;:  PyTorch 实现"&lt;a href="https://arxiv.org/abs/1611.09940" rel="nofollow"&gt;通过强化学习实现神经组合优化&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mjacar/pytorch-nec"&gt;pytorch-nec&lt;/a&gt;: PyTorch实现神经情景控制(&lt;a href="https://arxiv.org/abs/1703.01988" rel="nofollow"&gt;NEC，Neural Episodic Control&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/eladhoffer/seq2seq.pytorch"&gt;seq2seq.pytorch&lt;/a&gt;: 使用PyTorch进行Sequence-to-Sequence学习。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/alexis-jacq/Pytorch-Sketch-RNN"&gt;Pytorch-Sketch-RNN&lt;/a&gt;: PyTorch实现 “&lt;a href="arxiv.org/abs/1704.03477"&gt;A Neural Representation of Sketch Drawings&lt;/a&gt;”。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jacobgil/pytorch-pruning"&gt;pytorch-pruning&lt;/a&gt;: PyTorch实现 [1611.06440] &lt;a href="https://arxiv.org/abs/1611.06440" rel="nofollow"&gt;用于资源有效推理的剪枝卷积神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hitvoice/DrQA"&gt;DrQA&lt;/a&gt;: PyTorch实现自动阅读维基百科并回答开放领域问题。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/JianGoForIt/YellowFin_Pytorch"&gt;YellowFin_Pytorch&lt;/a&gt;: 基于动量梯度下降（momentum SGD）的自动调优优化器，无需手动指定学习速率和动量。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/deepsound-project/samplernn-pytorch"&gt;samplernn-pytorch&lt;/a&gt;: PyTorch实现SampleRNN: 一种无条件端到端神经音频生成模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tymokvo/AEGeAN"&gt;AEGeAN&lt;/a&gt;: 基于AE稳定的更深的深度卷积生成对抗网络(DCGAN, Deep Convolution Generative Adversarial Networks)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/twtygqyy/pytorch-SRResNet"&gt;/pytorch-SRResNet&lt;/a&gt;: PyTorch实现“&lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;基于生成对抗网络的实感单幅图像超分辨率&lt;/a&gt;”。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/fartashf/vsepp"&gt;vsepp&lt;/a&gt;: 论文代码，"&lt;a href="https://arxiv.org/abs/1707.05612" rel="nofollow"&gt;VSE++:使用难分样本(Hard Negative)改善视觉语义联合嵌入&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/alexis-jacq/Pytorch-DPPO"&gt;Pytorch-DPPO&lt;/a&gt;: Pytorch实现分布式近端策略优化(&lt;a href="https://arxiv.org/abs/1707.02286" rel="nofollow"&gt;Distributed Proximal Policy Optimization&lt;/a&gt;)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1400+&lt;/kbd&gt; &lt;a href="https://github.com/mingyuliutw/UNIT"&gt;UNIT&lt;/a&gt;: 无监督的图像到图像转换网络，&lt;a href="https://arxiv.org/abs/1703.00848" rel="nofollow"&gt;论文&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000+&lt;/kbd&gt; &lt;a href="https://github.com/gpleiss/efficient_densenet_pytorch"&gt;efficient_densenet_pytorch&lt;/a&gt;: DenseNets的内存高效实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/yjxiong/tsn-pytorch"&gt;tsn-pytorch&lt;/a&gt;: PyTorch实现时间分割网络(TSN, Temporal Segment Networks)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ajbrock/SMASH"&gt;SMASH&lt;/a&gt;: &lt;a href="https://arxiv.org/abs/1708.05344" rel="nofollow"&gt;SMASH&lt;/a&gt;，一种高效地探索神经体系结构的实验技术。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kuangliu/pytorch-retinanet"&gt;pytorch-retinanet&lt;/a&gt;: RetinaNet。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/aosokin/biogans"&gt;biogans&lt;/a&gt;: 实现 ICCV 2017 论文 "&lt;a href="https://arxiv.org/abs/1708.04692" rel="nofollow"&gt;利用GANs进行生物图像合成&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/woozzu/dong_iccv_2017"&gt;Semantic Image Synthesis via Adversarial Learning&lt;/a&gt;: PyTorch 实现 ICCV 2017 论文 "&lt;a href="https://arxiv.org/abs/1707.06873" rel="nofollow"&gt;基于对抗学习的语义图像合成&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jmhessel/fmpytorch"&gt;fmpytorch&lt;/a&gt;: PyTorch在Cython中实现分析机（Factorization Machine）模块。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ZhouYanzhao/ORN"&gt;ORN&lt;/a&gt;: PyTorch 实现 CVPR 2017 论文 "&lt;a href="https://arxiv.org/pdf/1701.01833.pdf" rel="nofollow"&gt;Oriented Response Networks&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/katerakelly/pytorch-maml"&gt;pytorch-maml&lt;/a&gt;: PyTorch实现 &lt;a href="https://arxiv.org/abs/1703.03400" rel="nofollow"&gt;MAML&lt;/a&gt;（Model-Agnostic Meta-Learning，与模型无关的元学习）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1800+&lt;/kbd&gt; &lt;a href="https://github.com/znxlwm/pytorch-generative-model-collections"&gt;pytorch-generative-model-collections&lt;/a&gt;: PyTorch中的各种生成模型集合。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/markdtw/vqa-winner-cvprw-2017"&gt;vqa-winner-cvprw-2017&lt;/a&gt;: Pytorch 实现 CVPR'17 VQA( Visual Question Answer，视觉问答) 挑战冠军。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/r9y9/tacotron_pytorch"&gt;tacotron_pytorch&lt;/a&gt;:  PyTorch 实现 Tacotron 语音合成模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Lextal/pspnet-pytorch"&gt;pspnet-pytorch&lt;/a&gt;: PyTorch 实现 PSPNet 语义分割网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/LiyuanLucasLiu/LM-LSTM-CRF"&gt;LM-LSTM-CRF&lt;/a&gt;: Empower Sequence Labeling with Task-Aware Language Model &lt;a href="http://arxiv.org/abs/1709.04109" rel="nofollow"&gt;http://arxiv.org/abs/1709.04109&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;3200+&lt;/kbd&gt; &lt;a href="https://github.com/1adrianb/face-alignment"&gt;face-alignment&lt;/a&gt;: 使用PyTorch构建2D和3D人脸对齐库。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ClementPinard/DepthNet"&gt;DepthNet&lt;/a&gt;: PyTorch 在Still Box数据集上训练DepthNet。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/thstkdgus35/EDSR-PyTorch"&gt;EDSR-PyTorch&lt;/a&gt;: PyTorch version of the paper 'Enhanced Deep Residual Networks for Single Image Super-Resolution' (CVPRW 2017)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ethanluoyc/e2c-pytorch"&gt;e2c-pytorch&lt;/a&gt;: E2C，Embed to Control 实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1700+&lt;/kbd&gt; &lt;a href="https://github.com/kenshohara/3D-ResNets-PyTorch"&gt;3D-ResNets-PyTorch&lt;/a&gt;: 基于3D残差网络的动作识别。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/khanhptnk/bandit-nmt"&gt;bandit-nmt&lt;/a&gt;: This is code repo for our EMNLP 2017 paper "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", which implements the A2C algorithm on top of a neural encoder-decoder model and benchmarks the combination under simulated noisy rewards.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1500+&lt;/kbd&gt; &lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr"&gt;pytorch-a2c-ppo-acktr&lt;/a&gt;: PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO) and Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR).&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/baldassarreFe/zalando-pytorch"&gt;zalando-pytorch&lt;/a&gt;: &lt;a href="https://github.com/zalandoresearch/fashion-mnist"&gt;Fashion-MNIST&lt;/a&gt;数据集上的各种实验。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/clcarwin/sphereface_pytorch"&gt;sphereface_pytorch&lt;/a&gt;: PyTorch实现SphereFace，人脸识别相关，&lt;a href="https://arxiv.org/abs/1704.08063" rel="nofollow"&gt;https://arxiv.org/abs/1704.08063&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/floringogianu/categorical-dqn"&gt;Categorical DQN&lt;/a&gt;: A PyTorch Implementation of Categorical DQN from &lt;a href="https://arxiv.org/abs/1707.06887" rel="nofollow"&gt;A Distributional Perspective on Reinforcement Learning&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/loudinthecloud/pytorch-ntm"&gt;pytorch-ntm&lt;/a&gt;: 神经网络图灵机。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/felixgwu/mask_rcnn_pytorch"&gt;mask_rcnn_pytorch&lt;/a&gt;: Mask RCNN in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/xbresson/graph_convnets_pytorch"&gt;graph_convnets_pytorch&lt;/a&gt;: PyTorch implementation of graph ConvNets, NIPS’16&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1300+&lt;/kbd&gt; &lt;a href="https://github.com/ruotianluo/pytorch-faster-rcnn"&gt;pytorch-faster-rcnn&lt;/a&gt;: A pytorch implementation of faster RCNN detection framework based on Xinlei Chen's tf-faster-rcnn.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/huggingface/torchMoji"&gt;torchMoji&lt;/a&gt;: A pyTorch implementation of the DeepMoji model: state-of-the-art deep learning model for analyzing sentiment, emotion, sarcasm etc.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2700+&lt;/kbd&gt; &lt;a href="https://github.com/hangzhaomit/semantic-segmentation-pytorch"&gt;semantic-segmentation-pytorch&lt;/a&gt;: 在&lt;a href="http://sceneparsing.csail.mit.edu" rel="nofollow"&gt;MIT ADE20K dataset&lt;/a&gt;数据集上实现语义分割/场景解析。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000+&lt;/kbd&gt; &lt;a href="https://github.com/salesforce/pytorch-qrnn"&gt;pytorch-qrnn&lt;/a&gt;: PyTorch implementation of the Quasi-Recurrent Neural Network - up to 16 times faster than NVIDIA's cuDNN LSTM&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/theeluwin/pytorch-sgns"&gt;pytorch-sgns&lt;/a&gt;: Skipgram Negative Sampling in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ClementPinard/SfmLearner-Pytorch"&gt;SfmLearner-Pytorch &lt;/a&gt;: Pytorch version of SfmLearner from Tinghui Zhou et al.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/1zb/deformable-convolution-pytorch"&gt;deformable-convolution-pytorch&lt;/a&gt;: PyTorch实现可变形卷积。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/fanglanting/skip-gram-pytorch"&gt;skip-gram-pytorch&lt;/a&gt;: A complete pytorch implementation of skipgram model (with subsampling and negative sampling). The embedding result is tested with Spearman's rank correlation.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hanzhanggit/StackGAN-v2"&gt;stackGAN-v2&lt;/a&gt;: Pytorch implementation for reproducing StackGAN_v2 results in the paper StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks by Han Zhang*, Tao Xu*, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ruotianluo/self-critical.pytorch"&gt;self-critical.pytorch&lt;/a&gt;: 非官方，PyTorch实现基于 self-critical 序列训练的图像标注。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1900+&lt;/kbd&gt; &lt;a href="https://github.com/tkipf/pygcn"&gt;pygcn&lt;/a&gt;: 图卷积网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ixaxaar/pytorch-dnc"&gt;dnc&lt;/a&gt;: 可微神经计算机、稀疏存取存储器与稀疏可微神经计算机。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ptrblck/prog_gans_pytorch_inference"&gt;prog_gans_pytorch_inference&lt;/a&gt;: PyTorch inference for "Progressive Growing of GANs" with CelebA snapshot.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/timomernick/pytorch-capsule"&gt;pytorch-capsule&lt;/a&gt;: Pytorch implementation of Hinton's Dynamic Routing Between Capsules.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dyhan0920/PyramidNet-PyTorch"&gt;PyramidNet-PyTorch&lt;/a&gt;: A PyTorch implementation for PyramidNets (Deep Pyramidal Residual Networks, arxiv.org/abs/1610.02915)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/gram-ai/radio-transformer-networks"&gt;radio-transformer-networks&lt;/a&gt;: A PyTorch implementation of Radio Transformer Networks from the paper "An Introduction to Deep Learning for the Physical Layer". arxiv.org/abs/1702.00832&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/castorini/honk"&gt;honk&lt;/a&gt;: PyTorch reimplementation of Google's TensorFlow CNNs for keyword spotting.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/SSARCandy/DeepCORAL"&gt;DeepCORAL&lt;/a&gt;: A PyTorch implementation of 'Deep CORAL: Correlation Alignment for Deep Domain Adaptation.', ECCV 2016&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/bearpaw/pytorch-pose"&gt;pytorch-pose&lt;/a&gt;: PyTorch工具包，用于2D人体姿态估计。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/karandesai-96/lang-emerge-parlai"&gt;lang-emerge-parlai&lt;/a&gt;: Implementation of EMNLP 2017 Paper "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog" using PyTorch and ParlAI&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Kaixhin/Rainbow"&gt;Rainbow&lt;/a&gt;: Rainbow: Combining Improvements in Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/gdlg/pytorch_compact_bilinear_pooling"&gt;pytorch_compact_bilinear_pooling v1&lt;/a&gt;: This repository has a pure Python implementation of Compact Bilinear Pooling and Count Sketch for PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/DeepInsight-PCALab/CompactBilinearPooling-Pytorch"&gt;CompactBilinearPooling-Pytorch v2&lt;/a&gt;: (Yang Gao, et al.) A Pytorch Implementation for Compact Bilinear Pooling.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/gitabcworld/FewShotLearning"&gt;FewShotLearning&lt;/a&gt;: Pytorch implementation of the paper "Optimization as a Model for Few-Shot Learning"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jklj077/meProp"&gt;meProp&lt;/a&gt;: Codes for "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting".&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/clcarwin/SFD_pytorch"&gt;SFD_pytorch&lt;/a&gt;: 单镜头尺度不变人脸检测器。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/GradientEpisodicMemory"&gt;GradientEpisodicMemory&lt;/a&gt;: Continuum Learning with GEM: Gradient Episodic Memory. &lt;a href="https://arxiv.org/abs/1706.08840" rel="nofollow"&gt;https://arxiv.org/abs/1706.08840&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1400+&lt;/kbd&gt; &lt;a href="https://github.com/KupynOrest/DeblurGAN"&gt;DeblurGAN&lt;/a&gt;: Pytorch implementation of the paper DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;4200+&lt;/kbd&gt; &lt;a href="https://github.com/yunjey/StarGAN"&gt;StarGAN&lt;/a&gt;: StarGAN: 多领域图像转换 GAN 网络，&lt;a href="https://arxiv.org/abs/1711.09020" rel="nofollow"&gt;https://arxiv.org/abs/1711.09020&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/adambielski/CapsNet-pytorch"&gt;CapsNet-pytorch&lt;/a&gt;: PyTorch 实现 NIPS 2017 论文 “&lt;a href="https://arxiv.org/abs/1710.09829" rel="nofollow"&gt;胶囊间的动态路由&lt;/a&gt;”。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ShichenLiu/CondenseNet"&gt;CondenseNet&lt;/a&gt;: CondenseNet: 面向移动设备的轻量级 CNN。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;5300+&lt;/kbd&gt; &lt;a href="https://github.com/DmitryUlyanov/deep-image-prior"&gt;deep-image-prior&lt;/a&gt;: 基于神经网络的图像修复，无学习过程。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/natanielruiz/deep-head-pose"&gt;deep-head-pose&lt;/a&gt;: 使用PyTorch进行深度学习头部姿势估计。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zhunzhong07/Random-Erasing"&gt;Random-Erasing&lt;/a&gt;: 论文代码，论文："&lt;a href="https://arxiv.org/abs/1708.04896" rel="nofollow"&gt;随机擦除数据增强&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/FaderNetworks"&gt;FaderNetworks&lt;/a&gt;: Fader Networks: 通过滑动属性重构图像 - NIPS 2017，&lt;a href="https://arxiv.org/pdf/1706.00409.pdf" rel="nofollow"&gt;https://arxiv.org/pdf/1706.00409.pdf&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1500+&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/flownet2-pytorch"&gt;FlowNet 2.0&lt;/a&gt;: FlowNet 2.0: 深度网络中光流估计的演化。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;4000+&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/pix2pixHD"&gt;pix2pixHD&lt;/a&gt;: 利用条件 GANs 合成和处理 HD 高清图像的 PyTorch 实现，&lt;a href="https://arxiv.org/pdf/1711.11585.pdf%E3%80%82" rel="nofollow"&gt;https://arxiv.org/pdf/1711.11585.pdf。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pkdn/pytorch-smoothgrad"&gt;pytorch-smoothgrad&lt;/a&gt;: SmoothGrad通过增加噪声来去除噪声。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/c0nn3r/RetinaNet"&gt;RetinaNet&lt;/a&gt;: RetinaNe实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;4300+&lt;/kbd&gt; &lt;a href="https://github.com/jwyang/faster-rcnn.pytorch"&gt;faster-rcnn.pytorch&lt;/a&gt;: This project is a faster faster R-CNN implementation, aimed to accelerating the training of faster R-CNN object detection models.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/leehomyc/mixup_pytorch"&gt;mixup_pytorch&lt;/a&gt;: A PyTorch implementation of the paper Mixup: Beyond Empirical Risk Minimization in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mapillary/inplace_abn"&gt;inplace_abn&lt;/a&gt;: In-Place Activated BatchNorm for Memory-Optimized Training of DNNs&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/xingyizhou/pytorch-pose-hg-3d"&gt;pytorch-pose-hg-3d&lt;/a&gt;: PyTorch implementation for 3D human pose estimation&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/HarshTrivedi/nmn-pytorch"&gt;nmn-pytorch&lt;/a&gt;: Neural Module Network for VQA in Pytorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kefirski/bytenet"&gt;bytenet&lt;/a&gt;: Pytorch implementation of bytenet from "Neural Machine Translation in Linear Time" paper&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hengyuan-hu/bottom-up-attention-vqa"&gt;bottom-up-attention-vqa&lt;/a&gt;: vqa, bottom-up-attention, pytorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ruiminshen/yolo2-pytorch"&gt;yolo2-pytorch&lt;/a&gt;: The YOLOv2 is one of the most popular one-stage object detector. This project adopts PyTorch as the developing framework to increase productivity, and utilize ONNX to convert models into Caffe 2 to benifit engineering deployment.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Wizaron/reseg-pytorch"&gt;reseg-pytorch&lt;/a&gt;: PyTorch 实现ReSeg。 (&lt;a href="https://arxiv.org/pdf/1511.07053.pdf" rel="nofollow"&gt;https://arxiv.org/pdf/1511.07053.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Wizaron/binary-stochastic-neurons"&gt;binary-stochastic-neurons&lt;/a&gt;: Binary Stochastic Neurons in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/DavexPro/pytorch-pose-estimation"&gt;pytorch-pose-estimation&lt;/a&gt;: PyTorch Implementation of Realtime Multi-Person Pose Estimation project.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/higgsfield/interaction_network_pytorch"&gt;interaction_network_pytorch&lt;/a&gt;: Pytorch Implementation of Interaction Networks for Learning about Objects, Relations and Physics.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wlwkgus/NoisyNaturalGradient"&gt;NoisyNaturalGradient&lt;/a&gt;: Pytorch Implementation of paper "Noisy Natural Gradient as Variational Inference".&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/moskomule/ewc.pytorch"&gt;ewc.pytorch&lt;/a&gt;: An implementation of Elastic Weight Consolidation (EWC), proposed in James Kirkpatrick et al. Overcoming catastrophic forgetting in neural networks 2016(10.1073/pnas.1611835114).&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jacobgil/pytorch-zssr"&gt;pytorch-zssr&lt;/a&gt;: PyTorch implementation of 1712.06087 "Zero-Shot" Super-Resolution using Deep Internal Learning&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/atiyo/deep_image_prior"&gt;deep_image_prior&lt;/a&gt;: 基于未训练神经网络的图像重建算法实现。算法：&lt;a href="https://arxiv.org/abs/1711.10925" rel="nofollow"&gt;Deep Image Prior&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/leviswind/pytorch-transformer"&gt;pytorch-transformer&lt;/a&gt;: PyTorch实现论文&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Attention Is All You Need&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/devendrachaplot/DeepRL-Grounding"&gt;DeepRL-Grounding&lt;/a&gt;: PyTorch实现AAAI-18论文&lt;a href="https://arxiv.org/abs/1706.07230" rel="nofollow"&gt;Gated-Attention Architectures for Task-Oriented Language Grounding&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Wizaron/deep-forecast-pytorch"&gt;deep-forecast-pytorch&lt;/a&gt;: 使用LSTMs进行风速预测，论文：&lt;a href="arxiv.org/pdf/1707.08110.pdf"&gt;Deep Forecast: Deep Learning-based Spatio-Temporal Forecasting&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/utiasSTARS/cat-net"&gt;cat-net&lt;/a&gt;:  正则外观变换（&lt;a href="https://arxiv.org/abs/1709.03009" rel="nofollow"&gt;Canonical Appearance Transformations&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tneumann/minimal_glo"&gt;minimal_glo&lt;/a&gt;: Minimal PyTorch implementation of Generative Latent Optimization from the paper "Optimizing the Latent Space of Generative Networks"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/dragen1860/LearningToCompare-Pytorch"&gt;LearningToCompare-Pytorch&lt;/a&gt;: Pytorch Implementation for Paper: Learning to Compare: Relation Network for Few-Shot Learning.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/poincare-embeddings"&gt;poincare-embeddings&lt;/a&gt;: PyTorch implementation of the NIPS-17 paper "Poincaré Embeddings for Learning Hierarchical Representations".&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/ikostrikov/pytorch-trpo"&gt;pytorch-trpo(Hessian-vector product version)&lt;/a&gt;: This is a PyTorch implementation of "Trust Region Policy Optimization (TRPO)" with exact Hessian-vector product instead of finite differences approximation.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/JamesChuanggg/ggnn.pytorch"&gt;ggnn.pytorch&lt;/a&gt;: A PyTorch Implementation of Gated Graph Sequence Neural Networks (GGNN).&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Mrgemy95/visual-interaction-networks-pytorch"&gt;visual-interaction-networks-pytorch&lt;/a&gt;: This's an implementation of deepmind Visual Interaction Networks paper using pytorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jhayes14/adversarial-patch"&gt;adversarial-patch&lt;/a&gt;: PyTorch实现对抗补丁。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch"&gt;Prototypical-Networks-for-Few-shot-Learning-PyTorch&lt;/a&gt;: Implementation of Prototypical Networks for Few Shot Learning (arxiv.org/abs/1703.05175) in Pytorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/orobix/Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch"&gt;Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch&lt;/a&gt;: Implementation of Visual Feature Attribution using Wasserstein GANs (arxiv.org/abs/1711.08998) in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Blade6570/PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch"&gt;PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch&lt;/a&gt;: 用级联优化网络生成照片级图像，&lt;a href="https://arxiv.org/abs/1707.09405" rel="nofollow"&gt;https://arxiv.org/abs/1707.09405&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1900+&lt;/kbd&gt; &lt;a href="https://github.com/carpedm20/ENAS-pytorch"&gt;ENAS-pytorch&lt;/a&gt;: PyTorch实现"&lt;a href="https://arxiv.org/abs/1802.03268" rel="nofollow"&gt;基于参数共享的高效神经网络结构搜索&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kentsyx/Neural-IMage-Assessment"&gt;Neural-IMage-Assessment&lt;/a&gt;: 神经图片评估，&lt;a href="https://arxiv.org/abs/1709.05424" rel="nofollow"&gt;https://arxiv.org/abs/1709.05424&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tfrerix/proxprop"&gt;proxprop&lt;/a&gt;: 近端回传(Proximal Backpropagation) - 隐式梯度代替显式梯度的神经网络训练算法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;9900+&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/FastPhotoStyle"&gt;FastPhotoStyle&lt;/a&gt;: 照片级逼真的图像风格化的一个封闭解。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Ben-Louis/Deep-Image-Analogy-PyTorch"&gt;Deep-Image-Analogy-PyTorch&lt;/a&gt;: 基于PyTorch的深度图像模拟的Python实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/layumi/Person_reID_baseline_pytorch"&gt;Person-reID_pytorch&lt;/a&gt;: 行人再识别Person-reID的PyTorch实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zalandoresearch/pt-dilate-rnn"&gt;pt-dilate-rnn&lt;/a&gt;: 空洞递归神经网络（Dilated RNNs）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jhjacobsen/pytorch-i-revnet"&gt;pytorch-i-revnet&lt;/a&gt;: Pytorch实现i-RevNets。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Orcuslc/OrthNet"&gt;OrthNet&lt;/a&gt;: TensorFlow、PyTorch和Numpy层生成正交多项式。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jt827859032/DRRN-pytorch"&gt;DRRN-pytorch&lt;/a&gt;: "&lt;a href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf" rel="nofollow"&gt;超分辨率的深递归残差网络(DRRN)&lt;/a&gt;", CVPR 2017&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/moskomule/shampoo.pytorch"&gt;shampoo.pytorch&lt;/a&gt;: Shampoo算法实现。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/truskovskiyk/nima.pytorch"&gt;Neural-IMage-Assessment 2&lt;/a&gt;: 神经图片评估，&lt;a href="https://arxiv.org/abs/1709.05424" rel="nofollow"&gt;https://arxiv.org/abs/1709.05424&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1900+&lt;/kbd&gt; &lt;a href="https://github.com/locuslab/TCN"&gt;TCN&lt;/a&gt;: Sequence modeling benchmarks and temporal convolutional networks locuslab/TCN&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/shahsohil/DCC"&gt;DCC&lt;/a&gt;: This repository contains the source code and data for reproducing results of Deep Continuous Clustering paper.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/arunmallya/packnet"&gt;packnet&lt;/a&gt;: Code for PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning arxiv.org/abs/1711.05769&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/github-pengge/PyTorch-progressive_growing_of_gans"&gt;PyTorch-progressive_growing_of_gans&lt;/a&gt;: PyTorch implementation of Progressive Growing of GANs for Improved Quality, Stability, and Variation.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/salesforce/nonauto-nmt"&gt;nonauto-nmt&lt;/a&gt;: PyTorch Implementation of "Non-Autoregressive Neural Machine Translation"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;4500+&lt;/kbd&gt; &lt;a href="https://github.com/eriklindernoren/PyTorch-GAN"&gt;PyTorch-GAN&lt;/a&gt;: PyTorch implementations of Generative Adversarial Networks.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tomrunia/PyTorchWavelets"&gt;PyTorchWavelets&lt;/a&gt;: PyTorch implementation of the wavelet analysis found in Torrence and Compo (1998)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/karpathy/pytorch-made"&gt;pytorch-made&lt;/a&gt;: MADE (Masked Autoencoder Density Estimation) implementation in PyTorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/emited/VariationalRecurrentNeuralNetwork"&gt;VRNN&lt;/a&gt;: Pytorch implementation of the Variational RNN (VRNN), from A Recurrent Latent Variable Model for Sequential Data.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/emited/flow"&gt;flow&lt;/a&gt;: Pytorch implementation of ICLR 2018 paper Deep Learning for Physical Processes: Integrating Prior Scientific Knowledge.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/r9y9/deepvoice3_pytorch"&gt;deepvoice3_pytorch&lt;/a&gt;: PyTorch实现基于卷积神经网络的语音合成模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/elanmart/psmm"&gt;psmm&lt;/a&gt;: imlementation of the the Pointer Sentinel Mixture Model, as described in the paper by Stephen Merity et al.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/tacotron2"&gt;tacotron2&lt;/a&gt;: Tacotron 2 - PyTorch implementation with faster-than-realtime inference.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/rahulkidambi/AccSGD"&gt;AccSGD&lt;/a&gt;: Implements pytorch code for the Accelerated SGD algorithm.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hengruo/QANet-pytorch"&gt;QANet-pytorch&lt;/a&gt;: an implementation of QANet with PyTorch (EM/F1 = 70.5/77.2 after 20 epoches for about 20 hours on one 1080Ti card.)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/TimDettmers/ConvE"&gt;ConvE&lt;/a&gt;: Convolutional 2D Knowledge Graph Embeddings&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kaushalshetty/Structured-Self-Attention"&gt;Structured-Self-Attention&lt;/a&gt;: Implementation for the paper A Structured Self-Attentive Sentence Embedding, which is published in ICLR 2017: arxiv.org/abs/1703.03130 .&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/williamleif/graphsage-simple"&gt;graphsage-simple&lt;/a&gt;: Simple reference implementation of GraphSAGE.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2500+&lt;/kbd&gt; &lt;a href="https://github.com/roytseng-tw/Detectron.pytorch"&gt;Detectron.pytorch&lt;/a&gt;: A pytorch implementation of Detectron. Both training from scratch and inferring directly from pretrained Detectron weights are available.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/irhumshafkat/R2Plus1D-PyTorch"&gt;R2Plus1D-PyTorch&lt;/a&gt;: PyTorch implementation of the R2Plus1D convolution based ResNet architecture described in the paper "A Closer Look at Spatiotemporal Convolutions for Action Recognition"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/viking-sudo-rm/StackNN"&gt;StackNN&lt;/a&gt;: A PyTorch implementation of differentiable stacks for use in neural networks.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/translagent"&gt;translagent&lt;/a&gt;: Code for Emergent Translation in Multi-Agent Communication.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jnhwkim/ban-vqa"&gt;ban-vqa&lt;/a&gt;: Bilinear attention networks for visual question answering.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/huggingface/pytorch-openai-transformer-lm"&gt;pytorch-openai-transformer-lm&lt;/a&gt;: This is a PyTorch implementation of the TensorFlow code provided with OpenAI's paper "Improving Language Understanding by Generative Pre-Training" by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/akanimax/T2F"&gt;T2F&lt;/a&gt;: 使用深度学习进行Text-to-Face生成。该项目结合了&lt;a href="https://arxiv.org/abs/1710.10916" rel="nofollow"&gt;StackGAN&lt;/a&gt;和&lt;a href="https://arxiv.org/abs/1710.10196" rel="nofollow"&gt;ProGAN&lt;/a&gt;，这两个模型可以基于文字描述合成人脸。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mseitzer/pytorch-fid"&gt;pytorch - fid&lt;/a&gt;: A Port of Fréchet Inception Distance (FID score) to PyTorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jmtomczak/vae_vpflows"&gt;vae_vpflows&lt;/a&gt;:Code in PyTorch for the convex combination linear IAF and the Householder Flow, J.M. Tomczak &amp;amp; M. Welling jmtomczak.github.io/deebmed.html&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mkocabas/CoordConv-pytorch"&gt;CoordConv-pytorch&lt;/a&gt;: Pytorch implementation of CoordConv introduced in 'An intriguing failing of convolutional neural networks and the CoordConv solution' paper. (arxiv.org/pdf/1807.03247.pdf)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/xternalz/SDPoint"&gt;SDPoint&lt;/a&gt;: Implementation of "Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks", published in CVPR 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wxywhu/SRDenseNet-pytorch"&gt;SRDenseNet-pytorch&lt;/a&gt;: 极深网络，SRDenseNet-pytorch，论文：&lt;a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf" rel="nofollow"&gt;基于密集跳跃连接的图像超分辨率（ICCV_2017）&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/LMescheder/GAN_stability"&gt;GAN_stability&lt;/a&gt;: Code for paper "Which Training Methods for GANs do actually Converge? (ICML 2018)"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wannabeOG/Mask-RCNN"&gt;Mask-RCNN&lt;/a&gt;: A PyTorch implementation of the architecture of Mask RCNN, serves as an introduction to working with PyTorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/chaoyuaw/pytorch-coviar"&gt;pytorch-coviar&lt;/a&gt;: Compressed Video Action Recognition&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/chenxi116/PNASNet.pytorch"&gt;PNASNet.pytorch&lt;/a&gt;: PyTorch implementation of PNASNet-5 on ImageNet.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kevinzakka/NALU-pytorch"&gt;NALU-pytorch&lt;/a&gt;: Basic pytorch implementation of NAC/NALU from Neural Arithmetic Logic Units arxiv.org/pdf/1808.00508.pdf&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/alexis-jacq/LOLA_DiCE"&gt;LOLA_DiCE&lt;/a&gt;: Pytorch 使用&lt;a href="arxiv.org/abs/1802.05098"&gt;DiCE&lt;/a&gt;实现&lt;a href="arxiv.org/abs/1709.04326"&gt;LOLA&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wohlert/generative-query-network-pytorch"&gt;generative-query-network-pytorch&lt;/a&gt;: Generative Query Network (GQN) in PyTorch as described in "Neural Scene Representation and Rendering"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/wmvanvliet/pytorch_hmax"&gt;pytorch_hmax&lt;/a&gt;: 在PyTorch中实现&lt;a href="https://maxlab.neuro.georgetown.edu/hmax.html#inside" rel="nofollow"&gt;HMAX(Hierarchical Model and X)&lt;/a&gt;视觉模型。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/yunlongdong/FCN-pytorch-easiest"&gt;FCN-pytorch-easiest&lt;/a&gt;: trying to be the most easiest and just get-to-use pytorch implementation of FCN (Fully Convolotional Networks)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/awni/transducer"&gt;transducer&lt;/a&gt;: A Fast Sequence Transducer Implementation with PyTorch Bindings.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/artix41/AVO-pytorch"&gt;AVO-pytorch&lt;/a&gt;: Implementation of Adversarial Variational Optimization in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/huguyuehuhu/HCN-pytorch"&gt;HCN-pytorch&lt;/a&gt;: A pytorch reimplementation of { Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation }.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/szagoruyko/binary-wide-resnet"&gt;binary-wide-resnet&lt;/a&gt;: PyTorch implementation of Wide Residual Networks with 1-bit weights by McDonnel (ICLR 2018)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/arunmallya/piggyback"&gt;piggyback&lt;/a&gt;: Code for Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights arxiv.org/abs/1801.06519&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;6900+&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/vid2vid"&gt;vid2vid&lt;/a&gt;: Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/cranmer/poisson-convolution-sum"&gt;poisson-convolution-sum&lt;/a&gt;: Implements an infinite sum of poisson-weighted convolutions&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/davidmascharka/tbd-nets"&gt;tbd-nets&lt;/a&gt;: PyTorch implementation of "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning" arxiv.org/abs/1803.05268&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/elbayadm/attn2d"&gt;attn2d&lt;/a&gt;: Pervasive Attention: 2D Convolutional Networks for Sequence-to-Sequence Prediction&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2600+&lt;/kbd&gt; &lt;a href="https://github.com/ultralytics/yolov3"&gt;yolov3&lt;/a&gt;: YOLOv3: 训练和推断，&lt;a href="https://www.ultralytics.com" rel="nofollow"&gt;https://www.ultralytics.com&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/duc0/deep-dream-in-pytorch"&gt;deep-dream-in-pytorch&lt;/a&gt;: Pytorch implementation of the DeepDream computer vision algorithm.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ikostrikov/pytorch-flows"&gt;pytorch-flows&lt;/a&gt;: PyTorch implementations of algorithms for density estimation&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ars-ashuha/quantile-regression-dqn-pytorch"&gt;quantile-regression-dqn-pytorch&lt;/a&gt;: Quantile Regression DQN a Minimal Working Example&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/L0SG/relational-rnn-pytorch"&gt;relational-rnn-pytorch&lt;/a&gt;: An implementation of DeepMind's Relational Recurrent Neural Networks in PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/scaelles/DEXTR-PyTorch"&gt;DEXTR-PyTorch&lt;/a&gt;: 深度极端切割，&lt;a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr" rel="nofollow"&gt;http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/rdspring1/PyTorch_GBW_LM"&gt;PyTorch_GBW_LM&lt;/a&gt;: PyTorch Language Model for Google Billion Word Dataset.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Stonesjtu/Pytorch-NCE"&gt;Pytorch-NCE&lt;/a&gt;: The Noise Contrastive Estimation for softmax output written in Pytorch&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/shayneobrien/generative-models"&gt;generative-models&lt;/a&gt;: Annotated, understandable, and visually interpretable PyTorch implementations of: VAE, BIRVAE, NSGAN, MMGAN, WGAN, WGANGP, LSGAN, DRAGAN, BEGAN, RaGAN, InfoGAN, fGAN, FisherGAN.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/andreasveit/convnet-aig"&gt;convnet-aig&lt;/a&gt;: PyTorch implementation for Convolutional Networks with Adaptive Inference Graphs.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/TianhongDai/integrated-gradient-pytorch"&gt;integrated-gradient-pytorch&lt;/a&gt;: This is the pytorch implementation of the paper - Axiomatic Attribution for Deep Networks.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Alexander-H-Liu/MalConv-Pytorch"&gt;MalConv-Pytorch&lt;/a&gt;: Pytorch implementation of MalConv.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/locuslab/trellisnet"&gt;trellisnet&lt;/a&gt;: Trellis Networks for Sequence Modeling&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/minqi/learning-to-communicate-pytorch"&gt;Learning to Communicate with Deep Multi-Agent Reinforcement Learning&lt;/a&gt;: pytorch implementation of  Learning to Communicate with Deep Multi-Agent Reinforcement Learning paper.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/michaelklachko/pnn.pytorch"&gt;pnn.pytorch&lt;/a&gt;: PyTorch implementation of CVPR'18 - Perturbative Neural Networks &lt;a href="http://xujuefei.com/pnn.html" rel="nofollow"&gt;http://xujuefei.com/pnn.html&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/rainofmine/Face_Attention_Network"&gt;Face_Attention_Network&lt;/a&gt;: Pytorch implementation of face attention network as described in Face Attention Network: An Effective Face Detector for the Occluded Faces.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1200+&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/waveglow"&gt;waveglow&lt;/a&gt;: 基于流的语音合成生成网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/deepfloat"&gt;deepfloat&lt;/a&gt;: This repository contains the SystemVerilog RTL, C++, HLS (Intel FPGA OpenCL to wrap RTL code) and Python needed to reproduce the numerical results in "Rethinking floating point for deep learning"&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/subeeshvasu/2018_subeesh_epsr_eccvw"&gt;EPSR&lt;/a&gt;: Pytorch implementation of &lt;a href="https://arxiv.org/pdf/1811.00344.pdf" rel="nofollow"&gt;Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network&lt;/a&gt;. This work has won the first place in PIRM2018-SR competition (region 1) held as part of the ECCV 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ksw0306/ClariNet"&gt;ClariNet&lt;/a&gt;: Pytorch实现&lt;a href="https://arxiv.org/abs/1807.07281" rel="nofollow"&gt;ClariNet&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;16600+&lt;/kbd&gt; &lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;pytorch-pretrained-BERT&lt;/a&gt;: PyTorch version of Google AI's BERT model with script to load Google's pre-trained models&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/npuichigo/waveglow"&gt;torch_waveglow&lt;/a&gt;: PyTorch实现WaveGlow: 基于流的语音合成生成网络。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2100+&lt;/kbd&gt; &lt;a href="https://github.com/cleardusk/3DDFA"&gt;3DDFA&lt;/a&gt;: The pytorch improved re-implementation of TPAMI 2017 paper: Face Alignment in Full Pose Range: A 3D Total Solution.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/tomgoldstein/loss-landscape"&gt;loss-landscape&lt;/a&gt;: loss-landscape Code for visualizing the loss landscape of neural nets.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zalandoresearch/famos"&gt;famos&lt;/a&gt;:（非）参数图像风格化马赛克的对抗性框架。论文：&lt;a href="http://arxiv.org/abs/1811.09236" rel="nofollow"&gt;http://arxiv.org/abs/1811.09236&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/anuragranj/back2future.pytorch"&gt;back2future.pytorch&lt;/a&gt;: This is a Pytorch implementation of
Janai, J., Güney, F., Ranjan, A., Black, M. and Geiger, A., Unsupervised Learning of Multi-Frame Optical Flow with Occlusions. ECCV 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mozilla/FFTNet"&gt;FFTNet&lt;/a&gt;: Unofficial Implementation of FFTNet vocode paper.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zisianw/FaceBoxes.PyTorch"&gt;FaceBoxes.PyTorch&lt;/a&gt;: PyTorch实现&lt;a href="https://arxiv.org/abs/1708.05234" rel="nofollow"&gt;FaceBoxes&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2100+&lt;/kbd&gt; &lt;a href="https://github.com/kimiyoung/transformer-xl"&gt;Transformer-XL&lt;/a&gt;: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Contexthttps://github.com/kimiyoung/transformer-xl&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jalexvig/associative_compression_networks"&gt;associative_compression_networks&lt;/a&gt;: Associative Compression Networks for Representation Learning.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jolibrain/fluidnet_cxx"&gt;fluidnet_cxx&lt;/a&gt;: FluidNet re-written with ATen tensor lib.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1700+&lt;/kbd&gt; &lt;a href="https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"&gt;Deep-Reinforcement-Learning-Algorithms-with-PyTorch&lt;/a&gt;: This repository contains PyTorch implementations of deep reinforcement learning algorithms.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ericsun99/Shufflenet-v2-Pytorch"&gt;Shufflenet-v2-Pytorch&lt;/a&gt;: This is a Pytorch implementation of faceplusplus's ShuffleNet-v2.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork"&gt;GraphWaveletNeuralNetwork&lt;/a&gt;: This is a Pytorch implementation of Graph Wavelet Neural Network. ICLR 2019.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/AttentionWalk"&gt;AttentionWalk&lt;/a&gt;: This is a Pytorch implementation of Watch Your Step: Learning Node Embeddings via Graph Attention. NIPS 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/SGCN"&gt;SGCN&lt;/a&gt;: This is a Pytorch implementation of Signed Graph Convolutional Network. ICDM 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/SINE"&gt;SINE&lt;/a&gt;: This is a Pytorch implementation of SINE: Scalable Incomplete Network Embedding. ICDM 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/GAM"&gt;GAM&lt;/a&gt;: This is a Pytorch implementation of Graph Classification using Structural Attention. KDD 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ProGamerGov/neural-style-pt"&gt;neural-style-pt&lt;/a&gt;: PyTorch 实现 Justin Johnson 的神经风格算法。论文：&lt;a href="https://arxiv.org/abs/1508.06576" rel="nofollow"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ibalazevic/TuckER"&gt;TuckER&lt;/a&gt;: TuckER: Tensor Factorization for Knowledge Graph Completion.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/BayesWatch/pytorch-prunes"&gt;pytorch-prunes&lt;/a&gt;: Pruning neural networks: is it time to nip it in the bud?&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/SimGNN"&gt;SimGNN&lt;/a&gt;: SimGNN: 一个快速图形相似度计算的神经网络方法。论文：A Neural Network Approach to Fast Graph Similarity Computation.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ahmedbesbes/character-based-cnn"&gt;Character CNN&lt;/a&gt;: PyTorch implementation of the Character-level Convolutional Networks for Text Classification paper.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1700+&lt;/kbd&gt; &lt;a href="https://github.com/facebookresearch/XLM"&gt;XLM&lt;/a&gt;: PyTorch original implementation of Cross-lingual Language Model Pretraining.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/eth-sri/diffai"&gt;DiffAI&lt;/a&gt;: A provable defense against adversarial examples and library for building compatible PyTorch models.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/APPNP"&gt;APPNP&lt;/a&gt;: Combining Neural Networks with Personalized PageRank for Classification on Graphs. ICLR 2019.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/MixHop-and-N-GCN"&gt;NGCN&lt;/a&gt;: A Higher-Order Graph Convolutional Layer. NeurIPS 2018.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/graykode/gpt-2-Pytorch"&gt;gpt-2-Pytorch&lt;/a&gt;: Simple Text-Generator with OpenAI gpt-2 Pytorch Implementation&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/Splitter"&gt;Splitter&lt;/a&gt;: Splitter: Learning Node Representations that Capture Multiple Social Contexts. (WWW 2019).&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/CapsGNN"&gt;CapsGNN&lt;/a&gt;: 胶囊图神经网络，&lt;a href="https://openreview.net/forum?id=Byl8BnRcYm" rel="nofollow"&gt;Capsule Graph Neural Network&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/ajbrock/BigGAN-PyTorch"&gt;BigGAN-PyTorch&lt;/a&gt;: PyTorch实现BigGAN（非官方）。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mhubii/ppo_pytorch_cpp"&gt;ppo_pytorch_cpp&lt;/a&gt;: 近端策略优化算法的C++ API。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/seungwonpark/RandWireNN"&gt;RandWireNN&lt;/a&gt;: 基于随机连接神经网络性能的图像识别。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/joel-huang/zeroshot-capsnet-pytorch"&gt;Zero-shot Intent CapsNet&lt;/a&gt;: GPU-accelerated PyTorch implementation of "Zero-shot User Intent Detection via Capsule Neural Networks".&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/SEAL-CI"&gt;SEAL-CI&lt;/a&gt; 半监督图分类：层次图视角，Semi-Supervised Graph Classification: A Hierarchical Graph Perspective. (WWW 2019)。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/MixHop-and-N-GCN"&gt;MixHop&lt;/a&gt;: MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. ICML 2019.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Lotayou/densebody_pytorch"&gt;densebody_pytorch&lt;/a&gt;: PyTorch implementation of CloudWalk's recent paper DenseBody.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mindslab-ai/voicefilter"&gt;voicefilter&lt;/a&gt;: Unofficial PyTorch implementation of Google AI's VoiceFilter system &lt;a href="http://swpark.me/voicefilter" rel="nofollow"&gt;http://swpark.me/voicefilter&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/NVIDIA/semantic-segmentation"&gt;NVIDIA/semantic-segmentation&lt;/a&gt;: PyTorch实现“利用视频传播和标签松弛改进语义分割”。论文：&lt;a href="https://arxiv.org/abs/1812.01593" rel="nofollow"&gt;Improving Semantic Segmentation via Video Propagation and Label Relaxation&lt;/a&gt;, In CVPR2019.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/benedekrozemberczki/ClusterGCN"&gt;ClusterGCN&lt;/a&gt;: A PyTorch implementation of "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks" (KDD 2019).&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/NVlabs/DG-Net"&gt;NVlabs/DG-Net&lt;/a&gt;: A PyTorch implementation of "Joint Discriminative and Generative Learning for Person Re-identification" (CVPR19 Oral).&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/baidu-research/NCRF"&gt;NCRF&lt;/a&gt;: 基于神经网络条件随机场(NCRF)的肿瘤转移检测，相关论文：&lt;a href="https://openreview.net/forum?id=S1aY66iiM%E3%80%82" rel="nofollow"&gt;https://openreview.net/forum?id=S1aY66iiM。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ducha-aiki/pytorch-sift"&gt;pytorch-sift&lt;/a&gt;: PyTorch实现SIFT（尺度不变特征变换匹配算法，Scale Invariant Feature Transform）描述子。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mateuszbuda/brain-segmentation-pytorch"&gt;brain-segmentation-pytorch&lt;/a&gt;: 深度学习分割网络U-Net的PyTorch模型实现，用于脑核磁共振中FLAIR异常的分割。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/rosinality/glow-pytorch"&gt;glow-pytorch&lt;/a&gt;: PyTorch 实现 "&lt;a href="https://arxiv.org/abs/1807.03039" rel="nofollow"&gt;Glow, Generative Flow with Invertible 1x1 Convolutions&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zsef123/EfficientNets-PyTorch"&gt;EfficientNets-PyTorch&lt;/a&gt;: PyTorch实现EfficientNet: 卷积神经网络模型尺度的再思考。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/nv-tlabs/STEAL"&gt;STEAL&lt;/a&gt;: STEAL - 从噪声标注中学习语义边界，&lt;a href="https://nv-tlabs.github.io/STEAL/" rel="nofollow"&gt;https://nv-tlabs.github.io/STEAL/&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/alecwangcq/EigenDamage-Pytorch"&gt;EigenDamage-Pytorch&lt;/a&gt;: 官方实现 ICML'19 论文 "&lt;a href="https://arxiv.org/abs/1905.05934" rel="nofollow"&gt;特征损伤：克罗内克分解特征基中的结构剪枝&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ruidan/Aspect-level-sentiment"&gt;Aspect-level-sentiment&lt;/a&gt;: 论文代码和数据集，ACL2018论文："&lt;a href="https://arxiv.org/abs/1806.04346" rel="nofollow"&gt;利用文档知识进行体层情感分类&lt;/a&gt;"。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/nyukat/breast_cancer_classifier"&gt;breast_cancer_classifier&lt;/a&gt;: 深层神经网络提高放射科医生乳腺癌筛查的效果，&lt;a href="https://arxiv.org/abs/1903.08297" rel="nofollow"&gt;https://arxiv.org/abs/1903.08297&lt;/a&gt; 。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/AaltoVision/DGC-Net"&gt;DGC-Net&lt;/a&gt;: PyTorch实现"&lt;a href="https://arxiv.org/abs/1810.08393" rel="nofollow"&gt;DGC-Net: 密集几何对应网络&lt;/a&gt;".&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Eric-Wallace/universal-triggers"&gt;universal-triggers&lt;/a&gt;: Universal Adversarial Triggers for Attacking and Analyzing NLP (EMNLP 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"&gt;Deep-Reinforcement-Learning-Algorithms-with-PyTorch&lt;/a&gt;: PyTorch implementations of deep reinforcement learning algorithms and environments.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/alibaba-edu/simple-effective-text-matching-pytorch"&gt;simple-effective-text-matching-pytorch&lt;/a&gt;: A pytorch implementation of the ACL2019 paper "Simple and Effective Text Matching with Richer Alignment Features".&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/utkuozbulak/adaptive-segmentation-mask-attack"&gt;Adaptive-segmentation-mask-attack (ASMA)&lt;/a&gt;: A pytorch implementation of the MICCAI2019 paper "Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation".&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/NVIDIA/unsupervised-video-interpolation"&gt;NVIDIA/unsupervised-video-interpolation&lt;/a&gt;: A PyTorch Implementation of &lt;a href="https://arxiv.org/abs/1906.05928" rel="nofollow"&gt;Unsupervised Video Interpolation Using Cycle Consistency&lt;/a&gt;, In ICCV 2019.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-talks--conferences报告--会议" class="anchor" aria-hidden="true" href="#talks--conferences报告--会议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Talks &amp;amp; conferences｜报告 &amp;amp; 会议&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://developers.facebook.com/videos/2018/pytorch-developer-conference/" rel="nofollow"&gt;PyTorch Conference 2018&lt;/a&gt;: 2018年首届PyTorch开发者大会。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-elsewhere--pytorch相关" class="anchor" aria-hidden="true" href="#pytorch-elsewhere--pytorch相关"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pytorch elsewhere ｜ Pytorch相关&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;kbd&gt;4100+&lt;/kbd&gt; &lt;a href="https://github.com/ritchieng/the-incredible-pytorch"&gt;the-incredible-pytorch&lt;/a&gt;**: 不可思议的Pythorch：一份PyTorch相关的教程、论文、项目、社区等的清单。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;5500+&lt;/kbd&gt; &lt;a href="https://github.com/wiseodd/generative-models"&gt;generative models&lt;/a&gt;: 各种生成模型，例如基于Pytorch和Tensorflow的GAN、VAE。 &lt;a href="http://wiseodd.github.io" rel="nofollow"&gt;http://wiseodd.github.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.reddit.com/r/MachineLearning/comments/5w3q74/d_so_pytorch_vs_tensorflow_whats_the_verdict_on/" rel="nofollow"&gt;pytorch vs tensorflow&lt;/a&gt;: Reddit上的PyTorch和TensorFlow的比较文章。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://discuss.pytorch.org/" rel="nofollow"&gt;Pytorch discussion forum&lt;/a&gt;: PyTorch论坛。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://hub.docker.com/r/escong/pytorch-notebook/" rel="nofollow"&gt;pytorch notebook: docker-stack&lt;/a&gt;: 类似于 &lt;a href="https://github.com/jupyter/docker-stacks/tree/master/scipy-notebook"&gt;Jupyter Notebook Scientific Python Stack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/kendricktan/drawlikebobross"&gt;drawlikebobross&lt;/a&gt;: 使用神经网络作画！&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/t-vi/pytorch-tvmisc"&gt;pytorch-tvmisc&lt;/a&gt;: 该仓库收集了作者用PyTorch实现的各种玩意儿。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/andrewliao11/pytorch-a3c-mujoco"&gt;pytorch-a3c-mujoco&lt;/a&gt;: 该项目旨在解决Mujoco中的控制问题，高度基于pytorch-a3c。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=nbJ-2G2GXL0&amp;amp;list=WL&amp;amp;index=9" rel="nofollow"&gt;PyTorch in 5 Minutes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jinfagang/pytorch_chatbot"&gt;pytorch_chatbot&lt;/a&gt;: 用PyTorch实现的聊天机器人。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Kaixhin/malmo-challenge"&gt;malmo-challenge&lt;/a&gt;: Malmo协作人工智能挑战-Pig Catcher团队。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jtoy/sketchnet"&gt;sketchnet&lt;/a&gt;: 指导计算机作画。&lt;a href="http://www.jtoy.net/projects/sketchnet/" rel="nofollow"&gt;http://www.jtoy.net/projects/sketchnet/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1100+&lt;/kbd&gt; &lt;a href="https://github.com/QuantScientist/Deep-Learning-Boot-Camp"&gt;Deep-Learning-Boot-Camp&lt;/a&gt;: 非盈利社区运营的5天深度学习训练营。 &lt;a href="http://deep-ml.com" rel="nofollow"&gt;http://deep-ml.com&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mratsim/Amazon_Forest_Computer_Vision"&gt;Amazon_Forest_Computer_Vision&lt;/a&gt;: 亚马逊森林计算机视觉：使用PyTorch标记卫星图像标记/Keras中的PyTorch技巧。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1700+&lt;/kbd&gt; &lt;a href="https://github.com/junxiaosong/AlphaZero_Gomoku"&gt;AlphaZero_Gomoku&lt;/a&gt;: 用AlphaZero算法玩五子棋。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;null&lt;/kbd&gt; &lt;a href="https://github.com/youansheng/pytorch-cv"&gt;pytorch-cv&lt;/a&gt;: null.&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1600+&lt;/kbd&gt; &lt;a href="https://github.com/KaiyangZhou/deep-person-reid"&gt;deep-person-reid&lt;/a&gt;: Pytorch实现深度学习行人重新识别方法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1300+&lt;/kbd&gt; &lt;a href="https://github.com/victoresque/pytorch-template"&gt;pytorch-template&lt;/a&gt;: PyTorch深度学习模版。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/svishnu88/DLwithPyTorch"&gt;Deep Learning With Pytorch&lt;/a&gt;: 随书代码《&lt;a href="https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-pytorch" rel="nofollow"&gt;Deep Learning With Pytorch TextBook&lt;/a&gt;》 PyTorch实用指南：使用PyTorch建立文本和视觉神经网络模型。&lt;a href="https://www.amazon.cn/dp/B078THDX3J/ref=sr_1_1?__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&amp;amp;keywords=Deep+Learning+with+PyTorch&amp;amp;qid=1568007543&amp;amp;s=gateway&amp;amp;sr=8-1" rel="nofollow"&gt;亚马逊中国电子版&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/jalola/compare-tensorflow-pytorch"&gt;compare-tensorflow-pytorch&lt;/a&gt;: 比较用Tensorflow编写的层和用Pytorch编写的层之间的输出。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/hasktorch/hasktorch"&gt;hasktorch&lt;/a&gt;: Haskell中的张量与神经网络。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.manning.com/books/deep-learning-with-pytorch" rel="nofollow"&gt;Deep Learning With Pytorch&lt;/a&gt; Deep Learning with PyTorch 教你如何用Python和PyTorch实现深度学习算法。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/fragcolor-xyz/nimtorch"&gt;nimtorch&lt;/a&gt;: PyTorch - Python + Nim，PyTorch的Nim前端。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/John-Ellis/derplearning"&gt;derplearning&lt;/a&gt;: 自动驾驶遥控车代码。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/tugstugi/pytorch-saltnet"&gt;pytorch-saltnet&lt;/a&gt;: Kaggle | TGS Salt Identification Challenge 第9名解决方案。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/peterjc123/pytorch-scripts"&gt;pytorch-scripts&lt;/a&gt;: 一些脚本，使在Windows上使用PyTorch更加容易。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/ptrblck/pytorch_misc"&gt;pytorch_misc&lt;/a&gt;: 为PyTorch讨论板创建的代码片段。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/arnas/awesome-pytorch-scholarship"&gt;awesome-pytorch-scholarship&lt;/a&gt;: 收集了一系列优秀的PyTorch学术文章、指南、博客、课程和其他资源。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/mmirman/MentisOculi"&gt;MentisOculi&lt;/a&gt;: PyTorch版raytracer。(raynet?)&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;2400+&lt;/kbd&gt; &lt;a href="https://github.com/karanchahal/DoodleMaster"&gt;DoodleMaster&lt;/a&gt;: “画出UI！”("Don't code your UI, Draw it !")&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/LaurentMazare/ocaml-torch"&gt;ocaml-torch&lt;/a&gt;: ocaml-torch为PyTorch张量库提供一些ocaml绑定。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/pytorch/extension-script"&gt;extension-script&lt;/a&gt;: TorchScript自定义C++/CUDA运算符的示例。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/zccyman/pytorch-inference"&gt;pytorch-inference&lt;/a&gt;:  Windows10 平台上 Pytorch 1.0在 C++ 中的推断。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/Wizaron/pytorch-cpp-inference"&gt;pytorch-cpp-inference&lt;/a&gt;: 包含使用PyTorch C++ API执行推断的各种示例。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/LaurentMazare/tch-rs"&gt;tch-rs&lt;/a&gt;: PyTorch的Rust绑定。&lt;/li&gt;
&lt;li&gt;&lt;kbd&gt;1000-&lt;/kbd&gt; &lt;a href="https://github.com/interesaaat/TorchSharp"&gt;TorchSharp&lt;/a&gt;: Pytorch引擎的.NET绑定。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ml-tooling/ml-workspace"&gt;ML Workspace&lt;/a&gt;: 面向机器学习和数据科学的一体化Web IDE。包含Jupyter, VS Code, PyTorch 和许多其他工具或库，这些都集合在一个Docker映像中。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Feedback: If you have any ideas or you want any other content to be added to this list, feel free to contribute.&lt;/strong&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>xavier-zy</author><guid isPermaLink="false">https://github.com/xavier-zy/Awesome-pytorch-list-CNVersion</guid><pubDate>Mon, 10 Feb 2020 00:24:00 GMT</pubDate></item><item><title>ageron/handson-ml #25 in Jupyter Notebook, Today</title><link>https://github.com/ageron/handson-ml</link><description>&lt;p&gt;&lt;i&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-notebooks" class="anchor" aria-hidden="true" href="#machine-learning-notebooks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning Notebooks&lt;/h1&gt;
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in
python. It contains the example code and solutions to the exercises in my O'Reilly book &lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;Hands-on Machine Learning with Scikit-Learn and TensorFlow&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8e10a44b0ddbb9530cc27d877f06db68d9fa1c7d/687474703a2f2f616b616d6169636f766572732e6f7265696c6c792e636f6d2f696d616765732f393738313439313936323238322f6361742e676966" alt="book" data-canonical-src="http://akamaicovers.oreilly.com/images/9781491962282/cat.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simply open the &lt;a href="http://jupyter.org/" rel="nofollow"&gt;Jupyter&lt;/a&gt; notebooks you are interested in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using &lt;a href="http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb" rel="nofollow"&gt;jupyter.org's notebook viewer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;note: &lt;a href="https://github.com/ageron/handson-ml/blob/master/index.ipynb"&gt;github.com's notebook viewer&lt;/a&gt; also works but it is slower and the math formulas are not displayed correctly,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;by cloning this repository and running Jupyter locally. This option lets you play around with the code. In this case, follow the installation instructions below,&lt;/li&gt;
&lt;li&gt;or by running the notebooks in &lt;a href="https://beta.deepnote.com" rel="nofollow"&gt;Deepnote&lt;/a&gt;. This allows you to play around with the code online in your browser. For example, here's a link to the first chapter: &lt;a href="https://beta.deepnote.com/launch?template=data-science&amp;amp;url=https%3A//github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb" rel="nofollow"&gt;&lt;img height="22" src="https://camo.githubusercontent.com/c3b9bd12a99f8de3301018192105256209bcf800/68747470733a2f2f626574612e646565706e6f74652e636f6d2f627574746f6e732f6c61756e63682d696e2d646565706e6f74652e737667" data-canonical-src="https://beta.deepnote.com/buttons/launch-in-deepnote.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;p&gt;First, you will need to install &lt;a href="https://git-scm.com/" rel="nofollow"&gt;git&lt;/a&gt;, if you don't have it already.&lt;/p&gt;
&lt;p&gt;Next, clone this repository by opening a terminal and typing the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd $HOME  # or any other development directory you prefer
$ git clone https://github.com/ageron/handson-ml.git
$ cd handson-ml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not want to install git, you can instead download &lt;a href="https://github.com/ageron/handson-ml/archive/master.zip"&gt;master.zip&lt;/a&gt;, unzip it, rename the resulting directory to &lt;code&gt;handson-ml&lt;/code&gt; and move it to your development directory.&lt;/p&gt;
&lt;p&gt;If you want to go through chapter 16 on Reinforcement Learning, you will need to &lt;a href="https://gym.openai.com/docs" rel="nofollow"&gt;install OpenAI gym&lt;/a&gt; and its dependencies for Atari simulations.&lt;/p&gt;
&lt;p&gt;If you are familiar with Python and you know how to install Python libraries, go ahead and install the libraries listed in &lt;code&gt;requirements.txt&lt;/code&gt; and jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section. If you need detailed instructions, please read on.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-python--required-libraries" class="anchor" aria-hidden="true" href="#python--required-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python &amp;amp; Required Libraries&lt;/h2&gt;
&lt;p&gt;Of course, you obviously need Python. Python 3 is already preinstalled on many systems nowadays. You can check which version you have by typing the following command (you may need to replace &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 --version  # for Python 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any Python 3 version should be fine, preferably 3.5 or above. If you don't have Python 3, I recommend installing it. To do so, you have several options: on Windows or MacOSX, you can just download it from &lt;a href="https://www.python.org/downloads/" rel="nofollow"&gt;python.org&lt;/a&gt;. On MacOSX, you can alternatively use &lt;a href="https://www.macports.org/" rel="nofollow"&gt;MacPorts&lt;/a&gt; or &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt;. If you are using Python 3.6 on MacOSX, you need to run the following command to install the &lt;code&gt;certifi&lt;/code&gt; package of certificates because Python 3.6 on MacOSX has no certificates to validate SSL connections (see this &lt;a href="https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error" rel="nofollow"&gt;StackOverflow question&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ /Applications/Python\ 3.6/Install\ Certificates.command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Linux, unless you know what you are doing, you should use your system's packaging system. For example, on Debian or Ubuntu, type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt-get update
$ sudo apt-get install python3 python3-pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to download and install &lt;a href="https://www.continuum.io/downloads" rel="nofollow"&gt;Anaconda&lt;/a&gt;. This is a package that includes both Python and many scientific libraries. You should prefer the Python 3 version.&lt;/p&gt;
&lt;p&gt;If you choose to use Anaconda, read the next section, or else jump to the &lt;a href="#using-pip"&gt;Using pip&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-anaconda" class="anchor" aria-hidden="true" href="#using-anaconda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using Anaconda&lt;/h2&gt;
&lt;p&gt;Once you have &lt;a href="https://docs.anaconda.com/anaconda/install/" rel="nofollow"&gt;installed Anaconda&lt;/a&gt; (or Miniconda), you can run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give you a conda environment named &lt;code&gt;mlbook&lt;/code&gt;, ready to use! Just activate it and you will have everything setup
for you:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ conda activate mlbook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You are all set! Next, jump to the &lt;a href="#starting-jupyter"&gt;Starting Jupyter&lt;/a&gt; section.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-pip" class="anchor" aria-hidden="true" href="#using-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using pip&lt;/h2&gt;
&lt;p&gt;If you are not using Anaconda, you need to install several scientific Python libraries that are necessary for this project, in particular NumPy, Matplotlib, Pandas, Jupyter and TensorFlow (and a few others). For this, you can either use Python's integrated packaging system, pip, or you may prefer to use your system's own packaging system (if available, e.g. on Linux, or on MacOSX when using MacPorts or Homebrew). The advantage of using pip is that it is easy to create multiple isolated Python environments with different libraries and different library versions (e.g. one environment for each project). The advantage of using your system's packaging system is that there is less risk of having conflicts between your Python libraries and your system's other packages. Since I have many projects with different library requirements, I prefer to use pip with isolated environments. Moreover, the pip packages are usually the most recent ones available, while Anaconda and system packages often lag behind a bit.&lt;/p&gt;
&lt;p&gt;These are the commands you need to type in a terminal if you want to use pip to install the required libraries. Note: in all the following commands, if you chose to use Python 2 rather than Python 3, you must replace &lt;code&gt;pip3&lt;/code&gt; with &lt;code&gt;pip&lt;/code&gt;, and &lt;code&gt;python3&lt;/code&gt; with &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First you need to make sure you have the latest version of pip installed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;--user&lt;/code&gt; option will install the latest version of pip only for the current user. If you prefer to install it system wide (i.e. for all users), you must have administrator rights (e.g. use &lt;code&gt;sudo python3&lt;/code&gt; instead of &lt;code&gt;python3&lt;/code&gt; on Linux), and you should remove the &lt;code&gt;--user&lt;/code&gt; option. The same is true of the command below that uses the &lt;code&gt;--user&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;Next, you can optionally create an isolated environment. This is recommended as it makes it possible to have a different environment for each project (e.g. one for this project), with potentially very different libraries, and different versions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --user --upgrade virtualenv
$ python3 -m virtualenv -p `which python3` env
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a new directory called &lt;code&gt;env&lt;/code&gt; in the current directory, containing an isolated Python environment based on Python 3. If you installed multiple versions of Python 3 on your system, you can replace &lt;code&gt;`which python3`&lt;/code&gt; with the path to the Python executable you prefer to use.&lt;/p&gt;
&lt;p&gt;Now you must activate this environment. You will need to run this command every time you want to use this environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ source ./env/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On Windows, the command is slightly different:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ .\env\Scripts\activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, use pip to install the required python packages. If you are not using virtualenv, you should add the &lt;code&gt;--user&lt;/code&gt; option (alternatively you could install the libraries system-wide, but this will probably require administrator rights, e.g. using &lt;code&gt;sudo pip3&lt;/code&gt; instead of &lt;code&gt;pip3&lt;/code&gt; on Linux).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python3 -m pip install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! You're all set, you just need to start Jupyter now.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-jupyter" class="anchor" aria-hidden="true" href="#starting-jupyter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting Jupyter&lt;/h2&gt;
&lt;p&gt;Okay! You can now start Jupyter, simply type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should open up your browser, and you should see Jupyter's tree view, with the contents of the current directory. If your browser does not open automatically, visit &lt;a href="http://127.0.0.1:8888/tree" rel="nofollow"&gt;127.0.0.1:8888&lt;/a&gt;. Click on &lt;code&gt;index.ipynb&lt;/code&gt; to get started!&lt;/p&gt;
&lt;p&gt;Congrats! You are ready to learn Machine Learning, hands on!&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h1&gt;
&lt;p&gt;I would like to thank everyone who contributed to this project, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ageron</author><guid isPermaLink="false">https://github.com/ageron/handson-ml</guid><pubDate>Mon, 10 Feb 2020 00:25:00 GMT</pubDate></item></channel></rss>